{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "12500\n",
      "2500\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(validloader))\n",
    "\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(device)  \n",
    "# If 'mps or cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortConn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=1, stride=stride, padding=0)\n",
    "            \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                             kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, \n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # skip connect projection from in_channels to out_channels\n",
    "        # depending on the stride and channels to do a projection\n",
    "        # or just pass on the input\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            self.shortcut = ShortConn2d(in_channels, out_channels, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reserve this now for the skip connection\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # not applying activation here for densenet implementation\n",
    "        return x + shortcut\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, bn_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_channels, \n",
    "                                kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(bn_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(bn_channels, bn_channels, \n",
    "                                kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(bn_channels)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(bn_channels, out_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = ShortConn2d(in_channels, out_channels, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        return x + shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6387e-01,  2.4741e-01, -8.1846e-02,  1.9173e-01,  8.6405e-01,\n",
       "          2.5488e-01, -1.4742e-01, -2.5082e-01,  1.7089e-02, -1.1815e+00],\n",
       "        [ 2.5183e-01,  2.4116e-01, -3.6751e-02,  2.0297e-01,  8.6995e-01,\n",
       "          1.5241e-01, -9.0700e-02, -2.3541e-01,  5.4563e-02, -1.1810e+00],\n",
       "        [ 2.5500e-01,  1.5679e-01, -1.7260e-04,  2.0089e-01,  1.0449e+00,\n",
       "          1.4832e-01,  5.7634e-04, -2.7198e-01,  8.1701e-02, -1.2878e+00],\n",
       "        [ 2.3501e-01,  2.0436e-01, -7.9636e-02,  1.8180e-01,  9.2297e-01,\n",
       "          1.7733e-01, -8.7504e-02, -2.4391e-01,  4.3700e-02, -1.1755e+00]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        ### RES 1\n",
    "        \n",
    "        # 3x32x32 to 10x32x32\n",
    "        self.rb1 = ResBlock(3, 10, 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        ### RES 2\n",
    "\n",
    "        # 10x32x32 to 20x32x32\n",
    "        self.rb2 = ResBlock(10, 20, 1)\n",
    "        # RES 1 skip connection 3x32x32 to 20x32x32\n",
    "        self.res1_sc1 = ShortConn2d(3, 20, 1)  \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        ### RES 3\n",
    "        \n",
    "        # 20x32x32 to 20x32x32\n",
    "        self.rb3 = ResBlock(20, 20, 1)\n",
    "        # RES 1 skip connection for 3x32x32 to 20x32x32\n",
    "        self.res1_sc2 = ShortConn2d(3, 20, 1)\n",
    "        # RES 2 skip connection 10x32x32 to 20x32x32\n",
    "        self.res2_sc1 = ShortConn2d(10, 20, 1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.bnrb1 = BNResBlock(20, 10, 30, 1)\n",
    "        # RES 1 skip connection for 3x32x32 to 30x32x32\n",
    "        self.res1_sc3 = ShortConn2d(3, 30, 1)\n",
    "        # RES 2 skip connection for 10x32x32 to 30x32x32\n",
    "        self.res2_sc2 = ShortConn2d(10, 30, 1)\n",
    "        # RES 3 skip connection for 20x32x32 to 30x32x32\n",
    "        self.res3_sc1 = ShortConn2d(20, 30, 1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        # we will do flatten height and width and then do global average pooling\n",
    "        \n",
    "        # affine operation from 20 neurons to output layer with 10 classes\n",
    "        self.fc = nn.Linear(30, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        res1_sc1 = self.res1_sc1(x)\n",
    "        res1_sc2 = self.res1_sc2(x)\n",
    "        res1_sc3 = self.res1_sc3(x)\n",
    "        x = self.rb1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        res2_sc1 = self.res2_sc1(x)\n",
    "        res2_sc2 = self.res2_sc2(x)\n",
    "        x = self.rb2(x)\n",
    "        x = self.act2(x + res1_sc1)\n",
    "        \n",
    "        res3_sc1 = self.res3_sc1(x)\n",
    "        x = self.rb3(x)\n",
    "        x = self.act3(x + res1_sc2 + res2_sc1)\n",
    "        \n",
    "        x = self.bnrb1(x)\n",
    "        x = self.act4(x + res1_sc3 + res2_sc2 + res3_sc1)\n",
    "        \n",
    "        # change x from shape [batch_size, channels, height, width] \n",
    "        # to [batch_size, channels, height*width]\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "        \n",
    "        # 4 (batch size) x 30 (channels) x 32 (height) x 32 (width)\n",
    "        # global average pooling over the spatial dimensions in each feature map\n",
    "        x = x.mean(dim=-1)\n",
    "        # becomes 4 x 30\n",
    "         \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "test = DenseNet()\n",
    "test.to(device)\n",
    "test(torch.randn(4, 3, 32, 32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> [Start of epoch 0]  lr: 0.002000\n",
      "[epoch: 0, i:   124]  train_loss: 2.253  |  valid_loss: 2.223\n",
      "[epoch: 0, i:   249]  train_loss: 2.132  |  valid_loss: 2.251\n",
      "[epoch: 0, i:   374]  train_loss: 2.086  |  valid_loss: 1.981\n",
      "[epoch: 0, i:   499]  train_loss: 2.093  |  valid_loss: 2.106\n",
      "[epoch: 0, i:   624]  train_loss: 2.028  |  valid_loss: 2.012\n",
      "[epoch: 0, i:   749]  train_loss: 2.002  |  valid_loss: 1.914\n",
      "[epoch: 0, i:   874]  train_loss: 1.953  |  valid_loss: 2.020\n",
      "[epoch: 0, i:   999]  train_loss: 1.942  |  valid_loss: 1.951\n",
      "[epoch: 0, i:  1124]  train_loss: 1.983  |  valid_loss: 1.822\n",
      "[epoch: 0, i:  1249]  train_loss: 1.969  |  valid_loss: 1.890\n",
      "[epoch: 0, i:  1374]  train_loss: 1.959  |  valid_loss: 1.822\n",
      "[epoch: 0, i:  1499]  train_loss: 1.891  |  valid_loss: 1.810\n",
      "[epoch: 0, i:  1624]  train_loss: 1.972  |  valid_loss: 1.874\n",
      "[epoch: 0, i:  1749]  train_loss: 1.958  |  valid_loss: 1.868\n",
      "[epoch: 0, i:  1874]  train_loss: 1.884  |  valid_loss: 1.673\n",
      "[epoch: 0, i:  1999]  train_loss: 1.834  |  valid_loss: 1.835\n",
      "[epoch: 0, i:  2124]  train_loss: 1.886  |  valid_loss: 1.752\n",
      "[epoch: 0, i:  2249]  train_loss: 1.875  |  valid_loss: 1.628\n",
      "[epoch: 0, i:  2374]  train_loss: 1.897  |  valid_loss: 1.777\n",
      "[epoch: 0, i:  2499]  train_loss: 1.823  |  valid_loss: 1.743\n",
      "[epoch: 0, i:  2624]  train_loss: 1.904  |  valid_loss: 1.628\n",
      "[epoch: 0, i:  2749]  train_loss: 1.785  |  valid_loss: 1.809\n",
      "[epoch: 0, i:  2874]  train_loss: 1.886  |  valid_loss: 1.764\n",
      "[epoch: 0, i:  2999]  train_loss: 1.857  |  valid_loss: 1.734\n",
      "[epoch: 0, i:  3124]  train_loss: 1.757  |  valid_loss: 1.707\n",
      "[epoch: 0, i:  3249]  train_loss: 1.763  |  valid_loss: 1.877\n",
      "[epoch: 0, i:  3374]  train_loss: 1.752  |  valid_loss: 1.601\n",
      "[epoch: 0, i:  3499]  train_loss: 1.730  |  valid_loss: 1.729\n",
      "[epoch: 0, i:  3624]  train_loss: 1.786  |  valid_loss: 1.867\n",
      "[epoch: 0, i:  3749]  train_loss: 1.800  |  valid_loss: 1.687\n",
      "[epoch: 0, i:  3874]  train_loss: 1.741  |  valid_loss: 1.678\n",
      "[epoch: 0, i:  3999]  train_loss: 1.778  |  valid_loss: 1.561\n",
      "[epoch: 0, i:  4124]  train_loss: 1.754  |  valid_loss: 1.677\n",
      "[epoch: 0, i:  4249]  train_loss: 1.776  |  valid_loss: 1.672\n",
      "[epoch: 0, i:  4374]  train_loss: 1.690  |  valid_loss: 1.620\n",
      "[epoch: 0, i:  4499]  train_loss: 1.624  |  valid_loss: 1.462\n",
      "[epoch: 0, i:  4624]  train_loss: 1.706  |  valid_loss: 1.673\n",
      "[epoch: 0, i:  4749]  train_loss: 1.685  |  valid_loss: 1.653\n",
      "[epoch: 0, i:  4874]  train_loss: 1.714  |  valid_loss: 1.583\n",
      "[epoch: 0, i:  4999]  train_loss: 1.721  |  valid_loss: 1.577\n",
      "[epoch: 0, i:  5124]  train_loss: 1.705  |  valid_loss: 1.950\n",
      "[epoch: 0, i:  5249]  train_loss: 1.748  |  valid_loss: 1.578\n",
      "[epoch: 0, i:  5374]  train_loss: 1.686  |  valid_loss: 1.516\n",
      "[epoch: 0, i:  5499]  train_loss: 1.684  |  valid_loss: 1.597\n",
      "[epoch: 0, i:  5624]  train_loss: 1.622  |  valid_loss: 1.734\n",
      "[epoch: 0, i:  5749]  train_loss: 1.739  |  valid_loss: 1.700\n",
      "[epoch: 0, i:  5874]  train_loss: 1.658  |  valid_loss: 1.558\n",
      "[epoch: 0, i:  5999]  train_loss: 1.594  |  valid_loss: 1.735\n",
      "[epoch: 0, i:  6124]  train_loss: 1.645  |  valid_loss: 1.613\n",
      "[epoch: 0, i:  6249]  train_loss: 1.562  |  valid_loss: 1.434\n",
      "[epoch: 0, i:  6374]  train_loss: 1.586  |  valid_loss: 1.562\n",
      "[epoch: 0, i:  6499]  train_loss: 1.588  |  valid_loss: 1.677\n",
      "[epoch: 0, i:  6624]  train_loss: 1.662  |  valid_loss: 1.520\n",
      "[epoch: 0, i:  6749]  train_loss: 1.652  |  valid_loss: 1.600\n",
      "[epoch: 0, i:  6874]  train_loss: 1.635  |  valid_loss: 1.606\n",
      "[epoch: 0, i:  6999]  train_loss: 1.595  |  valid_loss: 1.718\n",
      "[epoch: 0, i:  7124]  train_loss: 1.557  |  valid_loss: 1.563\n",
      "[epoch: 0, i:  7249]  train_loss: 1.632  |  valid_loss: 1.403\n",
      "[epoch: 0, i:  7374]  train_loss: 1.574  |  valid_loss: 1.624\n",
      "[epoch: 0, i:  7499]  train_loss: 1.588  |  valid_loss: 1.490\n",
      "[epoch: 0, i:  7624]  train_loss: 1.627  |  valid_loss: 1.573\n",
      "[epoch: 0, i:  7749]  train_loss: 1.630  |  valid_loss: 1.539\n",
      "[epoch: 0, i:  7874]  train_loss: 1.618  |  valid_loss: 1.509\n",
      "[epoch: 0, i:  7999]  train_loss: 1.596  |  valid_loss: 1.523\n",
      "[epoch: 0, i:  8124]  train_loss: 1.516  |  valid_loss: 1.721\n",
      "[epoch: 0, i:  8249]  train_loss: 1.558  |  valid_loss: 1.441\n",
      "[epoch: 0, i:  8374]  train_loss: 1.543  |  valid_loss: 1.522\n",
      "[epoch: 0, i:  8499]  train_loss: 1.526  |  valid_loss: 1.528\n",
      "[epoch: 0, i:  8624]  train_loss: 1.534  |  valid_loss: 1.461\n",
      "[epoch: 0, i:  8749]  train_loss: 1.602  |  valid_loss: 1.544\n",
      "[epoch: 0, i:  8874]  train_loss: 1.569  |  valid_loss: 1.451\n",
      "[epoch: 0, i:  8999]  train_loss: 1.536  |  valid_loss: 1.285\n",
      "[epoch: 0, i:  9124]  train_loss: 1.520  |  valid_loss: 1.439\n",
      "[epoch: 0, i:  9249]  train_loss: 1.528  |  valid_loss: 1.245\n",
      "[epoch: 0, i:  9374]  train_loss: 1.535  |  valid_loss: 1.483\n",
      "[epoch: 0, i:  9499]  train_loss: 1.499  |  valid_loss: 1.436\n",
      "[epoch: 0, i:  9624]  train_loss: 1.497  |  valid_loss: 1.422\n",
      "[epoch: 0, i:  9749]  train_loss: 1.478  |  valid_loss: 1.450\n",
      "[epoch: 0, i:  9874]  train_loss: 1.613  |  valid_loss: 1.502\n",
      "[epoch: 0, i:  9999]  train_loss: 1.532  |  valid_loss: 1.529\n",
      "[epoch: 0, i: 10124]  train_loss: 1.510  |  valid_loss: 1.356\n",
      "[epoch: 0, i: 10249]  train_loss: 1.529  |  valid_loss: 1.374\n",
      "[epoch: 0, i: 10374]  train_loss: 1.543  |  valid_loss: 1.519\n",
      "[epoch: 0, i: 10499]  train_loss: 1.483  |  valid_loss: 1.565\n",
      "[epoch: 0, i: 10624]  train_loss: 1.429  |  valid_loss: 1.619\n",
      "[epoch: 0, i: 10749]  train_loss: 1.484  |  valid_loss: 1.453\n",
      "[epoch: 0, i: 10874]  train_loss: 1.562  |  valid_loss: 1.433\n",
      "[epoch: 0, i: 10999]  train_loss: 1.490  |  valid_loss: 1.422\n",
      "[epoch: 0, i: 11124]  train_loss: 1.467  |  valid_loss: 1.465\n",
      "[epoch: 0, i: 11249]  train_loss: 1.481  |  valid_loss: 1.263\n",
      "[epoch: 0, i: 11374]  train_loss: 1.462  |  valid_loss: 1.313\n",
      "[epoch: 0, i: 11499]  train_loss: 1.438  |  valid_loss: 1.271\n",
      "[epoch: 0, i: 11624]  train_loss: 1.336  |  valid_loss: 1.262\n",
      "[epoch: 0, i: 11749]  train_loss: 1.437  |  valid_loss: 1.372\n",
      "[epoch: 0, i: 11874]  train_loss: 1.437  |  valid_loss: 1.418\n",
      "[epoch: 0, i: 11999]  train_loss: 1.422  |  valid_loss: 1.246\n",
      "[epoch: 0, i: 12124]  train_loss: 1.551  |  valid_loss: 1.325\n",
      "[epoch: 0, i: 12249]  train_loss: 1.433  |  valid_loss: 1.508\n",
      "[epoch: 0, i: 12374]  train_loss: 1.384  |  valid_loss: 1.399\n",
      "[epoch: 0, i: 12499]  train_loss: 1.491  |  valid_loss: 1.348\n",
      "--> [End of epoch 0] train_accuracy: 38.15%  |  valid_accuracy: 40.56%\n",
      "--> [Start of epoch 1]  lr: 0.001600\n",
      "[epoch: 1, i:   124]  train_loss: 1.453  |  valid_loss: 1.278\n",
      "[epoch: 1, i:   249]  train_loss: 1.364  |  valid_loss: 1.490\n",
      "[epoch: 1, i:   374]  train_loss: 1.369  |  valid_loss: 1.352\n",
      "[epoch: 1, i:   499]  train_loss: 1.414  |  valid_loss: 1.221\n",
      "[epoch: 1, i:   624]  train_loss: 1.417  |  valid_loss: 1.358\n",
      "[epoch: 1, i:   749]  train_loss: 1.405  |  valid_loss: 1.146\n",
      "[epoch: 1, i:   874]  train_loss: 1.505  |  valid_loss: 1.286\n",
      "[epoch: 1, i:   999]  train_loss: 1.385  |  valid_loss: 1.378\n",
      "[epoch: 1, i:  1124]  train_loss: 1.351  |  valid_loss: 1.467\n",
      "[epoch: 1, i:  1249]  train_loss: 1.379  |  valid_loss: 1.219\n",
      "[epoch: 1, i:  1374]  train_loss: 1.458  |  valid_loss: 1.120\n",
      "[epoch: 1, i:  1499]  train_loss: 1.454  |  valid_loss: 1.384\n",
      "[epoch: 1, i:  1624]  train_loss: 1.267  |  valid_loss: 1.274\n",
      "[epoch: 1, i:  1749]  train_loss: 1.337  |  valid_loss: 1.336\n",
      "[epoch: 1, i:  1874]  train_loss: 1.348  |  valid_loss: 1.240\n",
      "[epoch: 1, i:  1999]  train_loss: 1.357  |  valid_loss: 1.505\n",
      "[epoch: 1, i:  2124]  train_loss: 1.336  |  valid_loss: 1.234\n",
      "[epoch: 1, i:  2249]  train_loss: 1.396  |  valid_loss: 1.283\n",
      "[epoch: 1, i:  2374]  train_loss: 1.320  |  valid_loss: 1.212\n",
      "[epoch: 1, i:  2499]  train_loss: 1.355  |  valid_loss: 1.341\n",
      "[epoch: 1, i:  2624]  train_loss: 1.362  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  2749]  train_loss: 1.323  |  valid_loss: 1.394\n",
      "[epoch: 1, i:  2874]  train_loss: 1.418  |  valid_loss: 1.276\n",
      "[epoch: 1, i:  2999]  train_loss: 1.410  |  valid_loss: 1.416\n",
      "[epoch: 1, i:  3124]  train_loss: 1.326  |  valid_loss: 1.225\n",
      "[epoch: 1, i:  3249]  train_loss: 1.298  |  valid_loss: 1.567\n",
      "[epoch: 1, i:  3374]  train_loss: 1.307  |  valid_loss: 1.344\n",
      "[epoch: 1, i:  3499]  train_loss: 1.401  |  valid_loss: 1.286\n",
      "[epoch: 1, i:  3624]  train_loss: 1.371  |  valid_loss: 1.432\n",
      "[epoch: 1, i:  3749]  train_loss: 1.314  |  valid_loss: 1.207\n",
      "[epoch: 1, i:  3874]  train_loss: 1.317  |  valid_loss: 1.207\n",
      "[epoch: 1, i:  3999]  train_loss: 1.288  |  valid_loss: 1.114\n",
      "[epoch: 1, i:  4124]  train_loss: 1.278  |  valid_loss: 1.432\n",
      "[epoch: 1, i:  4249]  train_loss: 1.284  |  valid_loss: 1.338\n",
      "[epoch: 1, i:  4374]  train_loss: 1.296  |  valid_loss: 1.316\n",
      "[epoch: 1, i:  4499]  train_loss: 1.339  |  valid_loss: 1.261\n",
      "[epoch: 1, i:  4624]  train_loss: 1.349  |  valid_loss: 1.438\n",
      "[epoch: 1, i:  4749]  train_loss: 1.345  |  valid_loss: 1.345\n",
      "[epoch: 1, i:  4874]  train_loss: 1.315  |  valid_loss: 1.301\n",
      "[epoch: 1, i:  4999]  train_loss: 1.383  |  valid_loss: 1.265\n",
      "[epoch: 1, i:  5124]  train_loss: 1.402  |  valid_loss: 1.251\n",
      "[epoch: 1, i:  5249]  train_loss: 1.310  |  valid_loss: 1.271\n",
      "[epoch: 1, i:  5374]  train_loss: 1.258  |  valid_loss: 1.005\n",
      "[epoch: 1, i:  5499]  train_loss: 1.311  |  valid_loss: 1.049\n",
      "[epoch: 1, i:  5624]  train_loss: 1.282  |  valid_loss: 1.355\n",
      "[epoch: 1, i:  5749]  train_loss: 1.382  |  valid_loss: 1.345\n",
      "[epoch: 1, i:  5874]  train_loss: 1.304  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  5999]  train_loss: 1.309  |  valid_loss: 1.258\n",
      "[epoch: 1, i:  6124]  train_loss: 1.257  |  valid_loss: 1.099\n",
      "[epoch: 1, i:  6249]  train_loss: 1.369  |  valid_loss: 1.239\n",
      "[epoch: 1, i:  6374]  train_loss: 1.304  |  valid_loss: 1.226\n",
      "[epoch: 1, i:  6499]  train_loss: 1.313  |  valid_loss: 1.208\n",
      "[epoch: 1, i:  6624]  train_loss: 1.248  |  valid_loss: 1.155\n",
      "[epoch: 1, i:  6749]  train_loss: 1.306  |  valid_loss: 1.421\n",
      "[epoch: 1, i:  6874]  train_loss: 1.335  |  valid_loss: 1.305\n",
      "[epoch: 1, i:  6999]  train_loss: 1.332  |  valid_loss: 1.357\n",
      "[epoch: 1, i:  7124]  train_loss: 1.230  |  valid_loss: 1.288\n",
      "[epoch: 1, i:  7249]  train_loss: 1.268  |  valid_loss: 1.069\n",
      "[epoch: 1, i:  7374]  train_loss: 1.303  |  valid_loss: 1.325\n",
      "[epoch: 1, i:  7499]  train_loss: 1.326  |  valid_loss: 1.232\n",
      "[epoch: 1, i:  7624]  train_loss: 1.241  |  valid_loss: 1.386\n",
      "[epoch: 1, i:  7749]  train_loss: 1.254  |  valid_loss: 1.332\n",
      "[epoch: 1, i:  7874]  train_loss: 1.258  |  valid_loss: 1.197\n",
      "[epoch: 1, i:  7999]  train_loss: 1.232  |  valid_loss: 1.270\n",
      "[epoch: 1, i:  8124]  train_loss: 1.315  |  valid_loss: 1.310\n",
      "[epoch: 1, i:  8249]  train_loss: 1.308  |  valid_loss: 1.263\n",
      "[epoch: 1, i:  8374]  train_loss: 1.354  |  valid_loss: 1.194\n",
      "[epoch: 1, i:  8499]  train_loss: 1.284  |  valid_loss: 1.317\n",
      "[epoch: 1, i:  8624]  train_loss: 1.307  |  valid_loss: 1.149\n",
      "[epoch: 1, i:  8749]  train_loss: 1.273  |  valid_loss: 1.249\n",
      "[epoch: 1, i:  8874]  train_loss: 1.270  |  valid_loss: 1.299\n",
      "[epoch: 1, i:  8999]  train_loss: 1.232  |  valid_loss: 1.116\n",
      "[epoch: 1, i:  9124]  train_loss: 1.235  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  9249]  train_loss: 1.241  |  valid_loss: 1.017\n",
      "[epoch: 1, i:  9374]  train_loss: 1.236  |  valid_loss: 1.325\n",
      "[epoch: 1, i:  9499]  train_loss: 1.284  |  valid_loss: 1.257\n",
      "[epoch: 1, i:  9624]  train_loss: 1.364  |  valid_loss: 1.179\n",
      "[epoch: 1, i:  9749]  train_loss: 1.272  |  valid_loss: 1.210\n",
      "[epoch: 1, i:  9874]  train_loss: 1.289  |  valid_loss: 1.439\n",
      "[epoch: 1, i:  9999]  train_loss: 1.222  |  valid_loss: 1.330\n",
      "[epoch: 1, i: 10124]  train_loss: 1.239  |  valid_loss: 1.097\n",
      "[epoch: 1, i: 10249]  train_loss: 1.255  |  valid_loss: 1.426\n",
      "[epoch: 1, i: 10374]  train_loss: 1.298  |  valid_loss: 1.275\n",
      "[epoch: 1, i: 10499]  train_loss: 1.250  |  valid_loss: 1.421\n",
      "[epoch: 1, i: 10624]  train_loss: 1.298  |  valid_loss: 1.417\n",
      "[epoch: 1, i: 10749]  train_loss: 1.276  |  valid_loss: 1.189\n",
      "[epoch: 1, i: 10874]  train_loss: 1.281  |  valid_loss: 1.247\n",
      "[epoch: 1, i: 10999]  train_loss: 1.220  |  valid_loss: 1.215\n",
      "[epoch: 1, i: 11124]  train_loss: 1.285  |  valid_loss: 1.236\n",
      "[epoch: 1, i: 11249]  train_loss: 1.278  |  valid_loss: 1.144\n",
      "[epoch: 1, i: 11374]  train_loss: 1.237  |  valid_loss: 1.115\n",
      "[epoch: 1, i: 11499]  train_loss: 1.301  |  valid_loss: 1.148\n",
      "[epoch: 1, i: 11624]  train_loss: 1.258  |  valid_loss: 1.095\n",
      "[epoch: 1, i: 11749]  train_loss: 1.250  |  valid_loss: 1.229\n",
      "[epoch: 1, i: 11874]  train_loss: 1.189  |  valid_loss: 1.244\n",
      "[epoch: 1, i: 11999]  train_loss: 1.307  |  valid_loss: 1.175\n",
      "[epoch: 1, i: 12124]  train_loss: 1.130  |  valid_loss: 1.251\n",
      "[epoch: 1, i: 12249]  train_loss: 1.303  |  valid_loss: 1.380\n",
      "[epoch: 1, i: 12374]  train_loss: 1.227  |  valid_loss: 1.206\n",
      "[epoch: 1, i: 12499]  train_loss: 1.257  |  valid_loss: 1.131\n",
      "--> [End of epoch 1] train_accuracy: 52.60%  |  valid_accuracy: 54.78%\n",
      "--> [Start of epoch 2]  lr: 0.001280\n",
      "[epoch: 2, i:   124]  train_loss: 1.267  |  valid_loss: 1.069\n",
      "[epoch: 2, i:   249]  train_loss: 1.239  |  valid_loss: 1.184\n",
      "[epoch: 2, i:   374]  train_loss: 1.214  |  valid_loss: 1.123\n",
      "[epoch: 2, i:   499]  train_loss: 1.260  |  valid_loss: 1.248\n",
      "[epoch: 2, i:   624]  train_loss: 1.220  |  valid_loss: 1.246\n",
      "[epoch: 2, i:   749]  train_loss: 1.215  |  valid_loss: 0.998\n",
      "[epoch: 2, i:   874]  train_loss: 1.216  |  valid_loss: 1.203\n",
      "[epoch: 2, i:   999]  train_loss: 1.230  |  valid_loss: 1.249\n",
      "[epoch: 2, i:  1124]  train_loss: 1.194  |  valid_loss: 1.250\n",
      "[epoch: 2, i:  1249]  train_loss: 1.204  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  1374]  train_loss: 1.209  |  valid_loss: 0.991\n",
      "[epoch: 2, i:  1499]  train_loss: 1.203  |  valid_loss: 1.162\n",
      "[epoch: 2, i:  1624]  train_loss: 1.208  |  valid_loss: 1.146\n",
      "[epoch: 2, i:  1749]  train_loss: 1.269  |  valid_loss: 1.360\n",
      "[epoch: 2, i:  1874]  train_loss: 1.167  |  valid_loss: 1.082\n",
      "[epoch: 2, i:  1999]  train_loss: 1.207  |  valid_loss: 1.370\n",
      "[epoch: 2, i:  2124]  train_loss: 1.180  |  valid_loss: 1.197\n",
      "[epoch: 2, i:  2249]  train_loss: 1.256  |  valid_loss: 1.093\n",
      "[epoch: 2, i:  2374]  train_loss: 1.161  |  valid_loss: 0.961\n",
      "[epoch: 2, i:  2499]  train_loss: 1.161  |  valid_loss: 1.251\n",
      "[epoch: 2, i:  2624]  train_loss: 1.174  |  valid_loss: 1.142\n",
      "[epoch: 2, i:  2749]  train_loss: 1.158  |  valid_loss: 1.174\n",
      "[epoch: 2, i:  2874]  train_loss: 1.212  |  valid_loss: 1.322\n",
      "[epoch: 2, i:  2999]  train_loss: 1.177  |  valid_loss: 1.247\n",
      "[epoch: 2, i:  3124]  train_loss: 1.257  |  valid_loss: 1.129\n",
      "[epoch: 2, i:  3249]  train_loss: 1.119  |  valid_loss: 1.409\n",
      "[epoch: 2, i:  3374]  train_loss: 1.182  |  valid_loss: 1.144\n",
      "[epoch: 2, i:  3499]  train_loss: 1.196  |  valid_loss: 1.128\n",
      "[epoch: 2, i:  3624]  train_loss: 1.222  |  valid_loss: 1.234\n",
      "[epoch: 2, i:  3749]  train_loss: 1.226  |  valid_loss: 1.056\n",
      "[epoch: 2, i:  3874]  train_loss: 1.160  |  valid_loss: 1.032\n",
      "[epoch: 2, i:  3999]  train_loss: 1.186  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  4124]  train_loss: 1.164  |  valid_loss: 1.222\n",
      "[epoch: 2, i:  4249]  train_loss: 1.190  |  valid_loss: 1.171\n",
      "[epoch: 2, i:  4374]  train_loss: 1.195  |  valid_loss: 1.253\n",
      "[epoch: 2, i:  4499]  train_loss: 1.127  |  valid_loss: 1.173\n",
      "[epoch: 2, i:  4624]  train_loss: 1.232  |  valid_loss: 1.233\n",
      "[epoch: 2, i:  4749]  train_loss: 1.200  |  valid_loss: 1.253\n",
      "[epoch: 2, i:  4874]  train_loss: 1.226  |  valid_loss: 1.203\n",
      "[epoch: 2, i:  4999]  train_loss: 1.252  |  valid_loss: 1.099\n",
      "[epoch: 2, i:  5124]  train_loss: 1.130  |  valid_loss: 1.163\n",
      "[epoch: 2, i:  5249]  train_loss: 1.141  |  valid_loss: 1.146\n",
      "[epoch: 2, i:  5374]  train_loss: 1.292  |  valid_loss: 0.925\n",
      "[epoch: 2, i:  5499]  train_loss: 1.241  |  valid_loss: 1.013\n",
      "[epoch: 2, i:  5624]  train_loss: 1.250  |  valid_loss: 1.393\n",
      "[epoch: 2, i:  5749]  train_loss: 1.182  |  valid_loss: 1.079\n",
      "[epoch: 2, i:  5874]  train_loss: 1.135  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  5999]  train_loss: 1.180  |  valid_loss: 1.061\n",
      "[epoch: 2, i:  6124]  train_loss: 1.185  |  valid_loss: 1.000\n",
      "[epoch: 2, i:  6249]  train_loss: 1.168  |  valid_loss: 1.153\n",
      "[epoch: 2, i:  6374]  train_loss: 1.166  |  valid_loss: 1.126\n",
      "[epoch: 2, i:  6499]  train_loss: 1.177  |  valid_loss: 1.170\n",
      "[epoch: 2, i:  6624]  train_loss: 1.102  |  valid_loss: 1.009\n",
      "[epoch: 2, i:  6749]  train_loss: 1.268  |  valid_loss: 1.234\n",
      "[epoch: 2, i:  6874]  train_loss: 1.180  |  valid_loss: 1.043\n",
      "[epoch: 2, i:  6999]  train_loss: 1.220  |  valid_loss: 1.215\n",
      "[epoch: 2, i:  7124]  train_loss: 1.188  |  valid_loss: 1.259\n",
      "[epoch: 2, i:  7249]  train_loss: 1.123  |  valid_loss: 1.013\n",
      "[epoch: 2, i:  7374]  train_loss: 1.178  |  valid_loss: 1.314\n",
      "[epoch: 2, i:  7499]  train_loss: 1.159  |  valid_loss: 1.202\n",
      "[epoch: 2, i:  7624]  train_loss: 1.117  |  valid_loss: 1.226\n",
      "[epoch: 2, i:  7749]  train_loss: 1.183  |  valid_loss: 1.187\n",
      "[epoch: 2, i:  7874]  train_loss: 1.135  |  valid_loss: 1.066\n",
      "[epoch: 2, i:  7999]  train_loss: 1.103  |  valid_loss: 1.081\n",
      "[epoch: 2, i:  8124]  train_loss: 1.136  |  valid_loss: 1.205\n",
      "[epoch: 2, i:  8249]  train_loss: 1.234  |  valid_loss: 1.090\n",
      "[epoch: 2, i:  8374]  train_loss: 1.182  |  valid_loss: 1.160\n",
      "[epoch: 2, i:  8499]  train_loss: 1.164  |  valid_loss: 1.180\n",
      "[epoch: 2, i:  8624]  train_loss: 1.245  |  valid_loss: 1.106\n",
      "[epoch: 2, i:  8749]  train_loss: 1.193  |  valid_loss: 1.294\n",
      "[epoch: 2, i:  8874]  train_loss: 1.122  |  valid_loss: 1.239\n",
      "[epoch: 2, i:  8999]  train_loss: 1.138  |  valid_loss: 1.059\n",
      "[epoch: 2, i:  9124]  train_loss: 1.106  |  valid_loss: 1.041\n",
      "[epoch: 2, i:  9249]  train_loss: 1.160  |  valid_loss: 0.997\n",
      "[epoch: 2, i:  9374]  train_loss: 1.187  |  valid_loss: 1.174\n",
      "[epoch: 2, i:  9499]  train_loss: 1.161  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  9624]  train_loss: 1.047  |  valid_loss: 1.110\n",
      "[epoch: 2, i:  9749]  train_loss: 1.143  |  valid_loss: 1.153\n",
      "[epoch: 2, i:  9874]  train_loss: 1.144  |  valid_loss: 1.123\n",
      "[epoch: 2, i:  9999]  train_loss: 1.186  |  valid_loss: 1.197\n",
      "[epoch: 2, i: 10124]  train_loss: 1.126  |  valid_loss: 1.050\n",
      "[epoch: 2, i: 10249]  train_loss: 1.134  |  valid_loss: 1.272\n",
      "[epoch: 2, i: 10374]  train_loss: 1.187  |  valid_loss: 1.129\n",
      "[epoch: 2, i: 10499]  train_loss: 1.197  |  valid_loss: 1.326\n",
      "[epoch: 2, i: 10624]  train_loss: 1.022  |  valid_loss: 1.310\n",
      "[epoch: 2, i: 10749]  train_loss: 1.166  |  valid_loss: 1.074\n",
      "[epoch: 2, i: 10874]  train_loss: 1.128  |  valid_loss: 1.165\n",
      "[epoch: 2, i: 10999]  train_loss: 1.109  |  valid_loss: 1.078\n",
      "[epoch: 2, i: 11124]  train_loss: 1.170  |  valid_loss: 1.153\n",
      "[epoch: 2, i: 11249]  train_loss: 1.093  |  valid_loss: 1.057\n",
      "[epoch: 2, i: 11374]  train_loss: 1.028  |  valid_loss: 1.072\n",
      "[epoch: 2, i: 11499]  train_loss: 1.133  |  valid_loss: 1.054\n",
      "[epoch: 2, i: 11624]  train_loss: 1.147  |  valid_loss: 1.116\n",
      "[epoch: 2, i: 11749]  train_loss: 1.158  |  valid_loss: 1.126\n",
      "[epoch: 2, i: 11874]  train_loss: 1.095  |  valid_loss: 1.094\n",
      "[epoch: 2, i: 11999]  train_loss: 1.197  |  valid_loss: 0.931\n",
      "[epoch: 2, i: 12124]  train_loss: 1.171  |  valid_loss: 1.041\n",
      "[epoch: 2, i: 12249]  train_loss: 1.107  |  valid_loss: 1.226\n",
      "[epoch: 2, i: 12374]  train_loss: 1.103  |  valid_loss: 1.277\n",
      "[epoch: 2, i: 12499]  train_loss: 1.093  |  valid_loss: 1.166\n",
      "--> [End of epoch 2] train_accuracy: 57.62%  |  valid_accuracy: 59.40%\n",
      "--> [Start of epoch 3]  lr: 0.001024\n",
      "[epoch: 3, i:   124]  train_loss: 1.096  |  valid_loss: 0.970\n",
      "[epoch: 3, i:   249]  train_loss: 1.165  |  valid_loss: 1.044\n",
      "[epoch: 3, i:   374]  train_loss: 1.132  |  valid_loss: 1.056\n",
      "[epoch: 3, i:   499]  train_loss: 1.018  |  valid_loss: 1.080\n",
      "[epoch: 3, i:   624]  train_loss: 1.090  |  valid_loss: 1.124\n",
      "[epoch: 3, i:   749]  train_loss: 1.102  |  valid_loss: 0.916\n",
      "[epoch: 3, i:   874]  train_loss: 1.077  |  valid_loss: 1.047\n",
      "[epoch: 3, i:   999]  train_loss: 1.151  |  valid_loss: 1.168\n",
      "[epoch: 3, i:  1124]  train_loss: 1.101  |  valid_loss: 1.175\n",
      "[epoch: 3, i:  1249]  train_loss: 1.060  |  valid_loss: 0.991\n",
      "[epoch: 3, i:  1374]  train_loss: 1.124  |  valid_loss: 0.984\n",
      "[epoch: 3, i:  1499]  train_loss: 1.110  |  valid_loss: 1.171\n",
      "[epoch: 3, i:  1624]  train_loss: 1.152  |  valid_loss: 1.019\n",
      "[epoch: 3, i:  1749]  train_loss: 1.130  |  valid_loss: 1.286\n",
      "[epoch: 3, i:  1874]  train_loss: 1.131  |  valid_loss: 0.921\n",
      "[epoch: 3, i:  1999]  train_loss: 1.134  |  valid_loss: 1.354\n",
      "[epoch: 3, i:  2124]  train_loss: 1.092  |  valid_loss: 1.021\n",
      "[epoch: 3, i:  2249]  train_loss: 1.116  |  valid_loss: 1.053\n",
      "[epoch: 3, i:  2374]  train_loss: 1.084  |  valid_loss: 0.920\n",
      "[epoch: 3, i:  2499]  train_loss: 1.060  |  valid_loss: 1.180\n",
      "[epoch: 3, i:  2624]  train_loss: 1.105  |  valid_loss: 1.158\n",
      "[epoch: 3, i:  2749]  train_loss: 1.083  |  valid_loss: 1.164\n",
      "[epoch: 3, i:  2874]  train_loss: 1.059  |  valid_loss: 1.307\n",
      "[epoch: 3, i:  2999]  train_loss: 1.124  |  valid_loss: 1.083\n",
      "[epoch: 3, i:  3124]  train_loss: 1.151  |  valid_loss: 1.085\n",
      "[epoch: 3, i:  3249]  train_loss: 1.065  |  valid_loss: 1.253\n",
      "[epoch: 3, i:  3374]  train_loss: 1.089  |  valid_loss: 1.132\n",
      "[epoch: 3, i:  3499]  train_loss: 1.029  |  valid_loss: 1.039\n",
      "[epoch: 3, i:  3624]  train_loss: 1.149  |  valid_loss: 1.152\n",
      "[epoch: 3, i:  3749]  train_loss: 1.127  |  valid_loss: 0.966\n",
      "[epoch: 3, i:  3874]  train_loss: 1.056  |  valid_loss: 1.014\n",
      "[epoch: 3, i:  3999]  train_loss: 1.141  |  valid_loss: 0.858\n",
      "[epoch: 3, i:  4124]  train_loss: 1.045  |  valid_loss: 1.114\n",
      "[epoch: 3, i:  4249]  train_loss: 1.071  |  valid_loss: 1.148\n",
      "[epoch: 3, i:  4374]  train_loss: 1.117  |  valid_loss: 1.100\n",
      "[epoch: 3, i:  4499]  train_loss: 1.015  |  valid_loss: 1.046\n",
      "[epoch: 3, i:  4624]  train_loss: 1.156  |  valid_loss: 1.107\n",
      "[epoch: 3, i:  4749]  train_loss: 1.090  |  valid_loss: 1.126\n",
      "[epoch: 3, i:  4874]  train_loss: 1.088  |  valid_loss: 1.079\n",
      "[epoch: 3, i:  4999]  train_loss: 1.147  |  valid_loss: 1.049\n",
      "[epoch: 3, i:  5124]  train_loss: 1.121  |  valid_loss: 1.103\n",
      "[epoch: 3, i:  5249]  train_loss: 1.073  |  valid_loss: 1.150\n",
      "[epoch: 3, i:  5374]  train_loss: 1.106  |  valid_loss: 0.882\n",
      "[epoch: 3, i:  5499]  train_loss: 1.129  |  valid_loss: 0.829\n",
      "[epoch: 3, i:  5624]  train_loss: 1.107  |  valid_loss: 1.178\n",
      "[epoch: 3, i:  5749]  train_loss: 1.116  |  valid_loss: 1.054\n",
      "[epoch: 3, i:  5874]  train_loss: 1.097  |  valid_loss: 1.003\n",
      "[epoch: 3, i:  5999]  train_loss: 1.083  |  valid_loss: 1.034\n",
      "[epoch: 3, i:  6124]  train_loss: 1.019  |  valid_loss: 0.945\n",
      "[epoch: 3, i:  6249]  train_loss: 1.100  |  valid_loss: 1.118\n",
      "[epoch: 3, i:  6374]  train_loss: 1.078  |  valid_loss: 1.020\n",
      "[epoch: 3, i:  6499]  train_loss: 1.148  |  valid_loss: 1.006\n",
      "[epoch: 3, i:  6624]  train_loss: 1.093  |  valid_loss: 0.898\n",
      "[epoch: 3, i:  6749]  train_loss: 1.063  |  valid_loss: 1.125\n",
      "[epoch: 3, i:  6874]  train_loss: 1.071  |  valid_loss: 1.018\n",
      "[epoch: 3, i:  6999]  train_loss: 1.068  |  valid_loss: 1.250\n",
      "[epoch: 3, i:  7124]  train_loss: 1.081  |  valid_loss: 1.207\n",
      "[epoch: 3, i:  7249]  train_loss: 1.053  |  valid_loss: 0.919\n",
      "[epoch: 3, i:  7374]  train_loss: 1.046  |  valid_loss: 1.194\n",
      "[epoch: 3, i:  7499]  train_loss: 1.073  |  valid_loss: 1.168\n",
      "[epoch: 3, i:  7624]  train_loss: 1.126  |  valid_loss: 1.160\n",
      "[epoch: 3, i:  7749]  train_loss: 1.045  |  valid_loss: 1.012\n",
      "[epoch: 3, i:  7874]  train_loss: 1.098  |  valid_loss: 0.922\n",
      "[epoch: 3, i:  7999]  train_loss: 1.120  |  valid_loss: 1.024\n",
      "[epoch: 3, i:  8124]  train_loss: 1.129  |  valid_loss: 1.107\n",
      "[epoch: 3, i:  8249]  train_loss: 0.991  |  valid_loss: 1.077\n",
      "[epoch: 3, i:  8374]  train_loss: 1.098  |  valid_loss: 1.085\n",
      "[epoch: 3, i:  8499]  train_loss: 1.063  |  valid_loss: 1.088\n",
      "[epoch: 3, i:  8624]  train_loss: 1.109  |  valid_loss: 1.006\n",
      "[epoch: 3, i:  8749]  train_loss: 1.111  |  valid_loss: 1.108\n",
      "[epoch: 3, i:  8874]  train_loss: 1.195  |  valid_loss: 1.090\n",
      "[epoch: 3, i:  8999]  train_loss: 1.102  |  valid_loss: 0.870\n",
      "[epoch: 3, i:  9124]  train_loss: 1.079  |  valid_loss: 0.991\n",
      "[epoch: 3, i:  9249]  train_loss: 1.097  |  valid_loss: 0.916\n",
      "[epoch: 3, i:  9374]  train_loss: 1.091  |  valid_loss: 1.105\n",
      "[epoch: 3, i:  9499]  train_loss: 1.124  |  valid_loss: 0.995\n",
      "[epoch: 3, i:  9624]  train_loss: 0.991  |  valid_loss: 1.138\n",
      "[epoch: 3, i:  9749]  train_loss: 1.004  |  valid_loss: 1.077\n",
      "[epoch: 3, i:  9874]  train_loss: 1.137  |  valid_loss: 1.082\n",
      "[epoch: 3, i:  9999]  train_loss: 1.175  |  valid_loss: 1.089\n",
      "[epoch: 3, i: 10124]  train_loss: 1.069  |  valid_loss: 0.969\n",
      "[epoch: 3, i: 10249]  train_loss: 1.078  |  valid_loss: 1.185\n",
      "[epoch: 3, i: 10374]  train_loss: 1.043  |  valid_loss: 1.045\n",
      "[epoch: 3, i: 10499]  train_loss: 1.100  |  valid_loss: 1.209\n",
      "[epoch: 3, i: 10624]  train_loss: 0.958  |  valid_loss: 1.197\n",
      "[epoch: 3, i: 10749]  train_loss: 1.116  |  valid_loss: 1.036\n",
      "[epoch: 3, i: 10874]  train_loss: 1.079  |  valid_loss: 1.125\n",
      "[epoch: 3, i: 10999]  train_loss: 1.024  |  valid_loss: 1.163\n",
      "[epoch: 3, i: 11124]  train_loss: 1.061  |  valid_loss: 1.103\n",
      "[epoch: 3, i: 11249]  train_loss: 1.133  |  valid_loss: 1.073\n",
      "[epoch: 3, i: 11374]  train_loss: 1.063  |  valid_loss: 0.967\n",
      "[epoch: 3, i: 11499]  train_loss: 1.082  |  valid_loss: 0.940\n",
      "[epoch: 3, i: 11624]  train_loss: 1.101  |  valid_loss: 1.058\n",
      "[epoch: 3, i: 11749]  train_loss: 0.957  |  valid_loss: 1.016\n",
      "[epoch: 3, i: 11874]  train_loss: 1.046  |  valid_loss: 0.988\n",
      "[epoch: 3, i: 11999]  train_loss: 1.129  |  valid_loss: 0.797\n",
      "[epoch: 3, i: 12124]  train_loss: 1.046  |  valid_loss: 0.925\n",
      "[epoch: 3, i: 12249]  train_loss: 1.035  |  valid_loss: 1.186\n",
      "[epoch: 3, i: 12374]  train_loss: 1.141  |  valid_loss: 1.090\n",
      "[epoch: 3, i: 12499]  train_loss: 1.039  |  valid_loss: 1.078\n",
      "--> [End of epoch 3] train_accuracy: 60.84%  |  valid_accuracy: 62.21%\n",
      "--> [Start of epoch 4]  lr: 0.000819\n",
      "[epoch: 4, i:   124]  train_loss: 0.991  |  valid_loss: 0.943\n",
      "[epoch: 4, i:   249]  train_loss: 0.979  |  valid_loss: 0.957\n",
      "[epoch: 4, i:   374]  train_loss: 1.076  |  valid_loss: 1.126\n",
      "[epoch: 4, i:   499]  train_loss: 0.980  |  valid_loss: 1.071\n",
      "[epoch: 4, i:   624]  train_loss: 1.019  |  valid_loss: 1.156\n",
      "[epoch: 4, i:   749]  train_loss: 1.045  |  valid_loss: 0.868\n",
      "[epoch: 4, i:   874]  train_loss: 1.023  |  valid_loss: 1.046\n",
      "[epoch: 4, i:   999]  train_loss: 0.990  |  valid_loss: 1.094\n",
      "[epoch: 4, i:  1124]  train_loss: 1.044  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  1249]  train_loss: 1.046  |  valid_loss: 1.016\n",
      "[epoch: 4, i:  1374]  train_loss: 1.046  |  valid_loss: 0.922\n",
      "[epoch: 4, i:  1499]  train_loss: 1.059  |  valid_loss: 0.978\n",
      "[epoch: 4, i:  1624]  train_loss: 1.018  |  valid_loss: 0.859\n",
      "[epoch: 4, i:  1749]  train_loss: 1.039  |  valid_loss: 1.291\n",
      "[epoch: 4, i:  1874]  train_loss: 1.039  |  valid_loss: 0.961\n",
      "[epoch: 4, i:  1999]  train_loss: 1.121  |  valid_loss: 1.256\n",
      "[epoch: 4, i:  2124]  train_loss: 0.965  |  valid_loss: 1.005\n",
      "[epoch: 4, i:  2249]  train_loss: 1.032  |  valid_loss: 1.054\n",
      "[epoch: 4, i:  2374]  train_loss: 1.060  |  valid_loss: 0.941\n",
      "[epoch: 4, i:  2499]  train_loss: 1.018  |  valid_loss: 1.100\n",
      "[epoch: 4, i:  2624]  train_loss: 0.937  |  valid_loss: 0.995\n",
      "[epoch: 4, i:  2749]  train_loss: 1.079  |  valid_loss: 1.236\n",
      "[epoch: 4, i:  2874]  train_loss: 1.033  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  2999]  train_loss: 0.987  |  valid_loss: 1.079\n",
      "[epoch: 4, i:  3124]  train_loss: 0.986  |  valid_loss: 1.001\n",
      "[epoch: 4, i:  3249]  train_loss: 1.107  |  valid_loss: 1.224\n",
      "[epoch: 4, i:  3374]  train_loss: 1.082  |  valid_loss: 1.081\n",
      "[epoch: 4, i:  3499]  train_loss: 1.043  |  valid_loss: 0.902\n",
      "[epoch: 4, i:  3624]  train_loss: 1.048  |  valid_loss: 1.094\n",
      "[epoch: 4, i:  3749]  train_loss: 1.104  |  valid_loss: 0.943\n",
      "[epoch: 4, i:  3874]  train_loss: 0.966  |  valid_loss: 0.938\n",
      "[epoch: 4, i:  3999]  train_loss: 1.064  |  valid_loss: 0.801\n",
      "[epoch: 4, i:  4124]  train_loss: 1.073  |  valid_loss: 1.188\n",
      "[epoch: 4, i:  4249]  train_loss: 1.073  |  valid_loss: 1.079\n",
      "[epoch: 4, i:  4374]  train_loss: 1.044  |  valid_loss: 1.047\n",
      "[epoch: 4, i:  4499]  train_loss: 0.985  |  valid_loss: 1.050\n",
      "[epoch: 4, i:  4624]  train_loss: 0.994  |  valid_loss: 1.120\n",
      "[epoch: 4, i:  4749]  train_loss: 1.042  |  valid_loss: 1.102\n",
      "[epoch: 4, i:  4874]  train_loss: 1.041  |  valid_loss: 0.999\n",
      "[epoch: 4, i:  4999]  train_loss: 1.073  |  valid_loss: 0.972\n",
      "[epoch: 4, i:  5124]  train_loss: 1.014  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  5249]  train_loss: 1.018  |  valid_loss: 1.047\n",
      "[epoch: 4, i:  5374]  train_loss: 1.079  |  valid_loss: 0.817\n",
      "[epoch: 4, i:  5499]  train_loss: 1.014  |  valid_loss: 0.839\n",
      "[epoch: 4, i:  5624]  train_loss: 1.000  |  valid_loss: 1.212\n",
      "[epoch: 4, i:  5749]  train_loss: 1.070  |  valid_loss: 0.977\n",
      "[epoch: 4, i:  5874]  train_loss: 1.006  |  valid_loss: 0.951\n",
      "[epoch: 4, i:  5999]  train_loss: 1.082  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  6124]  train_loss: 1.069  |  valid_loss: 0.952\n",
      "[epoch: 4, i:  6249]  train_loss: 1.040  |  valid_loss: 1.056\n",
      "[epoch: 4, i:  6374]  train_loss: 1.052  |  valid_loss: 0.932\n",
      "[epoch: 4, i:  6499]  train_loss: 1.026  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  6624]  train_loss: 0.989  |  valid_loss: 0.868\n",
      "[epoch: 4, i:  6749]  train_loss: 0.985  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  6874]  train_loss: 1.032  |  valid_loss: 0.970\n",
      "[epoch: 4, i:  6999]  train_loss: 0.957  |  valid_loss: 1.264\n",
      "[epoch: 4, i:  7124]  train_loss: 1.122  |  valid_loss: 1.075\n",
      "[epoch: 4, i:  7249]  train_loss: 0.936  |  valid_loss: 0.860\n",
      "[epoch: 4, i:  7374]  train_loss: 0.971  |  valid_loss: 1.197\n",
      "[epoch: 4, i:  7499]  train_loss: 1.033  |  valid_loss: 1.009\n",
      "[epoch: 4, i:  7624]  train_loss: 1.024  |  valid_loss: 1.115\n",
      "[epoch: 4, i:  7749]  train_loss: 1.017  |  valid_loss: 0.934\n",
      "[epoch: 4, i:  7874]  train_loss: 0.966  |  valid_loss: 0.988\n",
      "[epoch: 4, i:  7999]  train_loss: 0.999  |  valid_loss: 0.964\n",
      "[epoch: 4, i:  8124]  train_loss: 1.085  |  valid_loss: 1.046\n",
      "[epoch: 4, i:  8249]  train_loss: 1.087  |  valid_loss: 0.996\n",
      "[epoch: 4, i:  8374]  train_loss: 1.085  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  8499]  train_loss: 1.016  |  valid_loss: 1.049\n",
      "[epoch: 4, i:  8624]  train_loss: 0.950  |  valid_loss: 1.033\n",
      "[epoch: 4, i:  8749]  train_loss: 1.004  |  valid_loss: 1.111\n",
      "[epoch: 4, i:  8874]  train_loss: 1.065  |  valid_loss: 1.194\n",
      "[epoch: 4, i:  8999]  train_loss: 1.001  |  valid_loss: 0.814\n",
      "[epoch: 4, i:  9124]  train_loss: 1.016  |  valid_loss: 0.916\n",
      "[epoch: 4, i:  9249]  train_loss: 0.976  |  valid_loss: 0.825\n",
      "[epoch: 4, i:  9374]  train_loss: 1.078  |  valid_loss: 1.116\n",
      "[epoch: 4, i:  9499]  train_loss: 1.015  |  valid_loss: 0.964\n",
      "[epoch: 4, i:  9624]  train_loss: 0.970  |  valid_loss: 0.984\n",
      "[epoch: 4, i:  9749]  train_loss: 0.949  |  valid_loss: 1.046\n",
      "[epoch: 4, i:  9874]  train_loss: 1.044  |  valid_loss: 1.140\n",
      "[epoch: 4, i:  9999]  train_loss: 1.031  |  valid_loss: 1.076\n",
      "[epoch: 4, i: 10124]  train_loss: 1.050  |  valid_loss: 0.884\n",
      "[epoch: 4, i: 10249]  train_loss: 1.049  |  valid_loss: 1.090\n",
      "[epoch: 4, i: 10374]  train_loss: 0.999  |  valid_loss: 0.949\n",
      "[epoch: 4, i: 10499]  train_loss: 0.934  |  valid_loss: 1.206\n",
      "[epoch: 4, i: 10624]  train_loss: 1.012  |  valid_loss: 1.160\n",
      "[epoch: 4, i: 10749]  train_loss: 1.018  |  valid_loss: 0.951\n",
      "[epoch: 4, i: 10874]  train_loss: 1.036  |  valid_loss: 1.094\n",
      "[epoch: 4, i: 10999]  train_loss: 1.115  |  valid_loss: 1.062\n",
      "[epoch: 4, i: 11124]  train_loss: 1.017  |  valid_loss: 1.021\n",
      "[epoch: 4, i: 11249]  train_loss: 1.021  |  valid_loss: 0.961\n",
      "[epoch: 4, i: 11374]  train_loss: 1.002  |  valid_loss: 0.920\n",
      "[epoch: 4, i: 11499]  train_loss: 1.018  |  valid_loss: 0.879\n",
      "[epoch: 4, i: 11624]  train_loss: 1.041  |  valid_loss: 0.868\n",
      "[epoch: 4, i: 11749]  train_loss: 1.039  |  valid_loss: 0.962\n",
      "[epoch: 4, i: 11874]  train_loss: 0.973  |  valid_loss: 0.955\n",
      "[epoch: 4, i: 11999]  train_loss: 1.021  |  valid_loss: 0.811\n",
      "[epoch: 4, i: 12124]  train_loss: 0.985  |  valid_loss: 0.915\n",
      "[epoch: 4, i: 12249]  train_loss: 1.016  |  valid_loss: 1.175\n",
      "[epoch: 4, i: 12374]  train_loss: 1.021  |  valid_loss: 1.089\n",
      "[epoch: 4, i: 12499]  train_loss: 1.078  |  valid_loss: 0.998\n",
      "--> [End of epoch 4] train_accuracy: 63.22%  |  valid_accuracy: 63.82%\n",
      "--> [Start of epoch 5]  lr: 0.000655\n",
      "[epoch: 5, i:   124]  train_loss: 0.965  |  valid_loss: 0.887\n",
      "[epoch: 5, i:   249]  train_loss: 1.080  |  valid_loss: 0.934\n",
      "[epoch: 5, i:   374]  train_loss: 1.008  |  valid_loss: 1.079\n",
      "[epoch: 5, i:   499]  train_loss: 0.903  |  valid_loss: 1.020\n",
      "[epoch: 5, i:   624]  train_loss: 0.965  |  valid_loss: 1.034\n",
      "[epoch: 5, i:   749]  train_loss: 0.943  |  valid_loss: 0.783\n",
      "[epoch: 5, i:   874]  train_loss: 0.988  |  valid_loss: 1.054\n",
      "[epoch: 5, i:   999]  train_loss: 0.951  |  valid_loss: 1.068\n",
      "[epoch: 5, i:  1124]  train_loss: 0.992  |  valid_loss: 1.005\n",
      "[epoch: 5, i:  1249]  train_loss: 1.021  |  valid_loss: 0.890\n",
      "[epoch: 5, i:  1374]  train_loss: 1.017  |  valid_loss: 0.960\n",
      "[epoch: 5, i:  1499]  train_loss: 0.941  |  valid_loss: 0.965\n",
      "[epoch: 5, i:  1624]  train_loss: 1.084  |  valid_loss: 0.857\n",
      "[epoch: 5, i:  1749]  train_loss: 0.953  |  valid_loss: 1.276\n",
      "[epoch: 5, i:  1874]  train_loss: 1.033  |  valid_loss: 0.825\n",
      "[epoch: 5, i:  1999]  train_loss: 0.952  |  valid_loss: 1.128\n",
      "[epoch: 5, i:  2124]  train_loss: 0.940  |  valid_loss: 0.852\n",
      "[epoch: 5, i:  2249]  train_loss: 1.043  |  valid_loss: 0.985\n",
      "[epoch: 5, i:  2374]  train_loss: 1.035  |  valid_loss: 0.863\n",
      "[epoch: 5, i:  2499]  train_loss: 1.054  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  2624]  train_loss: 0.958  |  valid_loss: 1.061\n",
      "[epoch: 5, i:  2749]  train_loss: 0.861  |  valid_loss: 1.100\n",
      "[epoch: 5, i:  2874]  train_loss: 1.003  |  valid_loss: 1.147\n",
      "[epoch: 5, i:  2999]  train_loss: 0.952  |  valid_loss: 0.991\n",
      "[epoch: 5, i:  3124]  train_loss: 1.027  |  valid_loss: 1.005\n",
      "[epoch: 5, i:  3249]  train_loss: 0.942  |  valid_loss: 1.228\n",
      "[epoch: 5, i:  3374]  train_loss: 0.978  |  valid_loss: 0.990\n",
      "[epoch: 5, i:  3499]  train_loss: 0.894  |  valid_loss: 0.907\n",
      "[epoch: 5, i:  3624]  train_loss: 1.009  |  valid_loss: 1.031\n",
      "[epoch: 5, i:  3749]  train_loss: 0.990  |  valid_loss: 0.830\n",
      "[epoch: 5, i:  3874]  train_loss: 0.862  |  valid_loss: 0.891\n",
      "[epoch: 5, i:  3999]  train_loss: 0.931  |  valid_loss: 0.763\n",
      "[epoch: 5, i:  4124]  train_loss: 0.956  |  valid_loss: 1.030\n",
      "[epoch: 5, i:  4249]  train_loss: 1.019  |  valid_loss: 1.081\n",
      "[epoch: 5, i:  4374]  train_loss: 0.979  |  valid_loss: 1.040\n",
      "[epoch: 5, i:  4499]  train_loss: 1.052  |  valid_loss: 0.997\n",
      "[epoch: 5, i:  4624]  train_loss: 0.978  |  valid_loss: 1.055\n",
      "[epoch: 5, i:  4749]  train_loss: 1.004  |  valid_loss: 1.035\n",
      "[epoch: 5, i:  4874]  train_loss: 1.013  |  valid_loss: 0.994\n",
      "[epoch: 5, i:  4999]  train_loss: 0.985  |  valid_loss: 0.923\n",
      "[epoch: 5, i:  5124]  train_loss: 0.983  |  valid_loss: 0.946\n",
      "[epoch: 5, i:  5249]  train_loss: 0.987  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  5374]  train_loss: 0.995  |  valid_loss: 0.730\n",
      "[epoch: 5, i:  5499]  train_loss: 1.039  |  valid_loss: 0.814\n",
      "[epoch: 5, i:  5624]  train_loss: 1.042  |  valid_loss: 1.030\n",
      "[epoch: 5, i:  5749]  train_loss: 0.941  |  valid_loss: 0.970\n",
      "[epoch: 5, i:  5874]  train_loss: 1.026  |  valid_loss: 0.864\n",
      "[epoch: 5, i:  5999]  train_loss: 1.020  |  valid_loss: 0.932\n",
      "[epoch: 5, i:  6124]  train_loss: 0.899  |  valid_loss: 0.813\n",
      "[epoch: 5, i:  6249]  train_loss: 1.023  |  valid_loss: 1.033\n",
      "[epoch: 5, i:  6374]  train_loss: 0.978  |  valid_loss: 0.945\n",
      "[epoch: 5, i:  6499]  train_loss: 0.969  |  valid_loss: 0.878\n",
      "[epoch: 5, i:  6624]  train_loss: 0.986  |  valid_loss: 0.863\n",
      "[epoch: 5, i:  6749]  train_loss: 0.948  |  valid_loss: 0.995\n",
      "[epoch: 5, i:  6874]  train_loss: 1.034  |  valid_loss: 0.949\n",
      "[epoch: 5, i:  6999]  train_loss: 0.980  |  valid_loss: 1.106\n",
      "[epoch: 5, i:  7124]  train_loss: 1.033  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  7249]  train_loss: 1.028  |  valid_loss: 0.784\n",
      "[epoch: 5, i:  7374]  train_loss: 0.957  |  valid_loss: 1.204\n",
      "[epoch: 5, i:  7499]  train_loss: 0.951  |  valid_loss: 1.039\n",
      "[epoch: 5, i:  7624]  train_loss: 0.993  |  valid_loss: 1.103\n",
      "[epoch: 5, i:  7749]  train_loss: 0.958  |  valid_loss: 0.946\n",
      "[epoch: 5, i:  7874]  train_loss: 1.057  |  valid_loss: 0.943\n",
      "[epoch: 5, i:  7999]  train_loss: 0.980  |  valid_loss: 0.894\n",
      "[epoch: 5, i:  8124]  train_loss: 0.900  |  valid_loss: 0.978\n",
      "[epoch: 5, i:  8249]  train_loss: 0.919  |  valid_loss: 0.961\n",
      "[epoch: 5, i:  8374]  train_loss: 0.954  |  valid_loss: 0.993\n",
      "[epoch: 5, i:  8499]  train_loss: 0.972  |  valid_loss: 1.024\n",
      "[epoch: 5, i:  8624]  train_loss: 0.960  |  valid_loss: 1.004\n",
      "[epoch: 5, i:  8749]  train_loss: 1.034  |  valid_loss: 1.031\n",
      "[epoch: 5, i:  8874]  train_loss: 0.997  |  valid_loss: 0.998\n",
      "[epoch: 5, i:  8999]  train_loss: 0.997  |  valid_loss: 0.833\n",
      "[epoch: 5, i:  9124]  train_loss: 0.969  |  valid_loss: 0.931\n",
      "[epoch: 5, i:  9249]  train_loss: 0.950  |  valid_loss: 0.764\n",
      "[epoch: 5, i:  9374]  train_loss: 1.056  |  valid_loss: 0.992\n",
      "[epoch: 5, i:  9499]  train_loss: 1.053  |  valid_loss: 0.932\n",
      "[epoch: 5, i:  9624]  train_loss: 0.943  |  valid_loss: 1.051\n",
      "[epoch: 5, i:  9749]  train_loss: 0.982  |  valid_loss: 1.055\n",
      "[epoch: 5, i:  9874]  train_loss: 0.958  |  valid_loss: 1.041\n",
      "[epoch: 5, i:  9999]  train_loss: 0.961  |  valid_loss: 0.972\n",
      "[epoch: 5, i: 10124]  train_loss: 1.086  |  valid_loss: 0.899\n",
      "[epoch: 5, i: 10249]  train_loss: 0.930  |  valid_loss: 1.041\n",
      "[epoch: 5, i: 10374]  train_loss: 0.974  |  valid_loss: 0.899\n",
      "[epoch: 5, i: 10499]  train_loss: 0.940  |  valid_loss: 1.036\n",
      "[epoch: 5, i: 10624]  train_loss: 0.990  |  valid_loss: 1.108\n",
      "[epoch: 5, i: 10749]  train_loss: 0.841  |  valid_loss: 0.897\n",
      "[epoch: 5, i: 10874]  train_loss: 0.944  |  valid_loss: 1.041\n",
      "[epoch: 5, i: 10999]  train_loss: 0.930  |  valid_loss: 1.010\n",
      "[epoch: 5, i: 11124]  train_loss: 1.017  |  valid_loss: 1.008\n",
      "[epoch: 5, i: 11249]  train_loss: 1.085  |  valid_loss: 1.000\n",
      "[epoch: 5, i: 11374]  train_loss: 0.916  |  valid_loss: 0.876\n",
      "[epoch: 5, i: 11499]  train_loss: 1.036  |  valid_loss: 0.859\n",
      "[epoch: 5, i: 11624]  train_loss: 0.995  |  valid_loss: 0.872\n",
      "[epoch: 5, i: 11749]  train_loss: 1.018  |  valid_loss: 0.914\n",
      "[epoch: 5, i: 11874]  train_loss: 0.965  |  valid_loss: 0.862\n",
      "[epoch: 5, i: 11999]  train_loss: 0.958  |  valid_loss: 0.837\n",
      "[epoch: 5, i: 12124]  train_loss: 1.004  |  valid_loss: 0.833\n",
      "[epoch: 5, i: 12249]  train_loss: 0.993  |  valid_loss: 1.136\n",
      "[epoch: 5, i: 12374]  train_loss: 1.057  |  valid_loss: 1.068\n",
      "[epoch: 5, i: 12499]  train_loss: 1.013  |  valid_loss: 0.980\n",
      "--> [End of epoch 5] train_accuracy: 65.07%  |  valid_accuracy: 66.21%\n",
      "--> [Start of epoch 6]  lr: 0.000524\n",
      "[epoch: 6, i:   124]  train_loss: 0.958  |  valid_loss: 0.851\n",
      "[epoch: 6, i:   249]  train_loss: 1.062  |  valid_loss: 0.853\n",
      "[epoch: 6, i:   374]  train_loss: 0.986  |  valid_loss: 0.978\n",
      "[epoch: 6, i:   499]  train_loss: 0.959  |  valid_loss: 0.906\n",
      "[epoch: 6, i:   624]  train_loss: 0.946  |  valid_loss: 1.059\n",
      "[epoch: 6, i:   749]  train_loss: 0.935  |  valid_loss: 0.697\n",
      "[epoch: 6, i:   874]  train_loss: 0.980  |  valid_loss: 0.973\n",
      "[epoch: 6, i:   999]  train_loss: 0.936  |  valid_loss: 1.043\n",
      "[epoch: 6, i:  1124]  train_loss: 0.929  |  valid_loss: 1.010\n",
      "[epoch: 6, i:  1249]  train_loss: 0.957  |  valid_loss: 0.877\n",
      "[epoch: 6, i:  1374]  train_loss: 0.894  |  valid_loss: 0.893\n",
      "[epoch: 6, i:  1499]  train_loss: 0.975  |  valid_loss: 0.885\n",
      "[epoch: 6, i:  1624]  train_loss: 0.926  |  valid_loss: 0.806\n",
      "[epoch: 6, i:  1749]  train_loss: 0.990  |  valid_loss: 1.214\n",
      "[epoch: 6, i:  1874]  train_loss: 1.013  |  valid_loss: 0.834\n",
      "[epoch: 6, i:  1999]  train_loss: 0.970  |  valid_loss: 1.141\n",
      "[epoch: 6, i:  2124]  train_loss: 0.952  |  valid_loss: 0.845\n",
      "[epoch: 6, i:  2249]  train_loss: 0.997  |  valid_loss: 0.977\n",
      "[epoch: 6, i:  2374]  train_loss: 0.984  |  valid_loss: 0.879\n",
      "[epoch: 6, i:  2499]  train_loss: 0.894  |  valid_loss: 1.071\n",
      "[epoch: 6, i:  2624]  train_loss: 0.899  |  valid_loss: 1.053\n",
      "[epoch: 6, i:  2749]  train_loss: 0.955  |  valid_loss: 1.063\n",
      "[epoch: 6, i:  2874]  train_loss: 0.898  |  valid_loss: 1.287\n",
      "[epoch: 6, i:  2999]  train_loss: 0.971  |  valid_loss: 1.051\n",
      "[epoch: 6, i:  3124]  train_loss: 0.956  |  valid_loss: 0.982\n",
      "[epoch: 6, i:  3249]  train_loss: 0.955  |  valid_loss: 1.173\n",
      "[epoch: 6, i:  3374]  train_loss: 0.976  |  valid_loss: 0.970\n",
      "[epoch: 6, i:  3499]  train_loss: 0.941  |  valid_loss: 0.817\n",
      "[epoch: 6, i:  3624]  train_loss: 0.887  |  valid_loss: 0.991\n",
      "[epoch: 6, i:  3749]  train_loss: 0.975  |  valid_loss: 0.821\n",
      "[epoch: 6, i:  3874]  train_loss: 0.875  |  valid_loss: 0.940\n",
      "[epoch: 6, i:  3999]  train_loss: 1.006  |  valid_loss: 0.752\n",
      "[epoch: 6, i:  4124]  train_loss: 0.928  |  valid_loss: 0.987\n",
      "[epoch: 6, i:  4249]  train_loss: 0.974  |  valid_loss: 1.018\n",
      "[epoch: 6, i:  4374]  train_loss: 0.976  |  valid_loss: 1.025\n",
      "[epoch: 6, i:  4499]  train_loss: 0.954  |  valid_loss: 0.889\n",
      "[epoch: 6, i:  4624]  train_loss: 1.024  |  valid_loss: 1.060\n",
      "[epoch: 6, i:  4749]  train_loss: 1.025  |  valid_loss: 0.961\n",
      "[epoch: 6, i:  4874]  train_loss: 0.892  |  valid_loss: 0.826\n",
      "[epoch: 6, i:  4999]  train_loss: 0.968  |  valid_loss: 0.931\n",
      "[epoch: 6, i:  5124]  train_loss: 0.933  |  valid_loss: 0.944\n",
      "[epoch: 6, i:  5249]  train_loss: 0.973  |  valid_loss: 1.041\n",
      "[epoch: 6, i:  5374]  train_loss: 0.938  |  valid_loss: 0.719\n",
      "[epoch: 6, i:  5499]  train_loss: 0.947  |  valid_loss: 0.720\n",
      "[epoch: 6, i:  5624]  train_loss: 0.946  |  valid_loss: 0.999\n",
      "[epoch: 6, i:  5749]  train_loss: 0.936  |  valid_loss: 0.957\n",
      "[epoch: 6, i:  5874]  train_loss: 0.913  |  valid_loss: 0.910\n",
      "[epoch: 6, i:  5999]  train_loss: 0.917  |  valid_loss: 0.974\n",
      "[epoch: 6, i:  6124]  train_loss: 0.959  |  valid_loss: 0.887\n",
      "[epoch: 6, i:  6249]  train_loss: 0.876  |  valid_loss: 0.946\n",
      "[epoch: 6, i:  6374]  train_loss: 0.957  |  valid_loss: 0.921\n",
      "[epoch: 6, i:  6499]  train_loss: 0.966  |  valid_loss: 0.875\n",
      "[epoch: 6, i:  6624]  train_loss: 0.887  |  valid_loss: 0.759\n",
      "[epoch: 6, i:  6749]  train_loss: 0.856  |  valid_loss: 0.981\n",
      "[epoch: 6, i:  6874]  train_loss: 0.927  |  valid_loss: 0.883\n",
      "[epoch: 6, i:  6999]  train_loss: 0.891  |  valid_loss: 1.104\n",
      "[epoch: 6, i:  7124]  train_loss: 1.011  |  valid_loss: 0.994\n",
      "[epoch: 6, i:  7249]  train_loss: 0.922  |  valid_loss: 0.760\n",
      "[epoch: 6, i:  7374]  train_loss: 0.857  |  valid_loss: 1.153\n",
      "[epoch: 6, i:  7499]  train_loss: 0.982  |  valid_loss: 0.959\n",
      "[epoch: 6, i:  7624]  train_loss: 0.923  |  valid_loss: 1.051\n",
      "[epoch: 6, i:  7749]  train_loss: 0.927  |  valid_loss: 0.933\n",
      "[epoch: 6, i:  7874]  train_loss: 0.977  |  valid_loss: 0.922\n",
      "[epoch: 6, i:  7999]  train_loss: 0.954  |  valid_loss: 0.883\n",
      "[epoch: 6, i:  8124]  train_loss: 0.936  |  valid_loss: 0.980\n",
      "[epoch: 6, i:  8249]  train_loss: 0.968  |  valid_loss: 0.904\n",
      "[epoch: 6, i:  8374]  train_loss: 0.989  |  valid_loss: 0.896\n",
      "[epoch: 6, i:  8499]  train_loss: 0.953  |  valid_loss: 1.045\n",
      "[epoch: 6, i:  8624]  train_loss: 0.965  |  valid_loss: 0.966\n",
      "[epoch: 6, i:  8749]  train_loss: 0.996  |  valid_loss: 1.034\n",
      "[epoch: 6, i:  8874]  train_loss: 0.945  |  valid_loss: 0.929\n",
      "[epoch: 6, i:  8999]  train_loss: 0.906  |  valid_loss: 0.714\n",
      "[epoch: 6, i:  9124]  train_loss: 0.932  |  valid_loss: 0.876\n",
      "[epoch: 6, i:  9249]  train_loss: 0.938  |  valid_loss: 0.752\n",
      "[epoch: 6, i:  9374]  train_loss: 0.919  |  valid_loss: 0.981\n",
      "[epoch: 6, i:  9499]  train_loss: 0.945  |  valid_loss: 0.900\n",
      "[epoch: 6, i:  9624]  train_loss: 0.973  |  valid_loss: 0.937\n",
      "[epoch: 6, i:  9749]  train_loss: 0.919  |  valid_loss: 1.012\n",
      "[epoch: 6, i:  9874]  train_loss: 0.954  |  valid_loss: 1.002\n",
      "[epoch: 6, i:  9999]  train_loss: 0.858  |  valid_loss: 0.940\n",
      "[epoch: 6, i: 10124]  train_loss: 0.984  |  valid_loss: 0.914\n",
      "[epoch: 6, i: 10249]  train_loss: 1.041  |  valid_loss: 1.072\n",
      "[epoch: 6, i: 10374]  train_loss: 0.997  |  valid_loss: 0.877\n",
      "[epoch: 6, i: 10499]  train_loss: 1.019  |  valid_loss: 1.058\n",
      "[epoch: 6, i: 10624]  train_loss: 0.972  |  valid_loss: 1.100\n",
      "[epoch: 6, i: 10749]  train_loss: 0.909  |  valid_loss: 0.925\n",
      "[epoch: 6, i: 10874]  train_loss: 0.920  |  valid_loss: 1.027\n",
      "[epoch: 6, i: 10999]  train_loss: 1.071  |  valid_loss: 1.061\n",
      "[epoch: 6, i: 11124]  train_loss: 0.927  |  valid_loss: 0.895\n",
      "[epoch: 6, i: 11249]  train_loss: 0.964  |  valid_loss: 0.983\n",
      "[epoch: 6, i: 11374]  train_loss: 0.865  |  valid_loss: 0.918\n",
      "[epoch: 6, i: 11499]  train_loss: 0.984  |  valid_loss: 0.818\n",
      "[epoch: 6, i: 11624]  train_loss: 0.933  |  valid_loss: 0.899\n",
      "[epoch: 6, i: 11749]  train_loss: 0.941  |  valid_loss: 0.880\n",
      "[epoch: 6, i: 11874]  train_loss: 0.922  |  valid_loss: 0.877\n",
      "[epoch: 6, i: 11999]  train_loss: 0.952  |  valid_loss: 0.822\n",
      "[epoch: 6, i: 12124]  train_loss: 0.917  |  valid_loss: 0.888\n",
      "[epoch: 6, i: 12249]  train_loss: 0.904  |  valid_loss: 1.186\n",
      "[epoch: 6, i: 12374]  train_loss: 1.028  |  valid_loss: 0.990\n",
      "[epoch: 6, i: 12499]  train_loss: 0.965  |  valid_loss: 0.944\n",
      "--> [End of epoch 6] train_accuracy: 66.08%  |  valid_accuracy: 66.49%\n",
      "--> [Start of epoch 7]  lr: 0.000419\n",
      "[epoch: 7, i:   124]  train_loss: 0.964  |  valid_loss: 0.843\n",
      "[epoch: 7, i:   249]  train_loss: 0.912  |  valid_loss: 0.839\n",
      "[epoch: 7, i:   374]  train_loss: 1.021  |  valid_loss: 1.046\n",
      "[epoch: 7, i:   499]  train_loss: 1.018  |  valid_loss: 0.897\n",
      "[epoch: 7, i:   624]  train_loss: 0.970  |  valid_loss: 0.970\n",
      "[epoch: 7, i:   749]  train_loss: 0.930  |  valid_loss: 0.664\n",
      "[epoch: 7, i:   874]  train_loss: 0.955  |  valid_loss: 0.919\n",
      "[epoch: 7, i:   999]  train_loss: 0.904  |  valid_loss: 1.048\n",
      "[epoch: 7, i:  1124]  train_loss: 0.887  |  valid_loss: 0.970\n",
      "[epoch: 7, i:  1249]  train_loss: 0.984  |  valid_loss: 0.873\n",
      "[epoch: 7, i:  1374]  train_loss: 0.944  |  valid_loss: 0.938\n",
      "[epoch: 7, i:  1499]  train_loss: 0.950  |  valid_loss: 0.877\n",
      "[epoch: 7, i:  1624]  train_loss: 0.880  |  valid_loss: 0.783\n",
      "[epoch: 7, i:  1749]  train_loss: 0.820  |  valid_loss: 1.258\n",
      "[epoch: 7, i:  1874]  train_loss: 0.802  |  valid_loss: 0.827\n",
      "[epoch: 7, i:  1999]  train_loss: 0.949  |  valid_loss: 1.088\n",
      "[epoch: 7, i:  2124]  train_loss: 0.956  |  valid_loss: 0.835\n",
      "[epoch: 7, i:  2249]  train_loss: 0.943  |  valid_loss: 0.950\n",
      "[epoch: 7, i:  2374]  train_loss: 0.872  |  valid_loss: 0.792\n",
      "[epoch: 7, i:  2499]  train_loss: 0.976  |  valid_loss: 1.033\n",
      "[epoch: 7, i:  2624]  train_loss: 0.925  |  valid_loss: 1.022\n",
      "[epoch: 7, i:  2749]  train_loss: 0.838  |  valid_loss: 1.021\n",
      "[epoch: 7, i:  2874]  train_loss: 0.920  |  valid_loss: 1.194\n",
      "[epoch: 7, i:  2999]  train_loss: 0.964  |  valid_loss: 0.963\n",
      "[epoch: 7, i:  3124]  train_loss: 0.937  |  valid_loss: 0.967\n",
      "[epoch: 7, i:  3249]  train_loss: 0.948  |  valid_loss: 1.145\n",
      "[epoch: 7, i:  3374]  train_loss: 0.922  |  valid_loss: 0.872\n",
      "[epoch: 7, i:  3499]  train_loss: 0.967  |  valid_loss: 0.781\n",
      "[epoch: 7, i:  3624]  train_loss: 0.877  |  valid_loss: 0.928\n",
      "[epoch: 7, i:  3749]  train_loss: 0.934  |  valid_loss: 0.746\n",
      "[epoch: 7, i:  3874]  train_loss: 0.931  |  valid_loss: 0.896\n",
      "[epoch: 7, i:  3999]  train_loss: 0.917  |  valid_loss: 0.714\n",
      "[epoch: 7, i:  4124]  train_loss: 0.877  |  valid_loss: 0.929\n",
      "[epoch: 7, i:  4249]  train_loss: 0.911  |  valid_loss: 1.065\n",
      "[epoch: 7, i:  4374]  train_loss: 0.943  |  valid_loss: 0.990\n",
      "[epoch: 7, i:  4499]  train_loss: 0.832  |  valid_loss: 0.830\n",
      "[epoch: 7, i:  4624]  train_loss: 1.012  |  valid_loss: 1.031\n",
      "[epoch: 7, i:  4749]  train_loss: 0.957  |  valid_loss: 0.959\n",
      "[epoch: 7, i:  4874]  train_loss: 0.945  |  valid_loss: 0.876\n",
      "[epoch: 7, i:  4999]  train_loss: 0.946  |  valid_loss: 0.852\n",
      "[epoch: 7, i:  5124]  train_loss: 0.850  |  valid_loss: 0.857\n",
      "[epoch: 7, i:  5249]  train_loss: 0.906  |  valid_loss: 0.981\n",
      "[epoch: 7, i:  5374]  train_loss: 0.845  |  valid_loss: 0.714\n",
      "[epoch: 7, i:  5499]  train_loss: 0.957  |  valid_loss: 0.794\n",
      "[epoch: 7, i:  5624]  train_loss: 0.914  |  valid_loss: 1.000\n",
      "[epoch: 7, i:  5749]  train_loss: 0.927  |  valid_loss: 0.891\n",
      "[epoch: 7, i:  5874]  train_loss: 0.950  |  valid_loss: 0.836\n",
      "[epoch: 7, i:  5999]  train_loss: 0.962  |  valid_loss: 0.924\n",
      "[epoch: 7, i:  6124]  train_loss: 0.881  |  valid_loss: 0.841\n",
      "[epoch: 7, i:  6249]  train_loss: 0.904  |  valid_loss: 0.996\n",
      "[epoch: 7, i:  6374]  train_loss: 0.976  |  valid_loss: 0.843\n",
      "[epoch: 7, i:  6499]  train_loss: 0.903  |  valid_loss: 0.863\n",
      "[epoch: 7, i:  6624]  train_loss: 0.914  |  valid_loss: 0.730\n",
      "[epoch: 7, i:  6749]  train_loss: 0.961  |  valid_loss: 0.964\n",
      "[epoch: 7, i:  6874]  train_loss: 0.877  |  valid_loss: 0.808\n",
      "[epoch: 7, i:  6999]  train_loss: 0.880  |  valid_loss: 1.025\n",
      "[epoch: 7, i:  7124]  train_loss: 0.909  |  valid_loss: 1.081\n",
      "[epoch: 7, i:  7249]  train_loss: 0.891  |  valid_loss: 0.711\n",
      "[epoch: 7, i:  7374]  train_loss: 0.858  |  valid_loss: 1.132\n",
      "[epoch: 7, i:  7499]  train_loss: 0.971  |  valid_loss: 1.060\n",
      "[epoch: 7, i:  7624]  train_loss: 0.901  |  valid_loss: 0.998\n",
      "[epoch: 7, i:  7749]  train_loss: 0.938  |  valid_loss: 0.816\n",
      "[epoch: 7, i:  7874]  train_loss: 0.910  |  valid_loss: 0.908\n",
      "[epoch: 7, i:  7999]  train_loss: 0.921  |  valid_loss: 0.860\n",
      "[epoch: 7, i:  8124]  train_loss: 0.933  |  valid_loss: 1.023\n",
      "[epoch: 7, i:  8249]  train_loss: 0.894  |  valid_loss: 0.966\n",
      "[epoch: 7, i:  8374]  train_loss: 0.859  |  valid_loss: 0.971\n",
      "[epoch: 7, i:  8499]  train_loss: 0.910  |  valid_loss: 0.992\n",
      "[epoch: 7, i:  8624]  train_loss: 0.960  |  valid_loss: 0.907\n",
      "[epoch: 7, i:  8749]  train_loss: 0.959  |  valid_loss: 1.074\n",
      "[epoch: 7, i:  8874]  train_loss: 0.936  |  valid_loss: 0.954\n",
      "[epoch: 7, i:  8999]  train_loss: 0.976  |  valid_loss: 0.749\n",
      "[epoch: 7, i:  9124]  train_loss: 0.964  |  valid_loss: 0.811\n",
      "[epoch: 7, i:  9249]  train_loss: 0.947  |  valid_loss: 0.730\n",
      "[epoch: 7, i:  9374]  train_loss: 0.905  |  valid_loss: 0.993\n",
      "[epoch: 7, i:  9499]  train_loss: 1.022  |  valid_loss: 0.818\n",
      "[epoch: 7, i:  9624]  train_loss: 0.968  |  valid_loss: 0.990\n",
      "[epoch: 7, i:  9749]  train_loss: 0.847  |  valid_loss: 0.982\n",
      "[epoch: 7, i:  9874]  train_loss: 0.929  |  valid_loss: 1.021\n",
      "[epoch: 7, i:  9999]  train_loss: 0.907  |  valid_loss: 0.916\n",
      "[epoch: 7, i: 10124]  train_loss: 0.835  |  valid_loss: 0.809\n",
      "[epoch: 7, i: 10249]  train_loss: 0.993  |  valid_loss: 0.934\n",
      "[epoch: 7, i: 10374]  train_loss: 0.920  |  valid_loss: 0.813\n",
      "[epoch: 7, i: 10499]  train_loss: 0.897  |  valid_loss: 1.059\n",
      "[epoch: 7, i: 10624]  train_loss: 0.910  |  valid_loss: 0.990\n",
      "[epoch: 7, i: 10749]  train_loss: 0.935  |  valid_loss: 0.899\n",
      "[epoch: 7, i: 10874]  train_loss: 0.861  |  valid_loss: 1.054\n",
      "[epoch: 7, i: 10999]  train_loss: 0.891  |  valid_loss: 0.981\n",
      "[epoch: 7, i: 11124]  train_loss: 0.886  |  valid_loss: 0.884\n",
      "[epoch: 7, i: 11249]  train_loss: 0.869  |  valid_loss: 0.911\n",
      "[epoch: 7, i: 11374]  train_loss: 0.862  |  valid_loss: 0.926\n",
      "[epoch: 7, i: 11499]  train_loss: 0.846  |  valid_loss: 0.738\n",
      "[epoch: 7, i: 11624]  train_loss: 0.989  |  valid_loss: 0.823\n",
      "[epoch: 7, i: 11749]  train_loss: 0.978  |  valid_loss: 0.897\n",
      "[epoch: 7, i: 11874]  train_loss: 0.927  |  valid_loss: 0.861\n",
      "[epoch: 7, i: 11999]  train_loss: 0.919  |  valid_loss: 0.759\n",
      "[epoch: 7, i: 12124]  train_loss: 0.942  |  valid_loss: 0.810\n",
      "[epoch: 7, i: 12249]  train_loss: 0.946  |  valid_loss: 1.158\n",
      "[epoch: 7, i: 12374]  train_loss: 0.894  |  valid_loss: 0.977\n",
      "[epoch: 7, i: 12499]  train_loss: 0.944  |  valid_loss: 0.914\n",
      "--> [End of epoch 7] train_accuracy: 67.23%  |  valid_accuracy: 67.77%\n",
      "--> [Start of epoch 8]  lr: 0.000336\n",
      "[epoch: 8, i:   124]  train_loss: 0.839  |  valid_loss: 0.790\n",
      "[epoch: 8, i:   249]  train_loss: 0.880  |  valid_loss: 0.834\n",
      "[epoch: 8, i:   374]  train_loss: 0.881  |  valid_loss: 0.963\n",
      "[epoch: 8, i:   499]  train_loss: 0.977  |  valid_loss: 0.899\n",
      "[epoch: 8, i:   624]  train_loss: 0.832  |  valid_loss: 0.970\n",
      "[epoch: 8, i:   749]  train_loss: 0.924  |  valid_loss: 0.693\n",
      "[epoch: 8, i:   874]  train_loss: 0.890  |  valid_loss: 0.940\n",
      "[epoch: 8, i:   999]  train_loss: 0.870  |  valid_loss: 1.027\n",
      "[epoch: 8, i:  1124]  train_loss: 0.919  |  valid_loss: 0.993\n",
      "[epoch: 8, i:  1249]  train_loss: 0.967  |  valid_loss: 0.827\n",
      "[epoch: 8, i:  1374]  train_loss: 0.964  |  valid_loss: 0.874\n",
      "[epoch: 8, i:  1499]  train_loss: 0.883  |  valid_loss: 0.875\n",
      "[epoch: 8, i:  1624]  train_loss: 0.849  |  valid_loss: 0.812\n",
      "[epoch: 8, i:  1749]  train_loss: 0.868  |  valid_loss: 1.156\n",
      "[epoch: 8, i:  1874]  train_loss: 0.910  |  valid_loss: 0.857\n",
      "[epoch: 8, i:  1999]  train_loss: 0.934  |  valid_loss: 1.121\n",
      "[epoch: 8, i:  2124]  train_loss: 0.968  |  valid_loss: 0.794\n",
      "[epoch: 8, i:  2249]  train_loss: 1.007  |  valid_loss: 0.887\n",
      "[epoch: 8, i:  2374]  train_loss: 0.843  |  valid_loss: 0.800\n",
      "[epoch: 8, i:  2499]  train_loss: 0.844  |  valid_loss: 1.036\n",
      "[epoch: 8, i:  2624]  train_loss: 0.938  |  valid_loss: 1.003\n",
      "[epoch: 8, i:  2749]  train_loss: 0.910  |  valid_loss: 1.016\n",
      "[epoch: 8, i:  2874]  train_loss: 0.971  |  valid_loss: 1.164\n",
      "[epoch: 8, i:  2999]  train_loss: 0.880  |  valid_loss: 0.914\n",
      "[epoch: 8, i:  3124]  train_loss: 0.892  |  valid_loss: 0.909\n",
      "[epoch: 8, i:  3249]  train_loss: 0.880  |  valid_loss: 1.128\n",
      "[epoch: 8, i:  3374]  train_loss: 0.898  |  valid_loss: 0.848\n",
      "[epoch: 8, i:  3499]  train_loss: 0.895  |  valid_loss: 0.784\n",
      "[epoch: 8, i:  3624]  train_loss: 0.930  |  valid_loss: 0.897\n",
      "[epoch: 8, i:  3749]  train_loss: 1.017  |  valid_loss: 0.743\n",
      "[epoch: 8, i:  3874]  train_loss: 0.975  |  valid_loss: 0.882\n",
      "[epoch: 8, i:  3999]  train_loss: 0.957  |  valid_loss: 0.694\n",
      "[epoch: 8, i:  4124]  train_loss: 0.939  |  valid_loss: 0.914\n",
      "[epoch: 8, i:  4249]  train_loss: 0.946  |  valid_loss: 0.982\n",
      "[epoch: 8, i:  4374]  train_loss: 0.905  |  valid_loss: 0.977\n",
      "[epoch: 8, i:  4499]  train_loss: 0.924  |  valid_loss: 0.872\n",
      "[epoch: 8, i:  4624]  train_loss: 0.870  |  valid_loss: 1.035\n",
      "[epoch: 8, i:  4749]  train_loss: 1.032  |  valid_loss: 0.928\n",
      "[epoch: 8, i:  4874]  train_loss: 0.872  |  valid_loss: 0.815\n",
      "[epoch: 8, i:  4999]  train_loss: 0.819  |  valid_loss: 0.786\n",
      "[epoch: 8, i:  5124]  train_loss: 0.864  |  valid_loss: 0.857\n",
      "[epoch: 8, i:  5249]  train_loss: 0.935  |  valid_loss: 0.896\n",
      "[epoch: 8, i:  5374]  train_loss: 0.904  |  valid_loss: 0.714\n",
      "[epoch: 8, i:  5499]  train_loss: 0.920  |  valid_loss: 0.756\n",
      "[epoch: 8, i:  5624]  train_loss: 0.918  |  valid_loss: 0.901\n",
      "[epoch: 8, i:  5749]  train_loss: 0.865  |  valid_loss: 0.898\n",
      "[epoch: 8, i:  5874]  train_loss: 0.859  |  valid_loss: 0.792\n",
      "[epoch: 8, i:  5999]  train_loss: 0.906  |  valid_loss: 0.883\n",
      "[epoch: 8, i:  6124]  train_loss: 0.955  |  valid_loss: 0.783\n",
      "[epoch: 8, i:  6249]  train_loss: 0.893  |  valid_loss: 1.014\n",
      "[epoch: 8, i:  6374]  train_loss: 0.878  |  valid_loss: 0.907\n",
      "[epoch: 8, i:  6499]  train_loss: 0.963  |  valid_loss: 0.840\n",
      "[epoch: 8, i:  6624]  train_loss: 0.826  |  valid_loss: 0.758\n",
      "[epoch: 8, i:  6749]  train_loss: 0.910  |  valid_loss: 0.915\n",
      "[epoch: 8, i:  6874]  train_loss: 0.878  |  valid_loss: 0.847\n",
      "[epoch: 8, i:  6999]  train_loss: 0.865  |  valid_loss: 1.114\n",
      "[epoch: 8, i:  7124]  train_loss: 0.872  |  valid_loss: 0.966\n",
      "[epoch: 8, i:  7249]  train_loss: 0.928  |  valid_loss: 0.739\n",
      "[epoch: 8, i:  7374]  train_loss: 0.921  |  valid_loss: 1.043\n",
      "[epoch: 8, i:  7499]  train_loss: 0.873  |  valid_loss: 0.971\n",
      "[epoch: 8, i:  7624]  train_loss: 0.860  |  valid_loss: 0.987\n",
      "[epoch: 8, i:  7749]  train_loss: 0.849  |  valid_loss: 0.878\n",
      "[epoch: 8, i:  7874]  train_loss: 0.876  |  valid_loss: 0.862\n",
      "[epoch: 8, i:  7999]  train_loss: 0.957  |  valid_loss: 0.812\n",
      "[epoch: 8, i:  8124]  train_loss: 0.884  |  valid_loss: 0.943\n",
      "[epoch: 8, i:  8249]  train_loss: 0.820  |  valid_loss: 0.964\n",
      "[epoch: 8, i:  8374]  train_loss: 0.943  |  valid_loss: 0.911\n",
      "[epoch: 8, i:  8499]  train_loss: 0.951  |  valid_loss: 0.954\n",
      "[epoch: 8, i:  8624]  train_loss: 0.966  |  valid_loss: 0.881\n",
      "[epoch: 8, i:  8749]  train_loss: 0.896  |  valid_loss: 0.929\n",
      "[epoch: 8, i:  8874]  train_loss: 0.988  |  valid_loss: 0.897\n",
      "[epoch: 8, i:  8999]  train_loss: 0.913  |  valid_loss: 0.745\n",
      "[epoch: 8, i:  9124]  train_loss: 0.909  |  valid_loss: 0.841\n",
      "[epoch: 8, i:  9249]  train_loss: 0.806  |  valid_loss: 0.698\n",
      "[epoch: 8, i:  9374]  train_loss: 0.814  |  valid_loss: 0.980\n",
      "[epoch: 8, i:  9499]  train_loss: 0.927  |  valid_loss: 0.916\n",
      "[epoch: 8, i:  9624]  train_loss: 0.934  |  valid_loss: 0.913\n",
      "[epoch: 8, i:  9749]  train_loss: 0.937  |  valid_loss: 0.930\n",
      "[epoch: 8, i:  9874]  train_loss: 0.929  |  valid_loss: 0.960\n",
      "[epoch: 8, i:  9999]  train_loss: 0.920  |  valid_loss: 0.913\n",
      "[epoch: 8, i: 10124]  train_loss: 0.874  |  valid_loss: 0.833\n",
      "[epoch: 8, i: 10249]  train_loss: 0.845  |  valid_loss: 0.931\n",
      "[epoch: 8, i: 10374]  train_loss: 0.875  |  valid_loss: 0.822\n",
      "[epoch: 8, i: 10499]  train_loss: 0.939  |  valid_loss: 0.994\n",
      "[epoch: 8, i: 10624]  train_loss: 0.946  |  valid_loss: 1.023\n",
      "[epoch: 8, i: 10749]  train_loss: 0.877  |  valid_loss: 0.896\n",
      "[epoch: 8, i: 10874]  train_loss: 0.876  |  valid_loss: 0.997\n",
      "[epoch: 8, i: 10999]  train_loss: 0.815  |  valid_loss: 1.010\n",
      "[epoch: 8, i: 11124]  train_loss: 0.867  |  valid_loss: 0.905\n",
      "[epoch: 8, i: 11249]  train_loss: 0.959  |  valid_loss: 0.896\n",
      "[epoch: 8, i: 11374]  train_loss: 0.994  |  valid_loss: 0.789\n",
      "[epoch: 8, i: 11499]  train_loss: 0.866  |  valid_loss: 0.672\n",
      "[epoch: 8, i: 11624]  train_loss: 0.903  |  valid_loss: 0.821\n",
      "[epoch: 8, i: 11749]  train_loss: 0.867  |  valid_loss: 0.833\n",
      "[epoch: 8, i: 11874]  train_loss: 0.894  |  valid_loss: 0.822\n",
      "[epoch: 8, i: 11999]  train_loss: 0.855  |  valid_loss: 0.769\n",
      "[epoch: 8, i: 12124]  train_loss: 0.944  |  valid_loss: 0.741\n",
      "[epoch: 8, i: 12249]  train_loss: 0.931  |  valid_loss: 1.140\n",
      "[epoch: 8, i: 12374]  train_loss: 0.861  |  valid_loss: 0.917\n",
      "[epoch: 8, i: 12499]  train_loss: 0.909  |  valid_loss: 0.874\n",
      "--> [End of epoch 8] train_accuracy: 68.07%  |  valid_accuracy: 68.79%\n",
      "--> [Start of epoch 9]  lr: 0.000268\n",
      "[epoch: 9, i:   124]  train_loss: 0.910  |  valid_loss: 0.744\n",
      "[epoch: 9, i:   249]  train_loss: 0.828  |  valid_loss: 0.808\n",
      "[epoch: 9, i:   374]  train_loss: 0.930  |  valid_loss: 0.919\n",
      "[epoch: 9, i:   499]  train_loss: 0.914  |  valid_loss: 0.917\n",
      "[epoch: 9, i:   624]  train_loss: 0.887  |  valid_loss: 1.002\n",
      "[epoch: 9, i:   749]  train_loss: 0.838  |  valid_loss: 0.665\n",
      "[epoch: 9, i:   874]  train_loss: 0.891  |  valid_loss: 0.910\n",
      "[epoch: 9, i:   999]  train_loss: 0.817  |  valid_loss: 1.036\n",
      "[epoch: 9, i:  1124]  train_loss: 0.930  |  valid_loss: 1.002\n",
      "[epoch: 9, i:  1249]  train_loss: 0.862  |  valid_loss: 0.777\n",
      "[epoch: 9, i:  1374]  train_loss: 0.921  |  valid_loss: 0.807\n",
      "[epoch: 9, i:  1499]  train_loss: 0.927  |  valid_loss: 0.819\n",
      "[epoch: 9, i:  1624]  train_loss: 0.901  |  valid_loss: 0.730\n",
      "[epoch: 9, i:  1749]  train_loss: 0.876  |  valid_loss: 1.089\n",
      "[epoch: 9, i:  1874]  train_loss: 0.922  |  valid_loss: 0.781\n",
      "[epoch: 9, i:  1999]  train_loss: 0.843  |  valid_loss: 1.047\n",
      "[epoch: 9, i:  2124]  train_loss: 0.884  |  valid_loss: 0.762\n",
      "[epoch: 9, i:  2249]  train_loss: 0.853  |  valid_loss: 0.906\n",
      "[epoch: 9, i:  2374]  train_loss: 0.886  |  valid_loss: 0.782\n",
      "[epoch: 9, i:  2499]  train_loss: 0.819  |  valid_loss: 1.061\n",
      "[epoch: 9, i:  2624]  train_loss: 0.918  |  valid_loss: 1.028\n",
      "[epoch: 9, i:  2749]  train_loss: 0.868  |  valid_loss: 1.054\n",
      "[epoch: 9, i:  2874]  train_loss: 0.943  |  valid_loss: 1.166\n",
      "[epoch: 9, i:  2999]  train_loss: 0.829  |  valid_loss: 0.919\n",
      "[epoch: 9, i:  3124]  train_loss: 0.910  |  valid_loss: 0.949\n",
      "[epoch: 9, i:  3249]  train_loss: 0.827  |  valid_loss: 1.131\n",
      "[epoch: 9, i:  3374]  train_loss: 0.906  |  valid_loss: 0.934\n",
      "[epoch: 9, i:  3499]  train_loss: 0.900  |  valid_loss: 0.796\n",
      "[epoch: 9, i:  3624]  train_loss: 0.945  |  valid_loss: 0.894\n",
      "[epoch: 9, i:  3749]  train_loss: 0.928  |  valid_loss: 0.767\n",
      "[epoch: 9, i:  3874]  train_loss: 0.881  |  valid_loss: 0.869\n",
      "[epoch: 9, i:  3999]  train_loss: 0.920  |  valid_loss: 0.657\n",
      "[epoch: 9, i:  4124]  train_loss: 0.907  |  valid_loss: 0.947\n",
      "[epoch: 9, i:  4249]  train_loss: 0.894  |  valid_loss: 0.987\n",
      "[epoch: 9, i:  4374]  train_loss: 0.918  |  valid_loss: 0.960\n",
      "[epoch: 9, i:  4499]  train_loss: 0.891  |  valid_loss: 0.849\n",
      "[epoch: 9, i:  4624]  train_loss: 0.835  |  valid_loss: 0.968\n",
      "[epoch: 9, i:  4749]  train_loss: 0.937  |  valid_loss: 0.955\n",
      "[epoch: 9, i:  4874]  train_loss: 0.934  |  valid_loss: 0.765\n",
      "[epoch: 9, i:  4999]  train_loss: 0.883  |  valid_loss: 0.838\n",
      "[epoch: 9, i:  5124]  train_loss: 0.928  |  valid_loss: 0.856\n",
      "[epoch: 9, i:  5249]  train_loss: 0.844  |  valid_loss: 0.945\n",
      "[epoch: 9, i:  5374]  train_loss: 0.957  |  valid_loss: 0.677\n",
      "[epoch: 9, i:  5499]  train_loss: 0.838  |  valid_loss: 0.720\n",
      "[epoch: 9, i:  5624]  train_loss: 0.846  |  valid_loss: 1.002\n",
      "[epoch: 9, i:  5749]  train_loss: 0.861  |  valid_loss: 0.938\n",
      "[epoch: 9, i:  5874]  train_loss: 0.861  |  valid_loss: 0.792\n",
      "[epoch: 9, i:  5999]  train_loss: 0.862  |  valid_loss: 0.949\n",
      "[epoch: 9, i:  6124]  train_loss: 0.907  |  valid_loss: 0.819\n",
      "[epoch: 9, i:  6249]  train_loss: 0.843  |  valid_loss: 0.901\n",
      "[epoch: 9, i:  6374]  train_loss: 0.821  |  valid_loss: 0.766\n",
      "[epoch: 9, i:  6499]  train_loss: 0.855  |  valid_loss: 0.780\n",
      "[epoch: 9, i:  6624]  train_loss: 0.828  |  valid_loss: 0.747\n",
      "[epoch: 9, i:  6749]  train_loss: 0.944  |  valid_loss: 0.852\n",
      "[epoch: 9, i:  6874]  train_loss: 0.825  |  valid_loss: 0.841\n",
      "[epoch: 9, i:  6999]  train_loss: 0.883  |  valid_loss: 1.020\n",
      "[epoch: 9, i:  7124]  train_loss: 0.833  |  valid_loss: 1.017\n",
      "[epoch: 9, i:  7249]  train_loss: 0.892  |  valid_loss: 0.705\n",
      "[epoch: 9, i:  7374]  train_loss: 0.935  |  valid_loss: 1.104\n",
      "[epoch: 9, i:  7499]  train_loss: 0.886  |  valid_loss: 0.979\n",
      "[epoch: 9, i:  7624]  train_loss: 0.896  |  valid_loss: 0.992\n",
      "[epoch: 9, i:  7749]  train_loss: 0.900  |  valid_loss: 0.891\n",
      "[epoch: 9, i:  7874]  train_loss: 0.923  |  valid_loss: 0.917\n",
      "[epoch: 9, i:  7999]  train_loss: 0.921  |  valid_loss: 0.760\n",
      "[epoch: 9, i:  8124]  train_loss: 0.870  |  valid_loss: 0.926\n",
      "[epoch: 9, i:  8249]  train_loss: 0.876  |  valid_loss: 0.920\n",
      "[epoch: 9, i:  8374]  train_loss: 0.857  |  valid_loss: 0.927\n",
      "[epoch: 9, i:  8499]  train_loss: 0.845  |  valid_loss: 0.971\n",
      "[epoch: 9, i:  8624]  train_loss: 0.861  |  valid_loss: 0.875\n",
      "[epoch: 9, i:  8749]  train_loss: 0.884  |  valid_loss: 0.957\n",
      "[epoch: 9, i:  8874]  train_loss: 0.916  |  valid_loss: 0.910\n",
      "[epoch: 9, i:  8999]  train_loss: 0.880  |  valid_loss: 0.769\n",
      "[epoch: 9, i:  9124]  train_loss: 0.840  |  valid_loss: 0.878\n",
      "[epoch: 9, i:  9249]  train_loss: 0.888  |  valid_loss: 0.652\n",
      "[epoch: 9, i:  9374]  train_loss: 0.911  |  valid_loss: 0.969\n",
      "[epoch: 9, i:  9499]  train_loss: 0.840  |  valid_loss: 0.817\n",
      "[epoch: 9, i:  9624]  train_loss: 0.837  |  valid_loss: 0.968\n",
      "[epoch: 9, i:  9749]  train_loss: 0.890  |  valid_loss: 0.983\n",
      "[epoch: 9, i:  9874]  train_loss: 0.948  |  valid_loss: 0.963\n",
      "[epoch: 9, i:  9999]  train_loss: 0.914  |  valid_loss: 0.896\n",
      "[epoch: 9, i: 10124]  train_loss: 0.734  |  valid_loss: 0.834\n",
      "[epoch: 9, i: 10249]  train_loss: 0.917  |  valid_loss: 0.983\n",
      "[epoch: 9, i: 10374]  train_loss: 0.876  |  valid_loss: 0.787\n",
      "[epoch: 9, i: 10499]  train_loss: 0.919  |  valid_loss: 1.003\n",
      "[epoch: 9, i: 10624]  train_loss: 0.962  |  valid_loss: 0.990\n",
      "[epoch: 9, i: 10749]  train_loss: 0.879  |  valid_loss: 0.863\n",
      "[epoch: 9, i: 10874]  train_loss: 0.935  |  valid_loss: 1.005\n",
      "[epoch: 9, i: 10999]  train_loss: 0.841  |  valid_loss: 0.978\n",
      "[epoch: 9, i: 11124]  train_loss: 0.858  |  valid_loss: 0.890\n",
      "[epoch: 9, i: 11249]  train_loss: 0.931  |  valid_loss: 0.883\n",
      "[epoch: 9, i: 11374]  train_loss: 0.815  |  valid_loss: 0.860\n",
      "[epoch: 9, i: 11499]  train_loss: 0.856  |  valid_loss: 0.720\n",
      "[epoch: 9, i: 11624]  train_loss: 0.887  |  valid_loss: 0.855\n",
      "[epoch: 9, i: 11749]  train_loss: 0.818  |  valid_loss: 0.842\n",
      "[epoch: 9, i: 11874]  train_loss: 0.927  |  valid_loss: 0.907\n",
      "[epoch: 9, i: 11999]  train_loss: 0.930  |  valid_loss: 0.725\n",
      "[epoch: 9, i: 12124]  train_loss: 0.884  |  valid_loss: 0.777\n",
      "[epoch: 9, i: 12249]  train_loss: 0.869  |  valid_loss: 1.068\n",
      "[epoch: 9, i: 12374]  train_loss: 0.783  |  valid_loss: 0.934\n",
      "[epoch: 9, i: 12499]  train_loss: 0.891  |  valid_loss: 0.911\n",
      "--> [End of epoch 9] train_accuracy: 68.62%  |  valid_accuracy: 68.96%\n",
      "--> [Start of epoch 10]  lr: 0.000215\n",
      "[epoch: 10, i:   124]  train_loss: 0.903  |  valid_loss: 0.778\n",
      "[epoch: 10, i:   249]  train_loss: 0.873  |  valid_loss: 0.753\n",
      "[epoch: 10, i:   374]  train_loss: 0.856  |  valid_loss: 0.930\n",
      "[epoch: 10, i:   499]  train_loss: 0.875  |  valid_loss: 0.826\n",
      "[epoch: 10, i:   624]  train_loss: 0.856  |  valid_loss: 0.936\n",
      "[epoch: 10, i:   749]  train_loss: 0.878  |  valid_loss: 0.643\n",
      "[epoch: 10, i:   874]  train_loss: 0.884  |  valid_loss: 0.874\n",
      "[epoch: 10, i:   999]  train_loss: 0.832  |  valid_loss: 1.024\n",
      "[epoch: 10, i:  1124]  train_loss: 0.858  |  valid_loss: 0.960\n",
      "[epoch: 10, i:  1249]  train_loss: 0.988  |  valid_loss: 0.754\n",
      "[epoch: 10, i:  1374]  train_loss: 0.845  |  valid_loss: 0.764\n",
      "[epoch: 10, i:  1499]  train_loss: 0.881  |  valid_loss: 0.848\n",
      "[epoch: 10, i:  1624]  train_loss: 0.891  |  valid_loss: 0.722\n",
      "[epoch: 10, i:  1749]  train_loss: 0.846  |  valid_loss: 1.135\n",
      "[epoch: 10, i:  1874]  train_loss: 0.897  |  valid_loss: 0.780\n",
      "[epoch: 10, i:  1999]  train_loss: 0.867  |  valid_loss: 1.053\n",
      "[epoch: 10, i:  2124]  train_loss: 0.974  |  valid_loss: 0.814\n",
      "[epoch: 10, i:  2249]  train_loss: 0.872  |  valid_loss: 0.882\n",
      "[epoch: 10, i:  2374]  train_loss: 0.904  |  valid_loss: 0.779\n",
      "[epoch: 10, i:  2499]  train_loss: 0.776  |  valid_loss: 1.044\n",
      "[epoch: 10, i:  2624]  train_loss: 0.805  |  valid_loss: 0.908\n",
      "[epoch: 10, i:  2749]  train_loss: 0.847  |  valid_loss: 0.978\n",
      "[epoch: 10, i:  2874]  train_loss: 0.903  |  valid_loss: 1.153\n",
      "[epoch: 10, i:  2999]  train_loss: 0.901  |  valid_loss: 0.889\n",
      "[epoch: 10, i:  3124]  train_loss: 0.807  |  valid_loss: 0.923\n",
      "[epoch: 10, i:  3249]  train_loss: 0.917  |  valid_loss: 1.083\n",
      "[epoch: 10, i:  3374]  train_loss: 0.906  |  valid_loss: 0.841\n",
      "[epoch: 10, i:  3499]  train_loss: 0.856  |  valid_loss: 0.799\n",
      "[epoch: 10, i:  3624]  train_loss: 0.885  |  valid_loss: 0.908\n",
      "[epoch: 10, i:  3749]  train_loss: 0.850  |  valid_loss: 0.711\n",
      "[epoch: 10, i:  3874]  train_loss: 0.914  |  valid_loss: 0.861\n",
      "[epoch: 10, i:  3999]  train_loss: 0.835  |  valid_loss: 0.687\n",
      "[epoch: 10, i:  4124]  train_loss: 0.775  |  valid_loss: 0.898\n",
      "[epoch: 10, i:  4249]  train_loss: 0.846  |  valid_loss: 0.971\n",
      "[epoch: 10, i:  4374]  train_loss: 0.867  |  valid_loss: 0.963\n",
      "[epoch: 10, i:  4499]  train_loss: 0.818  |  valid_loss: 0.829\n",
      "[epoch: 10, i:  4624]  train_loss: 0.859  |  valid_loss: 1.002\n",
      "[epoch: 10, i:  4749]  train_loss: 0.890  |  valid_loss: 0.941\n",
      "[epoch: 10, i:  4874]  train_loss: 0.831  |  valid_loss: 0.801\n",
      "[epoch: 10, i:  4999]  train_loss: 0.924  |  valid_loss: 0.815\n",
      "[epoch: 10, i:  5124]  train_loss: 0.884  |  valid_loss: 0.834\n",
      "[epoch: 10, i:  5249]  train_loss: 0.866  |  valid_loss: 0.925\n",
      "[epoch: 10, i:  5374]  train_loss: 0.872  |  valid_loss: 0.714\n",
      "[epoch: 10, i:  5499]  train_loss: 0.849  |  valid_loss: 0.736\n",
      "[epoch: 10, i:  5624]  train_loss: 0.957  |  valid_loss: 0.947\n",
      "[epoch: 10, i:  5749]  train_loss: 0.856  |  valid_loss: 0.846\n",
      "[epoch: 10, i:  5874]  train_loss: 0.886  |  valid_loss: 0.767\n",
      "[epoch: 10, i:  5999]  train_loss: 0.909  |  valid_loss: 0.867\n",
      "[epoch: 10, i:  6124]  train_loss: 0.927  |  valid_loss: 0.804\n",
      "[epoch: 10, i:  6249]  train_loss: 0.852  |  valid_loss: 0.899\n",
      "[epoch: 10, i:  6374]  train_loss: 0.823  |  valid_loss: 0.812\n",
      "[epoch: 10, i:  6499]  train_loss: 0.872  |  valid_loss: 0.824\n",
      "[epoch: 10, i:  6624]  train_loss: 0.834  |  valid_loss: 0.749\n",
      "[epoch: 10, i:  6749]  train_loss: 0.791  |  valid_loss: 0.876\n",
      "[epoch: 10, i:  6874]  train_loss: 0.871  |  valid_loss: 0.803\n",
      "[epoch: 10, i:  6999]  train_loss: 0.789  |  valid_loss: 1.045\n",
      "[epoch: 10, i:  7124]  train_loss: 0.848  |  valid_loss: 0.988\n",
      "[epoch: 10, i:  7249]  train_loss: 0.825  |  valid_loss: 0.697\n",
      "[epoch: 10, i:  7374]  train_loss: 0.852  |  valid_loss: 1.065\n",
      "[epoch: 10, i:  7499]  train_loss: 0.908  |  valid_loss: 0.954\n",
      "[epoch: 10, i:  7624]  train_loss: 0.890  |  valid_loss: 0.954\n",
      "[epoch: 10, i:  7749]  train_loss: 0.823  |  valid_loss: 0.856\n",
      "[epoch: 10, i:  7874]  train_loss: 0.916  |  valid_loss: 0.853\n",
      "[epoch: 10, i:  7999]  train_loss: 0.878  |  valid_loss: 0.760\n",
      "[epoch: 10, i:  8124]  train_loss: 0.901  |  valid_loss: 0.999\n",
      "[epoch: 10, i:  8249]  train_loss: 0.862  |  valid_loss: 0.939\n",
      "[epoch: 10, i:  8374]  train_loss: 0.818  |  valid_loss: 0.885\n",
      "[epoch: 10, i:  8499]  train_loss: 0.818  |  valid_loss: 0.974\n",
      "[epoch: 10, i:  8624]  train_loss: 0.898  |  valid_loss: 0.865\n",
      "[epoch: 10, i:  8749]  train_loss: 0.911  |  valid_loss: 0.940\n",
      "[epoch: 10, i:  8874]  train_loss: 0.882  |  valid_loss: 0.909\n",
      "[epoch: 10, i:  8999]  train_loss: 0.792  |  valid_loss: 0.699\n",
      "[epoch: 10, i:  9124]  train_loss: 0.941  |  valid_loss: 0.744\n",
      "[epoch: 10, i:  9249]  train_loss: 0.832  |  valid_loss: 0.628\n",
      "[epoch: 10, i:  9374]  train_loss: 0.926  |  valid_loss: 0.897\n",
      "[epoch: 10, i:  9499]  train_loss: 0.842  |  valid_loss: 0.838\n",
      "[epoch: 10, i:  9624]  train_loss: 0.884  |  valid_loss: 0.907\n",
      "[epoch: 10, i:  9749]  train_loss: 0.845  |  valid_loss: 0.957\n",
      "[epoch: 10, i:  9874]  train_loss: 0.852  |  valid_loss: 0.972\n",
      "[epoch: 10, i:  9999]  train_loss: 0.786  |  valid_loss: 0.842\n",
      "[epoch: 10, i: 10124]  train_loss: 0.859  |  valid_loss: 0.806\n",
      "[epoch: 10, i: 10249]  train_loss: 0.935  |  valid_loss: 0.910\n",
      "[epoch: 10, i: 10374]  train_loss: 0.877  |  valid_loss: 0.791\n",
      "[epoch: 10, i: 10499]  train_loss: 0.831  |  valid_loss: 0.987\n",
      "[epoch: 10, i: 10624]  train_loss: 0.911  |  valid_loss: 1.001\n",
      "[epoch: 10, i: 10749]  train_loss: 0.864  |  valid_loss: 0.867\n",
      "[epoch: 10, i: 10874]  train_loss: 0.838  |  valid_loss: 0.986\n",
      "[epoch: 10, i: 10999]  train_loss: 0.920  |  valid_loss: 1.023\n",
      "[epoch: 10, i: 11124]  train_loss: 0.765  |  valid_loss: 0.825\n",
      "[epoch: 10, i: 11249]  train_loss: 0.913  |  valid_loss: 0.886\n",
      "[epoch: 10, i: 11374]  train_loss: 0.866  |  valid_loss: 0.832\n",
      "[epoch: 10, i: 11499]  train_loss: 0.857  |  valid_loss: 0.704\n",
      "[epoch: 10, i: 11624]  train_loss: 0.951  |  valid_loss: 0.812\n",
      "[epoch: 10, i: 11749]  train_loss: 0.842  |  valid_loss: 0.788\n",
      "[epoch: 10, i: 11874]  train_loss: 0.909  |  valid_loss: 0.812\n",
      "[epoch: 10, i: 11999]  train_loss: 0.876  |  valid_loss: 0.748\n",
      "[epoch: 10, i: 12124]  train_loss: 0.840  |  valid_loss: 0.765\n",
      "[epoch: 10, i: 12249]  train_loss: 0.932  |  valid_loss: 1.083\n",
      "[epoch: 10, i: 12374]  train_loss: 0.817  |  valid_loss: 0.905\n",
      "[epoch: 10, i: 12499]  train_loss: 0.910  |  valid_loss: 0.884\n",
      "--> [End of epoch 10] train_accuracy: 69.33%  |  valid_accuracy: 69.45%\n",
      "--> [Start of epoch 11]  lr: 0.000172\n",
      "[epoch: 11, i:   124]  train_loss: 0.821  |  valid_loss: 0.735\n",
      "[epoch: 11, i:   249]  train_loss: 0.860  |  valid_loss: 0.734\n",
      "[epoch: 11, i:   374]  train_loss: 0.861  |  valid_loss: 0.949\n",
      "[epoch: 11, i:   499]  train_loss: 0.953  |  valid_loss: 0.835\n",
      "[epoch: 11, i:   624]  train_loss: 0.870  |  valid_loss: 0.967\n",
      "[epoch: 11, i:   749]  train_loss: 0.824  |  valid_loss: 0.634\n",
      "[epoch: 11, i:   874]  train_loss: 0.836  |  valid_loss: 0.898\n",
      "[epoch: 11, i:   999]  train_loss: 0.847  |  valid_loss: 0.956\n",
      "[epoch: 11, i:  1124]  train_loss: 0.888  |  valid_loss: 0.972\n",
      "[epoch: 11, i:  1249]  train_loss: 0.872  |  valid_loss: 0.755\n",
      "[epoch: 11, i:  1374]  train_loss: 0.848  |  valid_loss: 0.765\n",
      "[epoch: 11, i:  1499]  train_loss: 0.904  |  valid_loss: 0.856\n",
      "[epoch: 11, i:  1624]  train_loss: 0.825  |  valid_loss: 0.698\n",
      "[epoch: 11, i:  1749]  train_loss: 0.839  |  valid_loss: 0.999\n",
      "[epoch: 11, i:  1874]  train_loss: 0.842  |  valid_loss: 0.797\n",
      "[epoch: 11, i:  1999]  train_loss: 0.853  |  valid_loss: 1.032\n",
      "[epoch: 11, i:  2124]  train_loss: 0.831  |  valid_loss: 0.745\n",
      "[epoch: 11, i:  2249]  train_loss: 0.760  |  valid_loss: 0.879\n",
      "[epoch: 11, i:  2374]  train_loss: 0.892  |  valid_loss: 0.753\n",
      "[epoch: 11, i:  2499]  train_loss: 0.918  |  valid_loss: 1.010\n",
      "[epoch: 11, i:  2624]  train_loss: 0.842  |  valid_loss: 0.957\n",
      "[epoch: 11, i:  2749]  train_loss: 0.785  |  valid_loss: 0.943\n",
      "[epoch: 11, i:  2874]  train_loss: 0.845  |  valid_loss: 1.114\n",
      "[epoch: 11, i:  2999]  train_loss: 0.815  |  valid_loss: 0.851\n",
      "[epoch: 11, i:  3124]  train_loss: 0.857  |  valid_loss: 0.939\n",
      "[epoch: 11, i:  3249]  train_loss: 0.892  |  valid_loss: 1.095\n",
      "[epoch: 11, i:  3374]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 11, i:  3499]  train_loss: 0.834  |  valid_loss: 0.778\n",
      "[epoch: 11, i:  3624]  train_loss: 0.807  |  valid_loss: 0.840\n",
      "[epoch: 11, i:  3749]  train_loss: 0.885  |  valid_loss: 0.719\n",
      "[epoch: 11, i:  3874]  train_loss: 0.873  |  valid_loss: 0.802\n",
      "[epoch: 11, i:  3999]  train_loss: 0.835  |  valid_loss: 0.677\n",
      "[epoch: 11, i:  4124]  train_loss: 0.874  |  valid_loss: 0.917\n",
      "[epoch: 11, i:  4249]  train_loss: 0.894  |  valid_loss: 0.942\n",
      "[epoch: 11, i:  4374]  train_loss: 0.838  |  valid_loss: 0.906\n",
      "[epoch: 11, i:  4499]  train_loss: 0.879  |  valid_loss: 0.809\n",
      "[epoch: 11, i:  4624]  train_loss: 0.801  |  valid_loss: 0.969\n",
      "[epoch: 11, i:  4749]  train_loss: 0.834  |  valid_loss: 0.913\n",
      "[epoch: 11, i:  4874]  train_loss: 0.915  |  valid_loss: 0.843\n",
      "[epoch: 11, i:  4999]  train_loss: 0.892  |  valid_loss: 0.817\n",
      "[epoch: 11, i:  5124]  train_loss: 0.878  |  valid_loss: 0.810\n",
      "[epoch: 11, i:  5249]  train_loss: 0.921  |  valid_loss: 0.903\n",
      "[epoch: 11, i:  5374]  train_loss: 0.829  |  valid_loss: 0.661\n",
      "[epoch: 11, i:  5499]  train_loss: 0.802  |  valid_loss: 0.720\n",
      "[epoch: 11, i:  5624]  train_loss: 0.881  |  valid_loss: 1.015\n",
      "[epoch: 11, i:  5749]  train_loss: 0.744  |  valid_loss: 0.839\n",
      "[epoch: 11, i:  5874]  train_loss: 0.886  |  valid_loss: 0.772\n",
      "[epoch: 11, i:  5999]  train_loss: 0.895  |  valid_loss: 0.878\n",
      "[epoch: 11, i:  6124]  train_loss: 0.808  |  valid_loss: 0.816\n",
      "[epoch: 11, i:  6249]  train_loss: 0.884  |  valid_loss: 0.898\n",
      "[epoch: 11, i:  6374]  train_loss: 0.805  |  valid_loss: 0.785\n",
      "[epoch: 11, i:  6499]  train_loss: 0.887  |  valid_loss: 0.827\n",
      "[epoch: 11, i:  6624]  train_loss: 0.946  |  valid_loss: 0.746\n",
      "[epoch: 11, i:  6749]  train_loss: 0.851  |  valid_loss: 0.857\n",
      "[epoch: 11, i:  6874]  train_loss: 0.837  |  valid_loss: 0.801\n",
      "[epoch: 11, i:  6999]  train_loss: 0.816  |  valid_loss: 1.004\n",
      "[epoch: 11, i:  7124]  train_loss: 0.908  |  valid_loss: 0.978\n",
      "[epoch: 11, i:  7249]  train_loss: 0.877  |  valid_loss: 0.700\n",
      "[epoch: 11, i:  7374]  train_loss: 0.792  |  valid_loss: 1.084\n",
      "[epoch: 11, i:  7499]  train_loss: 0.877  |  valid_loss: 0.959\n",
      "[epoch: 11, i:  7624]  train_loss: 0.929  |  valid_loss: 0.946\n",
      "[epoch: 11, i:  7749]  train_loss: 0.914  |  valid_loss: 0.831\n",
      "[epoch: 11, i:  7874]  train_loss: 0.913  |  valid_loss: 0.884\n",
      "[epoch: 11, i:  7999]  train_loss: 0.904  |  valid_loss: 0.730\n",
      "[epoch: 11, i:  8124]  train_loss: 0.857  |  valid_loss: 0.937\n",
      "[epoch: 11, i:  8249]  train_loss: 0.890  |  valid_loss: 0.926\n",
      "[epoch: 11, i:  8374]  train_loss: 0.915  |  valid_loss: 0.870\n",
      "[epoch: 11, i:  8499]  train_loss: 0.865  |  valid_loss: 0.935\n",
      "[epoch: 11, i:  8624]  train_loss: 0.822  |  valid_loss: 0.867\n",
      "[epoch: 11, i:  8749]  train_loss: 0.831  |  valid_loss: 0.963\n",
      "[epoch: 11, i:  8874]  train_loss: 0.775  |  valid_loss: 0.897\n",
      "[epoch: 11, i:  8999]  train_loss: 0.837  |  valid_loss: 0.700\n",
      "[epoch: 11, i:  9124]  train_loss: 0.866  |  valid_loss: 0.825\n",
      "[epoch: 11, i:  9249]  train_loss: 0.876  |  valid_loss: 0.651\n",
      "[epoch: 11, i:  9374]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 11, i:  9499]  train_loss: 0.798  |  valid_loss: 0.817\n",
      "[epoch: 11, i:  9624]  train_loss: 0.866  |  valid_loss: 0.943\n",
      "[epoch: 11, i:  9749]  train_loss: 0.899  |  valid_loss: 0.909\n",
      "[epoch: 11, i:  9874]  train_loss: 0.839  |  valid_loss: 0.939\n",
      "[epoch: 11, i:  9999]  train_loss: 0.903  |  valid_loss: 0.849\n",
      "[epoch: 11, i: 10124]  train_loss: 0.829  |  valid_loss: 0.777\n",
      "[epoch: 11, i: 10249]  train_loss: 0.782  |  valid_loss: 0.914\n",
      "[epoch: 11, i: 10374]  train_loss: 0.885  |  valid_loss: 0.795\n",
      "[epoch: 11, i: 10499]  train_loss: 0.826  |  valid_loss: 1.030\n",
      "[epoch: 11, i: 10624]  train_loss: 0.816  |  valid_loss: 0.957\n",
      "[epoch: 11, i: 10749]  train_loss: 0.886  |  valid_loss: 0.877\n",
      "[epoch: 11, i: 10874]  train_loss: 0.855  |  valid_loss: 0.973\n",
      "[epoch: 11, i: 10999]  train_loss: 0.873  |  valid_loss: 0.955\n",
      "[epoch: 11, i: 11124]  train_loss: 0.856  |  valid_loss: 0.820\n",
      "[epoch: 11, i: 11249]  train_loss: 0.797  |  valid_loss: 0.964\n",
      "[epoch: 11, i: 11374]  train_loss: 0.915  |  valid_loss: 0.847\n",
      "[epoch: 11, i: 11499]  train_loss: 0.932  |  valid_loss: 0.665\n",
      "[epoch: 11, i: 11624]  train_loss: 0.796  |  valid_loss: 0.822\n",
      "[epoch: 11, i: 11749]  train_loss: 0.873  |  valid_loss: 0.810\n",
      "[epoch: 11, i: 11874]  train_loss: 0.916  |  valid_loss: 0.822\n",
      "[epoch: 11, i: 11999]  train_loss: 0.843  |  valid_loss: 0.745\n",
      "[epoch: 11, i: 12124]  train_loss: 0.853  |  valid_loss: 0.759\n",
      "[epoch: 11, i: 12249]  train_loss: 0.898  |  valid_loss: 1.021\n",
      "[epoch: 11, i: 12374]  train_loss: 0.848  |  valid_loss: 0.903\n",
      "[epoch: 11, i: 12499]  train_loss: 0.788  |  valid_loss: 0.872\n",
      "--> [End of epoch 11] train_accuracy: 69.77%  |  valid_accuracy: 69.80%\n",
      "--> [Start of epoch 12]  lr: 0.000137\n",
      "[epoch: 12, i:   124]  train_loss: 0.876  |  valid_loss: 0.742\n",
      "[epoch: 12, i:   249]  train_loss: 0.856  |  valid_loss: 0.732\n",
      "[epoch: 12, i:   374]  train_loss: 0.894  |  valid_loss: 0.928\n",
      "[epoch: 12, i:   499]  train_loss: 0.808  |  valid_loss: 0.822\n",
      "[epoch: 12, i:   624]  train_loss: 0.803  |  valid_loss: 0.943\n",
      "[epoch: 12, i:   749]  train_loss: 0.827  |  valid_loss: 0.605\n",
      "[epoch: 12, i:   874]  train_loss: 0.840  |  valid_loss: 0.882\n",
      "[epoch: 12, i:   999]  train_loss: 0.784  |  valid_loss: 0.922\n",
      "[epoch: 12, i:  1124]  train_loss: 0.788  |  valid_loss: 0.932\n",
      "[epoch: 12, i:  1249]  train_loss: 0.870  |  valid_loss: 0.753\n",
      "[epoch: 12, i:  1374]  train_loss: 0.773  |  valid_loss: 0.794\n",
      "[epoch: 12, i:  1499]  train_loss: 0.784  |  valid_loss: 0.819\n",
      "[epoch: 12, i:  1624]  train_loss: 0.844  |  valid_loss: 0.795\n",
      "[epoch: 12, i:  1749]  train_loss: 0.914  |  valid_loss: 1.072\n",
      "[epoch: 12, i:  1874]  train_loss: 0.882  |  valid_loss: 0.775\n",
      "[epoch: 12, i:  1999]  train_loss: 0.880  |  valid_loss: 1.018\n",
      "[epoch: 12, i:  2124]  train_loss: 0.880  |  valid_loss: 0.781\n",
      "[epoch: 12, i:  2249]  train_loss: 0.823  |  valid_loss: 0.901\n",
      "[epoch: 12, i:  2374]  train_loss: 0.864  |  valid_loss: 0.767\n",
      "[epoch: 12, i:  2499]  train_loss: 0.763  |  valid_loss: 1.055\n",
      "[epoch: 12, i:  2624]  train_loss: 0.813  |  valid_loss: 0.924\n",
      "[epoch: 12, i:  2749]  train_loss: 0.904  |  valid_loss: 0.943\n",
      "[epoch: 12, i:  2874]  train_loss: 0.879  |  valid_loss: 1.113\n",
      "[epoch: 12, i:  2999]  train_loss: 0.831  |  valid_loss: 0.873\n",
      "[epoch: 12, i:  3124]  train_loss: 0.808  |  valid_loss: 0.909\n",
      "[epoch: 12, i:  3249]  train_loss: 0.918  |  valid_loss: 1.086\n",
      "[epoch: 12, i:  3374]  train_loss: 0.889  |  valid_loss: 0.852\n",
      "[epoch: 12, i:  3499]  train_loss: 0.821  |  valid_loss: 0.775\n",
      "[epoch: 12, i:  3624]  train_loss: 0.804  |  valid_loss: 0.862\n",
      "[epoch: 12, i:  3749]  train_loss: 0.834  |  valid_loss: 0.697\n",
      "[epoch: 12, i:  3874]  train_loss: 0.969  |  valid_loss: 0.822\n",
      "[epoch: 12, i:  3999]  train_loss: 0.829  |  valid_loss: 0.636\n",
      "[epoch: 12, i:  4124]  train_loss: 0.895  |  valid_loss: 0.877\n",
      "[epoch: 12, i:  4249]  train_loss: 0.769  |  valid_loss: 0.929\n",
      "[epoch: 12, i:  4374]  train_loss: 0.843  |  valid_loss: 0.912\n",
      "[epoch: 12, i:  4499]  train_loss: 0.857  |  valid_loss: 0.820\n",
      "[epoch: 12, i:  4624]  train_loss: 0.858  |  valid_loss: 0.936\n",
      "[epoch: 12, i:  4749]  train_loss: 0.795  |  valid_loss: 0.926\n",
      "[epoch: 12, i:  4874]  train_loss: 0.827  |  valid_loss: 0.758\n",
      "[epoch: 12, i:  4999]  train_loss: 0.819  |  valid_loss: 0.780\n",
      "[epoch: 12, i:  5124]  train_loss: 0.798  |  valid_loss: 0.856\n",
      "[epoch: 12, i:  5249]  train_loss: 0.829  |  valid_loss: 0.914\n",
      "[epoch: 12, i:  5374]  train_loss: 0.876  |  valid_loss: 0.646\n",
      "[epoch: 12, i:  5499]  train_loss: 0.877  |  valid_loss: 0.731\n",
      "[epoch: 12, i:  5624]  train_loss: 0.874  |  valid_loss: 1.063\n",
      "[epoch: 12, i:  5749]  train_loss: 0.820  |  valid_loss: 0.837\n",
      "[epoch: 12, i:  5874]  train_loss: 0.814  |  valid_loss: 0.763\n",
      "[epoch: 12, i:  5999]  train_loss: 0.935  |  valid_loss: 0.903\n",
      "[epoch: 12, i:  6124]  train_loss: 0.778  |  valid_loss: 0.808\n",
      "[epoch: 12, i:  6249]  train_loss: 0.831  |  valid_loss: 0.900\n",
      "[epoch: 12, i:  6374]  train_loss: 0.818  |  valid_loss: 0.802\n",
      "[epoch: 12, i:  6499]  train_loss: 0.897  |  valid_loss: 0.828\n",
      "[epoch: 12, i:  6624]  train_loss: 0.865  |  valid_loss: 0.758\n",
      "[epoch: 12, i:  6749]  train_loss: 0.797  |  valid_loss: 0.835\n",
      "[epoch: 12, i:  6874]  train_loss: 0.846  |  valid_loss: 0.804\n",
      "[epoch: 12, i:  6999]  train_loss: 0.818  |  valid_loss: 0.975\n",
      "[epoch: 12, i:  7124]  train_loss: 0.885  |  valid_loss: 1.013\n",
      "[epoch: 12, i:  7249]  train_loss: 0.826  |  valid_loss: 0.695\n",
      "[epoch: 12, i:  7374]  train_loss: 0.880  |  valid_loss: 1.050\n",
      "[epoch: 12, i:  7499]  train_loss: 0.862  |  valid_loss: 0.916\n",
      "[epoch: 12, i:  7624]  train_loss: 0.825  |  valid_loss: 0.952\n",
      "[epoch: 12, i:  7749]  train_loss: 0.865  |  valid_loss: 0.852\n",
      "[epoch: 12, i:  7874]  train_loss: 0.860  |  valid_loss: 0.892\n",
      "[epoch: 12, i:  7999]  train_loss: 0.797  |  valid_loss: 0.769\n",
      "[epoch: 12, i:  8124]  train_loss: 0.798  |  valid_loss: 0.926\n",
      "[epoch: 12, i:  8249]  train_loss: 0.758  |  valid_loss: 0.907\n",
      "[epoch: 12, i:  8374]  train_loss: 0.883  |  valid_loss: 0.855\n",
      "[epoch: 12, i:  8499]  train_loss: 0.891  |  valid_loss: 0.963\n",
      "[epoch: 12, i:  8624]  train_loss: 0.827  |  valid_loss: 0.874\n",
      "[epoch: 12, i:  8749]  train_loss: 0.774  |  valid_loss: 0.905\n",
      "[epoch: 12, i:  8874]  train_loss: 0.785  |  valid_loss: 0.890\n",
      "[epoch: 12, i:  8999]  train_loss: 0.806  |  valid_loss: 0.683\n",
      "[epoch: 12, i:  9124]  train_loss: 0.843  |  valid_loss: 0.767\n",
      "[epoch: 12, i:  9249]  train_loss: 0.872  |  valid_loss: 0.652\n",
      "[epoch: 12, i:  9374]  train_loss: 0.783  |  valid_loss: 0.923\n",
      "[epoch: 12, i:  9499]  train_loss: 0.923  |  valid_loss: 0.765\n",
      "[epoch: 12, i:  9624]  train_loss: 0.838  |  valid_loss: 0.921\n",
      "[epoch: 12, i:  9749]  train_loss: 0.912  |  valid_loss: 0.943\n",
      "[epoch: 12, i:  9874]  train_loss: 0.836  |  valid_loss: 0.947\n",
      "[epoch: 12, i:  9999]  train_loss: 0.883  |  valid_loss: 0.850\n",
      "[epoch: 12, i: 10124]  train_loss: 0.833  |  valid_loss: 0.843\n",
      "[epoch: 12, i: 10249]  train_loss: 0.846  |  valid_loss: 0.884\n",
      "[epoch: 12, i: 10374]  train_loss: 0.801  |  valid_loss: 0.765\n",
      "[epoch: 12, i: 10499]  train_loss: 0.837  |  valid_loss: 1.010\n",
      "[epoch: 12, i: 10624]  train_loss: 0.849  |  valid_loss: 1.023\n",
      "[epoch: 12, i: 10749]  train_loss: 0.882  |  valid_loss: 0.941\n",
      "[epoch: 12, i: 10874]  train_loss: 0.849  |  valid_loss: 1.011\n",
      "[epoch: 12, i: 10999]  train_loss: 0.865  |  valid_loss: 0.950\n",
      "[epoch: 12, i: 11124]  train_loss: 0.873  |  valid_loss: 0.829\n",
      "[epoch: 12, i: 11249]  train_loss: 0.799  |  valid_loss: 0.889\n",
      "[epoch: 12, i: 11374]  train_loss: 0.825  |  valid_loss: 0.783\n",
      "[epoch: 12, i: 11499]  train_loss: 0.899  |  valid_loss: 0.622\n",
      "[epoch: 12, i: 11624]  train_loss: 0.897  |  valid_loss: 0.818\n",
      "[epoch: 12, i: 11749]  train_loss: 0.919  |  valid_loss: 0.801\n",
      "[epoch: 12, i: 11874]  train_loss: 0.818  |  valid_loss: 0.772\n",
      "[epoch: 12, i: 11999]  train_loss: 0.922  |  valid_loss: 0.679\n",
      "[epoch: 12, i: 12124]  train_loss: 0.877  |  valid_loss: 0.767\n",
      "[epoch: 12, i: 12249]  train_loss: 0.915  |  valid_loss: 1.071\n",
      "[epoch: 12, i: 12374]  train_loss: 0.863  |  valid_loss: 0.910\n",
      "[epoch: 12, i: 12499]  train_loss: 0.887  |  valid_loss: 0.891\n",
      "--> [End of epoch 12] train_accuracy: 70.12%  |  valid_accuracy: 70.22%\n",
      "--> [Start of epoch 13]  lr: 0.000110\n",
      "[epoch: 13, i:   124]  train_loss: 0.846  |  valid_loss: 0.739\n",
      "[epoch: 13, i:   249]  train_loss: 0.822  |  valid_loss: 0.748\n",
      "[epoch: 13, i:   374]  train_loss: 0.792  |  valid_loss: 0.917\n",
      "[epoch: 13, i:   499]  train_loss: 0.831  |  valid_loss: 0.846\n",
      "[epoch: 13, i:   624]  train_loss: 0.866  |  valid_loss: 0.952\n",
      "[epoch: 13, i:   749]  train_loss: 0.921  |  valid_loss: 0.605\n",
      "[epoch: 13, i:   874]  train_loss: 0.825  |  valid_loss: 0.916\n",
      "[epoch: 13, i:   999]  train_loss: 0.832  |  valid_loss: 0.953\n",
      "[epoch: 13, i:  1124]  train_loss: 0.804  |  valid_loss: 0.984\n",
      "[epoch: 13, i:  1249]  train_loss: 0.873  |  valid_loss: 0.741\n",
      "[epoch: 13, i:  1374]  train_loss: 0.912  |  valid_loss: 0.776\n",
      "[epoch: 13, i:  1499]  train_loss: 0.847  |  valid_loss: 0.793\n",
      "[epoch: 13, i:  1624]  train_loss: 0.831  |  valid_loss: 0.701\n",
      "[epoch: 13, i:  1749]  train_loss: 0.893  |  valid_loss: 1.012\n",
      "[epoch: 13, i:  1874]  train_loss: 0.827  |  valid_loss: 0.814\n",
      "[epoch: 13, i:  1999]  train_loss: 0.780  |  valid_loss: 0.994\n",
      "[epoch: 13, i:  2124]  train_loss: 0.778  |  valid_loss: 0.704\n",
      "[epoch: 13, i:  2249]  train_loss: 0.818  |  valid_loss: 0.893\n",
      "[epoch: 13, i:  2374]  train_loss: 0.808  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  2499]  train_loss: 0.807  |  valid_loss: 1.024\n",
      "[epoch: 13, i:  2624]  train_loss: 0.803  |  valid_loss: 0.903\n",
      "[epoch: 13, i:  2749]  train_loss: 0.829  |  valid_loss: 0.949\n",
      "[epoch: 13, i:  2874]  train_loss: 0.834  |  valid_loss: 1.098\n",
      "[epoch: 13, i:  2999]  train_loss: 0.855  |  valid_loss: 0.892\n",
      "[epoch: 13, i:  3124]  train_loss: 0.896  |  valid_loss: 0.892\n",
      "[epoch: 13, i:  3249]  train_loss: 0.882  |  valid_loss: 1.126\n",
      "[epoch: 13, i:  3374]  train_loss: 0.840  |  valid_loss: 0.811\n",
      "[epoch: 13, i:  3499]  train_loss: 0.828  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  3624]  train_loss: 0.865  |  valid_loss: 0.829\n",
      "[epoch: 13, i:  3749]  train_loss: 0.809  |  valid_loss: 0.686\n",
      "[epoch: 13, i:  3874]  train_loss: 0.842  |  valid_loss: 0.852\n",
      "[epoch: 13, i:  3999]  train_loss: 0.759  |  valid_loss: 0.645\n",
      "[epoch: 13, i:  4124]  train_loss: 0.837  |  valid_loss: 0.900\n",
      "[epoch: 13, i:  4249]  train_loss: 0.849  |  valid_loss: 0.904\n",
      "[epoch: 13, i:  4374]  train_loss: 0.781  |  valid_loss: 0.923\n",
      "[epoch: 13, i:  4499]  train_loss: 0.864  |  valid_loss: 0.812\n",
      "[epoch: 13, i:  4624]  train_loss: 0.896  |  valid_loss: 0.988\n",
      "[epoch: 13, i:  4749]  train_loss: 0.923  |  valid_loss: 0.937\n",
      "[epoch: 13, i:  4874]  train_loss: 0.843  |  valid_loss: 0.748\n",
      "[epoch: 13, i:  4999]  train_loss: 0.755  |  valid_loss: 0.800\n",
      "[epoch: 13, i:  5124]  train_loss: 0.830  |  valid_loss: 0.858\n",
      "[epoch: 13, i:  5249]  train_loss: 0.919  |  valid_loss: 0.907\n",
      "[epoch: 13, i:  5374]  train_loss: 0.821  |  valid_loss: 0.658\n",
      "[epoch: 13, i:  5499]  train_loss: 0.722  |  valid_loss: 0.713\n",
      "[epoch: 13, i:  5624]  train_loss: 0.889  |  valid_loss: 0.991\n",
      "[epoch: 13, i:  5749]  train_loss: 0.807  |  valid_loss: 0.845\n",
      "[epoch: 13, i:  5874]  train_loss: 0.846  |  valid_loss: 0.740\n",
      "[epoch: 13, i:  5999]  train_loss: 0.874  |  valid_loss: 0.852\n",
      "[epoch: 13, i:  6124]  train_loss: 0.804  |  valid_loss: 0.772\n",
      "[epoch: 13, i:  6249]  train_loss: 0.858  |  valid_loss: 0.920\n",
      "[epoch: 13, i:  6374]  train_loss: 0.780  |  valid_loss: 0.781\n",
      "[epoch: 13, i:  6499]  train_loss: 0.889  |  valid_loss: 0.787\n",
      "[epoch: 13, i:  6624]  train_loss: 0.889  |  valid_loss: 0.655\n",
      "[epoch: 13, i:  6749]  train_loss: 0.830  |  valid_loss: 0.864\n",
      "[epoch: 13, i:  6874]  train_loss: 0.902  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  6999]  train_loss: 0.848  |  valid_loss: 0.944\n",
      "[epoch: 13, i:  7124]  train_loss: 0.843  |  valid_loss: 1.021\n",
      "[epoch: 13, i:  7249]  train_loss: 0.958  |  valid_loss: 0.677\n",
      "[epoch: 13, i:  7374]  train_loss: 0.846  |  valid_loss: 1.080\n",
      "[epoch: 13, i:  7499]  train_loss: 0.851  |  valid_loss: 0.923\n",
      "[epoch: 13, i:  7624]  train_loss: 0.858  |  valid_loss: 0.943\n",
      "[epoch: 13, i:  7749]  train_loss: 0.888  |  valid_loss: 0.834\n",
      "[epoch: 13, i:  7874]  train_loss: 0.828  |  valid_loss: 0.899\n",
      "[epoch: 13, i:  7999]  train_loss: 0.873  |  valid_loss: 0.750\n",
      "[epoch: 13, i:  8124]  train_loss: 0.862  |  valid_loss: 0.927\n",
      "[epoch: 13, i:  8249]  train_loss: 0.843  |  valid_loss: 0.922\n",
      "[epoch: 13, i:  8374]  train_loss: 0.735  |  valid_loss: 0.861\n",
      "[epoch: 13, i:  8499]  train_loss: 0.899  |  valid_loss: 0.921\n",
      "[epoch: 13, i:  8624]  train_loss: 0.891  |  valid_loss: 0.887\n",
      "[epoch: 13, i:  8749]  train_loss: 0.867  |  valid_loss: 0.932\n",
      "[epoch: 13, i:  8874]  train_loss: 0.855  |  valid_loss: 0.913\n",
      "[epoch: 13, i:  8999]  train_loss: 0.843  |  valid_loss: 0.722\n",
      "[epoch: 13, i:  9124]  train_loss: 0.861  |  valid_loss: 0.766\n",
      "[epoch: 13, i:  9249]  train_loss: 0.851  |  valid_loss: 0.659\n",
      "[epoch: 13, i:  9374]  train_loss: 0.800  |  valid_loss: 0.978\n",
      "[epoch: 13, i:  9499]  train_loss: 0.856  |  valid_loss: 0.766\n",
      "[epoch: 13, i:  9624]  train_loss: 0.834  |  valid_loss: 0.891\n",
      "[epoch: 13, i:  9749]  train_loss: 0.795  |  valid_loss: 0.919\n",
      "[epoch: 13, i:  9874]  train_loss: 0.871  |  valid_loss: 0.909\n",
      "[epoch: 13, i:  9999]  train_loss: 0.824  |  valid_loss: 0.839\n",
      "[epoch: 13, i: 10124]  train_loss: 0.893  |  valid_loss: 0.789\n",
      "[epoch: 13, i: 10249]  train_loss: 0.807  |  valid_loss: 0.923\n",
      "[epoch: 13, i: 10374]  train_loss: 0.815  |  valid_loss: 0.750\n",
      "[epoch: 13, i: 10499]  train_loss: 0.876  |  valid_loss: 1.038\n",
      "[epoch: 13, i: 10624]  train_loss: 0.888  |  valid_loss: 0.979\n",
      "[epoch: 13, i: 10749]  train_loss: 0.867  |  valid_loss: 0.835\n",
      "[epoch: 13, i: 10874]  train_loss: 0.854  |  valid_loss: 0.993\n",
      "[epoch: 13, i: 10999]  train_loss: 0.810  |  valid_loss: 0.965\n",
      "[epoch: 13, i: 11124]  train_loss: 0.751  |  valid_loss: 0.857\n",
      "[epoch: 13, i: 11249]  train_loss: 0.809  |  valid_loss: 0.889\n",
      "[epoch: 13, i: 11374]  train_loss: 0.826  |  valid_loss: 0.790\n",
      "[epoch: 13, i: 11499]  train_loss: 0.835  |  valid_loss: 0.663\n",
      "[epoch: 13, i: 11624]  train_loss: 0.803  |  valid_loss: 0.825\n",
      "[epoch: 13, i: 11749]  train_loss: 0.815  |  valid_loss: 0.771\n",
      "[epoch: 13, i: 11874]  train_loss: 0.889  |  valid_loss: 0.801\n",
      "[epoch: 13, i: 11999]  train_loss: 0.842  |  valid_loss: 0.728\n",
      "[epoch: 13, i: 12124]  train_loss: 0.848  |  valid_loss: 0.762\n",
      "[epoch: 13, i: 12249]  train_loss: 0.860  |  valid_loss: 1.030\n",
      "[epoch: 13, i: 12374]  train_loss: 0.836  |  valid_loss: 0.946\n",
      "[epoch: 13, i: 12499]  train_loss: 0.913  |  valid_loss: 0.871\n",
      "--> [End of epoch 13] train_accuracy: 70.15%  |  valid_accuracy: 70.33%\n",
      "--> [Start of epoch 14]  lr: 0.000088\n",
      "[epoch: 14, i:   124]  train_loss: 0.795  |  valid_loss: 0.765\n",
      "[epoch: 14, i:   249]  train_loss: 0.864  |  valid_loss: 0.763\n",
      "[epoch: 14, i:   374]  train_loss: 0.778  |  valid_loss: 0.887\n",
      "[epoch: 14, i:   499]  train_loss: 0.762  |  valid_loss: 0.825\n",
      "[epoch: 14, i:   624]  train_loss: 0.838  |  valid_loss: 0.949\n",
      "[epoch: 14, i:   749]  train_loss: 0.787  |  valid_loss: 0.602\n",
      "[epoch: 14, i:   874]  train_loss: 0.839  |  valid_loss: 0.859\n",
      "[epoch: 14, i:   999]  train_loss: 0.820  |  valid_loss: 0.981\n",
      "[epoch: 14, i:  1124]  train_loss: 0.837  |  valid_loss: 0.997\n",
      "[epoch: 14, i:  1249]  train_loss: 0.839  |  valid_loss: 0.767\n",
      "[epoch: 14, i:  1374]  train_loss: 0.814  |  valid_loss: 0.772\n",
      "[epoch: 14, i:  1499]  train_loss: 0.785  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  1624]  train_loss: 0.905  |  valid_loss: 0.720\n",
      "[epoch: 14, i:  1749]  train_loss: 0.947  |  valid_loss: 0.965\n",
      "[epoch: 14, i:  1874]  train_loss: 0.765  |  valid_loss: 0.787\n",
      "[epoch: 14, i:  1999]  train_loss: 0.853  |  valid_loss: 1.105\n",
      "[epoch: 14, i:  2124]  train_loss: 0.843  |  valid_loss: 0.752\n",
      "[epoch: 14, i:  2249]  train_loss: 0.880  |  valid_loss: 0.866\n",
      "[epoch: 14, i:  2374]  train_loss: 0.816  |  valid_loss: 0.802\n",
      "[epoch: 14, i:  2499]  train_loss: 0.829  |  valid_loss: 1.013\n",
      "[epoch: 14, i:  2624]  train_loss: 0.837  |  valid_loss: 0.961\n",
      "[epoch: 14, i:  2749]  train_loss: 0.775  |  valid_loss: 1.031\n",
      "[epoch: 14, i:  2874]  train_loss: 0.822  |  valid_loss: 1.063\n",
      "[epoch: 14, i:  2999]  train_loss: 0.803  |  valid_loss: 0.901\n",
      "[epoch: 14, i:  3124]  train_loss: 0.831  |  valid_loss: 0.935\n",
      "[epoch: 14, i:  3249]  train_loss: 0.865  |  valid_loss: 1.069\n",
      "[epoch: 14, i:  3374]  train_loss: 0.928  |  valid_loss: 0.868\n",
      "[epoch: 14, i:  3499]  train_loss: 0.813  |  valid_loss: 0.785\n",
      "[epoch: 14, i:  3624]  train_loss: 0.854  |  valid_loss: 0.803\n",
      "[epoch: 14, i:  3749]  train_loss: 0.854  |  valid_loss: 0.719\n",
      "[epoch: 14, i:  3874]  train_loss: 0.826  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  3999]  train_loss: 0.818  |  valid_loss: 0.661\n",
      "[epoch: 14, i:  4124]  train_loss: 0.784  |  valid_loss: 0.858\n",
      "[epoch: 14, i:  4249]  train_loss: 0.843  |  valid_loss: 0.908\n",
      "[epoch: 14, i:  4374]  train_loss: 0.938  |  valid_loss: 0.924\n",
      "[epoch: 14, i:  4499]  train_loss: 0.889  |  valid_loss: 0.791\n",
      "[epoch: 14, i:  4624]  train_loss: 0.853  |  valid_loss: 0.983\n",
      "[epoch: 14, i:  4749]  train_loss: 0.825  |  valid_loss: 0.887\n",
      "[epoch: 14, i:  4874]  train_loss: 0.801  |  valid_loss: 0.768\n",
      "[epoch: 14, i:  4999]  train_loss: 0.780  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  5124]  train_loss: 0.901  |  valid_loss: 0.858\n",
      "[epoch: 14, i:  5249]  train_loss: 0.818  |  valid_loss: 0.904\n",
      "[epoch: 14, i:  5374]  train_loss: 0.825  |  valid_loss: 0.646\n",
      "[epoch: 14, i:  5499]  train_loss: 0.802  |  valid_loss: 0.714\n",
      "[epoch: 14, i:  5624]  train_loss: 0.842  |  valid_loss: 0.967\n",
      "[epoch: 14, i:  5749]  train_loss: 0.807  |  valid_loss: 0.831\n",
      "[epoch: 14, i:  5874]  train_loss: 0.901  |  valid_loss: 0.716\n",
      "[epoch: 14, i:  5999]  train_loss: 0.801  |  valid_loss: 0.875\n",
      "[epoch: 14, i:  6124]  train_loss: 0.837  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  6249]  train_loss: 0.879  |  valid_loss: 0.847\n",
      "[epoch: 14, i:  6374]  train_loss: 0.881  |  valid_loss: 0.764\n",
      "[epoch: 14, i:  6499]  train_loss: 0.940  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  6624]  train_loss: 0.831  |  valid_loss: 0.649\n",
      "[epoch: 14, i:  6749]  train_loss: 0.814  |  valid_loss: 0.844\n",
      "[epoch: 14, i:  6874]  train_loss: 0.924  |  valid_loss: 0.808\n",
      "[epoch: 14, i:  6999]  train_loss: 0.881  |  valid_loss: 0.987\n",
      "[epoch: 14, i:  7124]  train_loss: 0.823  |  valid_loss: 1.003\n",
      "[epoch: 14, i:  7249]  train_loss: 0.865  |  valid_loss: 0.665\n",
      "[epoch: 14, i:  7374]  train_loss: 0.789  |  valid_loss: 1.049\n",
      "[epoch: 14, i:  7499]  train_loss: 0.827  |  valid_loss: 0.961\n",
      "[epoch: 14, i:  7624]  train_loss: 0.880  |  valid_loss: 0.919\n",
      "[epoch: 14, i:  7749]  train_loss: 0.836  |  valid_loss: 0.801\n",
      "[epoch: 14, i:  7874]  train_loss: 0.838  |  valid_loss: 0.886\n",
      "[epoch: 14, i:  7999]  train_loss: 0.761  |  valid_loss: 0.734\n",
      "[epoch: 14, i:  8124]  train_loss: 0.811  |  valid_loss: 0.927\n",
      "[epoch: 14, i:  8249]  train_loss: 0.914  |  valid_loss: 0.898\n",
      "[epoch: 14, i:  8374]  train_loss: 0.755  |  valid_loss: 0.855\n",
      "[epoch: 14, i:  8499]  train_loss: 0.814  |  valid_loss: 0.881\n",
      "[epoch: 14, i:  8624]  train_loss: 0.882  |  valid_loss: 0.865\n",
      "[epoch: 14, i:  8749]  train_loss: 0.811  |  valid_loss: 0.955\n",
      "[epoch: 14, i:  8874]  train_loss: 0.812  |  valid_loss: 0.853\n",
      "[epoch: 14, i:  8999]  train_loss: 0.823  |  valid_loss: 0.696\n",
      "[epoch: 14, i:  9124]  train_loss: 0.812  |  valid_loss: 0.743\n",
      "[epoch: 14, i:  9249]  train_loss: 0.848  |  valid_loss: 0.641\n",
      "[epoch: 14, i:  9374]  train_loss: 0.830  |  valid_loss: 0.932\n",
      "[epoch: 14, i:  9499]  train_loss: 0.825  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  9624]  train_loss: 0.838  |  valid_loss: 0.866\n",
      "[epoch: 14, i:  9749]  train_loss: 0.801  |  valid_loss: 0.930\n",
      "[epoch: 14, i:  9874]  train_loss: 0.817  |  valid_loss: 0.875\n",
      "[epoch: 14, i:  9999]  train_loss: 0.836  |  valid_loss: 0.822\n",
      "[epoch: 14, i: 10124]  train_loss: 0.840  |  valid_loss: 0.783\n",
      "[epoch: 14, i: 10249]  train_loss: 0.846  |  valid_loss: 0.829\n",
      "[epoch: 14, i: 10374]  train_loss: 0.953  |  valid_loss: 0.757\n",
      "[epoch: 14, i: 10499]  train_loss: 0.833  |  valid_loss: 0.954\n",
      "[epoch: 14, i: 10624]  train_loss: 0.860  |  valid_loss: 0.992\n",
      "[epoch: 14, i: 10749]  train_loss: 0.815  |  valid_loss: 0.852\n",
      "[epoch: 14, i: 10874]  train_loss: 0.787  |  valid_loss: 0.969\n",
      "[epoch: 14, i: 10999]  train_loss: 0.744  |  valid_loss: 0.980\n",
      "[epoch: 14, i: 11124]  train_loss: 0.873  |  valid_loss: 0.851\n",
      "[epoch: 14, i: 11249]  train_loss: 0.806  |  valid_loss: 0.866\n",
      "[epoch: 14, i: 11374]  train_loss: 0.858  |  valid_loss: 0.791\n",
      "[epoch: 14, i: 11499]  train_loss: 0.808  |  valid_loss: 0.620\n",
      "[epoch: 14, i: 11624]  train_loss: 0.823  |  valid_loss: 0.807\n",
      "[epoch: 14, i: 11749]  train_loss: 0.878  |  valid_loss: 0.759\n",
      "[epoch: 14, i: 11874]  train_loss: 0.851  |  valid_loss: 0.811\n",
      "[epoch: 14, i: 11999]  train_loss: 0.859  |  valid_loss: 0.703\n",
      "[epoch: 14, i: 12124]  train_loss: 0.849  |  valid_loss: 0.789\n",
      "[epoch: 14, i: 12249]  train_loss: 0.913  |  valid_loss: 1.080\n",
      "[epoch: 14, i: 12374]  train_loss: 0.764  |  valid_loss: 0.899\n",
      "[epoch: 14, i: 12499]  train_loss: 0.829  |  valid_loss: 0.879\n",
      "--> [End of epoch 14] train_accuracy: 70.60%  |  valid_accuracy: 70.54%\n",
      "--> [Start of epoch 15]  lr: 0.000070\n",
      "[epoch: 15, i:   124]  train_loss: 0.894  |  valid_loss: 0.743\n",
      "[epoch: 15, i:   249]  train_loss: 0.837  |  valid_loss: 0.713\n",
      "[epoch: 15, i:   374]  train_loss: 0.794  |  valid_loss: 0.882\n",
      "[epoch: 15, i:   499]  train_loss: 0.868  |  valid_loss: 0.805\n",
      "[epoch: 15, i:   624]  train_loss: 0.860  |  valid_loss: 0.931\n",
      "[epoch: 15, i:   749]  train_loss: 0.792  |  valid_loss: 0.601\n",
      "[epoch: 15, i:   874]  train_loss: 0.767  |  valid_loss: 0.903\n",
      "[epoch: 15, i:   999]  train_loss: 0.782  |  valid_loss: 0.936\n",
      "[epoch: 15, i:  1124]  train_loss: 0.782  |  valid_loss: 0.965\n",
      "[epoch: 15, i:  1249]  train_loss: 0.875  |  valid_loss: 0.730\n",
      "[epoch: 15, i:  1374]  train_loss: 0.844  |  valid_loss: 0.766\n",
      "[epoch: 15, i:  1499]  train_loss: 0.808  |  valid_loss: 0.817\n",
      "[epoch: 15, i:  1624]  train_loss: 0.835  |  valid_loss: 0.686\n",
      "[epoch: 15, i:  1749]  train_loss: 0.774  |  valid_loss: 0.972\n",
      "[epoch: 15, i:  1874]  train_loss: 0.804  |  valid_loss: 0.784\n",
      "[epoch: 15, i:  1999]  train_loss: 0.864  |  valid_loss: 1.029\n",
      "[epoch: 15, i:  2124]  train_loss: 0.836  |  valid_loss: 0.740\n",
      "[epoch: 15, i:  2249]  train_loss: 0.833  |  valid_loss: 0.836\n",
      "[epoch: 15, i:  2374]  train_loss: 0.845  |  valid_loss: 0.788\n",
      "[epoch: 15, i:  2499]  train_loss: 0.841  |  valid_loss: 1.023\n",
      "[epoch: 15, i:  2624]  train_loss: 0.848  |  valid_loss: 0.926\n",
      "[epoch: 15, i:  2749]  train_loss: 0.906  |  valid_loss: 1.045\n",
      "[epoch: 15, i:  2874]  train_loss: 0.801  |  valid_loss: 1.048\n",
      "[epoch: 15, i:  2999]  train_loss: 0.842  |  valid_loss: 0.863\n",
      "[epoch: 15, i:  3124]  train_loss: 0.827  |  valid_loss: 0.918\n",
      "[epoch: 15, i:  3249]  train_loss: 0.864  |  valid_loss: 1.065\n",
      "[epoch: 15, i:  3374]  train_loss: 0.783  |  valid_loss: 0.788\n",
      "[epoch: 15, i:  3499]  train_loss: 0.863  |  valid_loss: 0.778\n",
      "[epoch: 15, i:  3624]  train_loss: 0.851  |  valid_loss: 0.805\n",
      "[epoch: 15, i:  3749]  train_loss: 0.839  |  valid_loss: 0.740\n",
      "[epoch: 15, i:  3874]  train_loss: 0.818  |  valid_loss: 0.807\n",
      "[epoch: 15, i:  3999]  train_loss: 0.831  |  valid_loss: 0.663\n",
      "[epoch: 15, i:  4124]  train_loss: 0.831  |  valid_loss: 0.852\n",
      "[epoch: 15, i:  4249]  train_loss: 0.841  |  valid_loss: 0.911\n",
      "[epoch: 15, i:  4374]  train_loss: 0.886  |  valid_loss: 0.914\n",
      "[epoch: 15, i:  4499]  train_loss: 0.819  |  valid_loss: 0.785\n",
      "[epoch: 15, i:  4624]  train_loss: 0.851  |  valid_loss: 0.957\n",
      "[epoch: 15, i:  4749]  train_loss: 0.765  |  valid_loss: 0.904\n",
      "[epoch: 15, i:  4874]  train_loss: 0.786  |  valid_loss: 0.815\n",
      "[epoch: 15, i:  4999]  train_loss: 0.836  |  valid_loss: 0.769\n",
      "[epoch: 15, i:  5124]  train_loss: 0.760  |  valid_loss: 0.833\n",
      "[epoch: 15, i:  5249]  train_loss: 0.846  |  valid_loss: 0.901\n",
      "[epoch: 15, i:  5374]  train_loss: 0.803  |  valid_loss: 0.646\n",
      "[epoch: 15, i:  5499]  train_loss: 0.767  |  valid_loss: 0.720\n",
      "[epoch: 15, i:  5624]  train_loss: 0.859  |  valid_loss: 0.917\n",
      "[epoch: 15, i:  5749]  train_loss: 0.871  |  valid_loss: 0.821\n",
      "[epoch: 15, i:  5874]  train_loss: 0.791  |  valid_loss: 0.738\n",
      "[epoch: 15, i:  5999]  train_loss: 0.809  |  valid_loss: 0.843\n",
      "[epoch: 15, i:  6124]  train_loss: 0.873  |  valid_loss: 0.780\n",
      "[epoch: 15, i:  6249]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 15, i:  6374]  train_loss: 0.798  |  valid_loss: 0.754\n",
      "[epoch: 15, i:  6499]  train_loss: 0.849  |  valid_loss: 0.810\n",
      "[epoch: 15, i:  6624]  train_loss: 0.872  |  valid_loss: 0.708\n",
      "[epoch: 15, i:  6749]  train_loss: 0.788  |  valid_loss: 0.842\n",
      "[epoch: 15, i:  6874]  train_loss: 0.851  |  valid_loss: 0.782\n",
      "[epoch: 15, i:  6999]  train_loss: 0.750  |  valid_loss: 0.951\n",
      "[epoch: 15, i:  7124]  train_loss: 0.872  |  valid_loss: 0.958\n",
      "[epoch: 15, i:  7249]  train_loss: 0.836  |  valid_loss: 0.654\n",
      "[epoch: 15, i:  7374]  train_loss: 0.781  |  valid_loss: 1.072\n",
      "[epoch: 15, i:  7499]  train_loss: 0.790  |  valid_loss: 0.937\n",
      "[epoch: 15, i:  7624]  train_loss: 0.777  |  valid_loss: 0.908\n",
      "[epoch: 15, i:  7749]  train_loss: 0.801  |  valid_loss: 0.811\n",
      "[epoch: 15, i:  7874]  train_loss: 0.772  |  valid_loss: 0.866\n",
      "[epoch: 15, i:  7999]  train_loss: 0.786  |  valid_loss: 0.730\n",
      "[epoch: 15, i:  8124]  train_loss: 0.844  |  valid_loss: 0.941\n",
      "[epoch: 15, i:  8249]  train_loss: 0.851  |  valid_loss: 0.891\n",
      "[epoch: 15, i:  8374]  train_loss: 0.833  |  valid_loss: 0.864\n",
      "[epoch: 15, i:  8499]  train_loss: 0.860  |  valid_loss: 0.920\n",
      "[epoch: 15, i:  8624]  train_loss: 0.828  |  valid_loss: 0.851\n",
      "[epoch: 15, i:  8749]  train_loss: 0.905  |  valid_loss: 0.871\n",
      "[epoch: 15, i:  8874]  train_loss: 0.876  |  valid_loss: 0.869\n",
      "[epoch: 15, i:  8999]  train_loss: 0.848  |  valid_loss: 0.721\n",
      "[epoch: 15, i:  9124]  train_loss: 0.820  |  valid_loss: 0.776\n",
      "[epoch: 15, i:  9249]  train_loss: 0.823  |  valid_loss: 0.626\n",
      "[epoch: 15, i:  9374]  train_loss: 0.751  |  valid_loss: 0.900\n",
      "[epoch: 15, i:  9499]  train_loss: 0.848  |  valid_loss: 0.799\n",
      "[epoch: 15, i:  9624]  train_loss: 0.799  |  valid_loss: 0.921\n",
      "[epoch: 15, i:  9749]  train_loss: 0.863  |  valid_loss: 0.945\n",
      "[epoch: 15, i:  9874]  train_loss: 0.803  |  valid_loss: 0.883\n",
      "[epoch: 15, i:  9999]  train_loss: 0.854  |  valid_loss: 0.806\n",
      "[epoch: 15, i: 10124]  train_loss: 0.906  |  valid_loss: 0.773\n",
      "[epoch: 15, i: 10249]  train_loss: 0.805  |  valid_loss: 0.876\n",
      "[epoch: 15, i: 10374]  train_loss: 0.866  |  valid_loss: 0.749\n",
      "[epoch: 15, i: 10499]  train_loss: 0.820  |  valid_loss: 0.992\n",
      "[epoch: 15, i: 10624]  train_loss: 0.840  |  valid_loss: 0.947\n",
      "[epoch: 15, i: 10749]  train_loss: 0.870  |  valid_loss: 0.850\n",
      "[epoch: 15, i: 10874]  train_loss: 0.858  |  valid_loss: 1.000\n",
      "[epoch: 15, i: 10999]  train_loss: 0.881  |  valid_loss: 0.960\n",
      "[epoch: 15, i: 11124]  train_loss: 0.805  |  valid_loss: 0.816\n",
      "[epoch: 15, i: 11249]  train_loss: 0.879  |  valid_loss: 0.880\n",
      "[epoch: 15, i: 11374]  train_loss: 0.808  |  valid_loss: 0.814\n",
      "[epoch: 15, i: 11499]  train_loss: 0.862  |  valid_loss: 0.613\n",
      "[epoch: 15, i: 11624]  train_loss: 0.823  |  valid_loss: 0.786\n",
      "[epoch: 15, i: 11749]  train_loss: 0.798  |  valid_loss: 0.766\n",
      "[epoch: 15, i: 11874]  train_loss: 0.814  |  valid_loss: 0.812\n",
      "[epoch: 15, i: 11999]  train_loss: 0.768  |  valid_loss: 0.724\n",
      "[epoch: 15, i: 12124]  train_loss: 0.789  |  valid_loss: 0.746\n",
      "[epoch: 15, i: 12249]  train_loss: 0.849  |  valid_loss: 1.043\n",
      "[epoch: 15, i: 12374]  train_loss: 0.888  |  valid_loss: 0.905\n",
      "[epoch: 15, i: 12499]  train_loss: 0.805  |  valid_loss: 0.875\n",
      "--> [End of epoch 15] train_accuracy: 70.76%  |  valid_accuracy: 70.46%\n",
      "--> [Start of epoch 16]  lr: 0.000056\n",
      "[epoch: 16, i:   124]  train_loss: 0.995  |  valid_loss: 0.772\n",
      "[epoch: 16, i:   249]  train_loss: 0.818  |  valid_loss: 0.740\n",
      "[epoch: 16, i:   374]  train_loss: 0.853  |  valid_loss: 0.844\n",
      "[epoch: 16, i:   499]  train_loss: 0.781  |  valid_loss: 0.785\n",
      "[epoch: 16, i:   624]  train_loss: 0.819  |  valid_loss: 0.931\n",
      "[epoch: 16, i:   749]  train_loss: 0.802  |  valid_loss: 0.606\n",
      "[epoch: 16, i:   874]  train_loss: 0.867  |  valid_loss: 0.882\n",
      "[epoch: 16, i:   999]  train_loss: 0.819  |  valid_loss: 0.956\n",
      "[epoch: 16, i:  1124]  train_loss: 0.896  |  valid_loss: 0.932\n",
      "[epoch: 16, i:  1249]  train_loss: 0.851  |  valid_loss: 0.728\n",
      "[epoch: 16, i:  1374]  train_loss: 0.803  |  valid_loss: 0.793\n",
      "[epoch: 16, i:  1499]  train_loss: 0.801  |  valid_loss: 0.843\n",
      "[epoch: 16, i:  1624]  train_loss: 0.860  |  valid_loss: 0.691\n",
      "[epoch: 16, i:  1749]  train_loss: 0.827  |  valid_loss: 0.944\n",
      "[epoch: 16, i:  1874]  train_loss: 0.835  |  valid_loss: 0.783\n",
      "[epoch: 16, i:  1999]  train_loss: 0.880  |  valid_loss: 1.011\n",
      "[epoch: 16, i:  2124]  train_loss: 0.824  |  valid_loss: 0.717\n",
      "[epoch: 16, i:  2249]  train_loss: 0.776  |  valid_loss: 0.863\n",
      "[epoch: 16, i:  2374]  train_loss: 0.832  |  valid_loss: 0.789\n",
      "[epoch: 16, i:  2499]  train_loss: 0.789  |  valid_loss: 1.010\n",
      "[epoch: 16, i:  2624]  train_loss: 0.816  |  valid_loss: 0.960\n",
      "[epoch: 16, i:  2749]  train_loss: 0.833  |  valid_loss: 0.938\n",
      "[epoch: 16, i:  2874]  train_loss: 0.827  |  valid_loss: 1.052\n",
      "[epoch: 16, i:  2999]  train_loss: 0.805  |  valid_loss: 0.845\n",
      "[epoch: 16, i:  3124]  train_loss: 0.867  |  valid_loss: 0.905\n",
      "[epoch: 16, i:  3249]  train_loss: 0.921  |  valid_loss: 1.120\n",
      "[epoch: 16, i:  3374]  train_loss: 0.876  |  valid_loss: 0.790\n",
      "[epoch: 16, i:  3499]  train_loss: 0.816  |  valid_loss: 0.753\n",
      "[epoch: 16, i:  3624]  train_loss: 0.787  |  valid_loss: 0.823\n",
      "[epoch: 16, i:  3749]  train_loss: 0.820  |  valid_loss: 0.692\n",
      "[epoch: 16, i:  3874]  train_loss: 0.893  |  valid_loss: 0.800\n",
      "[epoch: 16, i:  3999]  train_loss: 0.790  |  valid_loss: 0.645\n",
      "[epoch: 16, i:  4124]  train_loss: 0.751  |  valid_loss: 0.847\n",
      "[epoch: 16, i:  4249]  train_loss: 0.837  |  valid_loss: 0.890\n",
      "[epoch: 16, i:  4374]  train_loss: 0.779  |  valid_loss: 0.905\n",
      "[epoch: 16, i:  4499]  train_loss: 0.805  |  valid_loss: 0.770\n",
      "[epoch: 16, i:  4624]  train_loss: 0.779  |  valid_loss: 0.920\n",
      "[epoch: 16, i:  4749]  train_loss: 0.751  |  valid_loss: 0.893\n",
      "[epoch: 16, i:  4874]  train_loss: 0.852  |  valid_loss: 0.744\n",
      "[epoch: 16, i:  4999]  train_loss: 0.791  |  valid_loss: 0.743\n",
      "[epoch: 16, i:  5124]  train_loss: 0.728  |  valid_loss: 0.812\n",
      "[epoch: 16, i:  5249]  train_loss: 0.789  |  valid_loss: 0.899\n",
      "[epoch: 16, i:  5374]  train_loss: 0.823  |  valid_loss: 0.627\n",
      "[epoch: 16, i:  5499]  train_loss: 0.858  |  valid_loss: 0.732\n",
      "[epoch: 16, i:  5624]  train_loss: 0.814  |  valid_loss: 0.907\n",
      "[epoch: 16, i:  5749]  train_loss: 0.775  |  valid_loss: 0.831\n",
      "[epoch: 16, i:  5874]  train_loss: 0.845  |  valid_loss: 0.736\n",
      "[epoch: 16, i:  5999]  train_loss: 0.902  |  valid_loss: 0.896\n",
      "[epoch: 16, i:  6124]  train_loss: 0.762  |  valid_loss: 0.772\n",
      "[epoch: 16, i:  6249]  train_loss: 0.778  |  valid_loss: 0.894\n",
      "[epoch: 16, i:  6374]  train_loss: 0.821  |  valid_loss: 0.723\n",
      "[epoch: 16, i:  6499]  train_loss: 0.875  |  valid_loss: 0.768\n",
      "[epoch: 16, i:  6624]  train_loss: 0.867  |  valid_loss: 0.715\n",
      "[epoch: 16, i:  6749]  train_loss: 0.776  |  valid_loss: 0.857\n",
      "[epoch: 16, i:  6874]  train_loss: 0.905  |  valid_loss: 0.779\n",
      "[epoch: 16, i:  6999]  train_loss: 0.823  |  valid_loss: 0.976\n",
      "[epoch: 16, i:  7124]  train_loss: 0.762  |  valid_loss: 0.982\n",
      "[epoch: 16, i:  7249]  train_loss: 0.860  |  valid_loss: 0.648\n",
      "[epoch: 16, i:  7374]  train_loss: 0.799  |  valid_loss: 1.017\n",
      "[epoch: 16, i:  7499]  train_loss: 0.764  |  valid_loss: 0.962\n",
      "[epoch: 16, i:  7624]  train_loss: 0.838  |  valid_loss: 0.945\n",
      "[epoch: 16, i:  7749]  train_loss: 0.899  |  valid_loss: 0.799\n",
      "[epoch: 16, i:  7874]  train_loss: 0.836  |  valid_loss: 0.886\n",
      "[epoch: 16, i:  7999]  train_loss: 0.841  |  valid_loss: 0.710\n",
      "[epoch: 16, i:  8124]  train_loss: 0.809  |  valid_loss: 0.946\n",
      "[epoch: 16, i:  8249]  train_loss: 0.841  |  valid_loss: 0.903\n",
      "[epoch: 16, i:  8374]  train_loss: 0.850  |  valid_loss: 0.832\n",
      "[epoch: 16, i:  8499]  train_loss: 0.824  |  valid_loss: 0.910\n",
      "[epoch: 16, i:  8624]  train_loss: 0.827  |  valid_loss: 0.856\n",
      "[epoch: 16, i:  8749]  train_loss: 0.814  |  valid_loss: 0.921\n",
      "[epoch: 16, i:  8874]  train_loss: 0.777  |  valid_loss: 0.872\n",
      "[epoch: 16, i:  8999]  train_loss: 0.816  |  valid_loss: 0.688\n",
      "[epoch: 16, i:  9124]  train_loss: 0.830  |  valid_loss: 0.725\n",
      "[epoch: 16, i:  9249]  train_loss: 0.784  |  valid_loss: 0.624\n",
      "[epoch: 16, i:  9374]  train_loss: 0.828  |  valid_loss: 0.892\n",
      "[epoch: 16, i:  9499]  train_loss: 0.870  |  valid_loss: 0.752\n",
      "[epoch: 16, i:  9624]  train_loss: 0.835  |  valid_loss: 0.894\n",
      "[epoch: 16, i:  9749]  train_loss: 0.761  |  valid_loss: 0.925\n",
      "[epoch: 16, i:  9874]  train_loss: 0.869  |  valid_loss: 0.900\n",
      "[epoch: 16, i:  9999]  train_loss: 0.820  |  valid_loss: 0.840\n",
      "[epoch: 16, i: 10124]  train_loss: 0.785  |  valid_loss: 0.744\n",
      "[epoch: 16, i: 10249]  train_loss: 0.792  |  valid_loss: 0.877\n",
      "[epoch: 16, i: 10374]  train_loss: 0.853  |  valid_loss: 0.734\n",
      "[epoch: 16, i: 10499]  train_loss: 0.840  |  valid_loss: 0.935\n",
      "[epoch: 16, i: 10624]  train_loss: 0.853  |  valid_loss: 0.976\n",
      "[epoch: 16, i: 10749]  train_loss: 0.864  |  valid_loss: 0.847\n",
      "[epoch: 16, i: 10874]  train_loss: 0.895  |  valid_loss: 0.987\n",
      "[epoch: 16, i: 10999]  train_loss: 0.830  |  valid_loss: 0.985\n",
      "[epoch: 16, i: 11124]  train_loss: 0.825  |  valid_loss: 0.861\n",
      "[epoch: 16, i: 11249]  train_loss: 0.837  |  valid_loss: 0.895\n",
      "[epoch: 16, i: 11374]  train_loss: 0.855  |  valid_loss: 0.814\n",
      "[epoch: 16, i: 11499]  train_loss: 0.800  |  valid_loss: 0.639\n",
      "[epoch: 16, i: 11624]  train_loss: 0.805  |  valid_loss: 0.792\n",
      "[epoch: 16, i: 11749]  train_loss: 0.849  |  valid_loss: 0.767\n",
      "[epoch: 16, i: 11874]  train_loss: 0.783  |  valid_loss: 0.814\n",
      "[epoch: 16, i: 11999]  train_loss: 0.857  |  valid_loss: 0.719\n",
      "[epoch: 16, i: 12124]  train_loss: 0.795  |  valid_loss: 0.725\n",
      "[epoch: 16, i: 12249]  train_loss: 0.801  |  valid_loss: 1.039\n",
      "[epoch: 16, i: 12374]  train_loss: 0.865  |  valid_loss: 0.891\n",
      "[epoch: 16, i: 12499]  train_loss: 0.830  |  valid_loss: 0.881\n",
      "--> [End of epoch 16] train_accuracy: 70.86%  |  valid_accuracy: 70.93%\n",
      "--> [Start of epoch 17]  lr: 0.000045\n",
      "[epoch: 17, i:   124]  train_loss: 0.867  |  valid_loss: 0.769\n",
      "[epoch: 17, i:   249]  train_loss: 0.743  |  valid_loss: 0.725\n",
      "[epoch: 17, i:   374]  train_loss: 0.790  |  valid_loss: 0.891\n",
      "[epoch: 17, i:   499]  train_loss: 0.751  |  valid_loss: 0.801\n",
      "[epoch: 17, i:   624]  train_loss: 0.850  |  valid_loss: 0.927\n",
      "[epoch: 17, i:   749]  train_loss: 0.803  |  valid_loss: 0.588\n",
      "[epoch: 17, i:   874]  train_loss: 0.829  |  valid_loss: 0.879\n",
      "[epoch: 17, i:   999]  train_loss: 0.873  |  valid_loss: 0.933\n",
      "[epoch: 17, i:  1124]  train_loss: 0.800  |  valid_loss: 0.909\n",
      "[epoch: 17, i:  1249]  train_loss: 0.822  |  valid_loss: 0.751\n",
      "[epoch: 17, i:  1374]  train_loss: 0.824  |  valid_loss: 0.787\n",
      "[epoch: 17, i:  1499]  train_loss: 0.860  |  valid_loss: 0.819\n",
      "[epoch: 17, i:  1624]  train_loss: 0.765  |  valid_loss: 0.680\n",
      "[epoch: 17, i:  1749]  train_loss: 0.856  |  valid_loss: 0.995\n",
      "[epoch: 17, i:  1874]  train_loss: 0.803  |  valid_loss: 0.800\n",
      "[epoch: 17, i:  1999]  train_loss: 0.785  |  valid_loss: 1.021\n",
      "[epoch: 17, i:  2124]  train_loss: 0.782  |  valid_loss: 0.714\n",
      "[epoch: 17, i:  2249]  train_loss: 0.819  |  valid_loss: 0.845\n",
      "[epoch: 17, i:  2374]  train_loss: 0.887  |  valid_loss: 0.782\n",
      "[epoch: 17, i:  2499]  train_loss: 0.767  |  valid_loss: 1.012\n",
      "[epoch: 17, i:  2624]  train_loss: 0.818  |  valid_loss: 0.934\n",
      "[epoch: 17, i:  2749]  train_loss: 0.882  |  valid_loss: 0.982\n",
      "[epoch: 17, i:  2874]  train_loss: 0.786  |  valid_loss: 1.032\n",
      "[epoch: 17, i:  2999]  train_loss: 0.860  |  valid_loss: 0.847\n",
      "[epoch: 17, i:  3124]  train_loss: 0.824  |  valid_loss: 0.909\n",
      "[epoch: 17, i:  3249]  train_loss: 0.852  |  valid_loss: 1.058\n",
      "[epoch: 17, i:  3374]  train_loss: 0.800  |  valid_loss: 0.787\n",
      "[epoch: 17, i:  3499]  train_loss: 0.778  |  valid_loss: 0.767\n",
      "[epoch: 17, i:  3624]  train_loss: 0.869  |  valid_loss: 0.829\n",
      "[epoch: 17, i:  3749]  train_loss: 0.875  |  valid_loss: 0.706\n",
      "[epoch: 17, i:  3874]  train_loss: 0.841  |  valid_loss: 0.794\n",
      "[epoch: 17, i:  3999]  train_loss: 0.880  |  valid_loss: 0.655\n",
      "[epoch: 17, i:  4124]  train_loss: 0.750  |  valid_loss: 0.881\n",
      "[epoch: 17, i:  4249]  train_loss: 0.886  |  valid_loss: 0.896\n",
      "[epoch: 17, i:  4374]  train_loss: 0.861  |  valid_loss: 0.882\n",
      "[epoch: 17, i:  4499]  train_loss: 0.787  |  valid_loss: 0.803\n",
      "[epoch: 17, i:  4624]  train_loss: 0.772  |  valid_loss: 0.934\n",
      "[epoch: 17, i:  4749]  train_loss: 0.772  |  valid_loss: 0.890\n",
      "[epoch: 17, i:  4874]  train_loss: 0.892  |  valid_loss: 0.778\n",
      "[epoch: 17, i:  4999]  train_loss: 0.797  |  valid_loss: 0.780\n",
      "[epoch: 17, i:  5124]  train_loss: 0.831  |  valid_loss: 0.837\n",
      "[epoch: 17, i:  5249]  train_loss: 0.900  |  valid_loss: 0.906\n",
      "[epoch: 17, i:  5374]  train_loss: 0.821  |  valid_loss: 0.646\n",
      "[epoch: 17, i:  5499]  train_loss: 0.870  |  valid_loss: 0.725\n",
      "[epoch: 17, i:  5624]  train_loss: 0.821  |  valid_loss: 0.948\n",
      "[epoch: 17, i:  5749]  train_loss: 0.780  |  valid_loss: 0.848\n",
      "[epoch: 17, i:  5874]  train_loss: 0.800  |  valid_loss: 0.730\n",
      "[epoch: 17, i:  5999]  train_loss: 0.785  |  valid_loss: 0.815\n",
      "[epoch: 17, i:  6124]  train_loss: 0.800  |  valid_loss: 0.733\n",
      "[epoch: 17, i:  6249]  train_loss: 0.776  |  valid_loss: 0.883\n",
      "[epoch: 17, i:  6374]  train_loss: 0.826  |  valid_loss: 0.741\n",
      "[epoch: 17, i:  6499]  train_loss: 0.895  |  valid_loss: 0.785\n",
      "[epoch: 17, i:  6624]  train_loss: 0.861  |  valid_loss: 0.725\n",
      "[epoch: 17, i:  6749]  train_loss: 0.870  |  valid_loss: 0.776\n",
      "[epoch: 17, i:  6874]  train_loss: 0.819  |  valid_loss: 0.809\n",
      "[epoch: 17, i:  6999]  train_loss: 0.808  |  valid_loss: 0.997\n",
      "[epoch: 17, i:  7124]  train_loss: 0.831  |  valid_loss: 0.957\n",
      "[epoch: 17, i:  7249]  train_loss: 0.841  |  valid_loss: 0.657\n",
      "[epoch: 17, i:  7374]  train_loss: 0.746  |  valid_loss: 1.024\n",
      "[epoch: 17, i:  7499]  train_loss: 0.836  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  7624]  train_loss: 0.867  |  valid_loss: 0.949\n",
      "[epoch: 17, i:  7749]  train_loss: 0.836  |  valid_loss: 0.832\n",
      "[epoch: 17, i:  7874]  train_loss: 0.759  |  valid_loss: 0.879\n",
      "[epoch: 17, i:  7999]  train_loss: 0.924  |  valid_loss: 0.724\n",
      "[epoch: 17, i:  8124]  train_loss: 0.794  |  valid_loss: 0.944\n",
      "[epoch: 17, i:  8249]  train_loss: 0.787  |  valid_loss: 0.904\n",
      "[epoch: 17, i:  8374]  train_loss: 0.860  |  valid_loss: 0.845\n",
      "[epoch: 17, i:  8499]  train_loss: 0.798  |  valid_loss: 0.955\n",
      "[epoch: 17, i:  8624]  train_loss: 0.774  |  valid_loss: 0.851\n",
      "[epoch: 17, i:  8749]  train_loss: 0.748  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  8874]  train_loss: 0.860  |  valid_loss: 0.889\n",
      "[epoch: 17, i:  8999]  train_loss: 0.858  |  valid_loss: 0.697\n",
      "[epoch: 17, i:  9124]  train_loss: 0.695  |  valid_loss: 0.737\n",
      "[epoch: 17, i:  9249]  train_loss: 0.822  |  valid_loss: 0.608\n",
      "[epoch: 17, i:  9374]  train_loss: 0.845  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  9499]  train_loss: 0.828  |  valid_loss: 0.782\n",
      "[epoch: 17, i:  9624]  train_loss: 0.764  |  valid_loss: 0.919\n",
      "[epoch: 17, i:  9749]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 17, i:  9874]  train_loss: 0.817  |  valid_loss: 0.898\n",
      "[epoch: 17, i:  9999]  train_loss: 0.871  |  valid_loss: 0.820\n",
      "[epoch: 17, i: 10124]  train_loss: 0.801  |  valid_loss: 0.762\n",
      "[epoch: 17, i: 10249]  train_loss: 0.846  |  valid_loss: 0.882\n",
      "[epoch: 17, i: 10374]  train_loss: 0.850  |  valid_loss: 0.745\n",
      "[epoch: 17, i: 10499]  train_loss: 0.833  |  valid_loss: 0.955\n",
      "[epoch: 17, i: 10624]  train_loss: 0.820  |  valid_loss: 0.964\n",
      "[epoch: 17, i: 10749]  train_loss: 0.829  |  valid_loss: 0.862\n",
      "[epoch: 17, i: 10874]  train_loss: 0.776  |  valid_loss: 0.959\n",
      "[epoch: 17, i: 10999]  train_loss: 0.886  |  valid_loss: 0.985\n",
      "[epoch: 17, i: 11124]  train_loss: 0.793  |  valid_loss: 0.837\n",
      "[epoch: 17, i: 11249]  train_loss: 0.770  |  valid_loss: 0.876\n",
      "[epoch: 17, i: 11374]  train_loss: 0.837  |  valid_loss: 0.796\n",
      "[epoch: 17, i: 11499]  train_loss: 0.785  |  valid_loss: 0.586\n",
      "[epoch: 17, i: 11624]  train_loss: 0.837  |  valid_loss: 0.806\n",
      "[epoch: 17, i: 11749]  train_loss: 0.861  |  valid_loss: 0.747\n",
      "[epoch: 17, i: 11874]  train_loss: 0.915  |  valid_loss: 0.768\n",
      "[epoch: 17, i: 11999]  train_loss: 0.819  |  valid_loss: 0.673\n",
      "[epoch: 17, i: 12124]  train_loss: 0.786  |  valid_loss: 0.716\n",
      "[epoch: 17, i: 12249]  train_loss: 0.870  |  valid_loss: 1.092\n",
      "[epoch: 17, i: 12374]  train_loss: 0.862  |  valid_loss: 0.886\n",
      "[epoch: 17, i: 12499]  train_loss: 0.823  |  valid_loss: 0.888\n",
      "--> [End of epoch 17] train_accuracy: 71.02%  |  valid_accuracy: 71.04%\n",
      "--> [Start of epoch 18]  lr: 0.000036\n",
      "[epoch: 18, i:   124]  train_loss: 0.880  |  valid_loss: 0.730\n",
      "[epoch: 18, i:   249]  train_loss: 0.792  |  valid_loss: 0.758\n",
      "[epoch: 18, i:   374]  train_loss: 0.871  |  valid_loss: 0.865\n",
      "[epoch: 18, i:   499]  train_loss: 0.834  |  valid_loss: 0.783\n",
      "[epoch: 18, i:   624]  train_loss: 0.808  |  valid_loss: 0.930\n",
      "[epoch: 18, i:   749]  train_loss: 0.816  |  valid_loss: 0.566\n",
      "[epoch: 18, i:   874]  train_loss: 0.863  |  valid_loss: 0.859\n",
      "[epoch: 18, i:   999]  train_loss: 0.830  |  valid_loss: 0.946\n",
      "[epoch: 18, i:  1124]  train_loss: 0.826  |  valid_loss: 0.891\n",
      "[epoch: 18, i:  1249]  train_loss: 0.769  |  valid_loss: 0.755\n",
      "[epoch: 18, i:  1374]  train_loss: 0.796  |  valid_loss: 0.836\n",
      "[epoch: 18, i:  1499]  train_loss: 0.724  |  valid_loss: 0.820\n",
      "[epoch: 18, i:  1624]  train_loss: 0.835  |  valid_loss: 0.697\n",
      "[epoch: 18, i:  1749]  train_loss: 0.784  |  valid_loss: 1.024\n",
      "[epoch: 18, i:  1874]  train_loss: 0.792  |  valid_loss: 0.795\n",
      "[epoch: 18, i:  1999]  train_loss: 0.741  |  valid_loss: 1.009\n",
      "[epoch: 18, i:  2124]  train_loss: 0.739  |  valid_loss: 0.726\n",
      "[epoch: 18, i:  2249]  train_loss: 0.821  |  valid_loss: 0.837\n",
      "[epoch: 18, i:  2374]  train_loss: 0.889  |  valid_loss: 0.766\n",
      "[epoch: 18, i:  2499]  train_loss: 0.852  |  valid_loss: 0.990\n",
      "[epoch: 18, i:  2624]  train_loss: 0.779  |  valid_loss: 0.894\n",
      "[epoch: 18, i:  2749]  train_loss: 0.802  |  valid_loss: 0.956\n",
      "[epoch: 18, i:  2874]  train_loss: 0.882  |  valid_loss: 0.998\n",
      "[epoch: 18, i:  2999]  train_loss: 0.766  |  valid_loss: 0.870\n",
      "[epoch: 18, i:  3124]  train_loss: 0.822  |  valid_loss: 0.910\n",
      "[epoch: 18, i:  3249]  train_loss: 0.812  |  valid_loss: 1.060\n",
      "[epoch: 18, i:  3374]  train_loss: 0.829  |  valid_loss: 0.809\n",
      "[epoch: 18, i:  3499]  train_loss: 0.772  |  valid_loss: 0.773\n",
      "[epoch: 18, i:  3624]  train_loss: 0.760  |  valid_loss: 0.791\n",
      "[epoch: 18, i:  3749]  train_loss: 0.778  |  valid_loss: 0.721\n",
      "[epoch: 18, i:  3874]  train_loss: 0.877  |  valid_loss: 0.819\n",
      "[epoch: 18, i:  3999]  train_loss: 0.847  |  valid_loss: 0.646\n",
      "[epoch: 18, i:  4124]  train_loss: 0.812  |  valid_loss: 0.864\n",
      "[epoch: 18, i:  4249]  train_loss: 0.779  |  valid_loss: 0.895\n",
      "[epoch: 18, i:  4374]  train_loss: 0.844  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  4499]  train_loss: 0.802  |  valid_loss: 0.787\n",
      "[epoch: 18, i:  4624]  train_loss: 0.803  |  valid_loss: 0.963\n",
      "[epoch: 18, i:  4749]  train_loss: 0.799  |  valid_loss: 0.903\n",
      "[epoch: 18, i:  4874]  train_loss: 0.828  |  valid_loss: 0.764\n",
      "[epoch: 18, i:  4999]  train_loss: 0.833  |  valid_loss: 0.760\n",
      "[epoch: 18, i:  5124]  train_loss: 0.820  |  valid_loss: 0.801\n",
      "[epoch: 18, i:  5249]  train_loss: 0.829  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  5374]  train_loss: 0.852  |  valid_loss: 0.630\n",
      "[epoch: 18, i:  5499]  train_loss: 0.816  |  valid_loss: 0.695\n",
      "[epoch: 18, i:  5624]  train_loss: 0.823  |  valid_loss: 0.979\n",
      "[epoch: 18, i:  5749]  train_loss: 0.896  |  valid_loss: 0.824\n",
      "[epoch: 18, i:  5874]  train_loss: 0.840  |  valid_loss: 0.732\n",
      "[epoch: 18, i:  5999]  train_loss: 0.822  |  valid_loss: 0.863\n",
      "[epoch: 18, i:  6124]  train_loss: 0.782  |  valid_loss: 0.759\n",
      "[epoch: 18, i:  6249]  train_loss: 0.759  |  valid_loss: 0.850\n",
      "[epoch: 18, i:  6374]  train_loss: 0.848  |  valid_loss: 0.739\n",
      "[epoch: 18, i:  6499]  train_loss: 0.813  |  valid_loss: 0.773\n",
      "[epoch: 18, i:  6624]  train_loss: 0.800  |  valid_loss: 0.667\n",
      "[epoch: 18, i:  6749]  train_loss: 0.838  |  valid_loss: 0.829\n",
      "[epoch: 18, i:  6874]  train_loss: 0.844  |  valid_loss: 0.767\n",
      "[epoch: 18, i:  6999]  train_loss: 0.779  |  valid_loss: 1.001\n",
      "[epoch: 18, i:  7124]  train_loss: 0.805  |  valid_loss: 0.995\n",
      "[epoch: 18, i:  7249]  train_loss: 0.828  |  valid_loss: 0.674\n",
      "[epoch: 18, i:  7374]  train_loss: 0.936  |  valid_loss: 1.016\n",
      "[epoch: 18, i:  7499]  train_loss: 0.855  |  valid_loss: 0.912\n",
      "[epoch: 18, i:  7624]  train_loss: 0.855  |  valid_loss: 0.911\n",
      "[epoch: 18, i:  7749]  train_loss: 0.856  |  valid_loss: 0.807\n",
      "[epoch: 18, i:  7874]  train_loss: 0.865  |  valid_loss: 0.876\n",
      "[epoch: 18, i:  7999]  train_loss: 0.856  |  valid_loss: 0.726\n",
      "[epoch: 18, i:  8124]  train_loss: 0.851  |  valid_loss: 0.952\n",
      "[epoch: 18, i:  8249]  train_loss: 0.773  |  valid_loss: 0.873\n",
      "[epoch: 18, i:  8374]  train_loss: 0.804  |  valid_loss: 0.867\n",
      "[epoch: 18, i:  8499]  train_loss: 0.774  |  valid_loss: 0.937\n",
      "[epoch: 18, i:  8624]  train_loss: 0.790  |  valid_loss: 0.885\n",
      "[epoch: 18, i:  8749]  train_loss: 0.800  |  valid_loss: 0.897\n",
      "[epoch: 18, i:  8874]  train_loss: 0.863  |  valid_loss: 0.889\n",
      "[epoch: 18, i:  8999]  train_loss: 0.845  |  valid_loss: 0.703\n",
      "[epoch: 18, i:  9124]  train_loss: 0.846  |  valid_loss: 0.766\n",
      "[epoch: 18, i:  9249]  train_loss: 0.814  |  valid_loss: 0.631\n",
      "[epoch: 18, i:  9374]  train_loss: 0.841  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  9499]  train_loss: 0.829  |  valid_loss: 0.764\n",
      "[epoch: 18, i:  9624]  train_loss: 0.822  |  valid_loss: 0.886\n",
      "[epoch: 18, i:  9749]  train_loss: 0.822  |  valid_loss: 0.920\n",
      "[epoch: 18, i:  9874]  train_loss: 0.824  |  valid_loss: 0.911\n",
      "[epoch: 18, i:  9999]  train_loss: 0.779  |  valid_loss: 0.844\n",
      "[epoch: 18, i: 10124]  train_loss: 0.833  |  valid_loss: 0.774\n",
      "[epoch: 18, i: 10249]  train_loss: 0.834  |  valid_loss: 0.858\n",
      "[epoch: 18, i: 10374]  train_loss: 0.822  |  valid_loss: 0.748\n",
      "[epoch: 18, i: 10499]  train_loss: 0.825  |  valid_loss: 0.996\n",
      "[epoch: 18, i: 10624]  train_loss: 0.790  |  valid_loss: 0.928\n",
      "[epoch: 18, i: 10749]  train_loss: 0.801  |  valid_loss: 0.846\n",
      "[epoch: 18, i: 10874]  train_loss: 0.843  |  valid_loss: 0.971\n",
      "[epoch: 18, i: 10999]  train_loss: 0.804  |  valid_loss: 0.958\n",
      "[epoch: 18, i: 11124]  train_loss: 0.753  |  valid_loss: 0.816\n",
      "[epoch: 18, i: 11249]  train_loss: 0.866  |  valid_loss: 0.854\n",
      "[epoch: 18, i: 11374]  train_loss: 0.872  |  valid_loss: 0.805\n",
      "[epoch: 18, i: 11499]  train_loss: 0.791  |  valid_loss: 0.611\n",
      "[epoch: 18, i: 11624]  train_loss: 0.837  |  valid_loss: 0.815\n",
      "[epoch: 18, i: 11749]  train_loss: 0.767  |  valid_loss: 0.753\n",
      "[epoch: 18, i: 11874]  train_loss: 0.870  |  valid_loss: 0.813\n",
      "[epoch: 18, i: 11999]  train_loss: 0.875  |  valid_loss: 0.702\n",
      "[epoch: 18, i: 12124]  train_loss: 0.853  |  valid_loss: 0.765\n",
      "[epoch: 18, i: 12249]  train_loss: 0.824  |  valid_loss: 1.063\n",
      "[epoch: 18, i: 12374]  train_loss: 0.841  |  valid_loss: 0.898\n",
      "[epoch: 18, i: 12499]  train_loss: 0.871  |  valid_loss: 0.878\n",
      "--> [End of epoch 18] train_accuracy: 71.17%  |  valid_accuracy: 70.52%\n",
      "--> [Start of epoch 19]  lr: 0.000029\n",
      "[epoch: 19, i:   124]  train_loss: 0.884  |  valid_loss: 0.738\n",
      "[epoch: 19, i:   249]  train_loss: 0.837  |  valid_loss: 0.729\n",
      "[epoch: 19, i:   374]  train_loss: 0.802  |  valid_loss: 0.855\n",
      "[epoch: 19, i:   499]  train_loss: 0.850  |  valid_loss: 0.805\n",
      "[epoch: 19, i:   624]  train_loss: 0.786  |  valid_loss: 0.941\n",
      "[epoch: 19, i:   749]  train_loss: 0.844  |  valid_loss: 0.604\n",
      "[epoch: 19, i:   874]  train_loss: 0.807  |  valid_loss: 0.874\n",
      "[epoch: 19, i:   999]  train_loss: 0.829  |  valid_loss: 0.914\n",
      "[epoch: 19, i:  1124]  train_loss: 0.790  |  valid_loss: 0.944\n",
      "[epoch: 19, i:  1249]  train_loss: 0.822  |  valid_loss: 0.702\n",
      "[epoch: 19, i:  1374]  train_loss: 0.827  |  valid_loss: 0.819\n",
      "[epoch: 19, i:  1499]  train_loss: 0.821  |  valid_loss: 0.817\n",
      "[epoch: 19, i:  1624]  train_loss: 0.832  |  valid_loss: 0.692\n",
      "[epoch: 19, i:  1749]  train_loss: 0.807  |  valid_loss: 0.984\n",
      "[epoch: 19, i:  1874]  train_loss: 0.840  |  valid_loss: 0.780\n",
      "[epoch: 19, i:  1999]  train_loss: 0.774  |  valid_loss: 0.985\n",
      "[epoch: 19, i:  2124]  train_loss: 0.787  |  valid_loss: 0.734\n",
      "[epoch: 19, i:  2249]  train_loss: 0.822  |  valid_loss: 0.803\n",
      "[epoch: 19, i:  2374]  train_loss: 0.900  |  valid_loss: 0.792\n",
      "[epoch: 19, i:  2499]  train_loss: 0.811  |  valid_loss: 1.022\n",
      "[epoch: 19, i:  2624]  train_loss: 0.811  |  valid_loss: 0.934\n",
      "[epoch: 19, i:  2749]  train_loss: 0.717  |  valid_loss: 0.954\n",
      "[epoch: 19, i:  2874]  train_loss: 0.806  |  valid_loss: 0.997\n",
      "[epoch: 19, i:  2999]  train_loss: 0.818  |  valid_loss: 0.841\n",
      "[epoch: 19, i:  3124]  train_loss: 0.883  |  valid_loss: 0.911\n",
      "[epoch: 19, i:  3249]  train_loss: 0.736  |  valid_loss: 1.086\n",
      "[epoch: 19, i:  3374]  train_loss: 0.840  |  valid_loss: 0.784\n",
      "[epoch: 19, i:  3499]  train_loss: 0.794  |  valid_loss: 0.769\n",
      "[epoch: 19, i:  3624]  train_loss: 0.810  |  valid_loss: 0.800\n",
      "[epoch: 19, i:  3749]  train_loss: 0.732  |  valid_loss: 0.697\n",
      "[epoch: 19, i:  3874]  train_loss: 0.793  |  valid_loss: 0.826\n",
      "[epoch: 19, i:  3999]  train_loss: 0.953  |  valid_loss: 0.656\n",
      "[epoch: 19, i:  4124]  train_loss: 0.828  |  valid_loss: 0.870\n",
      "[epoch: 19, i:  4249]  train_loss: 0.870  |  valid_loss: 0.933\n",
      "[epoch: 19, i:  4374]  train_loss: 0.824  |  valid_loss: 0.872\n",
      "[epoch: 19, i:  4499]  train_loss: 0.882  |  valid_loss: 0.785\n",
      "[epoch: 19, i:  4624]  train_loss: 0.781  |  valid_loss: 0.963\n",
      "[epoch: 19, i:  4749]  train_loss: 0.840  |  valid_loss: 0.887\n",
      "[epoch: 19, i:  4874]  train_loss: 0.767  |  valid_loss: 0.737\n",
      "[epoch: 19, i:  4999]  train_loss: 0.833  |  valid_loss: 0.761\n",
      "[epoch: 19, i:  5124]  train_loss: 0.792  |  valid_loss: 0.799\n",
      "[epoch: 19, i:  5249]  train_loss: 0.811  |  valid_loss: 0.902\n",
      "[epoch: 19, i:  5374]  train_loss: 0.880  |  valid_loss: 0.646\n",
      "[epoch: 19, i:  5499]  train_loss: 0.798  |  valid_loss: 0.700\n",
      "[epoch: 19, i:  5624]  train_loss: 0.861  |  valid_loss: 1.028\n",
      "[epoch: 19, i:  5749]  train_loss: 0.748  |  valid_loss: 0.833\n",
      "[epoch: 19, i:  5874]  train_loss: 0.851  |  valid_loss: 0.741\n",
      "[epoch: 19, i:  5999]  train_loss: 0.914  |  valid_loss: 0.843\n",
      "[epoch: 19, i:  6124]  train_loss: 0.859  |  valid_loss: 0.755\n",
      "[epoch: 19, i:  6249]  train_loss: 0.786  |  valid_loss: 0.857\n",
      "[epoch: 19, i:  6374]  train_loss: 0.795  |  valid_loss: 0.749\n",
      "[epoch: 19, i:  6499]  train_loss: 0.773  |  valid_loss: 0.785\n",
      "[epoch: 19, i:  6624]  train_loss: 0.859  |  valid_loss: 0.675\n",
      "[epoch: 19, i:  6749]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 19, i:  6874]  train_loss: 0.778  |  valid_loss: 0.780\n",
      "[epoch: 19, i:  6999]  train_loss: 0.822  |  valid_loss: 0.972\n",
      "[epoch: 19, i:  7124]  train_loss: 0.803  |  valid_loss: 0.947\n",
      "[epoch: 19, i:  7249]  train_loss: 0.839  |  valid_loss: 0.665\n",
      "[epoch: 19, i:  7374]  train_loss: 0.778  |  valid_loss: 1.018\n",
      "[epoch: 19, i:  7499]  train_loss: 0.838  |  valid_loss: 0.920\n",
      "[epoch: 19, i:  7624]  train_loss: 0.804  |  valid_loss: 0.896\n",
      "[epoch: 19, i:  7749]  train_loss: 0.794  |  valid_loss: 0.833\n",
      "[epoch: 19, i:  7874]  train_loss: 0.816  |  valid_loss: 0.873\n",
      "[epoch: 19, i:  7999]  train_loss: 0.819  |  valid_loss: 0.722\n",
      "[epoch: 19, i:  8124]  train_loss: 0.809  |  valid_loss: 0.938\n",
      "[epoch: 19, i:  8249]  train_loss: 0.764  |  valid_loss: 0.894\n",
      "[epoch: 19, i:  8374]  train_loss: 0.767  |  valid_loss: 0.876\n",
      "[epoch: 19, i:  8499]  train_loss: 0.760  |  valid_loss: 0.920\n",
      "[epoch: 19, i:  8624]  train_loss: 0.776  |  valid_loss: 0.855\n",
      "[epoch: 19, i:  8749]  train_loss: 0.822  |  valid_loss: 0.892\n",
      "[epoch: 19, i:  8874]  train_loss: 0.858  |  valid_loss: 0.856\n",
      "[epoch: 19, i:  8999]  train_loss: 0.770  |  valid_loss: 0.684\n",
      "[epoch: 19, i:  9124]  train_loss: 0.816  |  valid_loss: 0.748\n",
      "[epoch: 19, i:  9249]  train_loss: 0.816  |  valid_loss: 0.609\n",
      "[epoch: 19, i:  9374]  train_loss: 0.898  |  valid_loss: 0.881\n",
      "[epoch: 19, i:  9499]  train_loss: 0.842  |  valid_loss: 0.755\n",
      "[epoch: 19, i:  9624]  train_loss: 0.832  |  valid_loss: 0.866\n",
      "[epoch: 19, i:  9749]  train_loss: 0.877  |  valid_loss: 0.938\n",
      "[epoch: 19, i:  9874]  train_loss: 0.803  |  valid_loss: 0.881\n",
      "[epoch: 19, i:  9999]  train_loss: 0.872  |  valid_loss: 0.810\n",
      "[epoch: 19, i: 10124]  train_loss: 0.863  |  valid_loss: 0.769\n",
      "[epoch: 19, i: 10249]  train_loss: 0.848  |  valid_loss: 0.863\n",
      "[epoch: 19, i: 10374]  train_loss: 0.808  |  valid_loss: 0.740\n",
      "[epoch: 19, i: 10499]  train_loss: 0.806  |  valid_loss: 0.949\n",
      "[epoch: 19, i: 10624]  train_loss: 0.853  |  valid_loss: 0.932\n",
      "[epoch: 19, i: 10749]  train_loss: 0.887  |  valid_loss: 0.858\n",
      "[epoch: 19, i: 10874]  train_loss: 0.787  |  valid_loss: 0.960\n",
      "[epoch: 19, i: 10999]  train_loss: 0.800  |  valid_loss: 0.969\n",
      "[epoch: 19, i: 11124]  train_loss: 0.708  |  valid_loss: 0.823\n",
      "[epoch: 19, i: 11249]  train_loss: 0.770  |  valid_loss: 0.868\n",
      "[epoch: 19, i: 11374]  train_loss: 0.831  |  valid_loss: 0.824\n",
      "[epoch: 19, i: 11499]  train_loss: 0.818  |  valid_loss: 0.641\n",
      "[epoch: 19, i: 11624]  train_loss: 0.829  |  valid_loss: 0.812\n",
      "[epoch: 19, i: 11749]  train_loss: 0.892  |  valid_loss: 0.756\n",
      "[epoch: 19, i: 11874]  train_loss: 0.791  |  valid_loss: 0.807\n",
      "[epoch: 19, i: 11999]  train_loss: 0.859  |  valid_loss: 0.687\n",
      "[epoch: 19, i: 12124]  train_loss: 0.864  |  valid_loss: 0.768\n",
      "[epoch: 19, i: 12249]  train_loss: 0.787  |  valid_loss: 1.088\n",
      "[epoch: 19, i: 12374]  train_loss: 0.838  |  valid_loss: 0.909\n",
      "[epoch: 19, i: 12499]  train_loss: 0.853  |  valid_loss: 0.860\n",
      "--> [End of epoch 19] train_accuracy: 71.12%  |  valid_accuracy: 70.64%\n",
      "--> [Start of epoch 20]  lr: 0.000023\n",
      "[epoch: 20, i:   124]  train_loss: 0.785  |  valid_loss: 0.742\n",
      "[epoch: 20, i:   249]  train_loss: 0.896  |  valid_loss: 0.742\n",
      "[epoch: 20, i:   374]  train_loss: 0.817  |  valid_loss: 0.862\n",
      "[epoch: 20, i:   499]  train_loss: 0.766  |  valid_loss: 0.793\n",
      "[epoch: 20, i:   624]  train_loss: 0.787  |  valid_loss: 0.924\n",
      "[epoch: 20, i:   749]  train_loss: 0.760  |  valid_loss: 0.568\n",
      "[epoch: 20, i:   874]  train_loss: 0.733  |  valid_loss: 0.874\n",
      "[epoch: 20, i:   999]  train_loss: 0.859  |  valid_loss: 0.951\n",
      "[epoch: 20, i:  1124]  train_loss: 0.819  |  valid_loss: 0.937\n",
      "[epoch: 20, i:  1249]  train_loss: 0.863  |  valid_loss: 0.770\n",
      "[epoch: 20, i:  1374]  train_loss: 0.903  |  valid_loss: 0.781\n",
      "[epoch: 20, i:  1499]  train_loss: 0.837  |  valid_loss: 0.817\n",
      "[epoch: 20, i:  1624]  train_loss: 0.829  |  valid_loss: 0.692\n",
      "[epoch: 20, i:  1749]  train_loss: 0.865  |  valid_loss: 0.997\n",
      "[epoch: 20, i:  1874]  train_loss: 0.820  |  valid_loss: 0.760\n",
      "[epoch: 20, i:  1999]  train_loss: 0.788  |  valid_loss: 1.017\n",
      "[epoch: 20, i:  2124]  train_loss: 0.809  |  valid_loss: 0.706\n",
      "[epoch: 20, i:  2249]  train_loss: 0.849  |  valid_loss: 0.847\n",
      "[epoch: 20, i:  2374]  train_loss: 0.856  |  valid_loss: 0.770\n",
      "[epoch: 20, i:  2499]  train_loss: 0.776  |  valid_loss: 1.029\n",
      "[epoch: 20, i:  2624]  train_loss: 0.855  |  valid_loss: 0.918\n",
      "[epoch: 20, i:  2749]  train_loss: 0.798  |  valid_loss: 0.949\n",
      "[epoch: 20, i:  2874]  train_loss: 0.777  |  valid_loss: 0.975\n",
      "[epoch: 20, i:  2999]  train_loss: 0.847  |  valid_loss: 0.852\n",
      "[epoch: 20, i:  3124]  train_loss: 0.813  |  valid_loss: 0.905\n",
      "[epoch: 20, i:  3249]  train_loss: 0.744  |  valid_loss: 1.070\n",
      "[epoch: 20, i:  3374]  train_loss: 0.843  |  valid_loss: 0.798\n",
      "[epoch: 20, i:  3499]  train_loss: 0.795  |  valid_loss: 0.754\n",
      "[epoch: 20, i:  3624]  train_loss: 0.757  |  valid_loss: 0.776\n",
      "[epoch: 20, i:  3749]  train_loss: 0.820  |  valid_loss: 0.700\n",
      "[epoch: 20, i:  3874]  train_loss: 0.804  |  valid_loss: 0.810\n",
      "[epoch: 20, i:  3999]  train_loss: 0.810  |  valid_loss: 0.645\n",
      "[epoch: 20, i:  4124]  train_loss: 0.756  |  valid_loss: 0.839\n",
      "[epoch: 20, i:  4249]  train_loss: 0.881  |  valid_loss: 0.891\n",
      "[epoch: 20, i:  4374]  train_loss: 0.768  |  valid_loss: 0.860\n",
      "[epoch: 20, i:  4499]  train_loss: 0.847  |  valid_loss: 0.818\n",
      "[epoch: 20, i:  4624]  train_loss: 0.791  |  valid_loss: 0.948\n",
      "[epoch: 20, i:  4749]  train_loss: 0.779  |  valid_loss: 0.866\n",
      "[epoch: 20, i:  4874]  train_loss: 0.779  |  valid_loss: 0.727\n",
      "[epoch: 20, i:  4999]  train_loss: 0.837  |  valid_loss: 0.753\n",
      "[epoch: 20, i:  5124]  train_loss: 0.795  |  valid_loss: 0.809\n",
      "[epoch: 20, i:  5249]  train_loss: 0.824  |  valid_loss: 0.884\n",
      "[epoch: 20, i:  5374]  train_loss: 0.866  |  valid_loss: 0.635\n",
      "[epoch: 20, i:  5499]  train_loss: 0.819  |  valid_loss: 0.724\n",
      "[epoch: 20, i:  5624]  train_loss: 0.788  |  valid_loss: 0.976\n",
      "[epoch: 20, i:  5749]  train_loss: 0.838  |  valid_loss: 0.812\n",
      "[epoch: 20, i:  5874]  train_loss: 0.836  |  valid_loss: 0.740\n",
      "[epoch: 20, i:  5999]  train_loss: 0.825  |  valid_loss: 0.856\n",
      "[epoch: 20, i:  6124]  train_loss: 0.800  |  valid_loss: 0.763\n",
      "[epoch: 20, i:  6249]  train_loss: 0.790  |  valid_loss: 0.861\n",
      "[epoch: 20, i:  6374]  train_loss: 0.862  |  valid_loss: 0.723\n",
      "[epoch: 20, i:  6499]  train_loss: 0.742  |  valid_loss: 0.786\n",
      "[epoch: 20, i:  6624]  train_loss: 0.740  |  valid_loss: 0.700\n",
      "[epoch: 20, i:  6749]  train_loss: 0.762  |  valid_loss: 0.803\n",
      "[epoch: 20, i:  6874]  train_loss: 0.858  |  valid_loss: 0.747\n",
      "[epoch: 20, i:  6999]  train_loss: 0.787  |  valid_loss: 0.982\n",
      "[epoch: 20, i:  7124]  train_loss: 0.784  |  valid_loss: 0.975\n",
      "[epoch: 20, i:  7249]  train_loss: 0.824  |  valid_loss: 0.667\n",
      "[epoch: 20, i:  7374]  train_loss: 0.850  |  valid_loss: 1.025\n",
      "[epoch: 20, i:  7499]  train_loss: 0.792  |  valid_loss: 0.896\n",
      "[epoch: 20, i:  7624]  train_loss: 0.842  |  valid_loss: 0.927\n",
      "[epoch: 20, i:  7749]  train_loss: 0.752  |  valid_loss: 0.812\n",
      "[epoch: 20, i:  7874]  train_loss: 0.738  |  valid_loss: 0.854\n",
      "[epoch: 20, i:  7999]  train_loss: 0.754  |  valid_loss: 0.713\n",
      "[epoch: 20, i:  8124]  train_loss: 0.750  |  valid_loss: 0.924\n",
      "[epoch: 20, i:  8249]  train_loss: 0.871  |  valid_loss: 0.888\n",
      "[epoch: 20, i:  8374]  train_loss: 0.806  |  valid_loss: 0.837\n",
      "[epoch: 20, i:  8499]  train_loss: 0.834  |  valid_loss: 0.919\n",
      "[epoch: 20, i:  8624]  train_loss: 0.783  |  valid_loss: 0.876\n",
      "[epoch: 20, i:  8749]  train_loss: 0.899  |  valid_loss: 0.899\n",
      "[epoch: 20, i:  8874]  train_loss: 0.790  |  valid_loss: 0.814\n",
      "[epoch: 20, i:  8999]  train_loss: 0.803  |  valid_loss: 0.686\n",
      "[epoch: 20, i:  9124]  train_loss: 0.797  |  valid_loss: 0.760\n",
      "[epoch: 20, i:  9249]  train_loss: 0.792  |  valid_loss: 0.616\n",
      "[epoch: 20, i:  9374]  train_loss: 0.842  |  valid_loss: 0.886\n",
      "[epoch: 20, i:  9499]  train_loss: 0.802  |  valid_loss: 0.768\n",
      "[epoch: 20, i:  9624]  train_loss: 0.849  |  valid_loss: 0.887\n",
      "[epoch: 20, i:  9749]  train_loss: 0.844  |  valid_loss: 0.894\n",
      "[epoch: 20, i:  9874]  train_loss: 0.836  |  valid_loss: 0.904\n",
      "[epoch: 20, i:  9999]  train_loss: 0.834  |  valid_loss: 0.815\n",
      "[epoch: 20, i: 10124]  train_loss: 0.814  |  valid_loss: 0.766\n",
      "[epoch: 20, i: 10249]  train_loss: 0.735  |  valid_loss: 0.827\n",
      "[epoch: 20, i: 10374]  train_loss: 0.790  |  valid_loss: 0.733\n",
      "[epoch: 20, i: 10499]  train_loss: 0.858  |  valid_loss: 0.977\n",
      "[epoch: 20, i: 10624]  train_loss: 0.793  |  valid_loss: 0.939\n",
      "[epoch: 20, i: 10749]  train_loss: 0.917  |  valid_loss: 0.848\n",
      "[epoch: 20, i: 10874]  train_loss: 0.847  |  valid_loss: 0.955\n",
      "[epoch: 20, i: 10999]  train_loss: 0.782  |  valid_loss: 0.983\n",
      "[epoch: 20, i: 11124]  train_loss: 0.811  |  valid_loss: 0.828\n",
      "[epoch: 20, i: 11249]  train_loss: 0.902  |  valid_loss: 0.876\n",
      "[epoch: 20, i: 11374]  train_loss: 0.855  |  valid_loss: 0.777\n",
      "[epoch: 20, i: 11499]  train_loss: 0.822  |  valid_loss: 0.648\n",
      "[epoch: 20, i: 11624]  train_loss: 0.763  |  valid_loss: 0.809\n",
      "[epoch: 20, i: 11749]  train_loss: 0.794  |  valid_loss: 0.731\n",
      "[epoch: 20, i: 11874]  train_loss: 0.817  |  valid_loss: 0.790\n",
      "[epoch: 20, i: 11999]  train_loss: 0.871  |  valid_loss: 0.706\n",
      "[epoch: 20, i: 12124]  train_loss: 0.841  |  valid_loss: 0.724\n",
      "[epoch: 20, i: 12249]  train_loss: 0.841  |  valid_loss: 1.048\n",
      "[epoch: 20, i: 12374]  train_loss: 0.819  |  valid_loss: 0.883\n",
      "[epoch: 20, i: 12499]  train_loss: 0.868  |  valid_loss: 0.855\n",
      "--> [End of epoch 20] train_accuracy: 71.26%  |  valid_accuracy: 70.68%\n",
      "--> [Start of epoch 21]  lr: 0.000018\n",
      "[epoch: 21, i:   124]  train_loss: 0.787  |  valid_loss: 0.743\n",
      "[epoch: 21, i:   249]  train_loss: 0.770  |  valid_loss: 0.706\n",
      "[epoch: 21, i:   374]  train_loss: 0.897  |  valid_loss: 0.852\n",
      "[epoch: 21, i:   499]  train_loss: 0.853  |  valid_loss: 0.811\n",
      "[epoch: 21, i:   624]  train_loss: 0.823  |  valid_loss: 0.905\n",
      "[epoch: 21, i:   749]  train_loss: 0.785  |  valid_loss: 0.597\n",
      "[epoch: 21, i:   874]  train_loss: 0.852  |  valid_loss: 0.840\n",
      "[epoch: 21, i:   999]  train_loss: 0.839  |  valid_loss: 0.958\n",
      "[epoch: 21, i:  1124]  train_loss: 0.852  |  valid_loss: 0.939\n",
      "[epoch: 21, i:  1249]  train_loss: 0.813  |  valid_loss: 0.757\n",
      "[epoch: 21, i:  1374]  train_loss: 0.821  |  valid_loss: 0.789\n",
      "[epoch: 21, i:  1499]  train_loss: 0.780  |  valid_loss: 0.809\n",
      "[epoch: 21, i:  1624]  train_loss: 0.805  |  valid_loss: 0.679\n",
      "[epoch: 21, i:  1749]  train_loss: 0.755  |  valid_loss: 1.025\n",
      "[epoch: 21, i:  1874]  train_loss: 0.827  |  valid_loss: 0.780\n",
      "[epoch: 21, i:  1999]  train_loss: 0.829  |  valid_loss: 1.023\n",
      "[epoch: 21, i:  2124]  train_loss: 0.913  |  valid_loss: 0.708\n",
      "[epoch: 21, i:  2249]  train_loss: 0.829  |  valid_loss: 0.821\n",
      "[epoch: 21, i:  2374]  train_loss: 0.824  |  valid_loss: 0.760\n",
      "[epoch: 21, i:  2499]  train_loss: 0.758  |  valid_loss: 0.992\n",
      "[epoch: 21, i:  2624]  train_loss: 0.830  |  valid_loss: 0.940\n",
      "[epoch: 21, i:  2749]  train_loss: 0.836  |  valid_loss: 0.932\n",
      "[epoch: 21, i:  2874]  train_loss: 0.785  |  valid_loss: 1.050\n",
      "[epoch: 21, i:  2999]  train_loss: 0.846  |  valid_loss: 0.855\n",
      "[epoch: 21, i:  3124]  train_loss: 0.839  |  valid_loss: 0.921\n",
      "[epoch: 21, i:  3249]  train_loss: 0.857  |  valid_loss: 1.061\n",
      "[epoch: 21, i:  3374]  train_loss: 0.823  |  valid_loss: 0.818\n",
      "[epoch: 21, i:  3499]  train_loss: 0.797  |  valid_loss: 0.765\n",
      "[epoch: 21, i:  3624]  train_loss: 0.790  |  valid_loss: 0.782\n",
      "[epoch: 21, i:  3749]  train_loss: 0.881  |  valid_loss: 0.690\n",
      "[epoch: 21, i:  3874]  train_loss: 0.850  |  valid_loss: 0.804\n",
      "[epoch: 21, i:  3999]  train_loss: 0.724  |  valid_loss: 0.644\n",
      "[epoch: 21, i:  4124]  train_loss: 0.700  |  valid_loss: 0.816\n",
      "[epoch: 21, i:  4249]  train_loss: 0.844  |  valid_loss: 0.878\n",
      "[epoch: 21, i:  4374]  train_loss: 0.789  |  valid_loss: 0.872\n",
      "[epoch: 21, i:  4499]  train_loss: 0.793  |  valid_loss: 0.789\n",
      "[epoch: 21, i:  4624]  train_loss: 0.792  |  valid_loss: 0.959\n",
      "[epoch: 21, i:  4749]  train_loss: 0.809  |  valid_loss: 0.926\n",
      "[epoch: 21, i:  4874]  train_loss: 0.794  |  valid_loss: 0.747\n",
      "[epoch: 21, i:  4999]  train_loss: 0.821  |  valid_loss: 0.766\n",
      "[epoch: 21, i:  5124]  train_loss: 0.806  |  valid_loss: 0.805\n",
      "[epoch: 21, i:  5249]  train_loss: 0.812  |  valid_loss: 0.879\n",
      "[epoch: 21, i:  5374]  train_loss: 0.807  |  valid_loss: 0.618\n",
      "[epoch: 21, i:  5499]  train_loss: 0.877  |  valid_loss: 0.695\n",
      "[epoch: 21, i:  5624]  train_loss: 0.806  |  valid_loss: 0.906\n",
      "[epoch: 21, i:  5749]  train_loss: 0.831  |  valid_loss: 0.841\n",
      "[epoch: 21, i:  5874]  train_loss: 0.786  |  valid_loss: 0.734\n",
      "[epoch: 21, i:  5999]  train_loss: 0.771  |  valid_loss: 0.843\n",
      "[epoch: 21, i:  6124]  train_loss: 0.788  |  valid_loss: 0.783\n",
      "[epoch: 21, i:  6249]  train_loss: 0.835  |  valid_loss: 0.898\n",
      "[epoch: 21, i:  6374]  train_loss: 0.816  |  valid_loss: 0.748\n",
      "[epoch: 21, i:  6499]  train_loss: 0.768  |  valid_loss: 0.791\n",
      "[epoch: 21, i:  6624]  train_loss: 0.832  |  valid_loss: 0.655\n",
      "[epoch: 21, i:  6749]  train_loss: 0.814  |  valid_loss: 0.808\n",
      "[epoch: 21, i:  6874]  train_loss: 0.819  |  valid_loss: 0.760\n",
      "[epoch: 21, i:  6999]  train_loss: 0.791  |  valid_loss: 1.013\n",
      "[epoch: 21, i:  7124]  train_loss: 0.840  |  valid_loss: 0.947\n",
      "[epoch: 21, i:  7249]  train_loss: 0.847  |  valid_loss: 0.650\n",
      "[epoch: 21, i:  7374]  train_loss: 0.792  |  valid_loss: 1.049\n",
      "[epoch: 21, i:  7499]  train_loss: 0.769  |  valid_loss: 0.910\n",
      "[epoch: 21, i:  7624]  train_loss: 0.809  |  valid_loss: 0.937\n",
      "[epoch: 21, i:  7749]  train_loss: 0.865  |  valid_loss: 0.793\n",
      "[epoch: 21, i:  7874]  train_loss: 0.869  |  valid_loss: 0.855\n",
      "[epoch: 21, i:  7999]  train_loss: 0.801  |  valid_loss: 0.725\n",
      "[epoch: 21, i:  8124]  train_loss: 0.865  |  valid_loss: 0.956\n",
      "[epoch: 21, i:  8249]  train_loss: 0.812  |  valid_loss: 0.920\n",
      "[epoch: 21, i:  8374]  train_loss: 0.835  |  valid_loss: 0.864\n",
      "[epoch: 21, i:  8499]  train_loss: 0.816  |  valid_loss: 0.913\n",
      "[epoch: 21, i:  8624]  train_loss: 0.786  |  valid_loss: 0.856\n",
      "[epoch: 21, i:  8749]  train_loss: 0.828  |  valid_loss: 0.889\n",
      "[epoch: 21, i:  8874]  train_loss: 0.729  |  valid_loss: 0.829\n",
      "[epoch: 21, i:  8999]  train_loss: 0.744  |  valid_loss: 0.675\n",
      "[epoch: 21, i:  9124]  train_loss: 0.772  |  valid_loss: 0.763\n",
      "[epoch: 21, i:  9249]  train_loss: 0.883  |  valid_loss: 0.628\n",
      "[epoch: 21, i:  9374]  train_loss: 0.829  |  valid_loss: 0.901\n",
      "[epoch: 21, i:  9499]  train_loss: 0.775  |  valid_loss: 0.779\n",
      "[epoch: 21, i:  9624]  train_loss: 0.839  |  valid_loss: 0.888\n",
      "[epoch: 21, i:  9749]  train_loss: 0.853  |  valid_loss: 0.946\n",
      "[epoch: 21, i:  9874]  train_loss: 0.791  |  valid_loss: 0.897\n",
      "[epoch: 21, i:  9999]  train_loss: 0.892  |  valid_loss: 0.814\n",
      "[epoch: 21, i: 10124]  train_loss: 0.758  |  valid_loss: 0.764\n",
      "[epoch: 21, i: 10249]  train_loss: 0.834  |  valid_loss: 0.911\n",
      "[epoch: 21, i: 10374]  train_loss: 0.830  |  valid_loss: 0.764\n",
      "[epoch: 21, i: 10499]  train_loss: 0.786  |  valid_loss: 0.970\n",
      "[epoch: 21, i: 10624]  train_loss: 0.862  |  valid_loss: 0.941\n",
      "[epoch: 21, i: 10749]  train_loss: 0.838  |  valid_loss: 0.839\n",
      "[epoch: 21, i: 10874]  train_loss: 0.800  |  valid_loss: 0.960\n",
      "[epoch: 21, i: 10999]  train_loss: 0.784  |  valid_loss: 0.959\n",
      "[epoch: 21, i: 11124]  train_loss: 0.742  |  valid_loss: 0.828\n",
      "[epoch: 21, i: 11249]  train_loss: 0.805  |  valid_loss: 0.878\n",
      "[epoch: 21, i: 11374]  train_loss: 0.822  |  valid_loss: 0.806\n",
      "[epoch: 21, i: 11499]  train_loss: 0.845  |  valid_loss: 0.619\n",
      "[epoch: 21, i: 11624]  train_loss: 0.732  |  valid_loss: 0.809\n",
      "[epoch: 21, i: 11749]  train_loss: 0.851  |  valid_loss: 0.760\n",
      "[epoch: 21, i: 11874]  train_loss: 0.858  |  valid_loss: 0.789\n",
      "[epoch: 21, i: 11999]  train_loss: 0.818  |  valid_loss: 0.685\n",
      "[epoch: 21, i: 12124]  train_loss: 0.878  |  valid_loss: 0.721\n",
      "[epoch: 21, i: 12249]  train_loss: 0.873  |  valid_loss: 1.057\n",
      "[epoch: 21, i: 12374]  train_loss: 0.857  |  valid_loss: 0.879\n",
      "[epoch: 21, i: 12499]  train_loss: 0.833  |  valid_loss: 0.877\n",
      "--> [End of epoch 21] train_accuracy: 71.34%  |  valid_accuracy: 71.14%\n",
      "--> [Start of epoch 22]  lr: 0.000015\n",
      "[epoch: 22, i:   124]  train_loss: 0.772  |  valid_loss: 0.753\n",
      "[epoch: 22, i:   249]  train_loss: 0.775  |  valid_loss: 0.735\n",
      "[epoch: 22, i:   374]  train_loss: 0.820  |  valid_loss: 0.894\n",
      "[epoch: 22, i:   499]  train_loss: 0.819  |  valid_loss: 0.788\n",
      "[epoch: 22, i:   624]  train_loss: 0.807  |  valid_loss: 0.911\n",
      "[epoch: 22, i:   749]  train_loss: 0.814  |  valid_loss: 0.592\n",
      "[epoch: 22, i:   874]  train_loss: 0.842  |  valid_loss: 0.903\n",
      "[epoch: 22, i:   999]  train_loss: 0.803  |  valid_loss: 0.911\n",
      "[epoch: 22, i:  1124]  train_loss: 0.826  |  valid_loss: 0.902\n",
      "[epoch: 22, i:  1249]  train_loss: 0.850  |  valid_loss: 0.716\n",
      "[epoch: 22, i:  1374]  train_loss: 0.853  |  valid_loss: 0.789\n",
      "[epoch: 22, i:  1499]  train_loss: 0.777  |  valid_loss: 0.834\n",
      "[epoch: 22, i:  1624]  train_loss: 0.779  |  valid_loss: 0.688\n",
      "[epoch: 22, i:  1749]  train_loss: 0.774  |  valid_loss: 1.007\n",
      "[epoch: 22, i:  1874]  train_loss: 0.718  |  valid_loss: 0.761\n",
      "[epoch: 22, i:  1999]  train_loss: 0.832  |  valid_loss: 0.993\n",
      "[epoch: 22, i:  2124]  train_loss: 0.760  |  valid_loss: 0.698\n",
      "[epoch: 22, i:  2249]  train_loss: 0.761  |  valid_loss: 0.829\n",
      "[epoch: 22, i:  2374]  train_loss: 0.761  |  valid_loss: 0.772\n",
      "[epoch: 22, i:  2499]  train_loss: 0.791  |  valid_loss: 0.996\n",
      "[epoch: 22, i:  2624]  train_loss: 0.829  |  valid_loss: 0.932\n",
      "[epoch: 22, i:  2749]  train_loss: 0.817  |  valid_loss: 0.938\n",
      "[epoch: 22, i:  2874]  train_loss: 0.835  |  valid_loss: 0.983\n",
      "[epoch: 22, i:  2999]  train_loss: 0.872  |  valid_loss: 0.843\n",
      "[epoch: 22, i:  3124]  train_loss: 0.808  |  valid_loss: 0.908\n",
      "[epoch: 22, i:  3249]  train_loss: 0.789  |  valid_loss: 1.072\n",
      "[epoch: 22, i:  3374]  train_loss: 0.816  |  valid_loss: 0.822\n",
      "[epoch: 22, i:  3499]  train_loss: 0.793  |  valid_loss: 0.769\n",
      "[epoch: 22, i:  3624]  train_loss: 0.826  |  valid_loss: 0.798\n",
      "[epoch: 22, i:  3749]  train_loss: 0.815  |  valid_loss: 0.680\n",
      "[epoch: 22, i:  3874]  train_loss: 0.835  |  valid_loss: 0.806\n",
      "[epoch: 22, i:  3999]  train_loss: 0.803  |  valid_loss: 0.624\n",
      "[epoch: 22, i:  4124]  train_loss: 0.800  |  valid_loss: 0.835\n",
      "[epoch: 22, i:  4249]  train_loss: 0.861  |  valid_loss: 0.883\n",
      "[epoch: 22, i:  4374]  train_loss: 0.844  |  valid_loss: 0.896\n",
      "[epoch: 22, i:  4499]  train_loss: 0.757  |  valid_loss: 0.792\n",
      "[epoch: 22, i:  4624]  train_loss: 0.847  |  valid_loss: 0.961\n",
      "[epoch: 22, i:  4749]  train_loss: 0.836  |  valid_loss: 0.873\n",
      "[epoch: 22, i:  4874]  train_loss: 0.795  |  valid_loss: 0.786\n",
      "[epoch: 22, i:  4999]  train_loss: 0.799  |  valid_loss: 0.757\n",
      "[epoch: 22, i:  5124]  train_loss: 0.813  |  valid_loss: 0.820\n",
      "[epoch: 22, i:  5249]  train_loss: 0.764  |  valid_loss: 0.885\n",
      "[epoch: 22, i:  5374]  train_loss: 0.795  |  valid_loss: 0.625\n",
      "[epoch: 22, i:  5499]  train_loss: 0.806  |  valid_loss: 0.697\n",
      "[epoch: 22, i:  5624]  train_loss: 0.861  |  valid_loss: 0.937\n",
      "[epoch: 22, i:  5749]  train_loss: 0.782  |  valid_loss: 0.824\n",
      "[epoch: 22, i:  5874]  train_loss: 0.833  |  valid_loss: 0.713\n",
      "[epoch: 22, i:  5999]  train_loss: 0.825  |  valid_loss: 0.875\n",
      "[epoch: 22, i:  6124]  train_loss: 0.806  |  valid_loss: 0.725\n",
      "[epoch: 22, i:  6249]  train_loss: 0.786  |  valid_loss: 0.888\n",
      "[epoch: 22, i:  6374]  train_loss: 0.802  |  valid_loss: 0.731\n",
      "[epoch: 22, i:  6499]  train_loss: 0.827  |  valid_loss: 0.763\n",
      "[epoch: 22, i:  6624]  train_loss: 0.887  |  valid_loss: 0.681\n",
      "[epoch: 22, i:  6749]  train_loss: 0.844  |  valid_loss: 0.812\n",
      "[epoch: 22, i:  6874]  train_loss: 0.816  |  valid_loss: 0.783\n",
      "[epoch: 22, i:  6999]  train_loss: 0.831  |  valid_loss: 0.955\n",
      "[epoch: 22, i:  7124]  train_loss: 0.848  |  valid_loss: 0.978\n",
      "[epoch: 22, i:  7249]  train_loss: 0.839  |  valid_loss: 0.676\n",
      "[epoch: 22, i:  7374]  train_loss: 0.767  |  valid_loss: 1.036\n",
      "[epoch: 22, i:  7499]  train_loss: 0.793  |  valid_loss: 0.898\n",
      "[epoch: 22, i:  7624]  train_loss: 0.844  |  valid_loss: 0.919\n",
      "[epoch: 22, i:  7749]  train_loss: 0.879  |  valid_loss: 0.788\n",
      "[epoch: 22, i:  7874]  train_loss: 0.728  |  valid_loss: 0.884\n",
      "[epoch: 22, i:  7999]  train_loss: 0.752  |  valid_loss: 0.715\n",
      "[epoch: 22, i:  8124]  train_loss: 0.820  |  valid_loss: 0.903\n",
      "[epoch: 22, i:  8249]  train_loss: 0.790  |  valid_loss: 0.865\n",
      "[epoch: 22, i:  8374]  train_loss: 0.814  |  valid_loss: 0.853\n",
      "[epoch: 22, i:  8499]  train_loss: 0.769  |  valid_loss: 0.895\n",
      "[epoch: 22, i:  8624]  train_loss: 0.727  |  valid_loss: 0.867\n",
      "[epoch: 22, i:  8749]  train_loss: 0.833  |  valid_loss: 0.899\n",
      "[epoch: 22, i:  8874]  train_loss: 0.806  |  valid_loss: 0.844\n",
      "[epoch: 22, i:  8999]  train_loss: 0.842  |  valid_loss: 0.688\n",
      "[epoch: 22, i:  9124]  train_loss: 0.822  |  valid_loss: 0.743\n",
      "[epoch: 22, i:  9249]  train_loss: 0.778  |  valid_loss: 0.620\n",
      "[epoch: 22, i:  9374]  train_loss: 0.822  |  valid_loss: 0.937\n",
      "[epoch: 22, i:  9499]  train_loss: 0.779  |  valid_loss: 0.763\n",
      "[epoch: 22, i:  9624]  train_loss: 0.849  |  valid_loss: 0.902\n",
      "[epoch: 22, i:  9749]  train_loss: 0.835  |  valid_loss: 0.918\n",
      "[epoch: 22, i:  9874]  train_loss: 0.848  |  valid_loss: 0.890\n",
      "[epoch: 22, i:  9999]  train_loss: 0.860  |  valid_loss: 0.811\n",
      "[epoch: 22, i: 10124]  train_loss: 0.788  |  valid_loss: 0.758\n",
      "[epoch: 22, i: 10249]  train_loss: 0.841  |  valid_loss: 0.867\n",
      "[epoch: 22, i: 10374]  train_loss: 0.853  |  valid_loss: 0.753\n",
      "[epoch: 22, i: 10499]  train_loss: 0.839  |  valid_loss: 0.978\n",
      "[epoch: 22, i: 10624]  train_loss: 0.810  |  valid_loss: 0.961\n",
      "[epoch: 22, i: 10749]  train_loss: 0.864  |  valid_loss: 0.871\n",
      "[epoch: 22, i: 10874]  train_loss: 0.842  |  valid_loss: 0.961\n",
      "[epoch: 22, i: 10999]  train_loss: 0.730  |  valid_loss: 0.963\n",
      "[epoch: 22, i: 11124]  train_loss: 0.806  |  valid_loss: 0.825\n",
      "[epoch: 22, i: 11249]  train_loss: 0.831  |  valid_loss: 0.857\n",
      "[epoch: 22, i: 11374]  train_loss: 0.802  |  valid_loss: 0.808\n",
      "[epoch: 22, i: 11499]  train_loss: 0.873  |  valid_loss: 0.603\n",
      "[epoch: 22, i: 11624]  train_loss: 0.746  |  valid_loss: 0.835\n",
      "[epoch: 22, i: 11749]  train_loss: 0.867  |  valid_loss: 0.762\n",
      "[epoch: 22, i: 11874]  train_loss: 0.927  |  valid_loss: 0.780\n",
      "[epoch: 22, i: 11999]  train_loss: 0.823  |  valid_loss: 0.712\n",
      "[epoch: 22, i: 12124]  train_loss: 0.866  |  valid_loss: 0.720\n",
      "[epoch: 22, i: 12249]  train_loss: 0.797  |  valid_loss: 1.079\n",
      "[epoch: 22, i: 12374]  train_loss: 0.837  |  valid_loss: 0.888\n",
      "[epoch: 22, i: 12499]  train_loss: 0.839  |  valid_loss: 0.865\n",
      "--> [End of epoch 22] train_accuracy: 71.11%  |  valid_accuracy: 70.87%\n",
      "--> [Start of epoch 23]  lr: 0.000012\n",
      "[epoch: 23, i:   124]  train_loss: 0.798  |  valid_loss: 0.739\n",
      "[epoch: 23, i:   249]  train_loss: 0.833  |  valid_loss: 0.735\n",
      "[epoch: 23, i:   374]  train_loss: 0.778  |  valid_loss: 0.848\n",
      "[epoch: 23, i:   499]  train_loss: 0.848  |  valid_loss: 0.781\n",
      "[epoch: 23, i:   624]  train_loss: 0.753  |  valid_loss: 0.898\n",
      "[epoch: 23, i:   749]  train_loss: 0.875  |  valid_loss: 0.570\n",
      "[epoch: 23, i:   874]  train_loss: 0.803  |  valid_loss: 0.859\n",
      "[epoch: 23, i:   999]  train_loss: 0.801  |  valid_loss: 0.952\n",
      "[epoch: 23, i:  1124]  train_loss: 0.854  |  valid_loss: 0.911\n",
      "[epoch: 23, i:  1249]  train_loss: 0.857  |  valid_loss: 0.734\n",
      "[epoch: 23, i:  1374]  train_loss: 0.820  |  valid_loss: 0.776\n",
      "[epoch: 23, i:  1499]  train_loss: 0.735  |  valid_loss: 0.811\n",
      "[epoch: 23, i:  1624]  train_loss: 0.727  |  valid_loss: 0.698\n",
      "[epoch: 23, i:  1749]  train_loss: 0.771  |  valid_loss: 0.967\n",
      "[epoch: 23, i:  1874]  train_loss: 0.759  |  valid_loss: 0.766\n",
      "[epoch: 23, i:  1999]  train_loss: 0.844  |  valid_loss: 1.028\n",
      "[epoch: 23, i:  2124]  train_loss: 0.788  |  valid_loss: 0.718\n",
      "[epoch: 23, i:  2249]  train_loss: 0.762  |  valid_loss: 0.810\n",
      "[epoch: 23, i:  2374]  train_loss: 0.803  |  valid_loss: 0.783\n",
      "[epoch: 23, i:  2499]  train_loss: 0.839  |  valid_loss: 0.997\n",
      "[epoch: 23, i:  2624]  train_loss: 0.851  |  valid_loss: 0.926\n",
      "[epoch: 23, i:  2749]  train_loss: 0.806  |  valid_loss: 0.915\n",
      "[epoch: 23, i:  2874]  train_loss: 0.808  |  valid_loss: 0.967\n",
      "[epoch: 23, i:  2999]  train_loss: 0.872  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  3124]  train_loss: 0.879  |  valid_loss: 0.893\n",
      "[epoch: 23, i:  3249]  train_loss: 0.895  |  valid_loss: 1.072\n",
      "[epoch: 23, i:  3374]  train_loss: 0.791  |  valid_loss: 0.794\n",
      "[epoch: 23, i:  3499]  train_loss: 0.843  |  valid_loss: 0.757\n",
      "[epoch: 23, i:  3624]  train_loss: 0.795  |  valid_loss: 0.801\n",
      "[epoch: 23, i:  3749]  train_loss: 0.748  |  valid_loss: 0.667\n",
      "[epoch: 23, i:  3874]  train_loss: 0.800  |  valid_loss: 0.781\n",
      "[epoch: 23, i:  3999]  train_loss: 0.810  |  valid_loss: 0.631\n",
      "[epoch: 23, i:  4124]  train_loss: 0.874  |  valid_loss: 0.869\n",
      "[epoch: 23, i:  4249]  train_loss: 0.922  |  valid_loss: 0.885\n",
      "[epoch: 23, i:  4374]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 23, i:  4499]  train_loss: 0.854  |  valid_loss: 0.772\n",
      "[epoch: 23, i:  4624]  train_loss: 0.806  |  valid_loss: 0.950\n",
      "[epoch: 23, i:  4749]  train_loss: 0.806  |  valid_loss: 0.893\n",
      "[epoch: 23, i:  4874]  train_loss: 0.821  |  valid_loss: 0.745\n",
      "[epoch: 23, i:  4999]  train_loss: 0.786  |  valid_loss: 0.765\n",
      "[epoch: 23, i:  5124]  train_loss: 0.729  |  valid_loss: 0.802\n",
      "[epoch: 23, i:  5249]  train_loss: 0.868  |  valid_loss: 0.885\n",
      "[epoch: 23, i:  5374]  train_loss: 0.784  |  valid_loss: 0.638\n",
      "[epoch: 23, i:  5499]  train_loss: 0.779  |  valid_loss: 0.703\n",
      "[epoch: 23, i:  5624]  train_loss: 0.737  |  valid_loss: 0.958\n",
      "[epoch: 23, i:  5749]  train_loss: 0.810  |  valid_loss: 0.828\n",
      "[epoch: 23, i:  5874]  train_loss: 0.806  |  valid_loss: 0.724\n",
      "[epoch: 23, i:  5999]  train_loss: 0.841  |  valid_loss: 0.854\n",
      "[epoch: 23, i:  6124]  train_loss: 0.804  |  valid_loss: 0.771\n",
      "[epoch: 23, i:  6249]  train_loss: 0.838  |  valid_loss: 0.889\n",
      "[epoch: 23, i:  6374]  train_loss: 0.763  |  valid_loss: 0.746\n",
      "[epoch: 23, i:  6499]  train_loss: 0.732  |  valid_loss: 0.760\n",
      "[epoch: 23, i:  6624]  train_loss: 0.813  |  valid_loss: 0.691\n",
      "[epoch: 23, i:  6749]  train_loss: 0.796  |  valid_loss: 0.797\n",
      "[epoch: 23, i:  6874]  train_loss: 0.784  |  valid_loss: 0.776\n",
      "[epoch: 23, i:  6999]  train_loss: 0.821  |  valid_loss: 0.955\n",
      "[epoch: 23, i:  7124]  train_loss: 0.749  |  valid_loss: 1.012\n",
      "[epoch: 23, i:  7249]  train_loss: 0.787  |  valid_loss: 0.637\n",
      "[epoch: 23, i:  7374]  train_loss: 0.937  |  valid_loss: 1.040\n",
      "[epoch: 23, i:  7499]  train_loss: 0.795  |  valid_loss: 0.909\n",
      "[epoch: 23, i:  7624]  train_loss: 0.787  |  valid_loss: 0.876\n",
      "[epoch: 23, i:  7749]  train_loss: 0.751  |  valid_loss: 0.819\n",
      "[epoch: 23, i:  7874]  train_loss: 0.805  |  valid_loss: 0.879\n",
      "[epoch: 23, i:  7999]  train_loss: 0.807  |  valid_loss: 0.729\n",
      "[epoch: 23, i:  8124]  train_loss: 0.832  |  valid_loss: 0.892\n",
      "[epoch: 23, i:  8249]  train_loss: 0.849  |  valid_loss: 0.892\n",
      "[epoch: 23, i:  8374]  train_loss: 0.920  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  8499]  train_loss: 0.871  |  valid_loss: 0.930\n",
      "[epoch: 23, i:  8624]  train_loss: 0.860  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  8749]  train_loss: 0.772  |  valid_loss: 0.891\n",
      "[epoch: 23, i:  8874]  train_loss: 0.840  |  valid_loss: 0.825\n",
      "[epoch: 23, i:  8999]  train_loss: 0.799  |  valid_loss: 0.706\n",
      "[epoch: 23, i:  9124]  train_loss: 0.793  |  valid_loss: 0.736\n",
      "[epoch: 23, i:  9249]  train_loss: 0.795  |  valid_loss: 0.619\n",
      "[epoch: 23, i:  9374]  train_loss: 0.736  |  valid_loss: 0.907\n",
      "[epoch: 23, i:  9499]  train_loss: 0.787  |  valid_loss: 0.777\n",
      "[epoch: 23, i:  9624]  train_loss: 0.853  |  valid_loss: 0.877\n",
      "[epoch: 23, i:  9749]  train_loss: 0.806  |  valid_loss: 0.914\n",
      "[epoch: 23, i:  9874]  train_loss: 0.743  |  valid_loss: 0.922\n",
      "[epoch: 23, i:  9999]  train_loss: 0.857  |  valid_loss: 0.828\n",
      "[epoch: 23, i: 10124]  train_loss: 0.773  |  valid_loss: 0.758\n",
      "[epoch: 23, i: 10249]  train_loss: 0.836  |  valid_loss: 0.853\n",
      "[epoch: 23, i: 10374]  train_loss: 0.874  |  valid_loss: 0.766\n",
      "[epoch: 23, i: 10499]  train_loss: 0.882  |  valid_loss: 0.955\n",
      "[epoch: 23, i: 10624]  train_loss: 0.794  |  valid_loss: 0.921\n",
      "[epoch: 23, i: 10749]  train_loss: 0.875  |  valid_loss: 0.818\n",
      "[epoch: 23, i: 10874]  train_loss: 0.856  |  valid_loss: 0.946\n",
      "[epoch: 23, i: 10999]  train_loss: 0.779  |  valid_loss: 0.972\n",
      "[epoch: 23, i: 11124]  train_loss: 0.909  |  valid_loss: 0.845\n",
      "[epoch: 23, i: 11249]  train_loss: 0.832  |  valid_loss: 0.868\n",
      "[epoch: 23, i: 11374]  train_loss: 0.806  |  valid_loss: 0.796\n",
      "[epoch: 23, i: 11499]  train_loss: 0.790  |  valid_loss: 0.618\n",
      "[epoch: 23, i: 11624]  train_loss: 0.768  |  valid_loss: 0.812\n",
      "[epoch: 23, i: 11749]  train_loss: 0.789  |  valid_loss: 0.749\n",
      "[epoch: 23, i: 11874]  train_loss: 0.811  |  valid_loss: 0.792\n",
      "[epoch: 23, i: 11999]  train_loss: 0.866  |  valid_loss: 0.683\n",
      "[epoch: 23, i: 12124]  train_loss: 0.842  |  valid_loss: 0.716\n",
      "[epoch: 23, i: 12249]  train_loss: 0.809  |  valid_loss: 1.096\n",
      "[epoch: 23, i: 12374]  train_loss: 0.844  |  valid_loss: 0.886\n",
      "[epoch: 23, i: 12499]  train_loss: 0.785  |  valid_loss: 0.864\n",
      "--> [End of epoch 23] train_accuracy: 71.31%  |  valid_accuracy: 70.91%\n",
      "--> [Start of epoch 24]  lr: 0.000009\n",
      "[epoch: 24, i:   124]  train_loss: 0.802  |  valid_loss: 0.738\n",
      "[epoch: 24, i:   249]  train_loss: 0.802  |  valid_loss: 0.702\n",
      "[epoch: 24, i:   374]  train_loss: 0.784  |  valid_loss: 0.898\n",
      "[epoch: 24, i:   499]  train_loss: 0.818  |  valid_loss: 0.795\n",
      "[epoch: 24, i:   624]  train_loss: 0.754  |  valid_loss: 0.921\n",
      "[epoch: 24, i:   749]  train_loss: 0.830  |  valid_loss: 0.576\n",
      "[epoch: 24, i:   874]  train_loss: 0.819  |  valid_loss: 0.855\n",
      "[epoch: 24, i:   999]  train_loss: 0.857  |  valid_loss: 0.945\n",
      "[epoch: 24, i:  1124]  train_loss: 0.719  |  valid_loss: 0.949\n",
      "[epoch: 24, i:  1249]  train_loss: 0.788  |  valid_loss: 0.714\n",
      "[epoch: 24, i:  1374]  train_loss: 0.817  |  valid_loss: 0.791\n",
      "[epoch: 24, i:  1499]  train_loss: 0.798  |  valid_loss: 0.807\n",
      "[epoch: 24, i:  1624]  train_loss: 0.812  |  valid_loss: 0.689\n",
      "[epoch: 24, i:  1749]  train_loss: 0.863  |  valid_loss: 0.999\n",
      "[epoch: 24, i:  1874]  train_loss: 0.916  |  valid_loss: 0.790\n",
      "[epoch: 24, i:  1999]  train_loss: 0.849  |  valid_loss: 0.961\n",
      "[epoch: 24, i:  2124]  train_loss: 0.860  |  valid_loss: 0.726\n",
      "[epoch: 24, i:  2249]  train_loss: 0.851  |  valid_loss: 0.820\n",
      "[epoch: 24, i:  2374]  train_loss: 0.781  |  valid_loss: 0.757\n",
      "[epoch: 24, i:  2499]  train_loss: 0.796  |  valid_loss: 1.007\n",
      "[epoch: 24, i:  2624]  train_loss: 0.887  |  valid_loss: 0.923\n",
      "[epoch: 24, i:  2749]  train_loss: 0.774  |  valid_loss: 0.955\n",
      "[epoch: 24, i:  2874]  train_loss: 0.834  |  valid_loss: 1.007\n",
      "[epoch: 24, i:  2999]  train_loss: 0.854  |  valid_loss: 0.846\n",
      "[epoch: 24, i:  3124]  train_loss: 0.794  |  valid_loss: 0.896\n",
      "[epoch: 24, i:  3249]  train_loss: 0.763  |  valid_loss: 1.039\n",
      "[epoch: 24, i:  3374]  train_loss: 0.905  |  valid_loss: 0.786\n",
      "[epoch: 24, i:  3499]  train_loss: 0.840  |  valid_loss: 0.758\n",
      "[epoch: 24, i:  3624]  train_loss: 0.841  |  valid_loss: 0.802\n",
      "[epoch: 24, i:  3749]  train_loss: 0.786  |  valid_loss: 0.693\n",
      "[epoch: 24, i:  3874]  train_loss: 0.810  |  valid_loss: 0.829\n",
      "[epoch: 24, i:  3999]  train_loss: 0.841  |  valid_loss: 0.625\n",
      "[epoch: 24, i:  4124]  train_loss: 0.849  |  valid_loss: 0.851\n",
      "[epoch: 24, i:  4249]  train_loss: 0.814  |  valid_loss: 0.907\n",
      "[epoch: 24, i:  4374]  train_loss: 0.834  |  valid_loss: 0.890\n",
      "[epoch: 24, i:  4499]  train_loss: 0.818  |  valid_loss: 0.792\n",
      "[epoch: 24, i:  4624]  train_loss: 0.820  |  valid_loss: 0.931\n",
      "[epoch: 24, i:  4749]  train_loss: 0.726  |  valid_loss: 0.887\n",
      "[epoch: 24, i:  4874]  train_loss: 0.798  |  valid_loss: 0.750\n",
      "[epoch: 24, i:  4999]  train_loss: 0.751  |  valid_loss: 0.766\n",
      "[epoch: 24, i:  5124]  train_loss: 0.822  |  valid_loss: 0.840\n",
      "[epoch: 24, i:  5249]  train_loss: 0.807  |  valid_loss: 0.898\n",
      "[epoch: 24, i:  5374]  train_loss: 0.839  |  valid_loss: 0.628\n",
      "[epoch: 24, i:  5499]  train_loss: 0.783  |  valid_loss: 0.710\n",
      "[epoch: 24, i:  5624]  train_loss: 0.849  |  valid_loss: 0.885\n",
      "[epoch: 24, i:  5749]  train_loss: 0.713  |  valid_loss: 0.828\n",
      "[epoch: 24, i:  5874]  train_loss: 0.866  |  valid_loss: 0.719\n",
      "[epoch: 24, i:  5999]  train_loss: 0.859  |  valid_loss: 0.856\n",
      "[epoch: 24, i:  6124]  train_loss: 0.831  |  valid_loss: 0.769\n",
      "[epoch: 24, i:  6249]  train_loss: 0.715  |  valid_loss: 0.893\n",
      "[epoch: 24, i:  6374]  train_loss: 0.789  |  valid_loss: 0.749\n",
      "[epoch: 24, i:  6499]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 24, i:  6624]  train_loss: 0.804  |  valid_loss: 0.691\n",
      "[epoch: 24, i:  6749]  train_loss: 0.761  |  valid_loss: 0.807\n",
      "[epoch: 24, i:  6874]  train_loss: 0.724  |  valid_loss: 0.763\n",
      "[epoch: 24, i:  6999]  train_loss: 0.781  |  valid_loss: 0.954\n",
      "[epoch: 24, i:  7124]  train_loss: 0.833  |  valid_loss: 0.968\n",
      "[epoch: 24, i:  7249]  train_loss: 0.765  |  valid_loss: 0.665\n",
      "[epoch: 24, i:  7374]  train_loss: 0.780  |  valid_loss: 1.041\n",
      "[epoch: 24, i:  7499]  train_loss: 0.808  |  valid_loss: 0.926\n",
      "[epoch: 24, i:  7624]  train_loss: 0.815  |  valid_loss: 0.923\n",
      "[epoch: 24, i:  7749]  train_loss: 0.758  |  valid_loss: 0.798\n",
      "[epoch: 24, i:  7874]  train_loss: 0.726  |  valid_loss: 0.886\n",
      "[epoch: 24, i:  7999]  train_loss: 0.800  |  valid_loss: 0.710\n",
      "[epoch: 24, i:  8124]  train_loss: 0.824  |  valid_loss: 0.893\n",
      "[epoch: 24, i:  8249]  train_loss: 0.850  |  valid_loss: 0.899\n",
      "[epoch: 24, i:  8374]  train_loss: 0.796  |  valid_loss: 0.840\n",
      "[epoch: 24, i:  8499]  train_loss: 0.802  |  valid_loss: 0.934\n",
      "[epoch: 24, i:  8624]  train_loss: 0.794  |  valid_loss: 0.819\n",
      "[epoch: 24, i:  8749]  train_loss: 0.889  |  valid_loss: 0.891\n",
      "[epoch: 24, i:  8874]  train_loss: 0.837  |  valid_loss: 0.867\n",
      "[epoch: 24, i:  8999]  train_loss: 0.835  |  valid_loss: 0.688\n",
      "[epoch: 24, i:  9124]  train_loss: 0.765  |  valid_loss: 0.774\n",
      "[epoch: 24, i:  9249]  train_loss: 0.871  |  valid_loss: 0.623\n",
      "[epoch: 24, i:  9374]  train_loss: 0.766  |  valid_loss: 0.910\n",
      "[epoch: 24, i:  9499]  train_loss: 0.802  |  valid_loss: 0.747\n",
      "[epoch: 24, i:  9624]  train_loss: 0.821  |  valid_loss: 0.889\n",
      "[epoch: 24, i:  9749]  train_loss: 0.873  |  valid_loss: 0.913\n",
      "[epoch: 24, i:  9874]  train_loss: 0.785  |  valid_loss: 0.922\n",
      "[epoch: 24, i:  9999]  train_loss: 0.769  |  valid_loss: 0.817\n",
      "[epoch: 24, i: 10124]  train_loss: 0.816  |  valid_loss: 0.762\n",
      "[epoch: 24, i: 10249]  train_loss: 0.795  |  valid_loss: 0.882\n",
      "[epoch: 24, i: 10374]  train_loss: 0.734  |  valid_loss: 0.741\n",
      "[epoch: 24, i: 10499]  train_loss: 0.816  |  valid_loss: 0.953\n",
      "[epoch: 24, i: 10624]  train_loss: 0.856  |  valid_loss: 0.940\n",
      "[epoch: 24, i: 10749]  train_loss: 0.815  |  valid_loss: 0.860\n",
      "[epoch: 24, i: 10874]  train_loss: 0.890  |  valid_loss: 0.955\n",
      "[epoch: 24, i: 10999]  train_loss: 0.784  |  valid_loss: 0.971\n",
      "[epoch: 24, i: 11124]  train_loss: 0.819  |  valid_loss: 0.839\n",
      "[epoch: 24, i: 11249]  train_loss: 0.841  |  valid_loss: 0.866\n",
      "[epoch: 24, i: 11374]  train_loss: 0.801  |  valid_loss: 0.797\n",
      "[epoch: 24, i: 11499]  train_loss: 0.870  |  valid_loss: 0.605\n",
      "[epoch: 24, i: 11624]  train_loss: 0.827  |  valid_loss: 0.800\n",
      "[epoch: 24, i: 11749]  train_loss: 0.823  |  valid_loss: 0.733\n",
      "[epoch: 24, i: 11874]  train_loss: 0.813  |  valid_loss: 0.793\n",
      "[epoch: 24, i: 11999]  train_loss: 0.827  |  valid_loss: 0.687\n",
      "[epoch: 24, i: 12124]  train_loss: 0.802  |  valid_loss: 0.733\n",
      "[epoch: 24, i: 12249]  train_loss: 0.832  |  valid_loss: 1.057\n",
      "[epoch: 24, i: 12374]  train_loss: 0.780  |  valid_loss: 0.861\n",
      "[epoch: 24, i: 12499]  train_loss: 0.836  |  valid_loss: 0.876\n",
      "--> [End of epoch 24] train_accuracy: 71.46%  |  valid_accuracy: 71.06%\n",
      "--> [Start of epoch 25]  lr: 0.000008\n",
      "[epoch: 25, i:   124]  train_loss: 0.831  |  valid_loss: 0.751\n",
      "[epoch: 25, i:   249]  train_loss: 0.767  |  valid_loss: 0.719\n",
      "[epoch: 25, i:   374]  train_loss: 0.779  |  valid_loss: 0.876\n",
      "[epoch: 25, i:   499]  train_loss: 0.895  |  valid_loss: 0.793\n",
      "[epoch: 25, i:   624]  train_loss: 0.840  |  valid_loss: 0.915\n",
      "[epoch: 25, i:   749]  train_loss: 0.793  |  valid_loss: 0.576\n",
      "[epoch: 25, i:   874]  train_loss: 0.844  |  valid_loss: 0.873\n",
      "[epoch: 25, i:   999]  train_loss: 0.828  |  valid_loss: 0.931\n",
      "[epoch: 25, i:  1124]  train_loss: 0.851  |  valid_loss: 0.910\n",
      "[epoch: 25, i:  1249]  train_loss: 0.803  |  valid_loss: 0.737\n",
      "[epoch: 25, i:  1374]  train_loss: 0.787  |  valid_loss: 0.764\n",
      "[epoch: 25, i:  1499]  train_loss: 0.837  |  valid_loss: 0.840\n",
      "[epoch: 25, i:  1624]  train_loss: 0.838  |  valid_loss: 0.676\n",
      "[epoch: 25, i:  1749]  train_loss: 0.777  |  valid_loss: 0.947\n",
      "[epoch: 25, i:  1874]  train_loss: 0.829  |  valid_loss: 0.761\n",
      "[epoch: 25, i:  1999]  train_loss: 0.854  |  valid_loss: 1.008\n",
      "[epoch: 25, i:  2124]  train_loss: 0.860  |  valid_loss: 0.703\n",
      "[epoch: 25, i:  2249]  train_loss: 0.815  |  valid_loss: 0.821\n",
      "[epoch: 25, i:  2374]  train_loss: 0.794  |  valid_loss: 0.767\n",
      "[epoch: 25, i:  2499]  train_loss: 0.768  |  valid_loss: 0.995\n",
      "[epoch: 25, i:  2624]  train_loss: 0.749  |  valid_loss: 0.935\n",
      "[epoch: 25, i:  2749]  train_loss: 0.826  |  valid_loss: 0.955\n",
      "[epoch: 25, i:  2874]  train_loss: 0.854  |  valid_loss: 1.005\n",
      "[epoch: 25, i:  2999]  train_loss: 0.885  |  valid_loss: 0.834\n",
      "[epoch: 25, i:  3124]  train_loss: 0.834  |  valid_loss: 0.902\n",
      "[epoch: 25, i:  3249]  train_loss: 0.854  |  valid_loss: 1.066\n",
      "[epoch: 25, i:  3374]  train_loss: 0.843  |  valid_loss: 0.779\n",
      "[epoch: 25, i:  3499]  train_loss: 0.784  |  valid_loss: 0.748\n",
      "[epoch: 25, i:  3624]  train_loss: 0.819  |  valid_loss: 0.780\n",
      "[epoch: 25, i:  3749]  train_loss: 0.814  |  valid_loss: 0.685\n",
      "[epoch: 25, i:  3874]  train_loss: 0.742  |  valid_loss: 0.807\n",
      "[epoch: 25, i:  3999]  train_loss: 0.905  |  valid_loss: 0.660\n",
      "[epoch: 25, i:  4124]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 25, i:  4249]  train_loss: 0.797  |  valid_loss: 0.874\n",
      "[epoch: 25, i:  4374]  train_loss: 0.849  |  valid_loss: 0.872\n",
      "[epoch: 25, i:  4499]  train_loss: 0.810  |  valid_loss: 0.794\n",
      "[epoch: 25, i:  4624]  train_loss: 0.794  |  valid_loss: 0.947\n",
      "[epoch: 25, i:  4749]  train_loss: 0.808  |  valid_loss: 0.893\n",
      "[epoch: 25, i:  4874]  train_loss: 0.805  |  valid_loss: 0.741\n",
      "[epoch: 25, i:  4999]  train_loss: 0.789  |  valid_loss: 0.725\n",
      "[epoch: 25, i:  5124]  train_loss: 0.814  |  valid_loss: 0.804\n",
      "[epoch: 25, i:  5249]  train_loss: 0.762  |  valid_loss: 0.880\n",
      "[epoch: 25, i:  5374]  train_loss: 0.807  |  valid_loss: 0.620\n",
      "[epoch: 25, i:  5499]  train_loss: 0.771  |  valid_loss: 0.714\n",
      "[epoch: 25, i:  5624]  train_loss: 0.801  |  valid_loss: 0.901\n",
      "[epoch: 25, i:  5749]  train_loss: 0.842  |  valid_loss: 0.820\n",
      "[epoch: 25, i:  5874]  train_loss: 0.778  |  valid_loss: 0.739\n",
      "[epoch: 25, i:  5999]  train_loss: 0.782  |  valid_loss: 0.873\n",
      "[epoch: 25, i:  6124]  train_loss: 0.827  |  valid_loss: 0.749\n",
      "[epoch: 25, i:  6249]  train_loss: 0.804  |  valid_loss: 0.851\n",
      "[epoch: 25, i:  6374]  train_loss: 0.850  |  valid_loss: 0.745\n",
      "[epoch: 25, i:  6499]  train_loss: 0.798  |  valid_loss: 0.801\n",
      "[epoch: 25, i:  6624]  train_loss: 0.850  |  valid_loss: 0.673\n",
      "[epoch: 25, i:  6749]  train_loss: 0.808  |  valid_loss: 0.820\n",
      "[epoch: 25, i:  6874]  train_loss: 0.753  |  valid_loss: 0.753\n",
      "[epoch: 25, i:  6999]  train_loss: 0.815  |  valid_loss: 0.964\n",
      "[epoch: 25, i:  7124]  train_loss: 0.829  |  valid_loss: 0.972\n",
      "[epoch: 25, i:  7249]  train_loss: 0.829  |  valid_loss: 0.659\n",
      "[epoch: 25, i:  7374]  train_loss: 0.902  |  valid_loss: 1.044\n",
      "[epoch: 25, i:  7499]  train_loss: 0.800  |  valid_loss: 0.903\n",
      "[epoch: 25, i:  7624]  train_loss: 0.781  |  valid_loss: 0.901\n",
      "[epoch: 25, i:  7749]  train_loss: 0.878  |  valid_loss: 0.798\n",
      "[epoch: 25, i:  7874]  train_loss: 0.809  |  valid_loss: 0.868\n",
      "[epoch: 25, i:  7999]  train_loss: 0.770  |  valid_loss: 0.710\n",
      "[epoch: 25, i:  8124]  train_loss: 0.821  |  valid_loss: 0.903\n",
      "[epoch: 25, i:  8249]  train_loss: 0.821  |  valid_loss: 0.888\n",
      "[epoch: 25, i:  8374]  train_loss: 0.834  |  valid_loss: 0.832\n",
      "[epoch: 25, i:  8499]  train_loss: 0.748  |  valid_loss: 0.919\n",
      "[epoch: 25, i:  8624]  train_loss: 0.857  |  valid_loss: 0.869\n",
      "[epoch: 25, i:  8749]  train_loss: 0.800  |  valid_loss: 0.888\n",
      "[epoch: 25, i:  8874]  train_loss: 0.798  |  valid_loss: 0.839\n",
      "[epoch: 25, i:  8999]  train_loss: 0.843  |  valid_loss: 0.704\n",
      "[epoch: 25, i:  9124]  train_loss: 0.841  |  valid_loss: 0.716\n",
      "[epoch: 25, i:  9249]  train_loss: 0.793  |  valid_loss: 0.618\n",
      "[epoch: 25, i:  9374]  train_loss: 0.795  |  valid_loss: 0.920\n",
      "[epoch: 25, i:  9499]  train_loss: 0.761  |  valid_loss: 0.747\n",
      "[epoch: 25, i:  9624]  train_loss: 0.863  |  valid_loss: 0.872\n",
      "[epoch: 25, i:  9749]  train_loss: 0.868  |  valid_loss: 0.931\n",
      "[epoch: 25, i:  9874]  train_loss: 0.683  |  valid_loss: 0.891\n",
      "[epoch: 25, i:  9999]  train_loss: 0.794  |  valid_loss: 0.822\n",
      "[epoch: 25, i: 10124]  train_loss: 0.867  |  valid_loss: 0.773\n",
      "[epoch: 25, i: 10249]  train_loss: 0.793  |  valid_loss: 0.856\n",
      "[epoch: 25, i: 10374]  train_loss: 0.910  |  valid_loss: 0.768\n",
      "[epoch: 25, i: 10499]  train_loss: 0.866  |  valid_loss: 0.991\n",
      "[epoch: 25, i: 10624]  train_loss: 0.738  |  valid_loss: 0.910\n",
      "[epoch: 25, i: 10749]  train_loss: 0.853  |  valid_loss: 0.825\n",
      "[epoch: 25, i: 10874]  train_loss: 0.817  |  valid_loss: 0.976\n",
      "[epoch: 25, i: 10999]  train_loss: 0.720  |  valid_loss: 0.975\n",
      "[epoch: 25, i: 11124]  train_loss: 0.819  |  valid_loss: 0.830\n",
      "[epoch: 25, i: 11249]  train_loss: 0.860  |  valid_loss: 0.867\n",
      "[epoch: 25, i: 11374]  train_loss: 0.775  |  valid_loss: 0.794\n",
      "[epoch: 25, i: 11499]  train_loss: 0.797  |  valid_loss: 0.594\n",
      "[epoch: 25, i: 11624]  train_loss: 0.740  |  valid_loss: 0.783\n",
      "[epoch: 25, i: 11749]  train_loss: 0.790  |  valid_loss: 0.739\n",
      "[epoch: 25, i: 11874]  train_loss: 0.768  |  valid_loss: 0.771\n",
      "[epoch: 25, i: 11999]  train_loss: 0.793  |  valid_loss: 0.691\n",
      "[epoch: 25, i: 12124]  train_loss: 0.810  |  valid_loss: 0.716\n",
      "[epoch: 25, i: 12249]  train_loss: 0.799  |  valid_loss: 1.079\n",
      "[epoch: 25, i: 12374]  train_loss: 0.821  |  valid_loss: 0.881\n",
      "[epoch: 25, i: 12499]  train_loss: 0.857  |  valid_loss: 0.860\n",
      "--> [End of epoch 25] train_accuracy: 71.20%  |  valid_accuracy: 71.08%\n",
      "--> [Start of epoch 26]  lr: 0.000006\n",
      "[epoch: 26, i:   124]  train_loss: 0.774  |  valid_loss: 0.719\n",
      "[epoch: 26, i:   249]  train_loss: 0.767  |  valid_loss: 0.731\n",
      "[epoch: 26, i:   374]  train_loss: 0.784  |  valid_loss: 0.874\n",
      "[epoch: 26, i:   499]  train_loss: 0.762  |  valid_loss: 0.793\n",
      "[epoch: 26, i:   624]  train_loss: 0.807  |  valid_loss: 0.924\n",
      "[epoch: 26, i:   749]  train_loss: 0.832  |  valid_loss: 0.571\n",
      "[epoch: 26, i:   874]  train_loss: 0.807  |  valid_loss: 0.862\n",
      "[epoch: 26, i:   999]  train_loss: 0.690  |  valid_loss: 0.901\n",
      "[epoch: 26, i:  1124]  train_loss: 0.811  |  valid_loss: 0.925\n",
      "[epoch: 26, i:  1249]  train_loss: 0.819  |  valid_loss: 0.757\n",
      "[epoch: 26, i:  1374]  train_loss: 0.756  |  valid_loss: 0.791\n",
      "[epoch: 26, i:  1499]  train_loss: 0.818  |  valid_loss: 0.826\n",
      "[epoch: 26, i:  1624]  train_loss: 0.893  |  valid_loss: 0.671\n",
      "[epoch: 26, i:  1749]  train_loss: 0.809  |  valid_loss: 0.957\n",
      "[epoch: 26, i:  1874]  train_loss: 0.812  |  valid_loss: 0.775\n",
      "[epoch: 26, i:  1999]  train_loss: 0.900  |  valid_loss: 1.002\n",
      "[epoch: 26, i:  2124]  train_loss: 0.810  |  valid_loss: 0.723\n",
      "[epoch: 26, i:  2249]  train_loss: 0.815  |  valid_loss: 0.813\n",
      "[epoch: 26, i:  2374]  train_loss: 0.859  |  valid_loss: 0.760\n",
      "[epoch: 26, i:  2499]  train_loss: 0.812  |  valid_loss: 1.008\n",
      "[epoch: 26, i:  2624]  train_loss: 0.831  |  valid_loss: 0.918\n",
      "[epoch: 26, i:  2749]  train_loss: 0.807  |  valid_loss: 0.922\n",
      "[epoch: 26, i:  2874]  train_loss: 0.818  |  valid_loss: 1.026\n",
      "[epoch: 26, i:  2999]  train_loss: 0.838  |  valid_loss: 0.824\n",
      "[epoch: 26, i:  3124]  train_loss: 0.849  |  valid_loss: 0.920\n",
      "[epoch: 26, i:  3249]  train_loss: 0.837  |  valid_loss: 1.039\n",
      "[epoch: 26, i:  3374]  train_loss: 0.843  |  valid_loss: 0.808\n",
      "[epoch: 26, i:  3499]  train_loss: 0.751  |  valid_loss: 0.744\n",
      "[epoch: 26, i:  3624]  train_loss: 0.741  |  valid_loss: 0.780\n",
      "[epoch: 26, i:  3749]  train_loss: 0.794  |  valid_loss: 0.691\n",
      "[epoch: 26, i:  3874]  train_loss: 0.763  |  valid_loss: 0.791\n",
      "[epoch: 26, i:  3999]  train_loss: 0.859  |  valid_loss: 0.642\n",
      "[epoch: 26, i:  4124]  train_loss: 0.886  |  valid_loss: 0.831\n",
      "[epoch: 26, i:  4249]  train_loss: 0.817  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  4374]  train_loss: 0.770  |  valid_loss: 0.859\n",
      "[epoch: 26, i:  4499]  train_loss: 0.760  |  valid_loss: 0.801\n",
      "[epoch: 26, i:  4624]  train_loss: 0.860  |  valid_loss: 0.929\n",
      "[epoch: 26, i:  4749]  train_loss: 0.800  |  valid_loss: 0.868\n",
      "[epoch: 26, i:  4874]  train_loss: 0.805  |  valid_loss: 0.749\n",
      "[epoch: 26, i:  4999]  train_loss: 0.881  |  valid_loss: 0.742\n",
      "[epoch: 26, i:  5124]  train_loss: 0.799  |  valid_loss: 0.832\n",
      "[epoch: 26, i:  5249]  train_loss: 0.781  |  valid_loss: 0.920\n",
      "[epoch: 26, i:  5374]  train_loss: 0.819  |  valid_loss: 0.607\n",
      "[epoch: 26, i:  5499]  train_loss: 0.731  |  valid_loss: 0.708\n",
      "[epoch: 26, i:  5624]  train_loss: 0.831  |  valid_loss: 0.994\n",
      "[epoch: 26, i:  5749]  train_loss: 0.822  |  valid_loss: 0.814\n",
      "[epoch: 26, i:  5874]  train_loss: 0.879  |  valid_loss: 0.710\n",
      "[epoch: 26, i:  5999]  train_loss: 0.829  |  valid_loss: 0.839\n",
      "[epoch: 26, i:  6124]  train_loss: 0.825  |  valid_loss: 0.733\n",
      "[epoch: 26, i:  6249]  train_loss: 0.874  |  valid_loss: 0.894\n",
      "[epoch: 26, i:  6374]  train_loss: 0.843  |  valid_loss: 0.707\n",
      "[epoch: 26, i:  6499]  train_loss: 0.811  |  valid_loss: 0.799\n",
      "[epoch: 26, i:  6624]  train_loss: 0.784  |  valid_loss: 0.673\n",
      "[epoch: 26, i:  6749]  train_loss: 0.765  |  valid_loss: 0.839\n",
      "[epoch: 26, i:  6874]  train_loss: 0.861  |  valid_loss: 0.762\n",
      "[epoch: 26, i:  6999]  train_loss: 0.801  |  valid_loss: 0.936\n",
      "[epoch: 26, i:  7124]  train_loss: 0.761  |  valid_loss: 0.981\n",
      "[epoch: 26, i:  7249]  train_loss: 0.836  |  valid_loss: 0.646\n",
      "[epoch: 26, i:  7374]  train_loss: 0.817  |  valid_loss: 1.057\n",
      "[epoch: 26, i:  7499]  train_loss: 0.862  |  valid_loss: 0.912\n",
      "[epoch: 26, i:  7624]  train_loss: 0.843  |  valid_loss: 0.909\n",
      "[epoch: 26, i:  7749]  train_loss: 0.843  |  valid_loss: 0.800\n",
      "[epoch: 26, i:  7874]  train_loss: 0.830  |  valid_loss: 0.868\n",
      "[epoch: 26, i:  7999]  train_loss: 0.850  |  valid_loss: 0.716\n",
      "[epoch: 26, i:  8124]  train_loss: 0.780  |  valid_loss: 0.912\n",
      "[epoch: 26, i:  8249]  train_loss: 0.745  |  valid_loss: 0.896\n",
      "[epoch: 26, i:  8374]  train_loss: 0.815  |  valid_loss: 0.842\n",
      "[epoch: 26, i:  8499]  train_loss: 0.830  |  valid_loss: 0.902\n",
      "[epoch: 26, i:  8624]  train_loss: 0.762  |  valid_loss: 0.863\n",
      "[epoch: 26, i:  8749]  train_loss: 0.790  |  valid_loss: 0.886\n",
      "[epoch: 26, i:  8874]  train_loss: 0.758  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  8999]  train_loss: 0.864  |  valid_loss: 0.694\n",
      "[epoch: 26, i:  9124]  train_loss: 0.842  |  valid_loss: 0.765\n",
      "[epoch: 26, i:  9249]  train_loss: 0.808  |  valid_loss: 0.601\n",
      "[epoch: 26, i:  9374]  train_loss: 0.829  |  valid_loss: 0.897\n",
      "[epoch: 26, i:  9499]  train_loss: 0.831  |  valid_loss: 0.731\n",
      "[epoch: 26, i:  9624]  train_loss: 0.833  |  valid_loss: 0.864\n",
      "[epoch: 26, i:  9749]  train_loss: 0.842  |  valid_loss: 0.953\n",
      "[epoch: 26, i:  9874]  train_loss: 0.771  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  9999]  train_loss: 0.813  |  valid_loss: 0.807\n",
      "[epoch: 26, i: 10124]  train_loss: 0.879  |  valid_loss: 0.782\n",
      "[epoch: 26, i: 10249]  train_loss: 0.774  |  valid_loss: 0.890\n",
      "[epoch: 26, i: 10374]  train_loss: 0.783  |  valid_loss: 0.750\n",
      "[epoch: 26, i: 10499]  train_loss: 0.780  |  valid_loss: 0.953\n",
      "[epoch: 26, i: 10624]  train_loss: 0.848  |  valid_loss: 0.921\n",
      "[epoch: 26, i: 10749]  train_loss: 0.859  |  valid_loss: 0.835\n",
      "[epoch: 26, i: 10874]  train_loss: 0.849  |  valid_loss: 0.964\n",
      "[epoch: 26, i: 10999]  train_loss: 0.702  |  valid_loss: 0.935\n",
      "[epoch: 26, i: 11124]  train_loss: 0.711  |  valid_loss: 0.835\n",
      "[epoch: 26, i: 11249]  train_loss: 0.765  |  valid_loss: 0.867\n",
      "[epoch: 26, i: 11374]  train_loss: 0.796  |  valid_loss: 0.785\n",
      "[epoch: 26, i: 11499]  train_loss: 0.816  |  valid_loss: 0.629\n",
      "[epoch: 26, i: 11624]  train_loss: 0.856  |  valid_loss: 0.790\n",
      "[epoch: 26, i: 11749]  train_loss: 0.806  |  valid_loss: 0.739\n",
      "[epoch: 26, i: 11874]  train_loss: 0.836  |  valid_loss: 0.779\n",
      "[epoch: 26, i: 11999]  train_loss: 0.822  |  valid_loss: 0.711\n",
      "[epoch: 26, i: 12124]  train_loss: 0.895  |  valid_loss: 0.731\n",
      "[epoch: 26, i: 12249]  train_loss: 0.925  |  valid_loss: 1.053\n",
      "[epoch: 26, i: 12374]  train_loss: 0.816  |  valid_loss: 0.878\n",
      "[epoch: 26, i: 12499]  train_loss: 0.819  |  valid_loss: 0.844\n",
      "--> [End of epoch 26] train_accuracy: 71.37%  |  valid_accuracy: 70.93%\n",
      "--> [Start of epoch 27]  lr: 0.000005\n",
      "[epoch: 27, i:   124]  train_loss: 0.800  |  valid_loss: 0.731\n",
      "[epoch: 27, i:   249]  train_loss: 0.863  |  valid_loss: 0.730\n",
      "[epoch: 27, i:   374]  train_loss: 0.741  |  valid_loss: 0.857\n",
      "[epoch: 27, i:   499]  train_loss: 0.783  |  valid_loss: 0.781\n",
      "[epoch: 27, i:   624]  train_loss: 0.805  |  valid_loss: 0.942\n",
      "[epoch: 27, i:   749]  train_loss: 0.783  |  valid_loss: 0.600\n",
      "[epoch: 27, i:   874]  train_loss: 0.852  |  valid_loss: 0.864\n",
      "[epoch: 27, i:   999]  train_loss: 0.844  |  valid_loss: 0.949\n",
      "[epoch: 27, i:  1124]  train_loss: 0.789  |  valid_loss: 0.951\n",
      "[epoch: 27, i:  1249]  train_loss: 0.885  |  valid_loss: 0.728\n",
      "[epoch: 27, i:  1374]  train_loss: 0.740  |  valid_loss: 0.743\n",
      "[epoch: 27, i:  1499]  train_loss: 0.792  |  valid_loss: 0.820\n",
      "[epoch: 27, i:  1624]  train_loss: 0.842  |  valid_loss: 0.693\n",
      "[epoch: 27, i:  1749]  train_loss: 0.800  |  valid_loss: 0.967\n",
      "[epoch: 27, i:  1874]  train_loss: 0.863  |  valid_loss: 0.744\n",
      "[epoch: 27, i:  1999]  train_loss: 0.846  |  valid_loss: 0.977\n",
      "[epoch: 27, i:  2124]  train_loss: 0.787  |  valid_loss: 0.749\n",
      "[epoch: 27, i:  2249]  train_loss: 0.874  |  valid_loss: 0.827\n",
      "[epoch: 27, i:  2374]  train_loss: 0.794  |  valid_loss: 0.756\n",
      "[epoch: 27, i:  2499]  train_loss: 0.782  |  valid_loss: 0.994\n",
      "[epoch: 27, i:  2624]  train_loss: 0.782  |  valid_loss: 0.946\n",
      "[epoch: 27, i:  2749]  train_loss: 0.782  |  valid_loss: 0.895\n",
      "[epoch: 27, i:  2874]  train_loss: 0.808  |  valid_loss: 0.986\n",
      "[epoch: 27, i:  2999]  train_loss: 0.829  |  valid_loss: 0.846\n",
      "[epoch: 27, i:  3124]  train_loss: 0.848  |  valid_loss: 0.906\n",
      "[epoch: 27, i:  3249]  train_loss: 0.772  |  valid_loss: 1.047\n",
      "[epoch: 27, i:  3374]  train_loss: 0.804  |  valid_loss: 0.781\n",
      "[epoch: 27, i:  3499]  train_loss: 0.812  |  valid_loss: 0.757\n",
      "[epoch: 27, i:  3624]  train_loss: 0.828  |  valid_loss: 0.790\n",
      "[epoch: 27, i:  3749]  train_loss: 0.786  |  valid_loss: 0.675\n",
      "[epoch: 27, i:  3874]  train_loss: 0.846  |  valid_loss: 0.790\n",
      "[epoch: 27, i:  3999]  train_loss: 0.815  |  valid_loss: 0.659\n",
      "[epoch: 27, i:  4124]  train_loss: 0.865  |  valid_loss: 0.828\n",
      "[epoch: 27, i:  4249]  train_loss: 0.872  |  valid_loss: 0.876\n",
      "[epoch: 27, i:  4374]  train_loss: 0.762  |  valid_loss: 0.861\n",
      "[epoch: 27, i:  4499]  train_loss: 0.776  |  valid_loss: 0.778\n",
      "[epoch: 27, i:  4624]  train_loss: 0.837  |  valid_loss: 0.926\n",
      "[epoch: 27, i:  4749]  train_loss: 0.849  |  valid_loss: 0.865\n",
      "[epoch: 27, i:  4874]  train_loss: 0.767  |  valid_loss: 0.726\n",
      "[epoch: 27, i:  4999]  train_loss: 0.838  |  valid_loss: 0.760\n",
      "[epoch: 27, i:  5124]  train_loss: 0.829  |  valid_loss: 0.855\n",
      "[epoch: 27, i:  5249]  train_loss: 0.773  |  valid_loss: 0.891\n",
      "[epoch: 27, i:  5374]  train_loss: 0.793  |  valid_loss: 0.586\n",
      "[epoch: 27, i:  5499]  train_loss: 0.812  |  valid_loss: 0.722\n",
      "[epoch: 27, i:  5624]  train_loss: 0.803  |  valid_loss: 0.991\n",
      "[epoch: 27, i:  5749]  train_loss: 0.810  |  valid_loss: 0.818\n",
      "[epoch: 27, i:  5874]  train_loss: 0.874  |  valid_loss: 0.712\n",
      "[epoch: 27, i:  5999]  train_loss: 0.794  |  valid_loss: 0.835\n",
      "[epoch: 27, i:  6124]  train_loss: 0.764  |  valid_loss: 0.763\n",
      "[epoch: 27, i:  6249]  train_loss: 0.777  |  valid_loss: 0.898\n",
      "[epoch: 27, i:  6374]  train_loss: 0.819  |  valid_loss: 0.740\n",
      "[epoch: 27, i:  6499]  train_loss: 0.859  |  valid_loss: 0.800\n",
      "[epoch: 27, i:  6624]  train_loss: 0.808  |  valid_loss: 0.681\n",
      "[epoch: 27, i:  6749]  train_loss: 0.800  |  valid_loss: 0.816\n",
      "[epoch: 27, i:  6874]  train_loss: 0.833  |  valid_loss: 0.777\n",
      "[epoch: 27, i:  6999]  train_loss: 0.790  |  valid_loss: 0.981\n",
      "[epoch: 27, i:  7124]  train_loss: 0.799  |  valid_loss: 0.952\n",
      "[epoch: 27, i:  7249]  train_loss: 0.755  |  valid_loss: 0.672\n",
      "[epoch: 27, i:  7374]  train_loss: 0.870  |  valid_loss: 1.032\n",
      "[epoch: 27, i:  7499]  train_loss: 0.771  |  valid_loss: 0.888\n",
      "[epoch: 27, i:  7624]  train_loss: 0.773  |  valid_loss: 0.909\n",
      "[epoch: 27, i:  7749]  train_loss: 0.809  |  valid_loss: 0.787\n",
      "[epoch: 27, i:  7874]  train_loss: 0.752  |  valid_loss: 0.870\n",
      "[epoch: 27, i:  7999]  train_loss: 0.816  |  valid_loss: 0.722\n",
      "[epoch: 27, i:  8124]  train_loss: 0.762  |  valid_loss: 0.927\n",
      "[epoch: 27, i:  8249]  train_loss: 0.834  |  valid_loss: 0.894\n",
      "[epoch: 27, i:  8374]  train_loss: 0.776  |  valid_loss: 0.853\n",
      "[epoch: 27, i:  8499]  train_loss: 0.916  |  valid_loss: 0.917\n",
      "[epoch: 27, i:  8624]  train_loss: 0.738  |  valid_loss: 0.873\n",
      "[epoch: 27, i:  8749]  train_loss: 0.817  |  valid_loss: 0.900\n",
      "[epoch: 27, i:  8874]  train_loss: 0.817  |  valid_loss: 0.851\n",
      "[epoch: 27, i:  8999]  train_loss: 0.779  |  valid_loss: 0.705\n",
      "[epoch: 27, i:  9124]  train_loss: 0.784  |  valid_loss: 0.758\n",
      "[epoch: 27, i:  9249]  train_loss: 0.792  |  valid_loss: 0.600\n",
      "[epoch: 27, i:  9374]  train_loss: 0.808  |  valid_loss: 0.907\n",
      "[epoch: 27, i:  9499]  train_loss: 0.790  |  valid_loss: 0.779\n",
      "[epoch: 27, i:  9624]  train_loss: 0.808  |  valid_loss: 0.880\n",
      "[epoch: 27, i:  9749]  train_loss: 0.773  |  valid_loss: 0.894\n",
      "[epoch: 27, i:  9874]  train_loss: 0.809  |  valid_loss: 0.919\n",
      "[epoch: 27, i:  9999]  train_loss: 0.925  |  valid_loss: 0.834\n",
      "[epoch: 27, i: 10124]  train_loss: 0.772  |  valid_loss: 0.752\n",
      "[epoch: 27, i: 10249]  train_loss: 0.785  |  valid_loss: 0.873\n",
      "[epoch: 27, i: 10374]  train_loss: 0.773  |  valid_loss: 0.745\n",
      "[epoch: 27, i: 10499]  train_loss: 0.858  |  valid_loss: 0.984\n",
      "[epoch: 27, i: 10624]  train_loss: 0.817  |  valid_loss: 0.931\n",
      "[epoch: 27, i: 10749]  train_loss: 0.761  |  valid_loss: 0.856\n",
      "[epoch: 27, i: 10874]  train_loss: 0.807  |  valid_loss: 0.968\n",
      "[epoch: 27, i: 10999]  train_loss: 0.836  |  valid_loss: 0.969\n",
      "[epoch: 27, i: 11124]  train_loss: 0.834  |  valid_loss: 0.844\n",
      "[epoch: 27, i: 11249]  train_loss: 0.835  |  valid_loss: 0.866\n",
      "[epoch: 27, i: 11374]  train_loss: 0.811  |  valid_loss: 0.801\n",
      "[epoch: 27, i: 11499]  train_loss: 0.800  |  valid_loss: 0.620\n",
      "[epoch: 27, i: 11624]  train_loss: 0.864  |  valid_loss: 0.823\n",
      "[epoch: 27, i: 11749]  train_loss: 0.816  |  valid_loss: 0.764\n",
      "[epoch: 27, i: 11874]  train_loss: 0.797  |  valid_loss: 0.757\n",
      "[epoch: 27, i: 11999]  train_loss: 0.801  |  valid_loss: 0.702\n",
      "[epoch: 27, i: 12124]  train_loss: 0.814  |  valid_loss: 0.730\n",
      "[epoch: 27, i: 12249]  train_loss: 0.854  |  valid_loss: 1.055\n",
      "[epoch: 27, i: 12374]  train_loss: 0.815  |  valid_loss: 0.876\n",
      "[epoch: 27, i: 12499]  train_loss: 0.814  |  valid_loss: 0.866\n",
      "--> [End of epoch 27] train_accuracy: 71.41%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 28]  lr: 0.000004\n",
      "[epoch: 28, i:   124]  train_loss: 0.745  |  valid_loss: 0.733\n",
      "[epoch: 28, i:   249]  train_loss: 0.776  |  valid_loss: 0.724\n",
      "[epoch: 28, i:   374]  train_loss: 0.775  |  valid_loss: 0.856\n",
      "[epoch: 28, i:   499]  train_loss: 0.789  |  valid_loss: 0.783\n",
      "[epoch: 28, i:   624]  train_loss: 0.848  |  valid_loss: 0.931\n",
      "[epoch: 28, i:   749]  train_loss: 0.820  |  valid_loss: 0.574\n",
      "[epoch: 28, i:   874]  train_loss: 0.900  |  valid_loss: 0.862\n",
      "[epoch: 28, i:   999]  train_loss: 0.813  |  valid_loss: 0.948\n",
      "[epoch: 28, i:  1124]  train_loss: 0.752  |  valid_loss: 0.919\n",
      "[epoch: 28, i:  1249]  train_loss: 0.798  |  valid_loss: 0.738\n",
      "[epoch: 28, i:  1374]  train_loss: 0.847  |  valid_loss: 0.783\n",
      "[epoch: 28, i:  1499]  train_loss: 0.751  |  valid_loss: 0.827\n",
      "[epoch: 28, i:  1624]  train_loss: 0.785  |  valid_loss: 0.691\n",
      "[epoch: 28, i:  1749]  train_loss: 0.874  |  valid_loss: 0.954\n",
      "[epoch: 28, i:  1874]  train_loss: 0.733  |  valid_loss: 0.766\n",
      "[epoch: 28, i:  1999]  train_loss: 0.811  |  valid_loss: 1.012\n",
      "[epoch: 28, i:  2124]  train_loss: 0.823  |  valid_loss: 0.716\n",
      "[epoch: 28, i:  2249]  train_loss: 0.754  |  valid_loss: 0.852\n",
      "[epoch: 28, i:  2374]  train_loss: 0.782  |  valid_loss: 0.777\n",
      "[epoch: 28, i:  2499]  train_loss: 0.819  |  valid_loss: 1.001\n",
      "[epoch: 28, i:  2624]  train_loss: 0.782  |  valid_loss: 0.931\n",
      "[epoch: 28, i:  2749]  train_loss: 0.932  |  valid_loss: 0.918\n",
      "[epoch: 28, i:  2874]  train_loss: 0.813  |  valid_loss: 1.028\n",
      "[epoch: 28, i:  2999]  train_loss: 0.837  |  valid_loss: 0.819\n",
      "[epoch: 28, i:  3124]  train_loss: 0.823  |  valid_loss: 0.909\n",
      "[epoch: 28, i:  3249]  train_loss: 0.841  |  valid_loss: 1.050\n",
      "[epoch: 28, i:  3374]  train_loss: 0.804  |  valid_loss: 0.826\n",
      "[epoch: 28, i:  3499]  train_loss: 0.729  |  valid_loss: 0.789\n",
      "[epoch: 28, i:  3624]  train_loss: 0.816  |  valid_loss: 0.791\n",
      "[epoch: 28, i:  3749]  train_loss: 0.746  |  valid_loss: 0.672\n",
      "[epoch: 28, i:  3874]  train_loss: 0.779  |  valid_loss: 0.791\n",
      "[epoch: 28, i:  3999]  train_loss: 0.737  |  valid_loss: 0.633\n",
      "[epoch: 28, i:  4124]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 28, i:  4249]  train_loss: 0.837  |  valid_loss: 0.867\n",
      "[epoch: 28, i:  4374]  train_loss: 0.838  |  valid_loss: 0.860\n",
      "[epoch: 28, i:  4499]  train_loss: 0.812  |  valid_loss: 0.778\n",
      "[epoch: 28, i:  4624]  train_loss: 0.831  |  valid_loss: 0.945\n",
      "[epoch: 28, i:  4749]  train_loss: 0.847  |  valid_loss: 0.868\n",
      "[epoch: 28, i:  4874]  train_loss: 0.889  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  4999]  train_loss: 0.856  |  valid_loss: 0.753\n",
      "[epoch: 28, i:  5124]  train_loss: 0.804  |  valid_loss: 0.836\n",
      "[epoch: 28, i:  5249]  train_loss: 0.813  |  valid_loss: 0.901\n",
      "[epoch: 28, i:  5374]  train_loss: 0.810  |  valid_loss: 0.630\n",
      "[epoch: 28, i:  5499]  train_loss: 0.808  |  valid_loss: 0.704\n",
      "[epoch: 28, i:  5624]  train_loss: 0.787  |  valid_loss: 0.930\n",
      "[epoch: 28, i:  5749]  train_loss: 0.901  |  valid_loss: 0.794\n",
      "[epoch: 28, i:  5874]  train_loss: 0.847  |  valid_loss: 0.728\n",
      "[epoch: 28, i:  5999]  train_loss: 0.777  |  valid_loss: 0.875\n",
      "[epoch: 28, i:  6124]  train_loss: 0.780  |  valid_loss: 0.748\n",
      "[epoch: 28, i:  6249]  train_loss: 0.875  |  valid_loss: 0.887\n",
      "[epoch: 28, i:  6374]  train_loss: 0.791  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  6499]  train_loss: 0.785  |  valid_loss: 0.766\n",
      "[epoch: 28, i:  6624]  train_loss: 0.715  |  valid_loss: 0.691\n",
      "[epoch: 28, i:  6749]  train_loss: 0.804  |  valid_loss: 0.820\n",
      "[epoch: 28, i:  6874]  train_loss: 0.794  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  6999]  train_loss: 0.840  |  valid_loss: 0.964\n",
      "[epoch: 28, i:  7124]  train_loss: 0.809  |  valid_loss: 0.989\n",
      "[epoch: 28, i:  7249]  train_loss: 0.788  |  valid_loss: 0.647\n",
      "[epoch: 28, i:  7374]  train_loss: 0.800  |  valid_loss: 1.030\n",
      "[epoch: 28, i:  7499]  train_loss: 0.815  |  valid_loss: 0.910\n",
      "[epoch: 28, i:  7624]  train_loss: 0.832  |  valid_loss: 0.907\n",
      "[epoch: 28, i:  7749]  train_loss: 0.769  |  valid_loss: 0.797\n",
      "[epoch: 28, i:  7874]  train_loss: 0.795  |  valid_loss: 0.890\n",
      "[epoch: 28, i:  7999]  train_loss: 0.863  |  valid_loss: 0.733\n",
      "[epoch: 28, i:  8124]  train_loss: 0.907  |  valid_loss: 0.903\n",
      "[epoch: 28, i:  8249]  train_loss: 0.849  |  valid_loss: 0.909\n",
      "[epoch: 28, i:  8374]  train_loss: 0.808  |  valid_loss: 0.861\n",
      "[epoch: 28, i:  8499]  train_loss: 0.805  |  valid_loss: 0.917\n",
      "[epoch: 28, i:  8624]  train_loss: 0.747  |  valid_loss: 0.867\n",
      "[epoch: 28, i:  8749]  train_loss: 0.766  |  valid_loss: 0.887\n",
      "[epoch: 28, i:  8874]  train_loss: 0.868  |  valid_loss: 0.872\n",
      "[epoch: 28, i:  8999]  train_loss: 0.859  |  valid_loss: 0.690\n",
      "[epoch: 28, i:  9124]  train_loss: 0.792  |  valid_loss: 0.754\n",
      "[epoch: 28, i:  9249]  train_loss: 0.817  |  valid_loss: 0.611\n",
      "[epoch: 28, i:  9374]  train_loss: 0.840  |  valid_loss: 0.899\n",
      "[epoch: 28, i:  9499]  train_loss: 0.783  |  valid_loss: 0.773\n",
      "[epoch: 28, i:  9624]  train_loss: 0.791  |  valid_loss: 0.877\n",
      "[epoch: 28, i:  9749]  train_loss: 0.790  |  valid_loss: 0.900\n",
      "[epoch: 28, i:  9874]  train_loss: 0.874  |  valid_loss: 0.932\n",
      "[epoch: 28, i:  9999]  train_loss: 0.842  |  valid_loss: 0.822\n",
      "[epoch: 28, i: 10124]  train_loss: 0.827  |  valid_loss: 0.758\n",
      "[epoch: 28, i: 10249]  train_loss: 0.775  |  valid_loss: 0.852\n",
      "[epoch: 28, i: 10374]  train_loss: 0.852  |  valid_loss: 0.746\n",
      "[epoch: 28, i: 10499]  train_loss: 0.865  |  valid_loss: 0.958\n",
      "[epoch: 28, i: 10624]  train_loss: 0.856  |  valid_loss: 0.938\n",
      "[epoch: 28, i: 10749]  train_loss: 0.840  |  valid_loss: 0.832\n",
      "[epoch: 28, i: 10874]  train_loss: 0.834  |  valid_loss: 0.969\n",
      "[epoch: 28, i: 10999]  train_loss: 0.895  |  valid_loss: 0.947\n",
      "[epoch: 28, i: 11124]  train_loss: 0.813  |  valid_loss: 0.833\n",
      "[epoch: 28, i: 11249]  train_loss: 0.802  |  valid_loss: 0.873\n",
      "[epoch: 28, i: 11374]  train_loss: 0.786  |  valid_loss: 0.803\n",
      "[epoch: 28, i: 11499]  train_loss: 0.825  |  valid_loss: 0.630\n",
      "[epoch: 28, i: 11624]  train_loss: 0.747  |  valid_loss: 0.820\n",
      "[epoch: 28, i: 11749]  train_loss: 0.789  |  valid_loss: 0.756\n",
      "[epoch: 28, i: 11874]  train_loss: 0.851  |  valid_loss: 0.787\n",
      "[epoch: 28, i: 11999]  train_loss: 0.788  |  valid_loss: 0.679\n",
      "[epoch: 28, i: 12124]  train_loss: 0.805  |  valid_loss: 0.713\n",
      "[epoch: 28, i: 12249]  train_loss: 0.821  |  valid_loss: 1.044\n",
      "[epoch: 28, i: 12374]  train_loss: 0.817  |  valid_loss: 0.876\n",
      "[epoch: 28, i: 12499]  train_loss: 0.841  |  valid_loss: 0.849\n",
      "--> [End of epoch 28] train_accuracy: 71.17%  |  valid_accuracy: 71.14%\n",
      "--> [Start of epoch 29]  lr: 0.000003\n",
      "[epoch: 29, i:   124]  train_loss: 0.774  |  valid_loss: 0.743\n",
      "[epoch: 29, i:   249]  train_loss: 0.812  |  valid_loss: 0.724\n",
      "[epoch: 29, i:   374]  train_loss: 0.831  |  valid_loss: 0.850\n",
      "[epoch: 29, i:   499]  train_loss: 0.790  |  valid_loss: 0.785\n",
      "[epoch: 29, i:   624]  train_loss: 0.835  |  valid_loss: 0.935\n",
      "[epoch: 29, i:   749]  train_loss: 0.777  |  valid_loss: 0.586\n",
      "[epoch: 29, i:   874]  train_loss: 0.748  |  valid_loss: 0.862\n",
      "[epoch: 29, i:   999]  train_loss: 0.782  |  valid_loss: 0.932\n",
      "[epoch: 29, i:  1124]  train_loss: 0.844  |  valid_loss: 0.971\n",
      "[epoch: 29, i:  1249]  train_loss: 0.808  |  valid_loss: 0.724\n",
      "[epoch: 29, i:  1374]  train_loss: 0.903  |  valid_loss: 0.749\n",
      "[epoch: 29, i:  1499]  train_loss: 0.812  |  valid_loss: 0.824\n",
      "[epoch: 29, i:  1624]  train_loss: 0.837  |  valid_loss: 0.681\n",
      "[epoch: 29, i:  1749]  train_loss: 0.778  |  valid_loss: 0.975\n",
      "[epoch: 29, i:  1874]  train_loss: 0.818  |  valid_loss: 0.765\n",
      "[epoch: 29, i:  1999]  train_loss: 0.771  |  valid_loss: 0.989\n",
      "[epoch: 29, i:  2124]  train_loss: 0.858  |  valid_loss: 0.737\n",
      "[epoch: 29, i:  2249]  train_loss: 0.773  |  valid_loss: 0.838\n",
      "[epoch: 29, i:  2374]  train_loss: 0.754  |  valid_loss: 0.775\n",
      "[epoch: 29, i:  2499]  train_loss: 0.810  |  valid_loss: 0.989\n",
      "[epoch: 29, i:  2624]  train_loss: 0.889  |  valid_loss: 0.911\n",
      "[epoch: 29, i:  2749]  train_loss: 0.846  |  valid_loss: 0.904\n",
      "[epoch: 29, i:  2874]  train_loss: 0.826  |  valid_loss: 1.019\n",
      "[epoch: 29, i:  2999]  train_loss: 0.855  |  valid_loss: 0.827\n",
      "[epoch: 29, i:  3124]  train_loss: 0.858  |  valid_loss: 0.904\n",
      "[epoch: 29, i:  3249]  train_loss: 0.831  |  valid_loss: 1.040\n",
      "[epoch: 29, i:  3374]  train_loss: 0.880  |  valid_loss: 0.784\n",
      "[epoch: 29, i:  3499]  train_loss: 0.861  |  valid_loss: 0.774\n",
      "[epoch: 29, i:  3624]  train_loss: 0.735  |  valid_loss: 0.783\n",
      "[epoch: 29, i:  3749]  train_loss: 0.778  |  valid_loss: 0.684\n",
      "[epoch: 29, i:  3874]  train_loss: 0.792  |  valid_loss: 0.825\n",
      "[epoch: 29, i:  3999]  train_loss: 0.828  |  valid_loss: 0.643\n",
      "[epoch: 29, i:  4124]  train_loss: 0.758  |  valid_loss: 0.895\n",
      "[epoch: 29, i:  4249]  train_loss: 0.871  |  valid_loss: 0.876\n",
      "[epoch: 29, i:  4374]  train_loss: 0.847  |  valid_loss: 0.881\n",
      "[epoch: 29, i:  4499]  train_loss: 0.770  |  valid_loss: 0.781\n",
      "[epoch: 29, i:  4624]  train_loss: 0.845  |  valid_loss: 0.940\n",
      "[epoch: 29, i:  4749]  train_loss: 0.819  |  valid_loss: 0.880\n",
      "[epoch: 29, i:  4874]  train_loss: 0.758  |  valid_loss: 0.753\n",
      "[epoch: 29, i:  4999]  train_loss: 0.739  |  valid_loss: 0.758\n",
      "[epoch: 29, i:  5124]  train_loss: 0.874  |  valid_loss: 0.800\n",
      "[epoch: 29, i:  5249]  train_loss: 0.791  |  valid_loss: 0.900\n",
      "[epoch: 29, i:  5374]  train_loss: 0.846  |  valid_loss: 0.623\n",
      "[epoch: 29, i:  5499]  train_loss: 0.858  |  valid_loss: 0.691\n",
      "[epoch: 29, i:  5624]  train_loss: 0.853  |  valid_loss: 0.906\n",
      "[epoch: 29, i:  5749]  train_loss: 0.811  |  valid_loss: 0.820\n",
      "[epoch: 29, i:  5874]  train_loss: 0.758  |  valid_loss: 0.751\n",
      "[epoch: 29, i:  5999]  train_loss: 0.869  |  valid_loss: 0.867\n",
      "[epoch: 29, i:  6124]  train_loss: 0.777  |  valid_loss: 0.747\n",
      "[epoch: 29, i:  6249]  train_loss: 0.839  |  valid_loss: 0.879\n",
      "[epoch: 29, i:  6374]  train_loss: 0.757  |  valid_loss: 0.740\n",
      "[epoch: 29, i:  6499]  train_loss: 0.807  |  valid_loss: 0.759\n",
      "[epoch: 29, i:  6624]  train_loss: 0.839  |  valid_loss: 0.677\n",
      "[epoch: 29, i:  6749]  train_loss: 0.753  |  valid_loss: 0.829\n",
      "[epoch: 29, i:  6874]  train_loss: 0.850  |  valid_loss: 0.764\n",
      "[epoch: 29, i:  6999]  train_loss: 0.795  |  valid_loss: 0.934\n",
      "[epoch: 29, i:  7124]  train_loss: 0.863  |  valid_loss: 0.952\n",
      "[epoch: 29, i:  7249]  train_loss: 0.821  |  valid_loss: 0.651\n",
      "[epoch: 29, i:  7374]  train_loss: 0.865  |  valid_loss: 1.031\n",
      "[epoch: 29, i:  7499]  train_loss: 0.801  |  valid_loss: 0.900\n",
      "[epoch: 29, i:  7624]  train_loss: 0.825  |  valid_loss: 0.911\n",
      "[epoch: 29, i:  7749]  train_loss: 0.862  |  valid_loss: 0.793\n",
      "[epoch: 29, i:  7874]  train_loss: 0.818  |  valid_loss: 0.853\n",
      "[epoch: 29, i:  7999]  train_loss: 0.775  |  valid_loss: 0.709\n",
      "[epoch: 29, i:  8124]  train_loss: 0.805  |  valid_loss: 0.932\n",
      "[epoch: 29, i:  8249]  train_loss: 0.781  |  valid_loss: 0.918\n",
      "[epoch: 29, i:  8374]  train_loss: 0.759  |  valid_loss: 0.834\n",
      "[epoch: 29, i:  8499]  train_loss: 0.718  |  valid_loss: 0.906\n",
      "[epoch: 29, i:  8624]  train_loss: 0.785  |  valid_loss: 0.848\n",
      "[epoch: 29, i:  8749]  train_loss: 0.827  |  valid_loss: 0.902\n",
      "[epoch: 29, i:  8874]  train_loss: 0.825  |  valid_loss: 0.851\n",
      "[epoch: 29, i:  8999]  train_loss: 0.833  |  valid_loss: 0.682\n",
      "[epoch: 29, i:  9124]  train_loss: 0.743  |  valid_loss: 0.720\n",
      "[epoch: 29, i:  9249]  train_loss: 0.810  |  valid_loss: 0.613\n",
      "[epoch: 29, i:  9374]  train_loss: 0.912  |  valid_loss: 0.893\n",
      "[epoch: 29, i:  9499]  train_loss: 0.842  |  valid_loss: 0.751\n",
      "[epoch: 29, i:  9624]  train_loss: 0.769  |  valid_loss: 0.898\n",
      "[epoch: 29, i:  9749]  train_loss: 0.794  |  valid_loss: 0.926\n",
      "[epoch: 29, i:  9874]  train_loss: 0.800  |  valid_loss: 0.890\n",
      "[epoch: 29, i:  9999]  train_loss: 0.768  |  valid_loss: 0.814\n",
      "[epoch: 29, i: 10124]  train_loss: 0.800  |  valid_loss: 0.757\n",
      "[epoch: 29, i: 10249]  train_loss: 0.817  |  valid_loss: 0.854\n",
      "[epoch: 29, i: 10374]  train_loss: 0.818  |  valid_loss: 0.743\n",
      "[epoch: 29, i: 10499]  train_loss: 0.817  |  valid_loss: 0.930\n",
      "[epoch: 29, i: 10624]  train_loss: 0.826  |  valid_loss: 0.956\n",
      "[epoch: 29, i: 10749]  train_loss: 0.783  |  valid_loss: 0.837\n",
      "[epoch: 29, i: 10874]  train_loss: 0.845  |  valid_loss: 0.961\n",
      "[epoch: 29, i: 10999]  train_loss: 0.847  |  valid_loss: 0.963\n",
      "[epoch: 29, i: 11124]  train_loss: 0.840  |  valid_loss: 0.823\n",
      "[epoch: 29, i: 11249]  train_loss: 0.852  |  valid_loss: 0.886\n",
      "[epoch: 29, i: 11374]  train_loss: 0.825  |  valid_loss: 0.796\n",
      "[epoch: 29, i: 11499]  train_loss: 0.773  |  valid_loss: 0.630\n",
      "[epoch: 29, i: 11624]  train_loss: 0.780  |  valid_loss: 0.805\n",
      "[epoch: 29, i: 11749]  train_loss: 0.806  |  valid_loss: 0.750\n",
      "[epoch: 29, i: 11874]  train_loss: 0.875  |  valid_loss: 0.793\n",
      "[epoch: 29, i: 11999]  train_loss: 0.767  |  valid_loss: 0.697\n",
      "[epoch: 29, i: 12124]  train_loss: 0.823  |  valid_loss: 0.697\n",
      "[epoch: 29, i: 12249]  train_loss: 0.841  |  valid_loss: 1.042\n",
      "[epoch: 29, i: 12374]  train_loss: 0.774  |  valid_loss: 0.906\n",
      "[epoch: 29, i: 12499]  train_loss: 0.807  |  valid_loss: 0.862\n",
      "--> [End of epoch 29] train_accuracy: 71.37%  |  valid_accuracy: 71.11%\n",
      "--> [Start of epoch 30]  lr: 0.000002\n",
      "[epoch: 30, i:   124]  train_loss: 0.720  |  valid_loss: 0.717\n",
      "[epoch: 30, i:   249]  train_loss: 0.885  |  valid_loss: 0.731\n",
      "[epoch: 30, i:   374]  train_loss: 0.836  |  valid_loss: 0.874\n",
      "[epoch: 30, i:   499]  train_loss: 0.808  |  valid_loss: 0.818\n",
      "[epoch: 30, i:   624]  train_loss: 0.816  |  valid_loss: 0.923\n",
      "[epoch: 30, i:   749]  train_loss: 0.837  |  valid_loss: 0.579\n",
      "[epoch: 30, i:   874]  train_loss: 0.823  |  valid_loss: 0.841\n",
      "[epoch: 30, i:   999]  train_loss: 0.846  |  valid_loss: 0.911\n",
      "[epoch: 30, i:  1124]  train_loss: 0.835  |  valid_loss: 0.918\n",
      "[epoch: 30, i:  1249]  train_loss: 0.745  |  valid_loss: 0.711\n",
      "[epoch: 30, i:  1374]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  1499]  train_loss: 0.774  |  valid_loss: 0.822\n",
      "[epoch: 30, i:  1624]  train_loss: 0.787  |  valid_loss: 0.683\n",
      "[epoch: 30, i:  1749]  train_loss: 0.846  |  valid_loss: 0.938\n",
      "[epoch: 30, i:  1874]  train_loss: 0.839  |  valid_loss: 0.767\n",
      "[epoch: 30, i:  1999]  train_loss: 0.766  |  valid_loss: 0.964\n",
      "[epoch: 30, i:  2124]  train_loss: 0.740  |  valid_loss: 0.705\n",
      "[epoch: 30, i:  2249]  train_loss: 0.854  |  valid_loss: 0.807\n",
      "[epoch: 30, i:  2374]  train_loss: 0.846  |  valid_loss: 0.756\n",
      "[epoch: 30, i:  2499]  train_loss: 0.818  |  valid_loss: 0.996\n",
      "[epoch: 30, i:  2624]  train_loss: 0.839  |  valid_loss: 0.923\n",
      "[epoch: 30, i:  2749]  train_loss: 0.861  |  valid_loss: 0.919\n",
      "[epoch: 30, i:  2874]  train_loss: 0.738  |  valid_loss: 1.005\n",
      "[epoch: 30, i:  2999]  train_loss: 0.844  |  valid_loss: 0.846\n",
      "[epoch: 30, i:  3124]  train_loss: 0.783  |  valid_loss: 0.894\n",
      "[epoch: 30, i:  3249]  train_loss: 0.812  |  valid_loss: 1.043\n",
      "[epoch: 30, i:  3374]  train_loss: 0.801  |  valid_loss: 0.783\n",
      "[epoch: 30, i:  3499]  train_loss: 0.887  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  3624]  train_loss: 0.816  |  valid_loss: 0.818\n",
      "[epoch: 30, i:  3749]  train_loss: 0.733  |  valid_loss: 0.703\n",
      "[epoch: 30, i:  3874]  train_loss: 0.773  |  valid_loss: 0.822\n",
      "[epoch: 30, i:  3999]  train_loss: 0.870  |  valid_loss: 0.633\n",
      "[epoch: 30, i:  4124]  train_loss: 0.841  |  valid_loss: 0.915\n",
      "[epoch: 30, i:  4249]  train_loss: 0.808  |  valid_loss: 0.886\n",
      "[epoch: 30, i:  4374]  train_loss: 0.825  |  valid_loss: 0.871\n",
      "[epoch: 30, i:  4499]  train_loss: 0.768  |  valid_loss: 0.773\n",
      "[epoch: 30, i:  4624]  train_loss: 0.841  |  valid_loss: 0.955\n",
      "[epoch: 30, i:  4749]  train_loss: 0.794  |  valid_loss: 0.884\n",
      "[epoch: 30, i:  4874]  train_loss: 0.842  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  4999]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 30, i:  5124]  train_loss: 0.743  |  valid_loss: 0.824\n",
      "[epoch: 30, i:  5249]  train_loss: 0.831  |  valid_loss: 0.865\n",
      "[epoch: 30, i:  5374]  train_loss: 0.863  |  valid_loss: 0.647\n",
      "[epoch: 30, i:  5499]  train_loss: 0.821  |  valid_loss: 0.706\n",
      "[epoch: 30, i:  5624]  train_loss: 0.791  |  valid_loss: 1.014\n",
      "[epoch: 30, i:  5749]  train_loss: 0.841  |  valid_loss: 0.808\n",
      "[epoch: 30, i:  5874]  train_loss: 0.808  |  valid_loss: 0.723\n",
      "[epoch: 30, i:  5999]  train_loss: 0.782  |  valid_loss: 0.880\n",
      "[epoch: 30, i:  6124]  train_loss: 0.751  |  valid_loss: 0.743\n",
      "[epoch: 30, i:  6249]  train_loss: 0.768  |  valid_loss: 0.881\n",
      "[epoch: 30, i:  6374]  train_loss: 0.786  |  valid_loss: 0.726\n",
      "[epoch: 30, i:  6499]  train_loss: 0.789  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  6624]  train_loss: 0.804  |  valid_loss: 0.655\n",
      "[epoch: 30, i:  6749]  train_loss: 0.811  |  valid_loss: 0.818\n",
      "[epoch: 30, i:  6874]  train_loss: 0.793  |  valid_loss: 0.774\n",
      "[epoch: 30, i:  6999]  train_loss: 0.763  |  valid_loss: 0.957\n",
      "[epoch: 30, i:  7124]  train_loss: 0.792  |  valid_loss: 0.994\n",
      "[epoch: 30, i:  7249]  train_loss: 0.816  |  valid_loss: 0.669\n",
      "[epoch: 30, i:  7374]  train_loss: 0.838  |  valid_loss: 1.019\n",
      "[epoch: 30, i:  7499]  train_loss: 0.797  |  valid_loss: 0.893\n",
      "[epoch: 30, i:  7624]  train_loss: 0.758  |  valid_loss: 0.910\n",
      "[epoch: 30, i:  7749]  train_loss: 0.871  |  valid_loss: 0.801\n",
      "[epoch: 30, i:  7874]  train_loss: 0.803  |  valid_loss: 0.863\n",
      "[epoch: 30, i:  7999]  train_loss: 0.827  |  valid_loss: 0.712\n",
      "[epoch: 30, i:  8124]  train_loss: 0.790  |  valid_loss: 0.906\n",
      "[epoch: 30, i:  8249]  train_loss: 0.804  |  valid_loss: 0.888\n",
      "[epoch: 30, i:  8374]  train_loss: 0.822  |  valid_loss: 0.824\n",
      "[epoch: 30, i:  8499]  train_loss: 0.847  |  valid_loss: 0.893\n",
      "[epoch: 30, i:  8624]  train_loss: 0.777  |  valid_loss: 0.854\n",
      "[epoch: 30, i:  8749]  train_loss: 0.774  |  valid_loss: 0.891\n",
      "[epoch: 30, i:  8874]  train_loss: 0.814  |  valid_loss: 0.865\n",
      "[epoch: 30, i:  8999]  train_loss: 0.845  |  valid_loss: 0.691\n",
      "[epoch: 30, i:  9124]  train_loss: 0.728  |  valid_loss: 0.747\n",
      "[epoch: 30, i:  9249]  train_loss: 0.876  |  valid_loss: 0.611\n",
      "[epoch: 30, i:  9374]  train_loss: 0.774  |  valid_loss: 0.900\n",
      "[epoch: 30, i:  9499]  train_loss: 0.789  |  valid_loss: 0.774\n",
      "[epoch: 30, i:  9624]  train_loss: 0.810  |  valid_loss: 0.869\n",
      "[epoch: 30, i:  9749]  train_loss: 0.936  |  valid_loss: 0.913\n",
      "[epoch: 30, i:  9874]  train_loss: 0.821  |  valid_loss: 0.898\n",
      "[epoch: 30, i:  9999]  train_loss: 0.881  |  valid_loss: 0.821\n",
      "[epoch: 30, i: 10124]  train_loss: 0.795  |  valid_loss: 0.751\n",
      "[epoch: 30, i: 10249]  train_loss: 0.756  |  valid_loss: 0.859\n",
      "[epoch: 30, i: 10374]  train_loss: 0.842  |  valid_loss: 0.736\n",
      "[epoch: 30, i: 10499]  train_loss: 0.738  |  valid_loss: 0.957\n",
      "[epoch: 30, i: 10624]  train_loss: 0.881  |  valid_loss: 0.938\n",
      "[epoch: 30, i: 10749]  train_loss: 0.784  |  valid_loss: 0.832\n",
      "[epoch: 30, i: 10874]  train_loss: 0.796  |  valid_loss: 0.932\n",
      "[epoch: 30, i: 10999]  train_loss: 0.877  |  valid_loss: 0.944\n",
      "[epoch: 30, i: 11124]  train_loss: 0.774  |  valid_loss: 0.820\n",
      "[epoch: 30, i: 11249]  train_loss: 0.827  |  valid_loss: 0.857\n",
      "[epoch: 30, i: 11374]  train_loss: 0.790  |  valid_loss: 0.796\n",
      "[epoch: 30, i: 11499]  train_loss: 0.748  |  valid_loss: 0.591\n",
      "[epoch: 30, i: 11624]  train_loss: 0.875  |  valid_loss: 0.792\n",
      "[epoch: 30, i: 11749]  train_loss: 0.873  |  valid_loss: 0.749\n",
      "[epoch: 30, i: 11874]  train_loss: 0.840  |  valid_loss: 0.788\n",
      "[epoch: 30, i: 11999]  train_loss: 0.752  |  valid_loss: 0.678\n",
      "[epoch: 30, i: 12124]  train_loss: 0.792  |  valid_loss: 0.716\n",
      "[epoch: 30, i: 12249]  train_loss: 0.752  |  valid_loss: 1.063\n",
      "[epoch: 30, i: 12374]  train_loss: 0.803  |  valid_loss: 0.868\n",
      "[epoch: 30, i: 12499]  train_loss: 0.792  |  valid_loss: 0.859\n",
      "--> [End of epoch 30] train_accuracy: 71.39%  |  valid_accuracy: 71.06%\n",
      "--> [Start of epoch 31]  lr: 0.000002\n",
      "[epoch: 31, i:   124]  train_loss: 0.792  |  valid_loss: 0.744\n",
      "[epoch: 31, i:   249]  train_loss: 0.792  |  valid_loss: 0.751\n",
      "[epoch: 31, i:   374]  train_loss: 0.840  |  valid_loss: 0.847\n",
      "[epoch: 31, i:   499]  train_loss: 0.742  |  valid_loss: 0.802\n",
      "[epoch: 31, i:   624]  train_loss: 0.839  |  valid_loss: 0.944\n",
      "[epoch: 31, i:   749]  train_loss: 0.881  |  valid_loss: 0.567\n",
      "[epoch: 31, i:   874]  train_loss: 0.875  |  valid_loss: 0.852\n",
      "[epoch: 31, i:   999]  train_loss: 0.776  |  valid_loss: 0.934\n",
      "[epoch: 31, i:  1124]  train_loss: 0.848  |  valid_loss: 0.888\n",
      "[epoch: 31, i:  1249]  train_loss: 0.853  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  1374]  train_loss: 0.829  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  1499]  train_loss: 0.803  |  valid_loss: 0.804\n",
      "[epoch: 31, i:  1624]  train_loss: 0.845  |  valid_loss: 0.683\n",
      "[epoch: 31, i:  1749]  train_loss: 0.840  |  valid_loss: 0.997\n",
      "[epoch: 31, i:  1874]  train_loss: 0.792  |  valid_loss: 0.738\n",
      "[epoch: 31, i:  1999]  train_loss: 0.807  |  valid_loss: 0.985\n",
      "[epoch: 31, i:  2124]  train_loss: 0.806  |  valid_loss: 0.721\n",
      "[epoch: 31, i:  2249]  train_loss: 0.794  |  valid_loss: 0.836\n",
      "[epoch: 31, i:  2374]  train_loss: 0.808  |  valid_loss: 0.771\n",
      "[epoch: 31, i:  2499]  train_loss: 0.848  |  valid_loss: 1.008\n",
      "[epoch: 31, i:  2624]  train_loss: 0.804  |  valid_loss: 0.909\n",
      "[epoch: 31, i:  2749]  train_loss: 0.748  |  valid_loss: 0.921\n",
      "[epoch: 31, i:  2874]  train_loss: 0.845  |  valid_loss: 1.024\n",
      "[epoch: 31, i:  2999]  train_loss: 0.837  |  valid_loss: 0.822\n",
      "[epoch: 31, i:  3124]  train_loss: 0.776  |  valid_loss: 0.913\n",
      "[epoch: 31, i:  3249]  train_loss: 0.782  |  valid_loss: 1.078\n",
      "[epoch: 31, i:  3374]  train_loss: 0.760  |  valid_loss: 0.806\n",
      "[epoch: 31, i:  3499]  train_loss: 0.772  |  valid_loss: 0.745\n",
      "[epoch: 31, i:  3624]  train_loss: 0.774  |  valid_loss: 0.788\n",
      "[epoch: 31, i:  3749]  train_loss: 0.760  |  valid_loss: 0.678\n",
      "[epoch: 31, i:  3874]  train_loss: 0.836  |  valid_loss: 0.790\n",
      "[epoch: 31, i:  3999]  train_loss: 0.782  |  valid_loss: 0.636\n",
      "[epoch: 31, i:  4124]  train_loss: 0.810  |  valid_loss: 0.850\n",
      "[epoch: 31, i:  4249]  train_loss: 0.813  |  valid_loss: 0.875\n",
      "[epoch: 31, i:  4374]  train_loss: 0.793  |  valid_loss: 0.860\n",
      "[epoch: 31, i:  4499]  train_loss: 0.818  |  valid_loss: 0.786\n",
      "[epoch: 31, i:  4624]  train_loss: 0.832  |  valid_loss: 0.934\n",
      "[epoch: 31, i:  4749]  train_loss: 0.809  |  valid_loss: 0.880\n",
      "[epoch: 31, i:  4874]  train_loss: 0.762  |  valid_loss: 0.735\n",
      "[epoch: 31, i:  4999]  train_loss: 0.789  |  valid_loss: 0.751\n",
      "[epoch: 31, i:  5124]  train_loss: 0.756  |  valid_loss: 0.812\n",
      "[epoch: 31, i:  5249]  train_loss: 0.847  |  valid_loss: 0.898\n",
      "[epoch: 31, i:  5374]  train_loss: 0.768  |  valid_loss: 0.641\n",
      "[epoch: 31, i:  5499]  train_loss: 0.817  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  5624]  train_loss: 0.815  |  valid_loss: 0.956\n",
      "[epoch: 31, i:  5749]  train_loss: 0.853  |  valid_loss: 0.818\n",
      "[epoch: 31, i:  5874]  train_loss: 0.825  |  valid_loss: 0.724\n",
      "[epoch: 31, i:  5999]  train_loss: 0.847  |  valid_loss: 0.855\n",
      "[epoch: 31, i:  6124]  train_loss: 0.875  |  valid_loss: 0.730\n",
      "[epoch: 31, i:  6249]  train_loss: 0.846  |  valid_loss: 0.858\n",
      "[epoch: 31, i:  6374]  train_loss: 0.834  |  valid_loss: 0.747\n",
      "[epoch: 31, i:  6499]  train_loss: 0.794  |  valid_loss: 0.775\n",
      "[epoch: 31, i:  6624]  train_loss: 0.751  |  valid_loss: 0.685\n",
      "[epoch: 31, i:  6749]  train_loss: 0.712  |  valid_loss: 0.831\n",
      "[epoch: 31, i:  6874]  train_loss: 0.805  |  valid_loss: 0.772\n",
      "[epoch: 31, i:  6999]  train_loss: 0.855  |  valid_loss: 0.961\n",
      "[epoch: 31, i:  7124]  train_loss: 0.808  |  valid_loss: 0.975\n",
      "[epoch: 31, i:  7249]  train_loss: 0.786  |  valid_loss: 0.633\n",
      "[epoch: 31, i:  7374]  train_loss: 0.772  |  valid_loss: 1.031\n",
      "[epoch: 31, i:  7499]  train_loss: 0.834  |  valid_loss: 0.916\n",
      "[epoch: 31, i:  7624]  train_loss: 0.831  |  valid_loss: 0.928\n",
      "[epoch: 31, i:  7749]  train_loss: 0.812  |  valid_loss: 0.815\n",
      "[epoch: 31, i:  7874]  train_loss: 0.820  |  valid_loss: 0.884\n",
      "[epoch: 31, i:  7999]  train_loss: 0.804  |  valid_loss: 0.706\n",
      "[epoch: 31, i:  8124]  train_loss: 0.836  |  valid_loss: 0.915\n",
      "[epoch: 31, i:  8249]  train_loss: 0.815  |  valid_loss: 0.914\n",
      "[epoch: 31, i:  8374]  train_loss: 0.805  |  valid_loss: 0.825\n",
      "[epoch: 31, i:  8499]  train_loss: 0.804  |  valid_loss: 0.889\n",
      "[epoch: 31, i:  8624]  train_loss: 0.802  |  valid_loss: 0.851\n",
      "[epoch: 31, i:  8749]  train_loss: 0.844  |  valid_loss: 0.904\n",
      "[epoch: 31, i:  8874]  train_loss: 0.789  |  valid_loss: 0.867\n",
      "[epoch: 31, i:  8999]  train_loss: 0.847  |  valid_loss: 0.687\n",
      "[epoch: 31, i:  9124]  train_loss: 0.812  |  valid_loss: 0.745\n",
      "[epoch: 31, i:  9249]  train_loss: 0.808  |  valid_loss: 0.626\n",
      "[epoch: 31, i:  9374]  train_loss: 0.806  |  valid_loss: 0.909\n",
      "[epoch: 31, i:  9499]  train_loss: 0.783  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  9624]  train_loss: 0.816  |  valid_loss: 0.876\n",
      "[epoch: 31, i:  9749]  train_loss: 0.790  |  valid_loss: 0.908\n",
      "[epoch: 31, i:  9874]  train_loss: 0.792  |  valid_loss: 0.891\n",
      "[epoch: 31, i:  9999]  train_loss: 0.804  |  valid_loss: 0.813\n",
      "[epoch: 31, i: 10124]  train_loss: 0.779  |  valid_loss: 0.751\n",
      "[epoch: 31, i: 10249]  train_loss: 0.833  |  valid_loss: 0.841\n",
      "[epoch: 31, i: 10374]  train_loss: 0.799  |  valid_loss: 0.778\n",
      "[epoch: 31, i: 10499]  train_loss: 0.832  |  valid_loss: 0.983\n",
      "[epoch: 31, i: 10624]  train_loss: 0.745  |  valid_loss: 0.975\n",
      "[epoch: 31, i: 10749]  train_loss: 0.861  |  valid_loss: 0.872\n",
      "[epoch: 31, i: 10874]  train_loss: 0.863  |  valid_loss: 0.935\n",
      "[epoch: 31, i: 10999]  train_loss: 0.871  |  valid_loss: 0.948\n",
      "[epoch: 31, i: 11124]  train_loss: 0.864  |  valid_loss: 0.831\n",
      "[epoch: 31, i: 11249]  train_loss: 0.807  |  valid_loss: 0.874\n",
      "[epoch: 31, i: 11374]  train_loss: 0.840  |  valid_loss: 0.792\n",
      "[epoch: 31, i: 11499]  train_loss: 0.769  |  valid_loss: 0.608\n",
      "[epoch: 31, i: 11624]  train_loss: 0.831  |  valid_loss: 0.783\n",
      "[epoch: 31, i: 11749]  train_loss: 0.826  |  valid_loss: 0.749\n",
      "[epoch: 31, i: 11874]  train_loss: 0.798  |  valid_loss: 0.779\n",
      "[epoch: 31, i: 11999]  train_loss: 0.803  |  valid_loss: 0.676\n",
      "[epoch: 31, i: 12124]  train_loss: 0.843  |  valid_loss: 0.742\n",
      "[epoch: 31, i: 12249]  train_loss: 0.828  |  valid_loss: 1.038\n",
      "[epoch: 31, i: 12374]  train_loss: 0.816  |  valid_loss: 0.901\n",
      "[epoch: 31, i: 12499]  train_loss: 0.773  |  valid_loss: 0.875\n",
      "--> [End of epoch 31] train_accuracy: 71.47%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 32]  lr: 0.000002\n",
      "[epoch: 32, i:   124]  train_loss: 0.837  |  valid_loss: 0.731\n",
      "[epoch: 32, i:   249]  train_loss: 0.810  |  valid_loss: 0.729\n",
      "[epoch: 32, i:   374]  train_loss: 0.862  |  valid_loss: 0.865\n",
      "[epoch: 32, i:   499]  train_loss: 0.797  |  valid_loss: 0.801\n",
      "[epoch: 32, i:   624]  train_loss: 0.800  |  valid_loss: 0.909\n",
      "[epoch: 32, i:   749]  train_loss: 0.910  |  valid_loss: 0.589\n",
      "[epoch: 32, i:   874]  train_loss: 0.844  |  valid_loss: 0.872\n",
      "[epoch: 32, i:   999]  train_loss: 0.840  |  valid_loss: 0.914\n",
      "[epoch: 32, i:  1124]  train_loss: 0.710  |  valid_loss: 0.918\n",
      "[epoch: 32, i:  1249]  train_loss: 0.864  |  valid_loss: 0.727\n",
      "[epoch: 32, i:  1374]  train_loss: 0.803  |  valid_loss: 0.794\n",
      "[epoch: 32, i:  1499]  train_loss: 0.821  |  valid_loss: 0.828\n",
      "[epoch: 32, i:  1624]  train_loss: 0.817  |  valid_loss: 0.668\n",
      "[epoch: 32, i:  1749]  train_loss: 0.802  |  valid_loss: 0.980\n",
      "[epoch: 32, i:  1874]  train_loss: 0.852  |  valid_loss: 0.774\n",
      "[epoch: 32, i:  1999]  train_loss: 0.820  |  valid_loss: 0.978\n",
      "[epoch: 32, i:  2124]  train_loss: 0.763  |  valid_loss: 0.734\n",
      "[epoch: 32, i:  2249]  train_loss: 0.769  |  valid_loss: 0.823\n",
      "[epoch: 32, i:  2374]  train_loss: 0.771  |  valid_loss: 0.764\n",
      "[epoch: 32, i:  2499]  train_loss: 0.811  |  valid_loss: 1.000\n",
      "[epoch: 32, i:  2624]  train_loss: 0.740  |  valid_loss: 0.902\n",
      "[epoch: 32, i:  2749]  train_loss: 0.811  |  valid_loss: 0.979\n",
      "[epoch: 32, i:  2874]  train_loss: 0.887  |  valid_loss: 1.049\n",
      "[epoch: 32, i:  2999]  train_loss: 0.831  |  valid_loss: 0.841\n",
      "[epoch: 32, i:  3124]  train_loss: 0.802  |  valid_loss: 0.897\n",
      "[epoch: 32, i:  3249]  train_loss: 0.817  |  valid_loss: 1.058\n",
      "[epoch: 32, i:  3374]  train_loss: 0.876  |  valid_loss: 0.829\n",
      "[epoch: 32, i:  3499]  train_loss: 0.782  |  valid_loss: 0.753\n",
      "[epoch: 32, i:  3624]  train_loss: 0.712  |  valid_loss: 0.771\n",
      "[epoch: 32, i:  3749]  train_loss: 0.798  |  valid_loss: 0.699\n",
      "[epoch: 32, i:  3874]  train_loss: 0.825  |  valid_loss: 0.812\n",
      "[epoch: 32, i:  3999]  train_loss: 0.786  |  valid_loss: 0.628\n",
      "[epoch: 32, i:  4124]  train_loss: 0.777  |  valid_loss: 0.827\n",
      "[epoch: 32, i:  4249]  train_loss: 0.785  |  valid_loss: 0.877\n",
      "[epoch: 32, i:  4374]  train_loss: 0.824  |  valid_loss: 0.879\n",
      "[epoch: 32, i:  4499]  train_loss: 0.733  |  valid_loss: 0.776\n",
      "[epoch: 32, i:  4624]  train_loss: 0.841  |  valid_loss: 0.937\n",
      "[epoch: 32, i:  4749]  train_loss: 0.816  |  valid_loss: 0.896\n",
      "[epoch: 32, i:  4874]  train_loss: 0.810  |  valid_loss: 0.744\n",
      "[epoch: 32, i:  4999]  train_loss: 0.860  |  valid_loss: 0.727\n",
      "[epoch: 32, i:  5124]  train_loss: 0.845  |  valid_loss: 0.824\n",
      "[epoch: 32, i:  5249]  train_loss: 0.782  |  valid_loss: 0.884\n",
      "[epoch: 32, i:  5374]  train_loss: 0.841  |  valid_loss: 0.640\n",
      "[epoch: 32, i:  5499]  train_loss: 0.819  |  valid_loss: 0.699\n",
      "[epoch: 32, i:  5624]  train_loss: 0.848  |  valid_loss: 0.870\n",
      "[epoch: 32, i:  5749]  train_loss: 0.741  |  valid_loss: 0.833\n",
      "[epoch: 32, i:  5874]  train_loss: 0.744  |  valid_loss: 0.744\n",
      "[epoch: 32, i:  5999]  train_loss: 0.837  |  valid_loss: 0.850\n",
      "[epoch: 32, i:  6124]  train_loss: 0.851  |  valid_loss: 0.768\n",
      "[epoch: 32, i:  6249]  train_loss: 0.837  |  valid_loss: 0.864\n",
      "[epoch: 32, i:  6374]  train_loss: 0.745  |  valid_loss: 0.734\n",
      "[epoch: 32, i:  6499]  train_loss: 0.795  |  valid_loss: 0.767\n",
      "[epoch: 32, i:  6624]  train_loss: 0.830  |  valid_loss: 0.671\n",
      "[epoch: 32, i:  6749]  train_loss: 0.843  |  valid_loss: 0.793\n",
      "[epoch: 32, i:  6874]  train_loss: 0.819  |  valid_loss: 0.749\n",
      "[epoch: 32, i:  6999]  train_loss: 0.830  |  valid_loss: 0.962\n",
      "[epoch: 32, i:  7124]  train_loss: 0.884  |  valid_loss: 0.961\n",
      "[epoch: 32, i:  7249]  train_loss: 0.880  |  valid_loss: 0.673\n",
      "[epoch: 32, i:  7374]  train_loss: 0.850  |  valid_loss: 1.050\n",
      "[epoch: 32, i:  7499]  train_loss: 0.782  |  valid_loss: 0.898\n",
      "[epoch: 32, i:  7624]  train_loss: 0.800  |  valid_loss: 0.895\n",
      "[epoch: 32, i:  7749]  train_loss: 0.799  |  valid_loss: 0.800\n",
      "[epoch: 32, i:  7874]  train_loss: 0.743  |  valid_loss: 0.865\n",
      "[epoch: 32, i:  7999]  train_loss: 0.861  |  valid_loss: 0.716\n",
      "[epoch: 32, i:  8124]  train_loss: 0.755  |  valid_loss: 0.934\n",
      "[epoch: 32, i:  8249]  train_loss: 0.839  |  valid_loss: 0.903\n",
      "[epoch: 32, i:  8374]  train_loss: 0.896  |  valid_loss: 0.846\n",
      "[epoch: 32, i:  8499]  train_loss: 0.826  |  valid_loss: 0.906\n",
      "[epoch: 32, i:  8624]  train_loss: 0.796  |  valid_loss: 0.884\n",
      "[epoch: 32, i:  8749]  train_loss: 0.858  |  valid_loss: 0.895\n",
      "[epoch: 32, i:  8874]  train_loss: 0.821  |  valid_loss: 0.830\n",
      "[epoch: 32, i:  8999]  train_loss: 0.814  |  valid_loss: 0.695\n",
      "[epoch: 32, i:  9124]  train_loss: 0.813  |  valid_loss: 0.750\n",
      "[epoch: 32, i:  9249]  train_loss: 0.796  |  valid_loss: 0.613\n",
      "[epoch: 32, i:  9374]  train_loss: 0.802  |  valid_loss: 0.879\n",
      "[epoch: 32, i:  9499]  train_loss: 0.777  |  valid_loss: 0.765\n",
      "[epoch: 32, i:  9624]  train_loss: 0.816  |  valid_loss: 0.860\n",
      "[epoch: 32, i:  9749]  train_loss: 0.799  |  valid_loss: 0.903\n",
      "[epoch: 32, i:  9874]  train_loss: 0.810  |  valid_loss: 0.890\n",
      "[epoch: 32, i:  9999]  train_loss: 0.825  |  valid_loss: 0.828\n",
      "[epoch: 32, i: 10124]  train_loss: 0.759  |  valid_loss: 0.770\n",
      "[epoch: 32, i: 10249]  train_loss: 0.795  |  valid_loss: 0.860\n",
      "[epoch: 32, i: 10374]  train_loss: 0.860  |  valid_loss: 0.757\n",
      "[epoch: 32, i: 10499]  train_loss: 0.741  |  valid_loss: 0.945\n",
      "[epoch: 32, i: 10624]  train_loss: 0.803  |  valid_loss: 0.962\n",
      "[epoch: 32, i: 10749]  train_loss: 0.845  |  valid_loss: 0.839\n",
      "[epoch: 32, i: 10874]  train_loss: 0.848  |  valid_loss: 0.961\n",
      "[epoch: 32, i: 10999]  train_loss: 0.791  |  valid_loss: 0.966\n",
      "[epoch: 32, i: 11124]  train_loss: 0.844  |  valid_loss: 0.818\n",
      "[epoch: 32, i: 11249]  train_loss: 0.800  |  valid_loss: 0.860\n",
      "[epoch: 32, i: 11374]  train_loss: 0.730  |  valid_loss: 0.789\n",
      "[epoch: 32, i: 11499]  train_loss: 0.849  |  valid_loss: 0.622\n",
      "[epoch: 32, i: 11624]  train_loss: 0.835  |  valid_loss: 0.797\n",
      "[epoch: 32, i: 11749]  train_loss: 0.765  |  valid_loss: 0.734\n",
      "[epoch: 32, i: 11874]  train_loss: 0.786  |  valid_loss: 0.776\n",
      "[epoch: 32, i: 11999]  train_loss: 0.759  |  valid_loss: 0.693\n",
      "[epoch: 32, i: 12124]  train_loss: 0.763  |  valid_loss: 0.725\n",
      "[epoch: 32, i: 12249]  train_loss: 0.864  |  valid_loss: 1.085\n",
      "[epoch: 32, i: 12374]  train_loss: 0.825  |  valid_loss: 0.923\n",
      "[epoch: 32, i: 12499]  train_loss: 0.848  |  valid_loss: 0.853\n",
      "--> [End of epoch 32] train_accuracy: 71.32%  |  valid_accuracy: 71.04%\n",
      "--> [Start of epoch 33]  lr: 0.000001\n",
      "[epoch: 33, i:   124]  train_loss: 0.879  |  valid_loss: 0.734\n",
      "[epoch: 33, i:   249]  train_loss: 0.751  |  valid_loss: 0.743\n",
      "[epoch: 33, i:   374]  train_loss: 0.818  |  valid_loss: 0.854\n",
      "[epoch: 33, i:   499]  train_loss: 0.803  |  valid_loss: 0.775\n",
      "[epoch: 33, i:   624]  train_loss: 0.855  |  valid_loss: 0.951\n",
      "[epoch: 33, i:   749]  train_loss: 0.899  |  valid_loss: 0.565\n",
      "[epoch: 33, i:   874]  train_loss: 0.823  |  valid_loss: 0.849\n",
      "[epoch: 33, i:   999]  train_loss: 0.782  |  valid_loss: 0.939\n",
      "[epoch: 33, i:  1124]  train_loss: 0.793  |  valid_loss: 0.922\n",
      "[epoch: 33, i:  1249]  train_loss: 0.856  |  valid_loss: 0.727\n",
      "[epoch: 33, i:  1374]  train_loss: 0.712  |  valid_loss: 0.762\n",
      "[epoch: 33, i:  1499]  train_loss: 0.808  |  valid_loss: 0.804\n",
      "[epoch: 33, i:  1624]  train_loss: 0.807  |  valid_loss: 0.695\n",
      "[epoch: 33, i:  1749]  train_loss: 0.839  |  valid_loss: 0.963\n",
      "[epoch: 33, i:  1874]  train_loss: 0.758  |  valid_loss: 0.779\n",
      "[epoch: 33, i:  1999]  train_loss: 0.842  |  valid_loss: 1.010\n",
      "[epoch: 33, i:  2124]  train_loss: 0.836  |  valid_loss: 0.705\n",
      "[epoch: 33, i:  2249]  train_loss: 0.799  |  valid_loss: 0.817\n",
      "[epoch: 33, i:  2374]  train_loss: 0.825  |  valid_loss: 0.781\n",
      "[epoch: 33, i:  2499]  train_loss: 0.835  |  valid_loss: 0.997\n",
      "[epoch: 33, i:  2624]  train_loss: 0.843  |  valid_loss: 0.940\n",
      "[epoch: 33, i:  2749]  train_loss: 0.814  |  valid_loss: 0.917\n",
      "[epoch: 33, i:  2874]  train_loss: 0.774  |  valid_loss: 0.962\n",
      "[epoch: 33, i:  2999]  train_loss: 0.777  |  valid_loss: 0.830\n",
      "[epoch: 33, i:  3124]  train_loss: 0.810  |  valid_loss: 0.897\n",
      "[epoch: 33, i:  3249]  train_loss: 0.727  |  valid_loss: 1.039\n",
      "[epoch: 33, i:  3374]  train_loss: 0.862  |  valid_loss: 0.799\n",
      "[epoch: 33, i:  3499]  train_loss: 0.776  |  valid_loss: 0.744\n",
      "[epoch: 33, i:  3624]  train_loss: 0.810  |  valid_loss: 0.780\n",
      "[epoch: 33, i:  3749]  train_loss: 0.849  |  valid_loss: 0.697\n",
      "[epoch: 33, i:  3874]  train_loss: 0.813  |  valid_loss: 0.797\n",
      "[epoch: 33, i:  3999]  train_loss: 0.870  |  valid_loss: 0.636\n",
      "[epoch: 33, i:  4124]  train_loss: 0.895  |  valid_loss: 0.855\n",
      "[epoch: 33, i:  4249]  train_loss: 0.887  |  valid_loss: 0.897\n",
      "[epoch: 33, i:  4374]  train_loss: 0.890  |  valid_loss: 0.881\n",
      "[epoch: 33, i:  4499]  train_loss: 0.712  |  valid_loss: 0.805\n",
      "[epoch: 33, i:  4624]  train_loss: 0.769  |  valid_loss: 0.930\n",
      "[epoch: 33, i:  4749]  train_loss: 0.897  |  valid_loss: 0.871\n",
      "[epoch: 33, i:  4874]  train_loss: 0.814  |  valid_loss: 0.748\n",
      "[epoch: 33, i:  4999]  train_loss: 0.875  |  valid_loss: 0.735\n",
      "[epoch: 33, i:  5124]  train_loss: 0.775  |  valid_loss: 0.839\n",
      "[epoch: 33, i:  5249]  train_loss: 0.790  |  valid_loss: 0.896\n",
      "[epoch: 33, i:  5374]  train_loss: 0.818  |  valid_loss: 0.632\n",
      "[epoch: 33, i:  5499]  train_loss: 0.827  |  valid_loss: 0.691\n",
      "[epoch: 33, i:  5624]  train_loss: 0.812  |  valid_loss: 0.929\n",
      "[epoch: 33, i:  5749]  train_loss: 0.872  |  valid_loss: 0.805\n",
      "[epoch: 33, i:  5874]  train_loss: 0.909  |  valid_loss: 0.718\n",
      "[epoch: 33, i:  5999]  train_loss: 0.886  |  valid_loss: 0.833\n",
      "[epoch: 33, i:  6124]  train_loss: 0.853  |  valid_loss: 0.774\n",
      "[epoch: 33, i:  6249]  train_loss: 0.842  |  valid_loss: 0.849\n",
      "[epoch: 33, i:  6374]  train_loss: 0.738  |  valid_loss: 0.744\n",
      "[epoch: 33, i:  6499]  train_loss: 0.823  |  valid_loss: 0.758\n",
      "[epoch: 33, i:  6624]  train_loss: 0.820  |  valid_loss: 0.656\n",
      "[epoch: 33, i:  6749]  train_loss: 0.700  |  valid_loss: 0.841\n",
      "[epoch: 33, i:  6874]  train_loss: 0.823  |  valid_loss: 0.760\n",
      "[epoch: 33, i:  6999]  train_loss: 0.821  |  valid_loss: 0.987\n",
      "[epoch: 33, i:  7124]  train_loss: 0.814  |  valid_loss: 0.961\n",
      "[epoch: 33, i:  7249]  train_loss: 0.826  |  valid_loss: 0.663\n",
      "[epoch: 33, i:  7374]  train_loss: 0.786  |  valid_loss: 1.014\n",
      "[epoch: 33, i:  7499]  train_loss: 0.858  |  valid_loss: 0.912\n",
      "[epoch: 33, i:  7624]  train_loss: 0.836  |  valid_loss: 0.906\n",
      "[epoch: 33, i:  7749]  train_loss: 0.753  |  valid_loss: 0.798\n",
      "[epoch: 33, i:  7874]  train_loss: 0.822  |  valid_loss: 0.872\n",
      "[epoch: 33, i:  7999]  train_loss: 0.781  |  valid_loss: 0.722\n",
      "[epoch: 33, i:  8124]  train_loss: 0.800  |  valid_loss: 0.899\n",
      "[epoch: 33, i:  8249]  train_loss: 0.768  |  valid_loss: 0.889\n",
      "[epoch: 33, i:  8374]  train_loss: 0.849  |  valid_loss: 0.830\n",
      "[epoch: 33, i:  8499]  train_loss: 0.702  |  valid_loss: 0.906\n",
      "[epoch: 33, i:  8624]  train_loss: 0.876  |  valid_loss: 0.859\n",
      "[epoch: 33, i:  8749]  train_loss: 0.813  |  valid_loss: 0.882\n",
      "[epoch: 33, i:  8874]  train_loss: 0.733  |  valid_loss: 0.853\n",
      "[epoch: 33, i:  8999]  train_loss: 0.891  |  valid_loss: 0.692\n",
      "[epoch: 33, i:  9124]  train_loss: 0.791  |  valid_loss: 0.754\n",
      "[epoch: 33, i:  9249]  train_loss: 0.729  |  valid_loss: 0.614\n",
      "[epoch: 33, i:  9374]  train_loss: 0.832  |  valid_loss: 0.893\n",
      "[epoch: 33, i:  9499]  train_loss: 0.780  |  valid_loss: 0.743\n",
      "[epoch: 33, i:  9624]  train_loss: 0.892  |  valid_loss: 0.873\n",
      "[epoch: 33, i:  9749]  train_loss: 0.799  |  valid_loss: 0.923\n",
      "[epoch: 33, i:  9874]  train_loss: 0.785  |  valid_loss: 0.902\n",
      "[epoch: 33, i:  9999]  train_loss: 0.844  |  valid_loss: 0.811\n",
      "[epoch: 33, i: 10124]  train_loss: 0.770  |  valid_loss: 0.724\n",
      "[epoch: 33, i: 10249]  train_loss: 0.723  |  valid_loss: 0.881\n",
      "[epoch: 33, i: 10374]  train_loss: 0.748  |  valid_loss: 0.737\n",
      "[epoch: 33, i: 10499]  train_loss: 0.842  |  valid_loss: 0.980\n",
      "[epoch: 33, i: 10624]  train_loss: 0.784  |  valid_loss: 0.944\n",
      "[epoch: 33, i: 10749]  train_loss: 0.866  |  valid_loss: 0.833\n",
      "[epoch: 33, i: 10874]  train_loss: 0.754  |  valid_loss: 0.961\n",
      "[epoch: 33, i: 10999]  train_loss: 0.840  |  valid_loss: 0.963\n",
      "[epoch: 33, i: 11124]  train_loss: 0.855  |  valid_loss: 0.847\n",
      "[epoch: 33, i: 11249]  train_loss: 0.745  |  valid_loss: 0.860\n",
      "[epoch: 33, i: 11374]  train_loss: 0.817  |  valid_loss: 0.805\n",
      "[epoch: 33, i: 11499]  train_loss: 0.818  |  valid_loss: 0.605\n",
      "[epoch: 33, i: 11624]  train_loss: 0.833  |  valid_loss: 0.788\n",
      "[epoch: 33, i: 11749]  train_loss: 0.764  |  valid_loss: 0.751\n",
      "[epoch: 33, i: 11874]  train_loss: 0.793  |  valid_loss: 0.761\n",
      "[epoch: 33, i: 11999]  train_loss: 0.838  |  valid_loss: 0.709\n",
      "[epoch: 33, i: 12124]  train_loss: 0.763  |  valid_loss: 0.726\n",
      "[epoch: 33, i: 12249]  train_loss: 0.712  |  valid_loss: 1.054\n",
      "[epoch: 33, i: 12374]  train_loss: 0.856  |  valid_loss: 0.887\n",
      "[epoch: 33, i: 12499]  train_loss: 0.873  |  valid_loss: 0.863\n",
      "--> [End of epoch 33] train_accuracy: 71.34%  |  valid_accuracy: 71.05%\n",
      "--> [Start of epoch 34]  lr: 0.000001\n",
      "[epoch: 34, i:   124]  train_loss: 0.786  |  valid_loss: 0.730\n",
      "[epoch: 34, i:   249]  train_loss: 0.842  |  valid_loss: 0.716\n",
      "[epoch: 34, i:   374]  train_loss: 0.819  |  valid_loss: 0.846\n",
      "[epoch: 34, i:   499]  train_loss: 0.895  |  valid_loss: 0.804\n",
      "[epoch: 34, i:   624]  train_loss: 0.761  |  valid_loss: 0.897\n",
      "[epoch: 34, i:   749]  train_loss: 0.832  |  valid_loss: 0.577\n",
      "[epoch: 34, i:   874]  train_loss: 0.746  |  valid_loss: 0.875\n",
      "[epoch: 34, i:   999]  train_loss: 0.830  |  valid_loss: 0.924\n",
      "[epoch: 34, i:  1124]  train_loss: 0.815  |  valid_loss: 0.912\n",
      "[epoch: 34, i:  1249]  train_loss: 0.830  |  valid_loss: 0.722\n",
      "[epoch: 34, i:  1374]  train_loss: 0.771  |  valid_loss: 0.751\n",
      "[epoch: 34, i:  1499]  train_loss: 0.812  |  valid_loss: 0.820\n",
      "[epoch: 34, i:  1624]  train_loss: 0.799  |  valid_loss: 0.713\n",
      "[epoch: 34, i:  1749]  train_loss: 0.865  |  valid_loss: 0.941\n",
      "[epoch: 34, i:  1874]  train_loss: 0.865  |  valid_loss: 0.760\n",
      "[epoch: 34, i:  1999]  train_loss: 0.772  |  valid_loss: 0.988\n",
      "[epoch: 34, i:  2124]  train_loss: 0.779  |  valid_loss: 0.719\n",
      "[epoch: 34, i:  2249]  train_loss: 0.876  |  valid_loss: 0.821\n",
      "[epoch: 34, i:  2374]  train_loss: 0.828  |  valid_loss: 0.787\n",
      "[epoch: 34, i:  2499]  train_loss: 0.804  |  valid_loss: 0.995\n",
      "[epoch: 34, i:  2624]  train_loss: 0.727  |  valid_loss: 0.928\n",
      "[epoch: 34, i:  2749]  train_loss: 0.881  |  valid_loss: 0.931\n",
      "[epoch: 34, i:  2874]  train_loss: 0.810  |  valid_loss: 1.003\n",
      "[epoch: 34, i:  2999]  train_loss: 0.721  |  valid_loss: 0.824\n",
      "[epoch: 34, i:  3124]  train_loss: 0.872  |  valid_loss: 0.886\n",
      "[epoch: 34, i:  3249]  train_loss: 0.832  |  valid_loss: 1.033\n",
      "[epoch: 34, i:  3374]  train_loss: 0.815  |  valid_loss: 0.799\n",
      "[epoch: 34, i:  3499]  train_loss: 0.831  |  valid_loss: 0.804\n",
      "[epoch: 34, i:  3624]  train_loss: 0.752  |  valid_loss: 0.795\n",
      "[epoch: 34, i:  3749]  train_loss: 0.852  |  valid_loss: 0.682\n",
      "[epoch: 34, i:  3874]  train_loss: 0.808  |  valid_loss: 0.830\n",
      "[epoch: 34, i:  3999]  train_loss: 0.791  |  valid_loss: 0.642\n",
      "[epoch: 34, i:  4124]  train_loss: 0.835  |  valid_loss: 0.851\n",
      "[epoch: 34, i:  4249]  train_loss: 0.813  |  valid_loss: 0.882\n",
      "[epoch: 34, i:  4374]  train_loss: 0.810  |  valid_loss: 0.858\n",
      "[epoch: 34, i:  4499]  train_loss: 0.783  |  valid_loss: 0.763\n",
      "[epoch: 34, i:  4624]  train_loss: 0.769  |  valid_loss: 0.937\n",
      "[epoch: 34, i:  4749]  train_loss: 0.791  |  valid_loss: 0.900\n",
      "[epoch: 34, i:  4874]  train_loss: 0.960  |  valid_loss: 0.757\n",
      "[epoch: 34, i:  4999]  train_loss: 0.837  |  valid_loss: 0.740\n",
      "[epoch: 34, i:  5124]  train_loss: 0.793  |  valid_loss: 0.842\n",
      "[epoch: 34, i:  5249]  train_loss: 0.837  |  valid_loss: 0.881\n",
      "[epoch: 34, i:  5374]  train_loss: 0.851  |  valid_loss: 0.643\n",
      "[epoch: 34, i:  5499]  train_loss: 0.810  |  valid_loss: 0.712\n",
      "[epoch: 34, i:  5624]  train_loss: 0.811  |  valid_loss: 0.928\n",
      "[epoch: 34, i:  5749]  train_loss: 0.815  |  valid_loss: 0.814\n",
      "[epoch: 34, i:  5874]  train_loss: 0.853  |  valid_loss: 0.736\n",
      "[epoch: 34, i:  5999]  train_loss: 0.803  |  valid_loss: 0.832\n",
      "[epoch: 34, i:  6124]  train_loss: 0.798  |  valid_loss: 0.751\n",
      "[epoch: 34, i:  6249]  train_loss: 0.788  |  valid_loss: 0.854\n",
      "[epoch: 34, i:  6374]  train_loss: 0.793  |  valid_loss: 0.737\n",
      "[epoch: 34, i:  6499]  train_loss: 0.789  |  valid_loss: 0.760\n",
      "[epoch: 34, i:  6624]  train_loss: 0.774  |  valid_loss: 0.658\n",
      "[epoch: 34, i:  6749]  train_loss: 0.770  |  valid_loss: 0.823\n",
      "[epoch: 34, i:  6874]  train_loss: 0.845  |  valid_loss: 0.769\n",
      "[epoch: 34, i:  6999]  train_loss: 0.878  |  valid_loss: 0.956\n",
      "[epoch: 34, i:  7124]  train_loss: 0.770  |  valid_loss: 0.964\n",
      "[epoch: 34, i:  7249]  train_loss: 0.801  |  valid_loss: 0.637\n",
      "[epoch: 34, i:  7374]  train_loss: 0.798  |  valid_loss: 1.012\n",
      "[epoch: 34, i:  7499]  train_loss: 0.779  |  valid_loss: 0.903\n",
      "[epoch: 34, i:  7624]  train_loss: 0.866  |  valid_loss: 0.887\n",
      "[epoch: 34, i:  7749]  train_loss: 0.755  |  valid_loss: 0.802\n",
      "[epoch: 34, i:  7874]  train_loss: 0.819  |  valid_loss: 0.863\n",
      "[epoch: 34, i:  7999]  train_loss: 0.735  |  valid_loss: 0.714\n",
      "[epoch: 34, i:  8124]  train_loss: 0.753  |  valid_loss: 0.940\n",
      "[epoch: 34, i:  8249]  train_loss: 0.855  |  valid_loss: 0.871\n",
      "[epoch: 34, i:  8374]  train_loss: 0.842  |  valid_loss: 0.853\n",
      "[epoch: 34, i:  8499]  train_loss: 0.770  |  valid_loss: 0.901\n",
      "[epoch: 34, i:  8624]  train_loss: 0.777  |  valid_loss: 0.868\n",
      "[epoch: 34, i:  8749]  train_loss: 0.813  |  valid_loss: 0.901\n",
      "[epoch: 34, i:  8874]  train_loss: 0.790  |  valid_loss: 0.839\n",
      "[epoch: 34, i:  8999]  train_loss: 0.804  |  valid_loss: 0.683\n",
      "[epoch: 34, i:  9124]  train_loss: 0.764  |  valid_loss: 0.743\n",
      "[epoch: 34, i:  9249]  train_loss: 0.702  |  valid_loss: 0.634\n",
      "[epoch: 34, i:  9374]  train_loss: 0.806  |  valid_loss: 0.890\n",
      "[epoch: 34, i:  9499]  train_loss: 0.865  |  valid_loss: 0.753\n",
      "[epoch: 34, i:  9624]  train_loss: 0.760  |  valid_loss: 0.876\n",
      "[epoch: 34, i:  9749]  train_loss: 0.893  |  valid_loss: 0.900\n",
      "[epoch: 34, i:  9874]  train_loss: 0.843  |  valid_loss: 0.912\n",
      "[epoch: 34, i:  9999]  train_loss: 0.876  |  valid_loss: 0.813\n",
      "[epoch: 34, i: 10124]  train_loss: 0.783  |  valid_loss: 0.739\n",
      "[epoch: 34, i: 10249]  train_loss: 0.766  |  valid_loss: 0.859\n",
      "[epoch: 34, i: 10374]  train_loss: 0.779  |  valid_loss: 0.748\n",
      "[epoch: 34, i: 10499]  train_loss: 0.856  |  valid_loss: 0.908\n",
      "[epoch: 34, i: 10624]  train_loss: 0.835  |  valid_loss: 0.948\n",
      "[epoch: 34, i: 10749]  train_loss: 0.761  |  valid_loss: 0.857\n",
      "[epoch: 34, i: 10874]  train_loss: 0.759  |  valid_loss: 0.985\n",
      "[epoch: 34, i: 10999]  train_loss: 0.768  |  valid_loss: 0.991\n",
      "[epoch: 34, i: 11124]  train_loss: 0.831  |  valid_loss: 0.823\n",
      "[epoch: 34, i: 11249]  train_loss: 0.849  |  valid_loss: 0.860\n",
      "[epoch: 34, i: 11374]  train_loss: 0.878  |  valid_loss: 0.805\n",
      "[epoch: 34, i: 11499]  train_loss: 0.842  |  valid_loss: 0.605\n",
      "[epoch: 34, i: 11624]  train_loss: 0.803  |  valid_loss: 0.794\n",
      "[epoch: 34, i: 11749]  train_loss: 0.756  |  valid_loss: 0.749\n",
      "[epoch: 34, i: 11874]  train_loss: 0.833  |  valid_loss: 0.767\n",
      "[epoch: 34, i: 11999]  train_loss: 0.786  |  valid_loss: 0.700\n",
      "[epoch: 34, i: 12124]  train_loss: 0.774  |  valid_loss: 0.736\n",
      "[epoch: 34, i: 12249]  train_loss: 0.744  |  valid_loss: 1.034\n",
      "[epoch: 34, i: 12374]  train_loss: 0.849  |  valid_loss: 0.902\n",
      "[epoch: 34, i: 12499]  train_loss: 0.922  |  valid_loss: 0.852\n",
      "--> [End of epoch 34] train_accuracy: 71.65%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 35]  lr: 0.000001\n",
      "[epoch: 35, i:   124]  train_loss: 0.735  |  valid_loss: 0.740\n",
      "[epoch: 35, i:   249]  train_loss: 0.806  |  valid_loss: 0.708\n",
      "[epoch: 35, i:   374]  train_loss: 0.757  |  valid_loss: 0.861\n",
      "[epoch: 35, i:   499]  train_loss: 0.901  |  valid_loss: 0.814\n",
      "[epoch: 35, i:   624]  train_loss: 0.797  |  valid_loss: 0.920\n",
      "[epoch: 35, i:   749]  train_loss: 0.812  |  valid_loss: 0.585\n",
      "[epoch: 35, i:   874]  train_loss: 0.750  |  valid_loss: 0.853\n",
      "[epoch: 35, i:   999]  train_loss: 0.764  |  valid_loss: 0.931\n",
      "[epoch: 35, i:  1124]  train_loss: 0.773  |  valid_loss: 0.920\n",
      "[epoch: 35, i:  1249]  train_loss: 0.914  |  valid_loss: 0.739\n",
      "[epoch: 35, i:  1374]  train_loss: 0.809  |  valid_loss: 0.754\n",
      "[epoch: 35, i:  1499]  train_loss: 0.749  |  valid_loss: 0.815\n",
      "[epoch: 35, i:  1624]  train_loss: 0.792  |  valid_loss: 0.690\n",
      "[epoch: 35, i:  1749]  train_loss: 0.841  |  valid_loss: 1.005\n",
      "[epoch: 35, i:  1874]  train_loss: 0.889  |  valid_loss: 0.766\n",
      "[epoch: 35, i:  1999]  train_loss: 0.816  |  valid_loss: 1.023\n",
      "[epoch: 35, i:  2124]  train_loss: 0.821  |  valid_loss: 0.722\n",
      "[epoch: 35, i:  2249]  train_loss: 0.809  |  valid_loss: 0.863\n",
      "[epoch: 35, i:  2374]  train_loss: 0.838  |  valid_loss: 0.768\n",
      "[epoch: 35, i:  2499]  train_loss: 0.795  |  valid_loss: 0.993\n",
      "[epoch: 35, i:  2624]  train_loss: 0.807  |  valid_loss: 0.927\n",
      "[epoch: 35, i:  2749]  train_loss: 0.816  |  valid_loss: 0.901\n",
      "[epoch: 35, i:  2874]  train_loss: 0.869  |  valid_loss: 1.031\n",
      "[epoch: 35, i:  2999]  train_loss: 0.783  |  valid_loss: 0.849\n",
      "[epoch: 35, i:  3124]  train_loss: 0.878  |  valid_loss: 0.903\n",
      "[epoch: 35, i:  3249]  train_loss: 0.755  |  valid_loss: 1.058\n",
      "[epoch: 35, i:  3374]  train_loss: 0.813  |  valid_loss: 0.779\n",
      "[epoch: 35, i:  3499]  train_loss: 0.865  |  valid_loss: 0.764\n",
      "[epoch: 35, i:  3624]  train_loss: 0.777  |  valid_loss: 0.801\n",
      "[epoch: 35, i:  3749]  train_loss: 0.752  |  valid_loss: 0.700\n",
      "[epoch: 35, i:  3874]  train_loss: 0.797  |  valid_loss: 0.792\n",
      "[epoch: 35, i:  3999]  train_loss: 0.806  |  valid_loss: 0.638\n",
      "[epoch: 35, i:  4124]  train_loss: 0.762  |  valid_loss: 0.876\n",
      "[epoch: 35, i:  4249]  train_loss: 0.794  |  valid_loss: 0.852\n",
      "[epoch: 35, i:  4374]  train_loss: 0.801  |  valid_loss: 0.857\n",
      "[epoch: 35, i:  4499]  train_loss: 0.837  |  valid_loss: 0.780\n",
      "[epoch: 35, i:  4624]  train_loss: 0.817  |  valid_loss: 0.944\n",
      "[epoch: 35, i:  4749]  train_loss: 0.814  |  valid_loss: 0.902\n",
      "[epoch: 35, i:  4874]  train_loss: 0.818  |  valid_loss: 0.749\n",
      "[epoch: 35, i:  4999]  train_loss: 0.845  |  valid_loss: 0.746\n",
      "[epoch: 35, i:  5124]  train_loss: 0.799  |  valid_loss: 0.845\n",
      "[epoch: 35, i:  5249]  train_loss: 0.800  |  valid_loss: 0.871\n",
      "[epoch: 35, i:  5374]  train_loss: 0.802  |  valid_loss: 0.631\n",
      "[epoch: 35, i:  5499]  train_loss: 0.783  |  valid_loss: 0.691\n",
      "[epoch: 35, i:  5624]  train_loss: 0.862  |  valid_loss: 0.919\n",
      "[epoch: 35, i:  5749]  train_loss: 0.794  |  valid_loss: 0.829\n",
      "[epoch: 35, i:  5874]  train_loss: 0.822  |  valid_loss: 0.720\n",
      "[epoch: 35, i:  5999]  train_loss: 0.727  |  valid_loss: 0.849\n",
      "[epoch: 35, i:  6124]  train_loss: 0.808  |  valid_loss: 0.751\n",
      "[epoch: 35, i:  6249]  train_loss: 0.787  |  valid_loss: 0.862\n",
      "[epoch: 35, i:  6374]  train_loss: 0.864  |  valid_loss: 0.740\n",
      "[epoch: 35, i:  6499]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 35, i:  6624]  train_loss: 0.863  |  valid_loss: 0.680\n",
      "[epoch: 35, i:  6749]  train_loss: 0.798  |  valid_loss: 0.771\n",
      "[epoch: 35, i:  6874]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 35, i:  6999]  train_loss: 0.872  |  valid_loss: 0.945\n",
      "[epoch: 35, i:  7124]  train_loss: 0.789  |  valid_loss: 0.967\n",
      "[epoch: 35, i:  7249]  train_loss: 0.793  |  valid_loss: 0.627\n",
      "[epoch: 35, i:  7374]  train_loss: 0.824  |  valid_loss: 1.032\n",
      "[epoch: 35, i:  7499]  train_loss: 0.793  |  valid_loss: 0.919\n",
      "[epoch: 35, i:  7624]  train_loss: 0.750  |  valid_loss: 0.914\n",
      "[epoch: 35, i:  7749]  train_loss: 0.784  |  valid_loss: 0.811\n",
      "[epoch: 35, i:  7874]  train_loss: 0.791  |  valid_loss: 0.888\n",
      "[epoch: 35, i:  7999]  train_loss: 0.804  |  valid_loss: 0.720\n",
      "[epoch: 35, i:  8124]  train_loss: 0.828  |  valid_loss: 0.908\n",
      "[epoch: 35, i:  8249]  train_loss: 0.732  |  valid_loss: 0.884\n",
      "[epoch: 35, i:  8374]  train_loss: 0.855  |  valid_loss: 0.818\n",
      "[epoch: 35, i:  8499]  train_loss: 0.722  |  valid_loss: 0.909\n",
      "[epoch: 35, i:  8624]  train_loss: 0.821  |  valid_loss: 0.874\n",
      "[epoch: 35, i:  8749]  train_loss: 0.836  |  valid_loss: 0.907\n",
      "[epoch: 35, i:  8874]  train_loss: 0.794  |  valid_loss: 0.857\n",
      "[epoch: 35, i:  8999]  train_loss: 0.824  |  valid_loss: 0.697\n",
      "[epoch: 35, i:  9124]  train_loss: 0.810  |  valid_loss: 0.716\n",
      "[epoch: 35, i:  9249]  train_loss: 0.858  |  valid_loss: 0.599\n",
      "[epoch: 35, i:  9374]  train_loss: 0.864  |  valid_loss: 0.910\n",
      "[epoch: 35, i:  9499]  train_loss: 0.743  |  valid_loss: 0.754\n",
      "[epoch: 35, i:  9624]  train_loss: 0.826  |  valid_loss: 0.908\n",
      "[epoch: 35, i:  9749]  train_loss: 0.764  |  valid_loss: 0.909\n",
      "[epoch: 35, i:  9874]  train_loss: 0.803  |  valid_loss: 0.884\n",
      "[epoch: 35, i:  9999]  train_loss: 0.723  |  valid_loss: 0.810\n",
      "[epoch: 35, i: 10124]  train_loss: 0.797  |  valid_loss: 0.741\n",
      "[epoch: 35, i: 10249]  train_loss: 0.837  |  valid_loss: 0.868\n",
      "[epoch: 35, i: 10374]  train_loss: 0.840  |  valid_loss: 0.748\n",
      "[epoch: 35, i: 10499]  train_loss: 0.806  |  valid_loss: 0.986\n",
      "[epoch: 35, i: 10624]  train_loss: 0.772  |  valid_loss: 0.934\n",
      "[epoch: 35, i: 10749]  train_loss: 0.858  |  valid_loss: 0.858\n",
      "[epoch: 35, i: 10874]  train_loss: 0.843  |  valid_loss: 0.948\n",
      "[epoch: 35, i: 10999]  train_loss: 0.813  |  valid_loss: 0.972\n",
      "[epoch: 35, i: 11124]  train_loss: 0.798  |  valid_loss: 0.811\n",
      "[epoch: 35, i: 11249]  train_loss: 0.816  |  valid_loss: 0.868\n",
      "[epoch: 35, i: 11374]  train_loss: 0.755  |  valid_loss: 0.789\n",
      "[epoch: 35, i: 11499]  train_loss: 0.851  |  valid_loss: 0.617\n",
      "[epoch: 35, i: 11624]  train_loss: 0.866  |  valid_loss: 0.807\n",
      "[epoch: 35, i: 11749]  train_loss: 0.836  |  valid_loss: 0.775\n",
      "[epoch: 35, i: 11874]  train_loss: 0.835  |  valid_loss: 0.767\n",
      "[epoch: 35, i: 11999]  train_loss: 0.824  |  valid_loss: 0.700\n",
      "[epoch: 35, i: 12124]  train_loss: 0.864  |  valid_loss: 0.696\n",
      "[epoch: 35, i: 12249]  train_loss: 0.834  |  valid_loss: 1.049\n",
      "[epoch: 35, i: 12374]  train_loss: 0.815  |  valid_loss: 0.870\n",
      "[epoch: 35, i: 12499]  train_loss: 0.762  |  valid_loss: 0.862\n",
      "--> [End of epoch 35] train_accuracy: 71.62%  |  valid_accuracy: 71.05%\n",
      "--> [Start of epoch 36]  lr: 0.000001\n",
      "[epoch: 36, i:   124]  train_loss: 0.757  |  valid_loss: 0.733\n",
      "[epoch: 36, i:   249]  train_loss: 0.745  |  valid_loss: 0.742\n",
      "[epoch: 36, i:   374]  train_loss: 0.843  |  valid_loss: 0.841\n",
      "[epoch: 36, i:   499]  train_loss: 0.740  |  valid_loss: 0.792\n",
      "[epoch: 36, i:   624]  train_loss: 0.766  |  valid_loss: 0.937\n",
      "[epoch: 36, i:   749]  train_loss: 0.823  |  valid_loss: 0.572\n",
      "[epoch: 36, i:   874]  train_loss: 0.806  |  valid_loss: 0.843\n",
      "[epoch: 36, i:   999]  train_loss: 0.805  |  valid_loss: 0.920\n",
      "[epoch: 36, i:  1124]  train_loss: 0.804  |  valid_loss: 0.957\n",
      "[epoch: 36, i:  1249]  train_loss: 0.850  |  valid_loss: 0.715\n",
      "[epoch: 36, i:  1374]  train_loss: 0.866  |  valid_loss: 0.795\n",
      "[epoch: 36, i:  1499]  train_loss: 0.826  |  valid_loss: 0.803\n",
      "[epoch: 36, i:  1624]  train_loss: 0.794  |  valid_loss: 0.671\n",
      "[epoch: 36, i:  1749]  train_loss: 0.810  |  valid_loss: 0.955\n",
      "[epoch: 36, i:  1874]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 36, i:  1999]  train_loss: 0.868  |  valid_loss: 0.988\n",
      "[epoch: 36, i:  2124]  train_loss: 0.841  |  valid_loss: 0.717\n",
      "[epoch: 36, i:  2249]  train_loss: 0.803  |  valid_loss: 0.833\n",
      "[epoch: 36, i:  2374]  train_loss: 0.789  |  valid_loss: 0.764\n",
      "[epoch: 36, i:  2499]  train_loss: 0.891  |  valid_loss: 1.004\n",
      "[epoch: 36, i:  2624]  train_loss: 0.739  |  valid_loss: 0.934\n",
      "[epoch: 36, i:  2749]  train_loss: 0.810  |  valid_loss: 0.948\n",
      "[epoch: 36, i:  2874]  train_loss: 0.809  |  valid_loss: 1.013\n",
      "[epoch: 36, i:  2999]  train_loss: 0.777  |  valid_loss: 0.823\n",
      "[epoch: 36, i:  3124]  train_loss: 0.856  |  valid_loss: 0.884\n",
      "[epoch: 36, i:  3249]  train_loss: 0.790  |  valid_loss: 1.066\n",
      "[epoch: 36, i:  3374]  train_loss: 0.773  |  valid_loss: 0.785\n",
      "[epoch: 36, i:  3499]  train_loss: 0.775  |  valid_loss: 0.765\n",
      "[epoch: 36, i:  3624]  train_loss: 0.795  |  valid_loss: 0.803\n",
      "[epoch: 36, i:  3749]  train_loss: 0.780  |  valid_loss: 0.688\n",
      "[epoch: 36, i:  3874]  train_loss: 0.804  |  valid_loss: 0.794\n",
      "[epoch: 36, i:  3999]  train_loss: 0.783  |  valid_loss: 0.652\n",
      "[epoch: 36, i:  4124]  train_loss: 0.842  |  valid_loss: 0.886\n",
      "[epoch: 36, i:  4249]  train_loss: 0.807  |  valid_loss: 0.857\n",
      "[epoch: 36, i:  4374]  train_loss: 0.849  |  valid_loss: 0.878\n",
      "[epoch: 36, i:  4499]  train_loss: 0.816  |  valid_loss: 0.784\n",
      "[epoch: 36, i:  4624]  train_loss: 0.837  |  valid_loss: 0.946\n",
      "[epoch: 36, i:  4749]  train_loss: 0.687  |  valid_loss: 0.871\n",
      "[epoch: 36, i:  4874]  train_loss: 0.868  |  valid_loss: 0.759\n",
      "[epoch: 36, i:  4999]  train_loss: 0.748  |  valid_loss: 0.732\n",
      "[epoch: 36, i:  5124]  train_loss: 0.784  |  valid_loss: 0.819\n",
      "[epoch: 36, i:  5249]  train_loss: 0.848  |  valid_loss: 0.881\n",
      "[epoch: 36, i:  5374]  train_loss: 0.826  |  valid_loss: 0.630\n",
      "[epoch: 36, i:  5499]  train_loss: 0.755  |  valid_loss: 0.724\n",
      "[epoch: 36, i:  5624]  train_loss: 0.799  |  valid_loss: 0.970\n",
      "[epoch: 36, i:  5749]  train_loss: 0.760  |  valid_loss: 0.818\n",
      "[epoch: 36, i:  5874]  train_loss: 0.766  |  valid_loss: 0.731\n",
      "[epoch: 36, i:  5999]  train_loss: 0.795  |  valid_loss: 0.844\n",
      "[epoch: 36, i:  6124]  train_loss: 0.855  |  valid_loss: 0.736\n",
      "[epoch: 36, i:  6249]  train_loss: 0.819  |  valid_loss: 0.877\n",
      "[epoch: 36, i:  6374]  train_loss: 0.813  |  valid_loss: 0.740\n",
      "[epoch: 36, i:  6499]  train_loss: 0.812  |  valid_loss: 0.784\n",
      "[epoch: 36, i:  6624]  train_loss: 0.845  |  valid_loss: 0.688\n",
      "[epoch: 36, i:  6749]  train_loss: 0.795  |  valid_loss: 0.821\n",
      "[epoch: 36, i:  6874]  train_loss: 0.845  |  valid_loss: 0.744\n",
      "[epoch: 36, i:  6999]  train_loss: 0.760  |  valid_loss: 0.954\n",
      "[epoch: 36, i:  7124]  train_loss: 0.827  |  valid_loss: 0.964\n",
      "[epoch: 36, i:  7249]  train_loss: 0.868  |  valid_loss: 0.665\n",
      "[epoch: 36, i:  7374]  train_loss: 0.815  |  valid_loss: 1.040\n",
      "[epoch: 36, i:  7499]  train_loss: 0.887  |  valid_loss: 0.880\n",
      "[epoch: 36, i:  7624]  train_loss: 0.791  |  valid_loss: 0.885\n",
      "[epoch: 36, i:  7749]  train_loss: 0.874  |  valid_loss: 0.811\n",
      "[epoch: 36, i:  7874]  train_loss: 0.808  |  valid_loss: 0.882\n",
      "[epoch: 36, i:  7999]  train_loss: 0.863  |  valid_loss: 0.710\n",
      "[epoch: 36, i:  8124]  train_loss: 0.852  |  valid_loss: 0.905\n",
      "[epoch: 36, i:  8249]  train_loss: 0.864  |  valid_loss: 0.924\n",
      "[epoch: 36, i:  8374]  train_loss: 0.809  |  valid_loss: 0.836\n",
      "[epoch: 36, i:  8499]  train_loss: 0.776  |  valid_loss: 0.934\n",
      "[epoch: 36, i:  8624]  train_loss: 0.807  |  valid_loss: 0.854\n",
      "[epoch: 36, i:  8749]  train_loss: 0.823  |  valid_loss: 0.898\n",
      "[epoch: 36, i:  8874]  train_loss: 0.816  |  valid_loss: 0.857\n",
      "[epoch: 36, i:  8999]  train_loss: 0.870  |  valid_loss: 0.691\n",
      "[epoch: 36, i:  9124]  train_loss: 0.835  |  valid_loss: 0.714\n",
      "[epoch: 36, i:  9249]  train_loss: 0.833  |  valid_loss: 0.614\n",
      "[epoch: 36, i:  9374]  train_loss: 0.802  |  valid_loss: 0.893\n",
      "[epoch: 36, i:  9499]  train_loss: 0.882  |  valid_loss: 0.752\n",
      "[epoch: 36, i:  9624]  train_loss: 0.790  |  valid_loss: 0.896\n",
      "[epoch: 36, i:  9749]  train_loss: 0.847  |  valid_loss: 0.911\n",
      "[epoch: 36, i:  9874]  train_loss: 0.823  |  valid_loss: 0.881\n",
      "[epoch: 36, i:  9999]  train_loss: 0.813  |  valid_loss: 0.813\n",
      "[epoch: 36, i: 10124]  train_loss: 0.812  |  valid_loss: 0.745\n",
      "[epoch: 36, i: 10249]  train_loss: 0.855  |  valid_loss: 0.845\n",
      "[epoch: 36, i: 10374]  train_loss: 0.795  |  valid_loss: 0.739\n",
      "[epoch: 36, i: 10499]  train_loss: 0.784  |  valid_loss: 0.949\n",
      "[epoch: 36, i: 10624]  train_loss: 0.735  |  valid_loss: 0.937\n",
      "[epoch: 36, i: 10749]  train_loss: 0.799  |  valid_loss: 0.846\n",
      "[epoch: 36, i: 10874]  train_loss: 0.800  |  valid_loss: 0.976\n",
      "[epoch: 36, i: 10999]  train_loss: 0.837  |  valid_loss: 0.937\n",
      "[epoch: 36, i: 11124]  train_loss: 0.779  |  valid_loss: 0.812\n",
      "[epoch: 36, i: 11249]  train_loss: 0.895  |  valid_loss: 0.856\n",
      "[epoch: 36, i: 11374]  train_loss: 0.761  |  valid_loss: 0.821\n",
      "[epoch: 36, i: 11499]  train_loss: 0.826  |  valid_loss: 0.597\n",
      "[epoch: 36, i: 11624]  train_loss: 0.790  |  valid_loss: 0.792\n",
      "[epoch: 36, i: 11749]  train_loss: 0.792  |  valid_loss: 0.735\n",
      "[epoch: 36, i: 11874]  train_loss: 0.892  |  valid_loss: 0.782\n",
      "[epoch: 36, i: 11999]  train_loss: 0.796  |  valid_loss: 0.695\n",
      "[epoch: 36, i: 12124]  train_loss: 0.765  |  valid_loss: 0.730\n",
      "[epoch: 36, i: 12249]  train_loss: 0.870  |  valid_loss: 1.040\n",
      "[epoch: 36, i: 12374]  train_loss: 0.828  |  valid_loss: 0.883\n",
      "[epoch: 36, i: 12499]  train_loss: 0.776  |  valid_loss: 0.876\n",
      "--> [End of epoch 36] train_accuracy: 71.46%  |  valid_accuracy: 71.19%\n",
      "--> [Start of epoch 37]  lr: 0.000001\n",
      "[epoch: 37, i:   124]  train_loss: 0.835  |  valid_loss: 0.763\n",
      "[epoch: 37, i:   249]  train_loss: 0.867  |  valid_loss: 0.729\n",
      "[epoch: 37, i:   374]  train_loss: 0.876  |  valid_loss: 0.834\n",
      "[epoch: 37, i:   499]  train_loss: 0.824  |  valid_loss: 0.796\n",
      "[epoch: 37, i:   624]  train_loss: 0.862  |  valid_loss: 0.904\n",
      "[epoch: 37, i:   749]  train_loss: 0.813  |  valid_loss: 0.569\n",
      "[epoch: 37, i:   874]  train_loss: 0.824  |  valid_loss: 0.864\n",
      "[epoch: 37, i:   999]  train_loss: 0.771  |  valid_loss: 0.931\n",
      "[epoch: 37, i:  1124]  train_loss: 0.841  |  valid_loss: 0.939\n",
      "[epoch: 37, i:  1249]  train_loss: 0.821  |  valid_loss: 0.711\n",
      "[epoch: 37, i:  1374]  train_loss: 0.810  |  valid_loss: 0.802\n",
      "[epoch: 37, i:  1499]  train_loss: 0.771  |  valid_loss: 0.819\n",
      "[epoch: 37, i:  1624]  train_loss: 0.695  |  valid_loss: 0.697\n",
      "[epoch: 37, i:  1749]  train_loss: 0.802  |  valid_loss: 0.940\n",
      "[epoch: 37, i:  1874]  train_loss: 0.813  |  valid_loss: 0.773\n",
      "[epoch: 37, i:  1999]  train_loss: 0.766  |  valid_loss: 0.998\n",
      "[epoch: 37, i:  2124]  train_loss: 0.713  |  valid_loss: 0.703\n",
      "[epoch: 37, i:  2249]  train_loss: 0.750  |  valid_loss: 0.831\n",
      "[epoch: 37, i:  2374]  train_loss: 0.798  |  valid_loss: 0.776\n",
      "[epoch: 37, i:  2499]  train_loss: 0.875  |  valid_loss: 1.008\n",
      "[epoch: 37, i:  2624]  train_loss: 0.813  |  valid_loss: 0.926\n",
      "[epoch: 37, i:  2749]  train_loss: 0.850  |  valid_loss: 0.994\n",
      "[epoch: 37, i:  2874]  train_loss: 0.773  |  valid_loss: 0.967\n",
      "[epoch: 37, i:  2999]  train_loss: 0.738  |  valid_loss: 0.842\n",
      "[epoch: 37, i:  3124]  train_loss: 0.766  |  valid_loss: 0.876\n",
      "[epoch: 37, i:  3249]  train_loss: 0.860  |  valid_loss: 1.050\n",
      "[epoch: 37, i:  3374]  train_loss: 0.857  |  valid_loss: 0.790\n",
      "[epoch: 37, i:  3499]  train_loss: 0.796  |  valid_loss: 0.749\n",
      "[epoch: 37, i:  3624]  train_loss: 0.845  |  valid_loss: 0.803\n",
      "[epoch: 37, i:  3749]  train_loss: 0.779  |  valid_loss: 0.700\n",
      "[epoch: 37, i:  3874]  train_loss: 0.885  |  valid_loss: 0.811\n",
      "[epoch: 37, i:  3999]  train_loss: 0.793  |  valid_loss: 0.643\n",
      "[epoch: 37, i:  4124]  train_loss: 0.872  |  valid_loss: 0.880\n",
      "[epoch: 37, i:  4249]  train_loss: 0.755  |  valid_loss: 0.874\n",
      "[epoch: 37, i:  4374]  train_loss: 0.783  |  valid_loss: 0.868\n",
      "[epoch: 37, i:  4499]  train_loss: 0.814  |  valid_loss: 0.780\n",
      "[epoch: 37, i:  4624]  train_loss: 0.840  |  valid_loss: 0.951\n",
      "[epoch: 37, i:  4749]  train_loss: 0.905  |  valid_loss: 0.888\n",
      "[epoch: 37, i:  4874]  train_loss: 0.771  |  valid_loss: 0.751\n",
      "[epoch: 37, i:  4999]  train_loss: 0.818  |  valid_loss: 0.727\n",
      "[epoch: 37, i:  5124]  train_loss: 0.834  |  valid_loss: 0.816\n",
      "[epoch: 37, i:  5249]  train_loss: 0.829  |  valid_loss: 0.881\n",
      "[epoch: 37, i:  5374]  train_loss: 0.824  |  valid_loss: 0.628\n",
      "[epoch: 37, i:  5499]  train_loss: 0.830  |  valid_loss: 0.702\n",
      "[epoch: 37, i:  5624]  train_loss: 0.772  |  valid_loss: 1.009\n",
      "[epoch: 37, i:  5749]  train_loss: 0.831  |  valid_loss: 0.812\n",
      "[epoch: 37, i:  5874]  train_loss: 0.764  |  valid_loss: 0.758\n",
      "[epoch: 37, i:  5999]  train_loss: 0.823  |  valid_loss: 0.858\n",
      "[epoch: 37, i:  6124]  train_loss: 0.756  |  valid_loss: 0.750\n",
      "[epoch: 37, i:  6249]  train_loss: 0.786  |  valid_loss: 0.870\n",
      "[epoch: 37, i:  6374]  train_loss: 0.765  |  valid_loss: 0.726\n",
      "[epoch: 37, i:  6499]  train_loss: 0.818  |  valid_loss: 0.769\n",
      "[epoch: 37, i:  6624]  train_loss: 0.731  |  valid_loss: 0.715\n",
      "[epoch: 37, i:  6749]  train_loss: 0.788  |  valid_loss: 0.796\n",
      "[epoch: 37, i:  6874]  train_loss: 0.817  |  valid_loss: 0.766\n",
      "[epoch: 37, i:  6999]  train_loss: 0.843  |  valid_loss: 0.925\n",
      "[epoch: 37, i:  7124]  train_loss: 0.823  |  valid_loss: 0.961\n",
      "[epoch: 37, i:  7249]  train_loss: 0.804  |  valid_loss: 0.642\n",
      "[epoch: 37, i:  7374]  train_loss: 0.884  |  valid_loss: 1.042\n",
      "[epoch: 37, i:  7499]  train_loss: 0.812  |  valid_loss: 0.907\n",
      "[epoch: 37, i:  7624]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 37, i:  7749]  train_loss: 0.769  |  valid_loss: 0.790\n",
      "[epoch: 37, i:  7874]  train_loss: 0.840  |  valid_loss: 0.861\n",
      "[epoch: 37, i:  7999]  train_loss: 0.734  |  valid_loss: 0.732\n",
      "[epoch: 37, i:  8124]  train_loss: 0.838  |  valid_loss: 0.895\n",
      "[epoch: 37, i:  8249]  train_loss: 0.790  |  valid_loss: 0.899\n",
      "[epoch: 37, i:  8374]  train_loss: 0.795  |  valid_loss: 0.844\n",
      "[epoch: 37, i:  8499]  train_loss: 0.783  |  valid_loss: 0.923\n",
      "[epoch: 37, i:  8624]  train_loss: 0.822  |  valid_loss: 0.858\n",
      "[epoch: 37, i:  8749]  train_loss: 0.831  |  valid_loss: 0.919\n",
      "[epoch: 37, i:  8874]  train_loss: 0.784  |  valid_loss: 0.846\n",
      "[epoch: 37, i:  8999]  train_loss: 0.861  |  valid_loss: 0.692\n",
      "[epoch: 37, i:  9124]  train_loss: 0.742  |  valid_loss: 0.724\n",
      "[epoch: 37, i:  9249]  train_loss: 0.824  |  valid_loss: 0.622\n",
      "[epoch: 37, i:  9374]  train_loss: 0.854  |  valid_loss: 0.886\n",
      "[epoch: 37, i:  9499]  train_loss: 0.773  |  valid_loss: 0.761\n",
      "[epoch: 37, i:  9624]  train_loss: 0.807  |  valid_loss: 0.879\n",
      "[epoch: 37, i:  9749]  train_loss: 0.815  |  valid_loss: 0.916\n",
      "[epoch: 37, i:  9874]  train_loss: 0.748  |  valid_loss: 0.891\n",
      "[epoch: 37, i:  9999]  train_loss: 0.794  |  valid_loss: 0.814\n",
      "[epoch: 37, i: 10124]  train_loss: 0.838  |  valid_loss: 0.766\n",
      "[epoch: 37, i: 10249]  train_loss: 0.892  |  valid_loss: 0.866\n",
      "[epoch: 37, i: 10374]  train_loss: 0.800  |  valid_loss: 0.756\n",
      "[epoch: 37, i: 10499]  train_loss: 0.749  |  valid_loss: 0.967\n",
      "[epoch: 37, i: 10624]  train_loss: 0.775  |  valid_loss: 0.934\n",
      "[epoch: 37, i: 10749]  train_loss: 0.799  |  valid_loss: 0.835\n",
      "[epoch: 37, i: 10874]  train_loss: 0.868  |  valid_loss: 0.947\n",
      "[epoch: 37, i: 10999]  train_loss: 0.833  |  valid_loss: 0.997\n",
      "[epoch: 37, i: 11124]  train_loss: 0.810  |  valid_loss: 0.816\n",
      "[epoch: 37, i: 11249]  train_loss: 0.828  |  valid_loss: 0.856\n",
      "[epoch: 37, i: 11374]  train_loss: 0.862  |  valid_loss: 0.772\n",
      "[epoch: 37, i: 11499]  train_loss: 0.822  |  valid_loss: 0.601\n",
      "[epoch: 37, i: 11624]  train_loss: 0.732  |  valid_loss: 0.791\n",
      "[epoch: 37, i: 11749]  train_loss: 0.877  |  valid_loss: 0.763\n",
      "[epoch: 37, i: 11874]  train_loss: 0.801  |  valid_loss: 0.790\n",
      "[epoch: 37, i: 11999]  train_loss: 0.904  |  valid_loss: 0.694\n",
      "[epoch: 37, i: 12124]  train_loss: 0.768  |  valid_loss: 0.723\n",
      "[epoch: 37, i: 12249]  train_loss: 0.844  |  valid_loss: 1.056\n",
      "[epoch: 37, i: 12374]  train_loss: 0.829  |  valid_loss: 0.881\n",
      "[epoch: 37, i: 12499]  train_loss: 0.752  |  valid_loss: 0.869\n",
      "--> [End of epoch 37] train_accuracy: 71.48%  |  valid_accuracy: 71.01%\n",
      "--> [Start of epoch 38]  lr: 0.000000\n",
      "[epoch: 38, i:   124]  train_loss: 0.801  |  valid_loss: 0.742\n",
      "[epoch: 38, i:   249]  train_loss: 0.817  |  valid_loss: 0.715\n",
      "[epoch: 38, i:   374]  train_loss: 0.826  |  valid_loss: 0.863\n",
      "[epoch: 38, i:   499]  train_loss: 0.765  |  valid_loss: 0.783\n",
      "[epoch: 38, i:   624]  train_loss: 0.838  |  valid_loss: 0.912\n",
      "[epoch: 38, i:   749]  train_loss: 0.837  |  valid_loss: 0.572\n",
      "[epoch: 38, i:   874]  train_loss: 0.827  |  valid_loss: 0.862\n",
      "[epoch: 38, i:   999]  train_loss: 0.811  |  valid_loss: 0.937\n",
      "[epoch: 38, i:  1124]  train_loss: 0.785  |  valid_loss: 0.917\n",
      "[epoch: 38, i:  1249]  train_loss: 0.860  |  valid_loss: 0.731\n",
      "[epoch: 38, i:  1374]  train_loss: 0.790  |  valid_loss: 0.773\n",
      "[epoch: 38, i:  1499]  train_loss: 0.856  |  valid_loss: 0.817\n",
      "[epoch: 38, i:  1624]  train_loss: 0.761  |  valid_loss: 0.670\n",
      "[epoch: 38, i:  1749]  train_loss: 0.734  |  valid_loss: 0.974\n",
      "[epoch: 38, i:  1874]  train_loss: 0.870  |  valid_loss: 0.771\n",
      "[epoch: 38, i:  1999]  train_loss: 0.859  |  valid_loss: 0.989\n",
      "[epoch: 38, i:  2124]  train_loss: 0.804  |  valid_loss: 0.700\n",
      "[epoch: 38, i:  2249]  train_loss: 0.820  |  valid_loss: 0.815\n",
      "[epoch: 38, i:  2374]  train_loss: 0.819  |  valid_loss: 0.754\n",
      "[epoch: 38, i:  2499]  train_loss: 0.830  |  valid_loss: 0.989\n",
      "[epoch: 38, i:  2624]  train_loss: 0.806  |  valid_loss: 0.911\n",
      "[epoch: 38, i:  2749]  train_loss: 0.850  |  valid_loss: 0.909\n",
      "[epoch: 38, i:  2874]  train_loss: 0.842  |  valid_loss: 0.964\n",
      "[epoch: 38, i:  2999]  train_loss: 0.808  |  valid_loss: 0.839\n",
      "[epoch: 38, i:  3124]  train_loss: 0.774  |  valid_loss: 0.890\n",
      "[epoch: 38, i:  3249]  train_loss: 0.826  |  valid_loss: 1.048\n",
      "[epoch: 38, i:  3374]  train_loss: 0.813  |  valid_loss: 0.839\n",
      "[epoch: 38, i:  3499]  train_loss: 0.733  |  valid_loss: 0.782\n",
      "[epoch: 38, i:  3624]  train_loss: 0.808  |  valid_loss: 0.806\n",
      "[epoch: 38, i:  3749]  train_loss: 0.772  |  valid_loss: 0.684\n",
      "[epoch: 38, i:  3874]  train_loss: 0.801  |  valid_loss: 0.790\n",
      "[epoch: 38, i:  3999]  train_loss: 0.858  |  valid_loss: 0.656\n",
      "[epoch: 38, i:  4124]  train_loss: 0.761  |  valid_loss: 0.913\n",
      "[epoch: 38, i:  4249]  train_loss: 0.805  |  valid_loss: 0.879\n",
      "[epoch: 38, i:  4374]  train_loss: 0.839  |  valid_loss: 0.878\n",
      "[epoch: 38, i:  4499]  train_loss: 0.762  |  valid_loss: 0.805\n",
      "[epoch: 38, i:  4624]  train_loss: 0.840  |  valid_loss: 0.927\n",
      "[epoch: 38, i:  4749]  train_loss: 0.747  |  valid_loss: 0.872\n",
      "[epoch: 38, i:  4874]  train_loss: 0.775  |  valid_loss: 0.743\n",
      "[epoch: 38, i:  4999]  train_loss: 0.831  |  valid_loss: 0.740\n",
      "[epoch: 38, i:  5124]  train_loss: 0.769  |  valid_loss: 0.811\n",
      "[epoch: 38, i:  5249]  train_loss: 0.833  |  valid_loss: 0.894\n",
      "[epoch: 38, i:  5374]  train_loss: 0.860  |  valid_loss: 0.612\n",
      "[epoch: 38, i:  5499]  train_loss: 0.829  |  valid_loss: 0.708\n",
      "[epoch: 38, i:  5624]  train_loss: 0.792  |  valid_loss: 0.798\n",
      "[epoch: 38, i:  5749]  train_loss: 0.802  |  valid_loss: 0.797\n",
      "[epoch: 38, i:  5874]  train_loss: 0.853  |  valid_loss: 0.741\n",
      "[epoch: 38, i:  5999]  train_loss: 0.810  |  valid_loss: 0.827\n",
      "[epoch: 38, i:  6124]  train_loss: 0.864  |  valid_loss: 0.755\n",
      "[epoch: 38, i:  6249]  train_loss: 0.794  |  valid_loss: 0.853\n",
      "[epoch: 38, i:  6374]  train_loss: 0.844  |  valid_loss: 0.741\n",
      "[epoch: 38, i:  6499]  train_loss: 0.855  |  valid_loss: 0.797\n",
      "[epoch: 38, i:  6624]  train_loss: 0.823  |  valid_loss: 0.723\n",
      "[epoch: 38, i:  6749]  train_loss: 0.826  |  valid_loss: 0.818\n",
      "[epoch: 38, i:  6874]  train_loss: 0.849  |  valid_loss: 0.760\n",
      "[epoch: 38, i:  6999]  train_loss: 0.792  |  valid_loss: 0.947\n",
      "[epoch: 38, i:  7124]  train_loss: 0.803  |  valid_loss: 0.960\n",
      "[epoch: 38, i:  7249]  train_loss: 0.757  |  valid_loss: 0.665\n",
      "[epoch: 38, i:  7374]  train_loss: 0.756  |  valid_loss: 1.038\n",
      "[epoch: 38, i:  7499]  train_loss: 0.832  |  valid_loss: 0.901\n",
      "[epoch: 38, i:  7624]  train_loss: 0.812  |  valid_loss: 0.912\n",
      "[epoch: 38, i:  7749]  train_loss: 0.876  |  valid_loss: 0.801\n",
      "[epoch: 38, i:  7874]  train_loss: 0.778  |  valid_loss: 0.851\n",
      "[epoch: 38, i:  7999]  train_loss: 0.855  |  valid_loss: 0.736\n",
      "[epoch: 38, i:  8124]  train_loss: 0.794  |  valid_loss: 0.939\n",
      "[epoch: 38, i:  8249]  train_loss: 0.810  |  valid_loss: 0.870\n",
      "[epoch: 38, i:  8374]  train_loss: 0.740  |  valid_loss: 0.855\n",
      "[epoch: 38, i:  8499]  train_loss: 0.771  |  valid_loss: 0.901\n",
      "[epoch: 38, i:  8624]  train_loss: 0.773  |  valid_loss: 0.873\n",
      "[epoch: 38, i:  8749]  train_loss: 0.819  |  valid_loss: 0.912\n",
      "[epoch: 38, i:  8874]  train_loss: 0.891  |  valid_loss: 0.879\n",
      "[epoch: 38, i:  8999]  train_loss: 0.877  |  valid_loss: 0.703\n",
      "[epoch: 38, i:  9124]  train_loss: 0.805  |  valid_loss: 0.753\n",
      "[epoch: 38, i:  9249]  train_loss: 0.808  |  valid_loss: 0.617\n",
      "[epoch: 38, i:  9374]  train_loss: 0.824  |  valid_loss: 0.880\n",
      "[epoch: 38, i:  9499]  train_loss: 0.799  |  valid_loss: 0.759\n",
      "[epoch: 38, i:  9624]  train_loss: 0.788  |  valid_loss: 0.895\n",
      "[epoch: 38, i:  9749]  train_loss: 0.825  |  valid_loss: 0.898\n",
      "[epoch: 38, i:  9874]  train_loss: 0.800  |  valid_loss: 0.905\n",
      "[epoch: 38, i:  9999]  train_loss: 0.841  |  valid_loss: 0.820\n",
      "[epoch: 38, i: 10124]  train_loss: 0.788  |  valid_loss: 0.751\n",
      "[epoch: 38, i: 10249]  train_loss: 0.813  |  valid_loss: 0.851\n",
      "[epoch: 38, i: 10374]  train_loss: 0.840  |  valid_loss: 0.748\n",
      "[epoch: 38, i: 10499]  train_loss: 0.779  |  valid_loss: 0.968\n",
      "[epoch: 38, i: 10624]  train_loss: 0.862  |  valid_loss: 0.916\n",
      "[epoch: 38, i: 10749]  train_loss: 0.787  |  valid_loss: 0.841\n",
      "[epoch: 38, i: 10874]  train_loss: 0.821  |  valid_loss: 0.980\n",
      "[epoch: 38, i: 10999]  train_loss: 0.930  |  valid_loss: 0.969\n",
      "[epoch: 38, i: 11124]  train_loss: 0.794  |  valid_loss: 0.834\n",
      "[epoch: 38, i: 11249]  train_loss: 0.864  |  valid_loss: 0.891\n",
      "[epoch: 38, i: 11374]  train_loss: 0.832  |  valid_loss: 0.795\n",
      "[epoch: 38, i: 11499]  train_loss: 0.796  |  valid_loss: 0.613\n",
      "[epoch: 38, i: 11624]  train_loss: 0.863  |  valid_loss: 0.822\n",
      "[epoch: 38, i: 11749]  train_loss: 0.785  |  valid_loss: 0.745\n",
      "[epoch: 38, i: 11874]  train_loss: 0.784  |  valid_loss: 0.782\n",
      "[epoch: 38, i: 11999]  train_loss: 0.691  |  valid_loss: 0.719\n",
      "[epoch: 38, i: 12124]  train_loss: 0.786  |  valid_loss: 0.705\n",
      "[epoch: 38, i: 12249]  train_loss: 0.833  |  valid_loss: 1.046\n",
      "[epoch: 38, i: 12374]  train_loss: 0.793  |  valid_loss: 0.865\n",
      "[epoch: 38, i: 12499]  train_loss: 0.876  |  valid_loss: 0.875\n",
      "--> [End of epoch 38] train_accuracy: 71.52%  |  valid_accuracy: 70.86%\n",
      "--> [Start of epoch 39]  lr: 0.000000\n",
      "[epoch: 39, i:   124]  train_loss: 0.858  |  valid_loss: 0.727\n",
      "[epoch: 39, i:   249]  train_loss: 0.857  |  valid_loss: 0.734\n",
      "[epoch: 39, i:   374]  train_loss: 0.779  |  valid_loss: 0.856\n",
      "[epoch: 39, i:   499]  train_loss: 0.745  |  valid_loss: 0.805\n",
      "[epoch: 39, i:   624]  train_loss: 0.811  |  valid_loss: 0.935\n",
      "[epoch: 39, i:   749]  train_loss: 0.802  |  valid_loss: 0.582\n",
      "[epoch: 39, i:   874]  train_loss: 0.743  |  valid_loss: 0.874\n",
      "[epoch: 39, i:   999]  train_loss: 0.807  |  valid_loss: 0.952\n",
      "[epoch: 39, i:  1124]  train_loss: 0.861  |  valid_loss: 0.943\n",
      "[epoch: 39, i:  1249]  train_loss: 0.827  |  valid_loss: 0.734\n",
      "[epoch: 39, i:  1374]  train_loss: 0.845  |  valid_loss: 0.760\n",
      "[epoch: 39, i:  1499]  train_loss: 0.747  |  valid_loss: 0.814\n",
      "[epoch: 39, i:  1624]  train_loss: 0.850  |  valid_loss: 0.689\n",
      "[epoch: 39, i:  1749]  train_loss: 0.783  |  valid_loss: 0.931\n",
      "[epoch: 39, i:  1874]  train_loss: 0.778  |  valid_loss: 0.779\n",
      "[epoch: 39, i:  1999]  train_loss: 0.721  |  valid_loss: 1.002\n",
      "[epoch: 39, i:  2124]  train_loss: 0.834  |  valid_loss: 0.731\n",
      "[epoch: 39, i:  2249]  train_loss: 0.781  |  valid_loss: 0.831\n",
      "[epoch: 39, i:  2374]  train_loss: 0.823  |  valid_loss: 0.767\n",
      "[epoch: 39, i:  2499]  train_loss: 0.830  |  valid_loss: 1.008\n",
      "[epoch: 39, i:  2624]  train_loss: 0.869  |  valid_loss: 0.939\n",
      "[epoch: 39, i:  2749]  train_loss: 0.770  |  valid_loss: 0.934\n",
      "[epoch: 39, i:  2874]  train_loss: 0.816  |  valid_loss: 0.951\n",
      "[epoch: 39, i:  2999]  train_loss: 0.794  |  valid_loss: 0.833\n",
      "[epoch: 39, i:  3124]  train_loss: 0.803  |  valid_loss: 0.907\n",
      "[epoch: 39, i:  3249]  train_loss: 0.858  |  valid_loss: 1.053\n",
      "[epoch: 39, i:  3374]  train_loss: 0.843  |  valid_loss: 0.809\n",
      "[epoch: 39, i:  3499]  train_loss: 0.896  |  valid_loss: 0.757\n",
      "[epoch: 39, i:  3624]  train_loss: 0.781  |  valid_loss: 0.777\n",
      "[epoch: 39, i:  3749]  train_loss: 0.809  |  valid_loss: 0.659\n",
      "[epoch: 39, i:  3874]  train_loss: 0.799  |  valid_loss: 0.812\n",
      "[epoch: 39, i:  3999]  train_loss: 0.799  |  valid_loss: 0.652\n",
      "[epoch: 39, i:  4124]  train_loss: 0.830  |  valid_loss: 0.864\n",
      "[epoch: 39, i:  4249]  train_loss: 0.819  |  valid_loss: 0.895\n",
      "[epoch: 39, i:  4374]  train_loss: 0.820  |  valid_loss: 0.873\n",
      "[epoch: 39, i:  4499]  train_loss: 0.778  |  valid_loss: 0.792\n",
      "[epoch: 39, i:  4624]  train_loss: 0.810  |  valid_loss: 0.961\n",
      "[epoch: 39, i:  4749]  train_loss: 0.799  |  valid_loss: 0.883\n",
      "[epoch: 39, i:  4874]  train_loss: 0.812  |  valid_loss: 0.759\n",
      "[epoch: 39, i:  4999]  train_loss: 0.743  |  valid_loss: 0.746\n",
      "[epoch: 39, i:  5124]  train_loss: 0.835  |  valid_loss: 0.818\n",
      "[epoch: 39, i:  5249]  train_loss: 0.775  |  valid_loss: 0.869\n",
      "[epoch: 39, i:  5374]  train_loss: 0.749  |  valid_loss: 0.599\n",
      "[epoch: 39, i:  5499]  train_loss: 0.837  |  valid_loss: 0.708\n",
      "[epoch: 39, i:  5624]  train_loss: 0.788  |  valid_loss: 0.956\n",
      "[epoch: 39, i:  5749]  train_loss: 0.833  |  valid_loss: 0.810\n",
      "[epoch: 39, i:  5874]  train_loss: 0.775  |  valid_loss: 0.722\n",
      "[epoch: 39, i:  5999]  train_loss: 0.853  |  valid_loss: 0.862\n",
      "[epoch: 39, i:  6124]  train_loss: 0.823  |  valid_loss: 0.747\n",
      "[epoch: 39, i:  6249]  train_loss: 0.847  |  valid_loss: 0.838\n",
      "[epoch: 39, i:  6374]  train_loss: 0.791  |  valid_loss: 0.708\n",
      "[epoch: 39, i:  6499]  train_loss: 0.860  |  valid_loss: 0.772\n",
      "[epoch: 39, i:  6624]  train_loss: 0.892  |  valid_loss: 0.672\n",
      "[epoch: 39, i:  6749]  train_loss: 0.838  |  valid_loss: 0.818\n",
      "[epoch: 39, i:  6874]  train_loss: 0.784  |  valid_loss: 0.741\n",
      "[epoch: 39, i:  6999]  train_loss: 0.785  |  valid_loss: 0.939\n",
      "[epoch: 39, i:  7124]  train_loss: 0.855  |  valid_loss: 0.982\n",
      "[epoch: 39, i:  7249]  train_loss: 0.809  |  valid_loss: 0.673\n",
      "[epoch: 39, i:  7374]  train_loss: 0.758  |  valid_loss: 1.020\n",
      "[epoch: 39, i:  7499]  train_loss: 0.839  |  valid_loss: 0.909\n",
      "[epoch: 39, i:  7624]  train_loss: 0.765  |  valid_loss: 0.912\n",
      "[epoch: 39, i:  7749]  train_loss: 0.785  |  valid_loss: 0.778\n",
      "[epoch: 39, i:  7874]  train_loss: 0.834  |  valid_loss: 0.887\n",
      "[epoch: 39, i:  7999]  train_loss: 0.858  |  valid_loss: 0.703\n",
      "[epoch: 39, i:  8124]  train_loss: 0.837  |  valid_loss: 0.910\n",
      "[epoch: 39, i:  8249]  train_loss: 0.760  |  valid_loss: 0.922\n",
      "[epoch: 39, i:  8374]  train_loss: 0.779  |  valid_loss: 0.856\n",
      "[epoch: 39, i:  8499]  train_loss: 0.810  |  valid_loss: 0.917\n",
      "[epoch: 39, i:  8624]  train_loss: 0.807  |  valid_loss: 0.863\n",
      "[epoch: 39, i:  8749]  train_loss: 0.794  |  valid_loss: 0.880\n",
      "[epoch: 39, i:  8874]  train_loss: 0.859  |  valid_loss: 0.849\n",
      "[epoch: 39, i:  8999]  train_loss: 0.771  |  valid_loss: 0.683\n",
      "[epoch: 39, i:  9124]  train_loss: 0.783  |  valid_loss: 0.750\n",
      "[epoch: 39, i:  9249]  train_loss: 0.769  |  valid_loss: 0.622\n",
      "[epoch: 39, i:  9374]  train_loss: 0.804  |  valid_loss: 0.910\n",
      "[epoch: 39, i:  9499]  train_loss: 0.782  |  valid_loss: 0.764\n",
      "[epoch: 39, i:  9624]  train_loss: 0.877  |  valid_loss: 0.861\n",
      "[epoch: 39, i:  9749]  train_loss: 0.813  |  valid_loss: 0.948\n",
      "[epoch: 39, i:  9874]  train_loss: 0.839  |  valid_loss: 0.895\n",
      "[epoch: 39, i:  9999]  train_loss: 0.840  |  valid_loss: 0.836\n",
      "[epoch: 39, i: 10124]  train_loss: 0.793  |  valid_loss: 0.742\n",
      "[epoch: 39, i: 10249]  train_loss: 0.794  |  valid_loss: 0.861\n",
      "[epoch: 39, i: 10374]  train_loss: 0.881  |  valid_loss: 0.735\n",
      "[epoch: 39, i: 10499]  train_loss: 0.756  |  valid_loss: 0.925\n",
      "[epoch: 39, i: 10624]  train_loss: 0.834  |  valid_loss: 0.931\n",
      "[epoch: 39, i: 10749]  train_loss: 0.826  |  valid_loss: 0.812\n",
      "[epoch: 39, i: 10874]  train_loss: 0.817  |  valid_loss: 0.958\n",
      "[epoch: 39, i: 10999]  train_loss: 0.862  |  valid_loss: 0.961\n",
      "[epoch: 39, i: 11124]  train_loss: 0.830  |  valid_loss: 0.848\n",
      "[epoch: 39, i: 11249]  train_loss: 0.909  |  valid_loss: 0.877\n",
      "[epoch: 39, i: 11374]  train_loss: 0.826  |  valid_loss: 0.796\n",
      "[epoch: 39, i: 11499]  train_loss: 0.695  |  valid_loss: 0.613\n",
      "[epoch: 39, i: 11624]  train_loss: 0.772  |  valid_loss: 0.801\n",
      "[epoch: 39, i: 11749]  train_loss: 0.798  |  valid_loss: 0.763\n",
      "[epoch: 39, i: 11874]  train_loss: 0.828  |  valid_loss: 0.801\n",
      "[epoch: 39, i: 11999]  train_loss: 0.800  |  valid_loss: 0.687\n",
      "[epoch: 39, i: 12124]  train_loss: 0.839  |  valid_loss: 0.743\n",
      "[epoch: 39, i: 12249]  train_loss: 0.845  |  valid_loss: 1.046\n",
      "[epoch: 39, i: 12374]  train_loss: 0.769  |  valid_loss: 0.873\n",
      "[epoch: 39, i: 12499]  train_loss: 0.770  |  valid_loss: 0.859\n",
      "--> [End of epoch 39] train_accuracy: 71.57%  |  valid_accuracy: 70.93%\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "PATH = './models/cifar_dense_avgpool_exp_adam.pth'\n",
    "\n",
    "epochs = 40             # Total epochs.\n",
    "iter_n = len(trainloader)  # number of iteration per epoch\n",
    "record_freq = iter_n // 100  # Record frequency.\n",
    "print_freq = iter_n // 100 # Print frequency, need to be multiple of record_freq.\n",
    "\n",
    "# a number for computing running validation loss in each iteration.\n",
    "train_per_valid = len(trainloader) / len(validloader)  \n",
    "\n",
    "avg_train_losses, avg_valid_losses = [], []   # Avg. losses.\n",
    "train_accuracies, valid_accuracies = [], []  # Train and test accuracies.\n",
    "\n",
    "dense_net = DenseNet()     # Create the network instance.\n",
    "dense_net.to(device)  # Move the network parameters to the specified device.\n",
    "\n",
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "\n",
    "# We use Adam as optimizer.\n",
    "opt = optim.Adam(dense_net.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# add plateau scheduling\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.8)\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_train_loss = 0.0       # Initialize running train_loss for training set.\n",
    "    running_val_loss = 0.0     # Initialize running train_loss for validation set.\n",
    "    \n",
    "    # Initialize running total and correct for computing train accuracy.\n",
    "    train_correct = 0 \n",
    "    train_total = 0\n",
    "    # Initialize running total and correct for computing test accuracy.\n",
    "    valid_total = 0   \n",
    "    valid_correct = 0\n",
    "    \n",
    "    print('--> [Start of epoch {}]'.format(epoch) +\n",
    "          '  lr: {:.6f}'.format(scheduler.get_last_lr()[0]))\n",
    "    \n",
    "    \n",
    "    validiter = iter(validloader)\n",
    "    for i, data in enumerate(iterable=trainloader, start=0):\n",
    "        \n",
    "        # Set the model to training mode.\n",
    "        dense_net.train()\n",
    "        \n",
    "        # Get the train_inputs.\n",
    "        train_inputs, train_labels = data\n",
    "        \n",
    "        # Move the train_inputs to the specified device.\n",
    "        train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step\n",
    "        train_output = dense_net(train_inputs)\n",
    "        train_loss = loss_func(train_output, train_labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "        \n",
    "        dense_net.eval()\n",
    "        # record statistics.\n",
    "        running_train_loss += train_loss.item()\n",
    "        \n",
    "        # record training accuracy in every mini-batch\n",
    "        _, train_predicted = torch.max(train_output.data, 1)\n",
    "        train_total += train_labels.size(0)\n",
    "        train_correct += (train_predicted == train_labels).sum().item()\n",
    "        \n",
    "        # only iterate through validation set according to train_per_valid ratio\n",
    "        if i % train_per_valid == train_per_valid - 1:\n",
    "            # Set the model to evaluation mode.\n",
    "            with torch.no_grad():\n",
    "                valid_inputs, valid_labels = next(validiter)\n",
    "                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)\n",
    "                valid_output = dense_net(valid_inputs)\n",
    "                valid_loss = loss_func(valid_output, valid_labels)\n",
    "                running_val_loss += valid_loss.item()\n",
    "                \n",
    "                _, valid_predicted = torch.max(valid_output.data, 1)\n",
    "                valid_total += valid_labels.size(0)\n",
    "                valid_correct += (valid_predicted == valid_labels).sum().item()\n",
    "        \n",
    "        # Record training/validation loss every several mini-batches.\n",
    "        if i % record_freq == record_freq - 1: \n",
    "            avg_train_loss = running_train_loss / record_freq\n",
    "            avg_train_losses.append(avg_train_loss)\n",
    "            avg_valid_loss = train_per_valid * running_val_loss / record_freq\n",
    "            avg_valid_losses.append(avg_valid_loss)\n",
    "            running_train_loss, running_val_loss = 0.0, 0.0\n",
    "            \n",
    "            # only print traing training loss once in a while to avoid cluster output\n",
    "            if i % print_freq == print_freq - 1:\n",
    "                print('[epoch: {}, i: {:5d}]'.format(epoch, i) +\n",
    "                      '  train_loss: {:.3f}'.format(avg_train_loss) + \n",
    "                      '  |  valid_loss: {:.3f}'.format(avg_valid_loss))\n",
    "\n",
    "    # do exponential scheduling\n",
    "    scheduler.step()\n",
    "                   \n",
    "    # calculating train accuracy\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    # Store the networks after each epochs.\n",
    "    # in case we want to do simple average or weighted average\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': dense_net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'train_loss': avg_train_losses[epoch],\n",
    "            'train_accuracy': train_accuracies[epoch],\n",
    "            'valid_accuracy': valid_accuracies[epoch],\n",
    "            }, PATH)\n",
    "    \n",
    "    print('--> [End of epoch {}]'.format(epoch) +\n",
    "                      ' train_accuracy: {:.2f}%'.format(train_accuracy) + \n",
    "                      '  |  valid_accuracy: {:.2f}%'.format(valid_accuracy))\n",
    "              \n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_train_losses)\n",
    "# plt.plot(avg_valid_losses)\n",
    "plt.title('DenseNet loss curve + pleateau scheduling')\n",
    "plt.xlabel('mini-batch index / {}'.format(record_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute the moving average of the data using a specified window size.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Calculate moving averages\n",
    "window_size = 100  # Adjust this based on your preference\n",
    "# train_losses_ma = moving_average(avg_train_losses, window_size)\n",
    "# valid_losses_ma = moving_average(avg_valid_losses, window_size)\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [1, 4, 9, 16]\n",
    "train_loss_ma = moving_average(x , 3)\n",
    "\n",
    "print(train_loss_ma)\n",
    "\n",
    "\n",
    "# Plot original loss curves\n",
    "plt.plot(avg_train_losses, label='Training Loss Trend')\n",
    "plt.plot(avg_valid_losses, label='Validation Loss Trend')\n",
    "\n",
    "# Plot trend lines (moving averages)\n",
    "# plt.plot(np.arange(window_size - 1, len(train_losses_ma) + window_size - 1), train_losses_ma, label='Training Loss Trend', linewidth=2)\n",
    "# plt.plot(np.arange(window_size - 1, len(valid_losses_ma) + window_size - 1), valid_losses_ma, label='Validation Loss Trend', linewidth=2)\n",
    "\n",
    "# plt.title('Loss Curve with Trend Lines')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies)\n",
    "plt.plot(valid_accuracies)\n",
    "plt.title('Accuracy Curve for DenseNet + ReduceLROnPlateau + Adam')\n",
    "plt.xlabel('epoch / {}'.format(record_freq))\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train accuracy', 'test accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to show an image.\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # Unnormalize.\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n",
      "Predicted:    cat  ship  ship plane\n"
     ]
    }
   ],
   "source": [
    "# Check several images.\n",
    "dataiter = iter(validloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "outputs = dense_net(images.to(device))\n",
    "\n",
    "# max compare along the row, return the index of the max value, which is the predicted class\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 73.13 %\n"
     ]
    }
   ],
   "source": [
    "dense_net.eval()\n",
    "# Get test accuracy.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 70 %\n",
      "Accuracy of   car : 84 %\n",
      "Accuracy of  bird : 59 %\n",
      "Accuracy of   cat : 48 %\n",
      "Accuracy of  deer : 69 %\n",
      "Accuracy of   dog : 65 %\n",
      "Accuracy of  frog : 84 %\n",
      "Accuracy of horse : 74 %\n",
      "Accuracy of  ship : 91 %\n",
      "Accuracy of truck : 83 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs181",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
