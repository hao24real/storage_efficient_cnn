{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "12500\n",
      "2500\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(validloader))\n",
    "\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(device)  \n",
    "# If 'mps or cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortConn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=1, stride=stride, padding=0)\n",
    "            \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                             kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, \n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # skip connect projection from in_channels to out_channels\n",
    "        # depending on the stride and channels to do a projection\n",
    "        # or just pass on the input\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            self.shortcut = ShortConn2d(in_channels, out_channels, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reserve this now for the skip connection\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # not applying activation here for densenet implementation\n",
    "        return x + shortcut\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, bn_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_channels, \n",
    "                                kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(bn_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(bn_channels, bn_channels, \n",
    "                                kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(bn_channels)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(bn_channels, out_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = ShortConn2d(in_channels, out_channels, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        return x + shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6387e-01,  2.4741e-01, -8.1846e-02,  1.9173e-01,  8.6405e-01,\n",
       "          2.5488e-01, -1.4742e-01, -2.5082e-01,  1.7089e-02, -1.1815e+00],\n",
       "        [ 2.5183e-01,  2.4116e-01, -3.6751e-02,  2.0297e-01,  8.6995e-01,\n",
       "          1.5241e-01, -9.0700e-02, -2.3541e-01,  5.4563e-02, -1.1810e+00],\n",
       "        [ 2.5500e-01,  1.5679e-01, -1.7260e-04,  2.0089e-01,  1.0449e+00,\n",
       "          1.4832e-01,  5.7634e-04, -2.7198e-01,  8.1701e-02, -1.2878e+00],\n",
       "        [ 2.3501e-01,  2.0436e-01, -7.9636e-02,  1.8180e-01,  9.2297e-01,\n",
       "          1.7733e-01, -8.7504e-02, -2.4391e-01,  4.3700e-02, -1.1755e+00]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        ### RES 1\n",
    "        \n",
    "        # 3x32x32 to 10x32x32\n",
    "        self.rb1 = ResBlock(3, 10, 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        ### RES 2\n",
    "\n",
    "        # 10x32x32 to 20x32x32\n",
    "        self.rb2 = ResBlock(10, 20, 1)\n",
    "        # RES 1 skip connection 3x32x32 to 20x32x32\n",
    "        self.res1_sc1 = ShortConn2d(3, 20, 1)  \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        ### RES 3\n",
    "        \n",
    "        # 20x32x32 to 20x32x32\n",
    "        self.rb3 = ResBlock(20, 20, 1)\n",
    "        # RES 1 skip connection for 3x32x32 to 20x32x32\n",
    "        self.res1_sc2 = ShortConn2d(3, 20, 1)\n",
    "        # RES 2 skip connection 10x32x32 to 20x32x32\n",
    "        self.res2_sc1 = ShortConn2d(10, 20, 1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.bnrb1 = BNResBlock(20, 10, 30, 1)\n",
    "        # RES 1 skip connection for 3x32x32 to 30x32x32\n",
    "        self.res1_sc3 = ShortConn2d(3, 30, 1)\n",
    "        # RES 2 skip connection for 10x32x32 to 30x32x32\n",
    "        self.res2_sc2 = ShortConn2d(10, 30, 1)\n",
    "        # RES 3 skip connection for 20x32x32 to 30x32x32\n",
    "        self.res3_sc1 = ShortConn2d(20, 30, 1)\n",
    "        self.act4 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        # we will do flatten height and width and then do global average pooling\n",
    "        \n",
    "        # affine operation from 20 neurons to output layer with 10 classes\n",
    "        self.fc = nn.Linear(30, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        res1_sc1 = self.res1_sc1(x)\n",
    "        res1_sc2 = self.res1_sc2(x)\n",
    "        res1_sc3 = self.res1_sc3(x)\n",
    "        x = self.rb1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        res2_sc1 = self.res2_sc1(x)\n",
    "        res2_sc2 = self.res2_sc2(x)\n",
    "        x = self.rb2(x)\n",
    "        x = self.act2(x + res1_sc1)\n",
    "        \n",
    "        res3_sc1 = self.res3_sc1(x)\n",
    "        x = self.rb3(x)\n",
    "        x = self.act3(x + res1_sc2 + res2_sc1)\n",
    "        \n",
    "        x = self.bnrb1(x)\n",
    "        x = self.act4(x + res1_sc3 + res2_sc2 + res3_sc1)\n",
    "        \n",
    "        # change x from shape [batch_size, channels, height, width] \n",
    "        # to [batch_size, channels, height*width]\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "        \n",
    "        # 4 (batch size) x 30 (channels) x 32 (height) x 32 (width)\n",
    "        # global average pooling over the spatial dimensions in each feature map\n",
    "        x = x.mean(dim=-1)\n",
    "        # becomes 4 x 30\n",
    "         \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "test = DenseNet()\n",
    "test.to(device)\n",
    "test(torch.randn(4, 3, 32, 32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> [Start of epoch 0]  lr: 0.002000\n",
      "[epoch: 0, i:   124]  train_loss: 2.253  |  valid_loss: 2.223\n",
      "[epoch: 0, i:   249]  train_loss: 2.132  |  valid_loss: 2.251\n",
      "[epoch: 0, i:   374]  train_loss: 2.086  |  valid_loss: 1.981\n",
      "[epoch: 0, i:   499]  train_loss: 2.093  |  valid_loss: 2.106\n",
      "[epoch: 0, i:   624]  train_loss: 2.028  |  valid_loss: 2.012\n",
      "[epoch: 0, i:   749]  train_loss: 2.002  |  valid_loss: 1.914\n",
      "[epoch: 0, i:   874]  train_loss: 1.953  |  valid_loss: 2.020\n",
      "[epoch: 0, i:   999]  train_loss: 1.942  |  valid_loss: 1.951\n",
      "[epoch: 0, i:  1124]  train_loss: 1.983  |  valid_loss: 1.822\n",
      "[epoch: 0, i:  1249]  train_loss: 1.969  |  valid_loss: 1.890\n",
      "[epoch: 0, i:  1374]  train_loss: 1.959  |  valid_loss: 1.822\n",
      "[epoch: 0, i:  1499]  train_loss: 1.891  |  valid_loss: 1.810\n",
      "[epoch: 0, i:  1624]  train_loss: 1.972  |  valid_loss: 1.874\n",
      "[epoch: 0, i:  1749]  train_loss: 1.958  |  valid_loss: 1.868\n",
      "[epoch: 0, i:  1874]  train_loss: 1.884  |  valid_loss: 1.673\n",
      "[epoch: 0, i:  1999]  train_loss: 1.834  |  valid_loss: 1.835\n",
      "[epoch: 0, i:  2124]  train_loss: 1.886  |  valid_loss: 1.752\n",
      "[epoch: 0, i:  2249]  train_loss: 1.875  |  valid_loss: 1.628\n",
      "[epoch: 0, i:  2374]  train_loss: 1.897  |  valid_loss: 1.777\n",
      "[epoch: 0, i:  2499]  train_loss: 1.823  |  valid_loss: 1.743\n",
      "[epoch: 0, i:  2624]  train_loss: 1.904  |  valid_loss: 1.628\n",
      "[epoch: 0, i:  2749]  train_loss: 1.785  |  valid_loss: 1.809\n",
      "[epoch: 0, i:  2874]  train_loss: 1.886  |  valid_loss: 1.764\n",
      "[epoch: 0, i:  2999]  train_loss: 1.857  |  valid_loss: 1.734\n",
      "[epoch: 0, i:  3124]  train_loss: 1.757  |  valid_loss: 1.707\n",
      "[epoch: 0, i:  3249]  train_loss: 1.763  |  valid_loss: 1.877\n",
      "[epoch: 0, i:  3374]  train_loss: 1.752  |  valid_loss: 1.601\n",
      "[epoch: 0, i:  3499]  train_loss: 1.730  |  valid_loss: 1.729\n",
      "[epoch: 0, i:  3624]  train_loss: 1.786  |  valid_loss: 1.867\n",
      "[epoch: 0, i:  3749]  train_loss: 1.800  |  valid_loss: 1.687\n",
      "[epoch: 0, i:  3874]  train_loss: 1.741  |  valid_loss: 1.678\n",
      "[epoch: 0, i:  3999]  train_loss: 1.778  |  valid_loss: 1.561\n",
      "[epoch: 0, i:  4124]  train_loss: 1.754  |  valid_loss: 1.677\n",
      "[epoch: 0, i:  4249]  train_loss: 1.776  |  valid_loss: 1.672\n",
      "[epoch: 0, i:  4374]  train_loss: 1.690  |  valid_loss: 1.620\n",
      "[epoch: 0, i:  4499]  train_loss: 1.624  |  valid_loss: 1.462\n",
      "[epoch: 0, i:  4624]  train_loss: 1.706  |  valid_loss: 1.673\n",
      "[epoch: 0, i:  4749]  train_loss: 1.685  |  valid_loss: 1.653\n",
      "[epoch: 0, i:  4874]  train_loss: 1.714  |  valid_loss: 1.583\n",
      "[epoch: 0, i:  4999]  train_loss: 1.721  |  valid_loss: 1.577\n",
      "[epoch: 0, i:  5124]  train_loss: 1.705  |  valid_loss: 1.950\n",
      "[epoch: 0, i:  5249]  train_loss: 1.748  |  valid_loss: 1.578\n",
      "[epoch: 0, i:  5374]  train_loss: 1.686  |  valid_loss: 1.516\n",
      "[epoch: 0, i:  5499]  train_loss: 1.684  |  valid_loss: 1.597\n",
      "[epoch: 0, i:  5624]  train_loss: 1.622  |  valid_loss: 1.734\n",
      "[epoch: 0, i:  5749]  train_loss: 1.739  |  valid_loss: 1.700\n",
      "[epoch: 0, i:  5874]  train_loss: 1.658  |  valid_loss: 1.558\n",
      "[epoch: 0, i:  5999]  train_loss: 1.594  |  valid_loss: 1.735\n",
      "[epoch: 0, i:  6124]  train_loss: 1.645  |  valid_loss: 1.613\n",
      "[epoch: 0, i:  6249]  train_loss: 1.562  |  valid_loss: 1.434\n",
      "[epoch: 0, i:  6374]  train_loss: 1.586  |  valid_loss: 1.562\n",
      "[epoch: 0, i:  6499]  train_loss: 1.588  |  valid_loss: 1.677\n",
      "[epoch: 0, i:  6624]  train_loss: 1.662  |  valid_loss: 1.520\n",
      "[epoch: 0, i:  6749]  train_loss: 1.652  |  valid_loss: 1.600\n",
      "[epoch: 0, i:  6874]  train_loss: 1.635  |  valid_loss: 1.606\n",
      "[epoch: 0, i:  6999]  train_loss: 1.595  |  valid_loss: 1.718\n",
      "[epoch: 0, i:  7124]  train_loss: 1.557  |  valid_loss: 1.563\n",
      "[epoch: 0, i:  7249]  train_loss: 1.632  |  valid_loss: 1.403\n",
      "[epoch: 0, i:  7374]  train_loss: 1.574  |  valid_loss: 1.624\n",
      "[epoch: 0, i:  7499]  train_loss: 1.588  |  valid_loss: 1.490\n",
      "[epoch: 0, i:  7624]  train_loss: 1.627  |  valid_loss: 1.573\n",
      "[epoch: 0, i:  7749]  train_loss: 1.630  |  valid_loss: 1.539\n",
      "[epoch: 0, i:  7874]  train_loss: 1.618  |  valid_loss: 1.509\n",
      "[epoch: 0, i:  7999]  train_loss: 1.596  |  valid_loss: 1.523\n",
      "[epoch: 0, i:  8124]  train_loss: 1.516  |  valid_loss: 1.721\n",
      "[epoch: 0, i:  8249]  train_loss: 1.558  |  valid_loss: 1.441\n",
      "[epoch: 0, i:  8374]  train_loss: 1.543  |  valid_loss: 1.522\n",
      "[epoch: 0, i:  8499]  train_loss: 1.526  |  valid_loss: 1.528\n",
      "[epoch: 0, i:  8624]  train_loss: 1.534  |  valid_loss: 1.461\n",
      "[epoch: 0, i:  8749]  train_loss: 1.602  |  valid_loss: 1.544\n",
      "[epoch: 0, i:  8874]  train_loss: 1.569  |  valid_loss: 1.451\n",
      "[epoch: 0, i:  8999]  train_loss: 1.536  |  valid_loss: 1.285\n",
      "[epoch: 0, i:  9124]  train_loss: 1.520  |  valid_loss: 1.439\n",
      "[epoch: 0, i:  9249]  train_loss: 1.528  |  valid_loss: 1.245\n",
      "[epoch: 0, i:  9374]  train_loss: 1.535  |  valid_loss: 1.483\n",
      "[epoch: 0, i:  9499]  train_loss: 1.499  |  valid_loss: 1.436\n",
      "[epoch: 0, i:  9624]  train_loss: 1.497  |  valid_loss: 1.422\n",
      "[epoch: 0, i:  9749]  train_loss: 1.478  |  valid_loss: 1.450\n",
      "[epoch: 0, i:  9874]  train_loss: 1.613  |  valid_loss: 1.502\n",
      "[epoch: 0, i:  9999]  train_loss: 1.532  |  valid_loss: 1.529\n",
      "[epoch: 0, i: 10124]  train_loss: 1.510  |  valid_loss: 1.356\n",
      "[epoch: 0, i: 10249]  train_loss: 1.529  |  valid_loss: 1.374\n",
      "[epoch: 0, i: 10374]  train_loss: 1.543  |  valid_loss: 1.519\n",
      "[epoch: 0, i: 10499]  train_loss: 1.483  |  valid_loss: 1.565\n",
      "[epoch: 0, i: 10624]  train_loss: 1.429  |  valid_loss: 1.619\n",
      "[epoch: 0, i: 10749]  train_loss: 1.484  |  valid_loss: 1.453\n",
      "[epoch: 0, i: 10874]  train_loss: 1.562  |  valid_loss: 1.433\n",
      "[epoch: 0, i: 10999]  train_loss: 1.490  |  valid_loss: 1.422\n",
      "[epoch: 0, i: 11124]  train_loss: 1.467  |  valid_loss: 1.465\n",
      "[epoch: 0, i: 11249]  train_loss: 1.481  |  valid_loss: 1.263\n",
      "[epoch: 0, i: 11374]  train_loss: 1.462  |  valid_loss: 1.313\n",
      "[epoch: 0, i: 11499]  train_loss: 1.438  |  valid_loss: 1.271\n",
      "[epoch: 0, i: 11624]  train_loss: 1.336  |  valid_loss: 1.262\n",
      "[epoch: 0, i: 11749]  train_loss: 1.437  |  valid_loss: 1.372\n",
      "[epoch: 0, i: 11874]  train_loss: 1.437  |  valid_loss: 1.418\n",
      "[epoch: 0, i: 11999]  train_loss: 1.422  |  valid_loss: 1.246\n",
      "[epoch: 0, i: 12124]  train_loss: 1.551  |  valid_loss: 1.325\n",
      "[epoch: 0, i: 12249]  train_loss: 1.433  |  valid_loss: 1.508\n",
      "[epoch: 0, i: 12374]  train_loss: 1.384  |  valid_loss: 1.399\n",
      "[epoch: 0, i: 12499]  train_loss: 1.491  |  valid_loss: 1.348\n",
      "--> [End of epoch 0] train_accuracy: 38.15%  |  valid_accuracy: 40.56%\n",
      "--> [Start of epoch 1]  lr: 0.001600\n",
      "[epoch: 1, i:   124]  train_loss: 1.453  |  valid_loss: 1.278\n",
      "[epoch: 1, i:   249]  train_loss: 1.364  |  valid_loss: 1.490\n",
      "[epoch: 1, i:   374]  train_loss: 1.369  |  valid_loss: 1.352\n",
      "[epoch: 1, i:   499]  train_loss: 1.414  |  valid_loss: 1.221\n",
      "[epoch: 1, i:   624]  train_loss: 1.417  |  valid_loss: 1.358\n",
      "[epoch: 1, i:   749]  train_loss: 1.405  |  valid_loss: 1.146\n",
      "[epoch: 1, i:   874]  train_loss: 1.505  |  valid_loss: 1.286\n",
      "[epoch: 1, i:   999]  train_loss: 1.385  |  valid_loss: 1.378\n",
      "[epoch: 1, i:  1124]  train_loss: 1.351  |  valid_loss: 1.467\n",
      "[epoch: 1, i:  1249]  train_loss: 1.379  |  valid_loss: 1.219\n",
      "[epoch: 1, i:  1374]  train_loss: 1.458  |  valid_loss: 1.120\n",
      "[epoch: 1, i:  1499]  train_loss: 1.454  |  valid_loss: 1.384\n",
      "[epoch: 1, i:  1624]  train_loss: 1.267  |  valid_loss: 1.274\n",
      "[epoch: 1, i:  1749]  train_loss: 1.337  |  valid_loss: 1.336\n",
      "[epoch: 1, i:  1874]  train_loss: 1.348  |  valid_loss: 1.240\n",
      "[epoch: 1, i:  1999]  train_loss: 1.357  |  valid_loss: 1.505\n",
      "[epoch: 1, i:  2124]  train_loss: 1.336  |  valid_loss: 1.234\n",
      "[epoch: 1, i:  2249]  train_loss: 1.396  |  valid_loss: 1.283\n",
      "[epoch: 1, i:  2374]  train_loss: 1.320  |  valid_loss: 1.212\n",
      "[epoch: 1, i:  2499]  train_loss: 1.355  |  valid_loss: 1.341\n",
      "[epoch: 1, i:  2624]  train_loss: 1.362  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  2749]  train_loss: 1.323  |  valid_loss: 1.394\n",
      "[epoch: 1, i:  2874]  train_loss: 1.418  |  valid_loss: 1.276\n",
      "[epoch: 1, i:  2999]  train_loss: 1.410  |  valid_loss: 1.416\n",
      "[epoch: 1, i:  3124]  train_loss: 1.326  |  valid_loss: 1.225\n",
      "[epoch: 1, i:  3249]  train_loss: 1.298  |  valid_loss: 1.567\n",
      "[epoch: 1, i:  3374]  train_loss: 1.307  |  valid_loss: 1.344\n",
      "[epoch: 1, i:  3499]  train_loss: 1.401  |  valid_loss: 1.286\n",
      "[epoch: 1, i:  3624]  train_loss: 1.371  |  valid_loss: 1.432\n",
      "[epoch: 1, i:  3749]  train_loss: 1.314  |  valid_loss: 1.207\n",
      "[epoch: 1, i:  3874]  train_loss: 1.317  |  valid_loss: 1.207\n",
      "[epoch: 1, i:  3999]  train_loss: 1.288  |  valid_loss: 1.114\n",
      "[epoch: 1, i:  4124]  train_loss: 1.278  |  valid_loss: 1.432\n",
      "[epoch: 1, i:  4249]  train_loss: 1.284  |  valid_loss: 1.338\n",
      "[epoch: 1, i:  4374]  train_loss: 1.296  |  valid_loss: 1.316\n",
      "[epoch: 1, i:  4499]  train_loss: 1.339  |  valid_loss: 1.261\n",
      "[epoch: 1, i:  4624]  train_loss: 1.349  |  valid_loss: 1.438\n",
      "[epoch: 1, i:  4749]  train_loss: 1.345  |  valid_loss: 1.345\n",
      "[epoch: 1, i:  4874]  train_loss: 1.315  |  valid_loss: 1.301\n",
      "[epoch: 1, i:  4999]  train_loss: 1.383  |  valid_loss: 1.265\n",
      "[epoch: 1, i:  5124]  train_loss: 1.402  |  valid_loss: 1.251\n",
      "[epoch: 1, i:  5249]  train_loss: 1.310  |  valid_loss: 1.271\n",
      "[epoch: 1, i:  5374]  train_loss: 1.258  |  valid_loss: 1.005\n",
      "[epoch: 1, i:  5499]  train_loss: 1.311  |  valid_loss: 1.049\n",
      "[epoch: 1, i:  5624]  train_loss: 1.282  |  valid_loss: 1.355\n",
      "[epoch: 1, i:  5749]  train_loss: 1.382  |  valid_loss: 1.345\n",
      "[epoch: 1, i:  5874]  train_loss: 1.304  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  5999]  train_loss: 1.309  |  valid_loss: 1.258\n",
      "[epoch: 1, i:  6124]  train_loss: 1.257  |  valid_loss: 1.099\n",
      "[epoch: 1, i:  6249]  train_loss: 1.369  |  valid_loss: 1.239\n",
      "[epoch: 1, i:  6374]  train_loss: 1.304  |  valid_loss: 1.226\n",
      "[epoch: 1, i:  6499]  train_loss: 1.313  |  valid_loss: 1.208\n",
      "[epoch: 1, i:  6624]  train_loss: 1.248  |  valid_loss: 1.155\n",
      "[epoch: 1, i:  6749]  train_loss: 1.306  |  valid_loss: 1.421\n",
      "[epoch: 1, i:  6874]  train_loss: 1.335  |  valid_loss: 1.305\n",
      "[epoch: 1, i:  6999]  train_loss: 1.332  |  valid_loss: 1.357\n",
      "[epoch: 1, i:  7124]  train_loss: 1.230  |  valid_loss: 1.288\n",
      "[epoch: 1, i:  7249]  train_loss: 1.268  |  valid_loss: 1.069\n",
      "[epoch: 1, i:  7374]  train_loss: 1.303  |  valid_loss: 1.325\n",
      "[epoch: 1, i:  7499]  train_loss: 1.326  |  valid_loss: 1.232\n",
      "[epoch: 1, i:  7624]  train_loss: 1.241  |  valid_loss: 1.386\n",
      "[epoch: 1, i:  7749]  train_loss: 1.254  |  valid_loss: 1.332\n",
      "[epoch: 1, i:  7874]  train_loss: 1.258  |  valid_loss: 1.197\n",
      "[epoch: 1, i:  7999]  train_loss: 1.232  |  valid_loss: 1.270\n",
      "[epoch: 1, i:  8124]  train_loss: 1.315  |  valid_loss: 1.310\n",
      "[epoch: 1, i:  8249]  train_loss: 1.308  |  valid_loss: 1.263\n",
      "[epoch: 1, i:  8374]  train_loss: 1.354  |  valid_loss: 1.194\n",
      "[epoch: 1, i:  8499]  train_loss: 1.284  |  valid_loss: 1.317\n",
      "[epoch: 1, i:  8624]  train_loss: 1.307  |  valid_loss: 1.149\n",
      "[epoch: 1, i:  8749]  train_loss: 1.273  |  valid_loss: 1.249\n",
      "[epoch: 1, i:  8874]  train_loss: 1.270  |  valid_loss: 1.299\n",
      "[epoch: 1, i:  8999]  train_loss: 1.232  |  valid_loss: 1.116\n",
      "[epoch: 1, i:  9124]  train_loss: 1.235  |  valid_loss: 1.189\n",
      "[epoch: 1, i:  9249]  train_loss: 1.241  |  valid_loss: 1.017\n",
      "[epoch: 1, i:  9374]  train_loss: 1.236  |  valid_loss: 1.325\n",
      "[epoch: 1, i:  9499]  train_loss: 1.284  |  valid_loss: 1.257\n",
      "[epoch: 1, i:  9624]  train_loss: 1.364  |  valid_loss: 1.179\n",
      "[epoch: 1, i:  9749]  train_loss: 1.272  |  valid_loss: 1.210\n",
      "[epoch: 1, i:  9874]  train_loss: 1.289  |  valid_loss: 1.439\n",
      "[epoch: 1, i:  9999]  train_loss: 1.222  |  valid_loss: 1.330\n",
      "[epoch: 1, i: 10124]  train_loss: 1.239  |  valid_loss: 1.097\n",
      "[epoch: 1, i: 10249]  train_loss: 1.255  |  valid_loss: 1.426\n",
      "[epoch: 1, i: 10374]  train_loss: 1.298  |  valid_loss: 1.275\n",
      "[epoch: 1, i: 10499]  train_loss: 1.250  |  valid_loss: 1.421\n",
      "[epoch: 1, i: 10624]  train_loss: 1.298  |  valid_loss: 1.417\n",
      "[epoch: 1, i: 10749]  train_loss: 1.276  |  valid_loss: 1.189\n",
      "[epoch: 1, i: 10874]  train_loss: 1.281  |  valid_loss: 1.247\n",
      "[epoch: 1, i: 10999]  train_loss: 1.220  |  valid_loss: 1.215\n",
      "[epoch: 1, i: 11124]  train_loss: 1.285  |  valid_loss: 1.236\n",
      "[epoch: 1, i: 11249]  train_loss: 1.278  |  valid_loss: 1.144\n",
      "[epoch: 1, i: 11374]  train_loss: 1.237  |  valid_loss: 1.115\n",
      "[epoch: 1, i: 11499]  train_loss: 1.301  |  valid_loss: 1.148\n",
      "[epoch: 1, i: 11624]  train_loss: 1.258  |  valid_loss: 1.095\n",
      "[epoch: 1, i: 11749]  train_loss: 1.250  |  valid_loss: 1.229\n",
      "[epoch: 1, i: 11874]  train_loss: 1.189  |  valid_loss: 1.244\n",
      "[epoch: 1, i: 11999]  train_loss: 1.307  |  valid_loss: 1.175\n",
      "[epoch: 1, i: 12124]  train_loss: 1.130  |  valid_loss: 1.251\n",
      "[epoch: 1, i: 12249]  train_loss: 1.303  |  valid_loss: 1.380\n",
      "[epoch: 1, i: 12374]  train_loss: 1.227  |  valid_loss: 1.206\n",
      "[epoch: 1, i: 12499]  train_loss: 1.257  |  valid_loss: 1.131\n",
      "--> [End of epoch 1] train_accuracy: 52.60%  |  valid_accuracy: 54.78%\n",
      "--> [Start of epoch 2]  lr: 0.001280\n",
      "[epoch: 2, i:   124]  train_loss: 1.267  |  valid_loss: 1.069\n",
      "[epoch: 2, i:   249]  train_loss: 1.239  |  valid_loss: 1.184\n",
      "[epoch: 2, i:   374]  train_loss: 1.214  |  valid_loss: 1.123\n",
      "[epoch: 2, i:   499]  train_loss: 1.260  |  valid_loss: 1.248\n",
      "[epoch: 2, i:   624]  train_loss: 1.220  |  valid_loss: 1.246\n",
      "[epoch: 2, i:   749]  train_loss: 1.215  |  valid_loss: 0.998\n",
      "[epoch: 2, i:   874]  train_loss: 1.216  |  valid_loss: 1.203\n",
      "[epoch: 2, i:   999]  train_loss: 1.230  |  valid_loss: 1.249\n",
      "[epoch: 2, i:  1124]  train_loss: 1.194  |  valid_loss: 1.250\n",
      "[epoch: 2, i:  1249]  train_loss: 1.204  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  1374]  train_loss: 1.209  |  valid_loss: 0.991\n",
      "[epoch: 2, i:  1499]  train_loss: 1.203  |  valid_loss: 1.162\n",
      "[epoch: 2, i:  1624]  train_loss: 1.208  |  valid_loss: 1.146\n",
      "[epoch: 2, i:  1749]  train_loss: 1.269  |  valid_loss: 1.360\n",
      "[epoch: 2, i:  1874]  train_loss: 1.167  |  valid_loss: 1.082\n",
      "[epoch: 2, i:  1999]  train_loss: 1.207  |  valid_loss: 1.370\n",
      "[epoch: 2, i:  2124]  train_loss: 1.180  |  valid_loss: 1.197\n",
      "[epoch: 2, i:  2249]  train_loss: 1.256  |  valid_loss: 1.093\n",
      "[epoch: 2, i:  2374]  train_loss: 1.161  |  valid_loss: 0.961\n",
      "[epoch: 2, i:  2499]  train_loss: 1.161  |  valid_loss: 1.251\n",
      "[epoch: 2, i:  2624]  train_loss: 1.174  |  valid_loss: 1.142\n",
      "[epoch: 2, i:  2749]  train_loss: 1.158  |  valid_loss: 1.174\n",
      "[epoch: 2, i:  2874]  train_loss: 1.212  |  valid_loss: 1.322\n",
      "[epoch: 2, i:  2999]  train_loss: 1.177  |  valid_loss: 1.247\n",
      "[epoch: 2, i:  3124]  train_loss: 1.257  |  valid_loss: 1.129\n",
      "[epoch: 2, i:  3249]  train_loss: 1.119  |  valid_loss: 1.409\n",
      "[epoch: 2, i:  3374]  train_loss: 1.182  |  valid_loss: 1.144\n",
      "[epoch: 2, i:  3499]  train_loss: 1.196  |  valid_loss: 1.128\n",
      "[epoch: 2, i:  3624]  train_loss: 1.222  |  valid_loss: 1.234\n",
      "[epoch: 2, i:  3749]  train_loss: 1.226  |  valid_loss: 1.056\n",
      "[epoch: 2, i:  3874]  train_loss: 1.160  |  valid_loss: 1.032\n",
      "[epoch: 2, i:  3999]  train_loss: 1.186  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  4124]  train_loss: 1.164  |  valid_loss: 1.222\n",
      "[epoch: 2, i:  4249]  train_loss: 1.190  |  valid_loss: 1.171\n",
      "[epoch: 2, i:  4374]  train_loss: 1.195  |  valid_loss: 1.253\n",
      "[epoch: 2, i:  4499]  train_loss: 1.127  |  valid_loss: 1.173\n",
      "[epoch: 2, i:  4624]  train_loss: 1.232  |  valid_loss: 1.233\n",
      "[epoch: 2, i:  4749]  train_loss: 1.200  |  valid_loss: 1.253\n",
      "[epoch: 2, i:  4874]  train_loss: 1.226  |  valid_loss: 1.203\n",
      "[epoch: 2, i:  4999]  train_loss: 1.252  |  valid_loss: 1.099\n",
      "[epoch: 2, i:  5124]  train_loss: 1.130  |  valid_loss: 1.163\n",
      "[epoch: 2, i:  5249]  train_loss: 1.141  |  valid_loss: 1.146\n",
      "[epoch: 2, i:  5374]  train_loss: 1.292  |  valid_loss: 0.925\n",
      "[epoch: 2, i:  5499]  train_loss: 1.241  |  valid_loss: 1.013\n",
      "[epoch: 2, i:  5624]  train_loss: 1.250  |  valid_loss: 1.393\n",
      "[epoch: 2, i:  5749]  train_loss: 1.182  |  valid_loss: 1.079\n",
      "[epoch: 2, i:  5874]  train_loss: 1.135  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  5999]  train_loss: 1.180  |  valid_loss: 1.061\n",
      "[epoch: 2, i:  6124]  train_loss: 1.185  |  valid_loss: 1.000\n",
      "[epoch: 2, i:  6249]  train_loss: 1.168  |  valid_loss: 1.153\n",
      "[epoch: 2, i:  6374]  train_loss: 1.166  |  valid_loss: 1.126\n",
      "[epoch: 2, i:  6499]  train_loss: 1.177  |  valid_loss: 1.170\n",
      "[epoch: 2, i:  6624]  train_loss: 1.102  |  valid_loss: 1.009\n",
      "[epoch: 2, i:  6749]  train_loss: 1.268  |  valid_loss: 1.234\n",
      "[epoch: 2, i:  6874]  train_loss: 1.180  |  valid_loss: 1.043\n",
      "[epoch: 2, i:  6999]  train_loss: 1.220  |  valid_loss: 1.215\n",
      "[epoch: 2, i:  7124]  train_loss: 1.188  |  valid_loss: 1.259\n",
      "[epoch: 2, i:  7249]  train_loss: 1.123  |  valid_loss: 1.013\n",
      "[epoch: 2, i:  7374]  train_loss: 1.178  |  valid_loss: 1.314\n",
      "[epoch: 2, i:  7499]  train_loss: 1.159  |  valid_loss: 1.202\n",
      "[epoch: 2, i:  7624]  train_loss: 1.117  |  valid_loss: 1.226\n",
      "[epoch: 2, i:  7749]  train_loss: 1.183  |  valid_loss: 1.187\n",
      "[epoch: 2, i:  7874]  train_loss: 1.135  |  valid_loss: 1.066\n",
      "[epoch: 2, i:  7999]  train_loss: 1.103  |  valid_loss: 1.081\n",
      "[epoch: 2, i:  8124]  train_loss: 1.136  |  valid_loss: 1.205\n",
      "[epoch: 2, i:  8249]  train_loss: 1.234  |  valid_loss: 1.090\n",
      "[epoch: 2, i:  8374]  train_loss: 1.182  |  valid_loss: 1.160\n",
      "[epoch: 2, i:  8499]  train_loss: 1.164  |  valid_loss: 1.180\n",
      "[epoch: 2, i:  8624]  train_loss: 1.245  |  valid_loss: 1.106\n",
      "[epoch: 2, i:  8749]  train_loss: 1.193  |  valid_loss: 1.294\n",
      "[epoch: 2, i:  8874]  train_loss: 1.122  |  valid_loss: 1.239\n",
      "[epoch: 2, i:  8999]  train_loss: 1.138  |  valid_loss: 1.059\n",
      "[epoch: 2, i:  9124]  train_loss: 1.106  |  valid_loss: 1.041\n",
      "[epoch: 2, i:  9249]  train_loss: 1.160  |  valid_loss: 0.997\n",
      "[epoch: 2, i:  9374]  train_loss: 1.187  |  valid_loss: 1.174\n",
      "[epoch: 2, i:  9499]  train_loss: 1.161  |  valid_loss: 1.031\n",
      "[epoch: 2, i:  9624]  train_loss: 1.047  |  valid_loss: 1.110\n",
      "[epoch: 2, i:  9749]  train_loss: 1.143  |  valid_loss: 1.153\n",
      "[epoch: 2, i:  9874]  train_loss: 1.144  |  valid_loss: 1.123\n",
      "[epoch: 2, i:  9999]  train_loss: 1.186  |  valid_loss: 1.197\n",
      "[epoch: 2, i: 10124]  train_loss: 1.126  |  valid_loss: 1.050\n",
      "[epoch: 2, i: 10249]  train_loss: 1.134  |  valid_loss: 1.272\n",
      "[epoch: 2, i: 10374]  train_loss: 1.187  |  valid_loss: 1.129\n",
      "[epoch: 2, i: 10499]  train_loss: 1.197  |  valid_loss: 1.326\n",
      "[epoch: 2, i: 10624]  train_loss: 1.022  |  valid_loss: 1.310\n",
      "[epoch: 2, i: 10749]  train_loss: 1.166  |  valid_loss: 1.074\n",
      "[epoch: 2, i: 10874]  train_loss: 1.128  |  valid_loss: 1.165\n",
      "[epoch: 2, i: 10999]  train_loss: 1.109  |  valid_loss: 1.078\n",
      "[epoch: 2, i: 11124]  train_loss: 1.170  |  valid_loss: 1.153\n",
      "[epoch: 2, i: 11249]  train_loss: 1.093  |  valid_loss: 1.057\n",
      "[epoch: 2, i: 11374]  train_loss: 1.028  |  valid_loss: 1.072\n",
      "[epoch: 2, i: 11499]  train_loss: 1.133  |  valid_loss: 1.054\n",
      "[epoch: 2, i: 11624]  train_loss: 1.147  |  valid_loss: 1.116\n",
      "[epoch: 2, i: 11749]  train_loss: 1.158  |  valid_loss: 1.126\n",
      "[epoch: 2, i: 11874]  train_loss: 1.095  |  valid_loss: 1.094\n",
      "[epoch: 2, i: 11999]  train_loss: 1.197  |  valid_loss: 0.931\n",
      "[epoch: 2, i: 12124]  train_loss: 1.171  |  valid_loss: 1.041\n",
      "[epoch: 2, i: 12249]  train_loss: 1.107  |  valid_loss: 1.226\n",
      "[epoch: 2, i: 12374]  train_loss: 1.103  |  valid_loss: 1.277\n",
      "[epoch: 2, i: 12499]  train_loss: 1.093  |  valid_loss: 1.166\n",
      "--> [End of epoch 2] train_accuracy: 57.62%  |  valid_accuracy: 59.40%\n",
      "--> [Start of epoch 3]  lr: 0.001024\n",
      "[epoch: 3, i:   124]  train_loss: 1.096  |  valid_loss: 0.970\n",
      "[epoch: 3, i:   249]  train_loss: 1.165  |  valid_loss: 1.044\n",
      "[epoch: 3, i:   374]  train_loss: 1.132  |  valid_loss: 1.056\n",
      "[epoch: 3, i:   499]  train_loss: 1.018  |  valid_loss: 1.080\n",
      "[epoch: 3, i:   624]  train_loss: 1.090  |  valid_loss: 1.124\n",
      "[epoch: 3, i:   749]  train_loss: 1.102  |  valid_loss: 0.916\n",
      "[epoch: 3, i:   874]  train_loss: 1.077  |  valid_loss: 1.047\n",
      "[epoch: 3, i:   999]  train_loss: 1.151  |  valid_loss: 1.168\n",
      "[epoch: 3, i:  1124]  train_loss: 1.101  |  valid_loss: 1.175\n",
      "[epoch: 3, i:  1249]  train_loss: 1.060  |  valid_loss: 0.991\n",
      "[epoch: 3, i:  1374]  train_loss: 1.124  |  valid_loss: 0.984\n",
      "[epoch: 3, i:  1499]  train_loss: 1.110  |  valid_loss: 1.171\n",
      "[epoch: 3, i:  1624]  train_loss: 1.152  |  valid_loss: 1.019\n",
      "[epoch: 3, i:  1749]  train_loss: 1.130  |  valid_loss: 1.286\n",
      "[epoch: 3, i:  1874]  train_loss: 1.131  |  valid_loss: 0.921\n",
      "[epoch: 3, i:  1999]  train_loss: 1.134  |  valid_loss: 1.354\n",
      "[epoch: 3, i:  2124]  train_loss: 1.092  |  valid_loss: 1.021\n",
      "[epoch: 3, i:  2249]  train_loss: 1.116  |  valid_loss: 1.053\n",
      "[epoch: 3, i:  2374]  train_loss: 1.084  |  valid_loss: 0.920\n",
      "[epoch: 3, i:  2499]  train_loss: 1.060  |  valid_loss: 1.180\n",
      "[epoch: 3, i:  2624]  train_loss: 1.105  |  valid_loss: 1.158\n",
      "[epoch: 3, i:  2749]  train_loss: 1.083  |  valid_loss: 1.164\n",
      "[epoch: 3, i:  2874]  train_loss: 1.059  |  valid_loss: 1.307\n",
      "[epoch: 3, i:  2999]  train_loss: 1.124  |  valid_loss: 1.083\n",
      "[epoch: 3, i:  3124]  train_loss: 1.151  |  valid_loss: 1.085\n",
      "[epoch: 3, i:  3249]  train_loss: 1.065  |  valid_loss: 1.253\n",
      "[epoch: 3, i:  3374]  train_loss: 1.089  |  valid_loss: 1.132\n",
      "[epoch: 3, i:  3499]  train_loss: 1.029  |  valid_loss: 1.039\n",
      "[epoch: 3, i:  3624]  train_loss: 1.149  |  valid_loss: 1.152\n",
      "[epoch: 3, i:  3749]  train_loss: 1.127  |  valid_loss: 0.966\n",
      "[epoch: 3, i:  3874]  train_loss: 1.056  |  valid_loss: 1.014\n",
      "[epoch: 3, i:  3999]  train_loss: 1.141  |  valid_loss: 0.858\n",
      "[epoch: 3, i:  4124]  train_loss: 1.045  |  valid_loss: 1.114\n",
      "[epoch: 3, i:  4249]  train_loss: 1.071  |  valid_loss: 1.148\n",
      "[epoch: 3, i:  4374]  train_loss: 1.117  |  valid_loss: 1.100\n",
      "[epoch: 3, i:  4499]  train_loss: 1.015  |  valid_loss: 1.046\n",
      "[epoch: 3, i:  4624]  train_loss: 1.156  |  valid_loss: 1.107\n",
      "[epoch: 3, i:  4749]  train_loss: 1.090  |  valid_loss: 1.126\n",
      "[epoch: 3, i:  4874]  train_loss: 1.088  |  valid_loss: 1.079\n",
      "[epoch: 3, i:  4999]  train_loss: 1.147  |  valid_loss: 1.049\n",
      "[epoch: 3, i:  5124]  train_loss: 1.121  |  valid_loss: 1.103\n",
      "[epoch: 3, i:  5249]  train_loss: 1.073  |  valid_loss: 1.150\n",
      "[epoch: 3, i:  5374]  train_loss: 1.106  |  valid_loss: 0.882\n",
      "[epoch: 3, i:  5499]  train_loss: 1.129  |  valid_loss: 0.829\n",
      "[epoch: 3, i:  5624]  train_loss: 1.107  |  valid_loss: 1.178\n",
      "[epoch: 3, i:  5749]  train_loss: 1.116  |  valid_loss: 1.054\n",
      "[epoch: 3, i:  5874]  train_loss: 1.097  |  valid_loss: 1.003\n",
      "[epoch: 3, i:  5999]  train_loss: 1.083  |  valid_loss: 1.034\n",
      "[epoch: 3, i:  6124]  train_loss: 1.019  |  valid_loss: 0.945\n",
      "[epoch: 3, i:  6249]  train_loss: 1.100  |  valid_loss: 1.118\n",
      "[epoch: 3, i:  6374]  train_loss: 1.078  |  valid_loss: 1.020\n",
      "[epoch: 3, i:  6499]  train_loss: 1.148  |  valid_loss: 1.006\n",
      "[epoch: 3, i:  6624]  train_loss: 1.093  |  valid_loss: 0.898\n",
      "[epoch: 3, i:  6749]  train_loss: 1.063  |  valid_loss: 1.125\n",
      "[epoch: 3, i:  6874]  train_loss: 1.071  |  valid_loss: 1.018\n",
      "[epoch: 3, i:  6999]  train_loss: 1.068  |  valid_loss: 1.250\n",
      "[epoch: 3, i:  7124]  train_loss: 1.081  |  valid_loss: 1.207\n",
      "[epoch: 3, i:  7249]  train_loss: 1.053  |  valid_loss: 0.919\n",
      "[epoch: 3, i:  7374]  train_loss: 1.046  |  valid_loss: 1.194\n",
      "[epoch: 3, i:  7499]  train_loss: 1.073  |  valid_loss: 1.168\n",
      "[epoch: 3, i:  7624]  train_loss: 1.126  |  valid_loss: 1.160\n",
      "[epoch: 3, i:  7749]  train_loss: 1.045  |  valid_loss: 1.012\n",
      "[epoch: 3, i:  7874]  train_loss: 1.098  |  valid_loss: 0.922\n",
      "[epoch: 3, i:  7999]  train_loss: 1.120  |  valid_loss: 1.024\n",
      "[epoch: 3, i:  8124]  train_loss: 1.129  |  valid_loss: 1.107\n",
      "[epoch: 3, i:  8249]  train_loss: 0.991  |  valid_loss: 1.077\n",
      "[epoch: 3, i:  8374]  train_loss: 1.098  |  valid_loss: 1.085\n",
      "[epoch: 3, i:  8499]  train_loss: 1.063  |  valid_loss: 1.088\n",
      "[epoch: 3, i:  8624]  train_loss: 1.109  |  valid_loss: 1.006\n",
      "[epoch: 3, i:  8749]  train_loss: 1.111  |  valid_loss: 1.108\n",
      "[epoch: 3, i:  8874]  train_loss: 1.195  |  valid_loss: 1.090\n",
      "[epoch: 3, i:  8999]  train_loss: 1.102  |  valid_loss: 0.870\n",
      "[epoch: 3, i:  9124]  train_loss: 1.079  |  valid_loss: 0.991\n",
      "[epoch: 3, i:  9249]  train_loss: 1.097  |  valid_loss: 0.916\n",
      "[epoch: 3, i:  9374]  train_loss: 1.091  |  valid_loss: 1.105\n",
      "[epoch: 3, i:  9499]  train_loss: 1.124  |  valid_loss: 0.995\n",
      "[epoch: 3, i:  9624]  train_loss: 0.991  |  valid_loss: 1.138\n",
      "[epoch: 3, i:  9749]  train_loss: 1.004  |  valid_loss: 1.077\n",
      "[epoch: 3, i:  9874]  train_loss: 1.137  |  valid_loss: 1.082\n",
      "[epoch: 3, i:  9999]  train_loss: 1.175  |  valid_loss: 1.089\n",
      "[epoch: 3, i: 10124]  train_loss: 1.069  |  valid_loss: 0.969\n",
      "[epoch: 3, i: 10249]  train_loss: 1.078  |  valid_loss: 1.185\n",
      "[epoch: 3, i: 10374]  train_loss: 1.043  |  valid_loss: 1.045\n",
      "[epoch: 3, i: 10499]  train_loss: 1.100  |  valid_loss: 1.209\n",
      "[epoch: 3, i: 10624]  train_loss: 0.958  |  valid_loss: 1.197\n",
      "[epoch: 3, i: 10749]  train_loss: 1.116  |  valid_loss: 1.036\n",
      "[epoch: 3, i: 10874]  train_loss: 1.079  |  valid_loss: 1.125\n",
      "[epoch: 3, i: 10999]  train_loss: 1.024  |  valid_loss: 1.163\n",
      "[epoch: 3, i: 11124]  train_loss: 1.061  |  valid_loss: 1.103\n",
      "[epoch: 3, i: 11249]  train_loss: 1.133  |  valid_loss: 1.073\n",
      "[epoch: 3, i: 11374]  train_loss: 1.063  |  valid_loss: 0.967\n",
      "[epoch: 3, i: 11499]  train_loss: 1.082  |  valid_loss: 0.940\n",
      "[epoch: 3, i: 11624]  train_loss: 1.101  |  valid_loss: 1.058\n",
      "[epoch: 3, i: 11749]  train_loss: 0.957  |  valid_loss: 1.016\n",
      "[epoch: 3, i: 11874]  train_loss: 1.046  |  valid_loss: 0.988\n",
      "[epoch: 3, i: 11999]  train_loss: 1.129  |  valid_loss: 0.797\n",
      "[epoch: 3, i: 12124]  train_loss: 1.046  |  valid_loss: 0.925\n",
      "[epoch: 3, i: 12249]  train_loss: 1.035  |  valid_loss: 1.186\n",
      "[epoch: 3, i: 12374]  train_loss: 1.141  |  valid_loss: 1.090\n",
      "[epoch: 3, i: 12499]  train_loss: 1.039  |  valid_loss: 1.078\n",
      "--> [End of epoch 3] train_accuracy: 60.84%  |  valid_accuracy: 62.21%\n",
      "--> [Start of epoch 4]  lr: 0.000819\n",
      "[epoch: 4, i:   124]  train_loss: 0.991  |  valid_loss: 0.943\n",
      "[epoch: 4, i:   249]  train_loss: 0.979  |  valid_loss: 0.957\n",
      "[epoch: 4, i:   374]  train_loss: 1.076  |  valid_loss: 1.126\n",
      "[epoch: 4, i:   499]  train_loss: 0.980  |  valid_loss: 1.071\n",
      "[epoch: 4, i:   624]  train_loss: 1.019  |  valid_loss: 1.156\n",
      "[epoch: 4, i:   749]  train_loss: 1.045  |  valid_loss: 0.868\n",
      "[epoch: 4, i:   874]  train_loss: 1.023  |  valid_loss: 1.046\n",
      "[epoch: 4, i:   999]  train_loss: 0.990  |  valid_loss: 1.094\n",
      "[epoch: 4, i:  1124]  train_loss: 1.044  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  1249]  train_loss: 1.046  |  valid_loss: 1.016\n",
      "[epoch: 4, i:  1374]  train_loss: 1.046  |  valid_loss: 0.922\n",
      "[epoch: 4, i:  1499]  train_loss: 1.059  |  valid_loss: 0.978\n",
      "[epoch: 4, i:  1624]  train_loss: 1.018  |  valid_loss: 0.859\n",
      "[epoch: 4, i:  1749]  train_loss: 1.039  |  valid_loss: 1.291\n",
      "[epoch: 4, i:  1874]  train_loss: 1.039  |  valid_loss: 0.961\n",
      "[epoch: 4, i:  1999]  train_loss: 1.121  |  valid_loss: 1.256\n",
      "[epoch: 4, i:  2124]  train_loss: 0.965  |  valid_loss: 1.005\n",
      "[epoch: 4, i:  2249]  train_loss: 1.032  |  valid_loss: 1.054\n",
      "[epoch: 4, i:  2374]  train_loss: 1.060  |  valid_loss: 0.941\n",
      "[epoch: 4, i:  2499]  train_loss: 1.018  |  valid_loss: 1.100\n",
      "[epoch: 4, i:  2624]  train_loss: 0.937  |  valid_loss: 0.995\n",
      "[epoch: 4, i:  2749]  train_loss: 1.079  |  valid_loss: 1.236\n",
      "[epoch: 4, i:  2874]  train_loss: 1.033  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  2999]  train_loss: 0.987  |  valid_loss: 1.079\n",
      "[epoch: 4, i:  3124]  train_loss: 0.986  |  valid_loss: 1.001\n",
      "[epoch: 4, i:  3249]  train_loss: 1.107  |  valid_loss: 1.224\n",
      "[epoch: 4, i:  3374]  train_loss: 1.082  |  valid_loss: 1.081\n",
      "[epoch: 4, i:  3499]  train_loss: 1.043  |  valid_loss: 0.902\n",
      "[epoch: 4, i:  3624]  train_loss: 1.048  |  valid_loss: 1.094\n",
      "[epoch: 4, i:  3749]  train_loss: 1.104  |  valid_loss: 0.943\n",
      "[epoch: 4, i:  3874]  train_loss: 0.966  |  valid_loss: 0.938\n",
      "[epoch: 4, i:  3999]  train_loss: 1.064  |  valid_loss: 0.801\n",
      "[epoch: 4, i:  4124]  train_loss: 1.073  |  valid_loss: 1.188\n",
      "[epoch: 4, i:  4249]  train_loss: 1.073  |  valid_loss: 1.079\n",
      "[epoch: 4, i:  4374]  train_loss: 1.044  |  valid_loss: 1.047\n",
      "[epoch: 4, i:  4499]  train_loss: 0.985  |  valid_loss: 1.050\n",
      "[epoch: 4, i:  4624]  train_loss: 0.994  |  valid_loss: 1.120\n",
      "[epoch: 4, i:  4749]  train_loss: 1.042  |  valid_loss: 1.102\n",
      "[epoch: 4, i:  4874]  train_loss: 1.041  |  valid_loss: 0.999\n",
      "[epoch: 4, i:  4999]  train_loss: 1.073  |  valid_loss: 0.972\n",
      "[epoch: 4, i:  5124]  train_loss: 1.014  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  5249]  train_loss: 1.018  |  valid_loss: 1.047\n",
      "[epoch: 4, i:  5374]  train_loss: 1.079  |  valid_loss: 0.817\n",
      "[epoch: 4, i:  5499]  train_loss: 1.014  |  valid_loss: 0.839\n",
      "[epoch: 4, i:  5624]  train_loss: 1.000  |  valid_loss: 1.212\n",
      "[epoch: 4, i:  5749]  train_loss: 1.070  |  valid_loss: 0.977\n",
      "[epoch: 4, i:  5874]  train_loss: 1.006  |  valid_loss: 0.951\n",
      "[epoch: 4, i:  5999]  train_loss: 1.082  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  6124]  train_loss: 1.069  |  valid_loss: 0.952\n",
      "[epoch: 4, i:  6249]  train_loss: 1.040  |  valid_loss: 1.056\n",
      "[epoch: 4, i:  6374]  train_loss: 1.052  |  valid_loss: 0.932\n",
      "[epoch: 4, i:  6499]  train_loss: 1.026  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  6624]  train_loss: 0.989  |  valid_loss: 0.868\n",
      "[epoch: 4, i:  6749]  train_loss: 0.985  |  valid_loss: 1.158\n",
      "[epoch: 4, i:  6874]  train_loss: 1.032  |  valid_loss: 0.970\n",
      "[epoch: 4, i:  6999]  train_loss: 0.957  |  valid_loss: 1.264\n",
      "[epoch: 4, i:  7124]  train_loss: 1.122  |  valid_loss: 1.075\n",
      "[epoch: 4, i:  7249]  train_loss: 0.936  |  valid_loss: 0.860\n",
      "[epoch: 4, i:  7374]  train_loss: 0.971  |  valid_loss: 1.197\n",
      "[epoch: 4, i:  7499]  train_loss: 1.033  |  valid_loss: 1.009\n",
      "[epoch: 4, i:  7624]  train_loss: 1.024  |  valid_loss: 1.115\n",
      "[epoch: 4, i:  7749]  train_loss: 1.017  |  valid_loss: 0.934\n",
      "[epoch: 4, i:  7874]  train_loss: 0.966  |  valid_loss: 0.988\n",
      "[epoch: 4, i:  7999]  train_loss: 0.999  |  valid_loss: 0.964\n",
      "[epoch: 4, i:  8124]  train_loss: 1.085  |  valid_loss: 1.046\n",
      "[epoch: 4, i:  8249]  train_loss: 1.087  |  valid_loss: 0.996\n",
      "[epoch: 4, i:  8374]  train_loss: 1.085  |  valid_loss: 1.038\n",
      "[epoch: 4, i:  8499]  train_loss: 1.016  |  valid_loss: 1.049\n",
      "[epoch: 4, i:  8624]  train_loss: 0.950  |  valid_loss: 1.033\n",
      "[epoch: 4, i:  8749]  train_loss: 1.004  |  valid_loss: 1.111\n",
      "[epoch: 4, i:  8874]  train_loss: 1.065  |  valid_loss: 1.194\n",
      "[epoch: 4, i:  8999]  train_loss: 1.001  |  valid_loss: 0.814\n",
      "[epoch: 4, i:  9124]  train_loss: 1.016  |  valid_loss: 0.916\n",
      "[epoch: 4, i:  9249]  train_loss: 0.976  |  valid_loss: 0.825\n",
      "[epoch: 4, i:  9374]  train_loss: 1.078  |  valid_loss: 1.116\n",
      "[epoch: 4, i:  9499]  train_loss: 1.015  |  valid_loss: 0.964\n",
      "[epoch: 4, i:  9624]  train_loss: 0.970  |  valid_loss: 0.984\n",
      "[epoch: 4, i:  9749]  train_loss: 0.949  |  valid_loss: 1.046\n",
      "[epoch: 4, i:  9874]  train_loss: 1.044  |  valid_loss: 1.140\n",
      "[epoch: 4, i:  9999]  train_loss: 1.031  |  valid_loss: 1.076\n",
      "[epoch: 4, i: 10124]  train_loss: 1.050  |  valid_loss: 0.884\n",
      "[epoch: 4, i: 10249]  train_loss: 1.049  |  valid_loss: 1.090\n",
      "[epoch: 4, i: 10374]  train_loss: 0.999  |  valid_loss: 0.949\n",
      "[epoch: 4, i: 10499]  train_loss: 0.934  |  valid_loss: 1.206\n",
      "[epoch: 4, i: 10624]  train_loss: 1.012  |  valid_loss: 1.160\n",
      "[epoch: 4, i: 10749]  train_loss: 1.018  |  valid_loss: 0.951\n",
      "[epoch: 4, i: 10874]  train_loss: 1.036  |  valid_loss: 1.094\n",
      "[epoch: 4, i: 10999]  train_loss: 1.115  |  valid_loss: 1.062\n",
      "[epoch: 4, i: 11124]  train_loss: 1.017  |  valid_loss: 1.021\n",
      "[epoch: 4, i: 11249]  train_loss: 1.021  |  valid_loss: 0.961\n",
      "[epoch: 4, i: 11374]  train_loss: 1.002  |  valid_loss: 0.920\n",
      "[epoch: 4, i: 11499]  train_loss: 1.018  |  valid_loss: 0.879\n",
      "[epoch: 4, i: 11624]  train_loss: 1.041  |  valid_loss: 0.868\n",
      "[epoch: 4, i: 11749]  train_loss: 1.039  |  valid_loss: 0.962\n",
      "[epoch: 4, i: 11874]  train_loss: 0.973  |  valid_loss: 0.955\n",
      "[epoch: 4, i: 11999]  train_loss: 1.021  |  valid_loss: 0.811\n",
      "[epoch: 4, i: 12124]  train_loss: 0.985  |  valid_loss: 0.915\n",
      "[epoch: 4, i: 12249]  train_loss: 1.016  |  valid_loss: 1.175\n",
      "[epoch: 4, i: 12374]  train_loss: 1.021  |  valid_loss: 1.089\n",
      "[epoch: 4, i: 12499]  train_loss: 1.078  |  valid_loss: 0.998\n",
      "--> [End of epoch 4] train_accuracy: 63.22%  |  valid_accuracy: 63.82%\n",
      "--> [Start of epoch 5]  lr: 0.000655\n",
      "[epoch: 5, i:   124]  train_loss: 0.965  |  valid_loss: 0.887\n",
      "[epoch: 5, i:   249]  train_loss: 1.080  |  valid_loss: 0.934\n",
      "[epoch: 5, i:   374]  train_loss: 1.008  |  valid_loss: 1.079\n",
      "[epoch: 5, i:   499]  train_loss: 0.903  |  valid_loss: 1.020\n",
      "[epoch: 5, i:   624]  train_loss: 0.965  |  valid_loss: 1.034\n",
      "[epoch: 5, i:   749]  train_loss: 0.943  |  valid_loss: 0.783\n",
      "[epoch: 5, i:   874]  train_loss: 0.988  |  valid_loss: 1.054\n",
      "[epoch: 5, i:   999]  train_loss: 0.951  |  valid_loss: 1.068\n",
      "[epoch: 5, i:  1124]  train_loss: 0.992  |  valid_loss: 1.005\n",
      "[epoch: 5, i:  1249]  train_loss: 1.021  |  valid_loss: 0.890\n",
      "[epoch: 5, i:  1374]  train_loss: 1.017  |  valid_loss: 0.960\n",
      "[epoch: 5, i:  1499]  train_loss: 0.941  |  valid_loss: 0.965\n",
      "[epoch: 5, i:  1624]  train_loss: 1.084  |  valid_loss: 0.857\n",
      "[epoch: 5, i:  1749]  train_loss: 0.953  |  valid_loss: 1.276\n",
      "[epoch: 5, i:  1874]  train_loss: 1.033  |  valid_loss: 0.825\n",
      "[epoch: 5, i:  1999]  train_loss: 0.952  |  valid_loss: 1.128\n",
      "[epoch: 5, i:  2124]  train_loss: 0.940  |  valid_loss: 0.852\n",
      "[epoch: 5, i:  2249]  train_loss: 1.043  |  valid_loss: 0.985\n",
      "[epoch: 5, i:  2374]  train_loss: 1.035  |  valid_loss: 0.863\n",
      "[epoch: 5, i:  2499]  train_loss: 1.054  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  2624]  train_loss: 0.958  |  valid_loss: 1.061\n",
      "[epoch: 5, i:  2749]  train_loss: 0.861  |  valid_loss: 1.100\n",
      "[epoch: 5, i:  2874]  train_loss: 1.003  |  valid_loss: 1.147\n",
      "[epoch: 5, i:  2999]  train_loss: 0.952  |  valid_loss: 0.991\n",
      "[epoch: 5, i:  3124]  train_loss: 1.027  |  valid_loss: 1.005\n",
      "[epoch: 5, i:  3249]  train_loss: 0.942  |  valid_loss: 1.228\n",
      "[epoch: 5, i:  3374]  train_loss: 0.978  |  valid_loss: 0.990\n",
      "[epoch: 5, i:  3499]  train_loss: 0.894  |  valid_loss: 0.907\n",
      "[epoch: 5, i:  3624]  train_loss: 1.009  |  valid_loss: 1.031\n",
      "[epoch: 5, i:  3749]  train_loss: 0.990  |  valid_loss: 0.830\n",
      "[epoch: 5, i:  3874]  train_loss: 0.862  |  valid_loss: 0.891\n",
      "[epoch: 5, i:  3999]  train_loss: 0.931  |  valid_loss: 0.763\n",
      "[epoch: 5, i:  4124]  train_loss: 0.956  |  valid_loss: 1.030\n",
      "[epoch: 5, i:  4249]  train_loss: 1.019  |  valid_loss: 1.081\n",
      "[epoch: 5, i:  4374]  train_loss: 0.979  |  valid_loss: 1.040\n",
      "[epoch: 5, i:  4499]  train_loss: 1.052  |  valid_loss: 0.997\n",
      "[epoch: 5, i:  4624]  train_loss: 0.978  |  valid_loss: 1.055\n",
      "[epoch: 5, i:  4749]  train_loss: 1.004  |  valid_loss: 1.035\n",
      "[epoch: 5, i:  4874]  train_loss: 1.013  |  valid_loss: 0.994\n",
      "[epoch: 5, i:  4999]  train_loss: 0.985  |  valid_loss: 0.923\n",
      "[epoch: 5, i:  5124]  train_loss: 0.983  |  valid_loss: 0.946\n",
      "[epoch: 5, i:  5249]  train_loss: 0.987  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  5374]  train_loss: 0.995  |  valid_loss: 0.730\n",
      "[epoch: 5, i:  5499]  train_loss: 1.039  |  valid_loss: 0.814\n",
      "[epoch: 5, i:  5624]  train_loss: 1.042  |  valid_loss: 1.030\n",
      "[epoch: 5, i:  5749]  train_loss: 0.941  |  valid_loss: 0.970\n",
      "[epoch: 5, i:  5874]  train_loss: 1.026  |  valid_loss: 0.864\n",
      "[epoch: 5, i:  5999]  train_loss: 1.020  |  valid_loss: 0.932\n",
      "[epoch: 5, i:  6124]  train_loss: 0.899  |  valid_loss: 0.813\n",
      "[epoch: 5, i:  6249]  train_loss: 1.023  |  valid_loss: 1.033\n",
      "[epoch: 5, i:  6374]  train_loss: 0.978  |  valid_loss: 0.945\n",
      "[epoch: 5, i:  6499]  train_loss: 0.969  |  valid_loss: 0.878\n",
      "[epoch: 5, i:  6624]  train_loss: 0.986  |  valid_loss: 0.863\n",
      "[epoch: 5, i:  6749]  train_loss: 0.948  |  valid_loss: 0.995\n",
      "[epoch: 5, i:  6874]  train_loss: 1.034  |  valid_loss: 0.949\n",
      "[epoch: 5, i:  6999]  train_loss: 0.980  |  valid_loss: 1.106\n",
      "[epoch: 5, i:  7124]  train_loss: 1.033  |  valid_loss: 1.028\n",
      "[epoch: 5, i:  7249]  train_loss: 1.028  |  valid_loss: 0.784\n",
      "[epoch: 5, i:  7374]  train_loss: 0.957  |  valid_loss: 1.204\n",
      "[epoch: 5, i:  7499]  train_loss: 0.951  |  valid_loss: 1.039\n",
      "[epoch: 5, i:  7624]  train_loss: 0.993  |  valid_loss: 1.103\n",
      "[epoch: 5, i:  7749]  train_loss: 0.958  |  valid_loss: 0.946\n",
      "[epoch: 5, i:  7874]  train_loss: 1.057  |  valid_loss: 0.943\n",
      "[epoch: 5, i:  7999]  train_loss: 0.980  |  valid_loss: 0.894\n",
      "[epoch: 5, i:  8124]  train_loss: 0.900  |  valid_loss: 0.978\n",
      "[epoch: 5, i:  8249]  train_loss: 0.919  |  valid_loss: 0.961\n",
      "[epoch: 5, i:  8374]  train_loss: 0.954  |  valid_loss: 0.993\n",
      "[epoch: 5, i:  8499]  train_loss: 0.972  |  valid_loss: 1.024\n",
      "[epoch: 5, i:  8624]  train_loss: 0.960  |  valid_loss: 1.004\n",
      "[epoch: 5, i:  8749]  train_loss: 1.034  |  valid_loss: 1.031\n",
      "[epoch: 5, i:  8874]  train_loss: 0.997  |  valid_loss: 0.998\n",
      "[epoch: 5, i:  8999]  train_loss: 0.997  |  valid_loss: 0.833\n",
      "[epoch: 5, i:  9124]  train_loss: 0.969  |  valid_loss: 0.931\n",
      "[epoch: 5, i:  9249]  train_loss: 0.950  |  valid_loss: 0.764\n",
      "[epoch: 5, i:  9374]  train_loss: 1.056  |  valid_loss: 0.992\n",
      "[epoch: 5, i:  9499]  train_loss: 1.053  |  valid_loss: 0.932\n",
      "[epoch: 5, i:  9624]  train_loss: 0.943  |  valid_loss: 1.051\n",
      "[epoch: 5, i:  9749]  train_loss: 0.982  |  valid_loss: 1.055\n",
      "[epoch: 5, i:  9874]  train_loss: 0.958  |  valid_loss: 1.041\n",
      "[epoch: 5, i:  9999]  train_loss: 0.961  |  valid_loss: 0.972\n",
      "[epoch: 5, i: 10124]  train_loss: 1.086  |  valid_loss: 0.899\n",
      "[epoch: 5, i: 10249]  train_loss: 0.930  |  valid_loss: 1.041\n",
      "[epoch: 5, i: 10374]  train_loss: 0.974  |  valid_loss: 0.899\n",
      "[epoch: 5, i: 10499]  train_loss: 0.940  |  valid_loss: 1.036\n",
      "[epoch: 5, i: 10624]  train_loss: 0.990  |  valid_loss: 1.108\n",
      "[epoch: 5, i: 10749]  train_loss: 0.841  |  valid_loss: 0.897\n",
      "[epoch: 5, i: 10874]  train_loss: 0.944  |  valid_loss: 1.041\n",
      "[epoch: 5, i: 10999]  train_loss: 0.930  |  valid_loss: 1.010\n",
      "[epoch: 5, i: 11124]  train_loss: 1.017  |  valid_loss: 1.008\n",
      "[epoch: 5, i: 11249]  train_loss: 1.085  |  valid_loss: 1.000\n",
      "[epoch: 5, i: 11374]  train_loss: 0.916  |  valid_loss: 0.876\n",
      "[epoch: 5, i: 11499]  train_loss: 1.036  |  valid_loss: 0.859\n",
      "[epoch: 5, i: 11624]  train_loss: 0.995  |  valid_loss: 0.872\n",
      "[epoch: 5, i: 11749]  train_loss: 1.018  |  valid_loss: 0.914\n",
      "[epoch: 5, i: 11874]  train_loss: 0.965  |  valid_loss: 0.862\n",
      "[epoch: 5, i: 11999]  train_loss: 0.958  |  valid_loss: 0.837\n",
      "[epoch: 5, i: 12124]  train_loss: 1.004  |  valid_loss: 0.833\n",
      "[epoch: 5, i: 12249]  train_loss: 0.993  |  valid_loss: 1.136\n",
      "[epoch: 5, i: 12374]  train_loss: 1.057  |  valid_loss: 1.068\n",
      "[epoch: 5, i: 12499]  train_loss: 1.013  |  valid_loss: 0.980\n",
      "--> [End of epoch 5] train_accuracy: 65.07%  |  valid_accuracy: 66.21%\n",
      "--> [Start of epoch 6]  lr: 0.000524\n",
      "[epoch: 6, i:   124]  train_loss: 0.958  |  valid_loss: 0.851\n",
      "[epoch: 6, i:   249]  train_loss: 1.062  |  valid_loss: 0.853\n",
      "[epoch: 6, i:   374]  train_loss: 0.986  |  valid_loss: 0.978\n",
      "[epoch: 6, i:   499]  train_loss: 0.959  |  valid_loss: 0.906\n",
      "[epoch: 6, i:   624]  train_loss: 0.946  |  valid_loss: 1.059\n",
      "[epoch: 6, i:   749]  train_loss: 0.935  |  valid_loss: 0.697\n",
      "[epoch: 6, i:   874]  train_loss: 0.980  |  valid_loss: 0.973\n",
      "[epoch: 6, i:   999]  train_loss: 0.936  |  valid_loss: 1.043\n",
      "[epoch: 6, i:  1124]  train_loss: 0.929  |  valid_loss: 1.010\n",
      "[epoch: 6, i:  1249]  train_loss: 0.957  |  valid_loss: 0.877\n",
      "[epoch: 6, i:  1374]  train_loss: 0.894  |  valid_loss: 0.893\n",
      "[epoch: 6, i:  1499]  train_loss: 0.975  |  valid_loss: 0.885\n",
      "[epoch: 6, i:  1624]  train_loss: 0.926  |  valid_loss: 0.806\n",
      "[epoch: 6, i:  1749]  train_loss: 0.990  |  valid_loss: 1.214\n",
      "[epoch: 6, i:  1874]  train_loss: 1.013  |  valid_loss: 0.834\n",
      "[epoch: 6, i:  1999]  train_loss: 0.970  |  valid_loss: 1.141\n",
      "[epoch: 6, i:  2124]  train_loss: 0.952  |  valid_loss: 0.845\n",
      "[epoch: 6, i:  2249]  train_loss: 0.997  |  valid_loss: 0.977\n",
      "[epoch: 6, i:  2374]  train_loss: 0.984  |  valid_loss: 0.879\n",
      "[epoch: 6, i:  2499]  train_loss: 0.894  |  valid_loss: 1.071\n",
      "[epoch: 6, i:  2624]  train_loss: 0.899  |  valid_loss: 1.053\n",
      "[epoch: 6, i:  2749]  train_loss: 0.955  |  valid_loss: 1.063\n",
      "[epoch: 6, i:  2874]  train_loss: 0.898  |  valid_loss: 1.287\n",
      "[epoch: 6, i:  2999]  train_loss: 0.971  |  valid_loss: 1.051\n",
      "[epoch: 6, i:  3124]  train_loss: 0.956  |  valid_loss: 0.982\n",
      "[epoch: 6, i:  3249]  train_loss: 0.955  |  valid_loss: 1.173\n",
      "[epoch: 6, i:  3374]  train_loss: 0.976  |  valid_loss: 0.970\n",
      "[epoch: 6, i:  3499]  train_loss: 0.941  |  valid_loss: 0.817\n",
      "[epoch: 6, i:  3624]  train_loss: 0.887  |  valid_loss: 0.991\n",
      "[epoch: 6, i:  3749]  train_loss: 0.975  |  valid_loss: 0.821\n",
      "[epoch: 6, i:  3874]  train_loss: 0.875  |  valid_loss: 0.940\n",
      "[epoch: 6, i:  3999]  train_loss: 1.006  |  valid_loss: 0.752\n",
      "[epoch: 6, i:  4124]  train_loss: 0.928  |  valid_loss: 0.987\n",
      "[epoch: 6, i:  4249]  train_loss: 0.974  |  valid_loss: 1.018\n",
      "[epoch: 6, i:  4374]  train_loss: 0.976  |  valid_loss: 1.025\n",
      "[epoch: 6, i:  4499]  train_loss: 0.954  |  valid_loss: 0.889\n",
      "[epoch: 6, i:  4624]  train_loss: 1.024  |  valid_loss: 1.060\n",
      "[epoch: 6, i:  4749]  train_loss: 1.025  |  valid_loss: 0.961\n",
      "[epoch: 6, i:  4874]  train_loss: 0.892  |  valid_loss: 0.826\n",
      "[epoch: 6, i:  4999]  train_loss: 0.968  |  valid_loss: 0.931\n",
      "[epoch: 6, i:  5124]  train_loss: 0.933  |  valid_loss: 0.944\n",
      "[epoch: 6, i:  5249]  train_loss: 0.973  |  valid_loss: 1.041\n",
      "[epoch: 6, i:  5374]  train_loss: 0.938  |  valid_loss: 0.719\n",
      "[epoch: 6, i:  5499]  train_loss: 0.947  |  valid_loss: 0.720\n",
      "[epoch: 6, i:  5624]  train_loss: 0.946  |  valid_loss: 0.999\n",
      "[epoch: 6, i:  5749]  train_loss: 0.936  |  valid_loss: 0.957\n",
      "[epoch: 6, i:  5874]  train_loss: 0.913  |  valid_loss: 0.910\n",
      "[epoch: 6, i:  5999]  train_loss: 0.917  |  valid_loss: 0.974\n",
      "[epoch: 6, i:  6124]  train_loss: 0.959  |  valid_loss: 0.887\n",
      "[epoch: 6, i:  6249]  train_loss: 0.876  |  valid_loss: 0.946\n",
      "[epoch: 6, i:  6374]  train_loss: 0.957  |  valid_loss: 0.921\n",
      "[epoch: 6, i:  6499]  train_loss: 0.966  |  valid_loss: 0.875\n",
      "[epoch: 6, i:  6624]  train_loss: 0.887  |  valid_loss: 0.759\n",
      "[epoch: 6, i:  6749]  train_loss: 0.856  |  valid_loss: 0.981\n",
      "[epoch: 6, i:  6874]  train_loss: 0.927  |  valid_loss: 0.883\n",
      "[epoch: 6, i:  6999]  train_loss: 0.891  |  valid_loss: 1.104\n",
      "[epoch: 6, i:  7124]  train_loss: 1.011  |  valid_loss: 0.994\n",
      "[epoch: 6, i:  7249]  train_loss: 0.922  |  valid_loss: 0.760\n",
      "[epoch: 6, i:  7374]  train_loss: 0.857  |  valid_loss: 1.153\n",
      "[epoch: 6, i:  7499]  train_loss: 0.982  |  valid_loss: 0.959\n",
      "[epoch: 6, i:  7624]  train_loss: 0.923  |  valid_loss: 1.051\n",
      "[epoch: 6, i:  7749]  train_loss: 0.927  |  valid_loss: 0.933\n",
      "[epoch: 6, i:  7874]  train_loss: 0.977  |  valid_loss: 0.922\n",
      "[epoch: 6, i:  7999]  train_loss: 0.954  |  valid_loss: 0.883\n",
      "[epoch: 6, i:  8124]  train_loss: 0.936  |  valid_loss: 0.980\n",
      "[epoch: 6, i:  8249]  train_loss: 0.968  |  valid_loss: 0.904\n",
      "[epoch: 6, i:  8374]  train_loss: 0.989  |  valid_loss: 0.896\n",
      "[epoch: 6, i:  8499]  train_loss: 0.953  |  valid_loss: 1.045\n",
      "[epoch: 6, i:  8624]  train_loss: 0.965  |  valid_loss: 0.966\n",
      "[epoch: 6, i:  8749]  train_loss: 0.996  |  valid_loss: 1.034\n",
      "[epoch: 6, i:  8874]  train_loss: 0.945  |  valid_loss: 0.929\n",
      "[epoch: 6, i:  8999]  train_loss: 0.906  |  valid_loss: 0.714\n",
      "[epoch: 6, i:  9124]  train_loss: 0.932  |  valid_loss: 0.876\n",
      "[epoch: 6, i:  9249]  train_loss: 0.938  |  valid_loss: 0.752\n",
      "[epoch: 6, i:  9374]  train_loss: 0.919  |  valid_loss: 0.981\n",
      "[epoch: 6, i:  9499]  train_loss: 0.945  |  valid_loss: 0.900\n",
      "[epoch: 6, i:  9624]  train_loss: 0.973  |  valid_loss: 0.937\n",
      "[epoch: 6, i:  9749]  train_loss: 0.919  |  valid_loss: 1.012\n",
      "[epoch: 6, i:  9874]  train_loss: 0.954  |  valid_loss: 1.002\n",
      "[epoch: 6, i:  9999]  train_loss: 0.858  |  valid_loss: 0.940\n",
      "[epoch: 6, i: 10124]  train_loss: 0.984  |  valid_loss: 0.914\n",
      "[epoch: 6, i: 10249]  train_loss: 1.041  |  valid_loss: 1.072\n",
      "[epoch: 6, i: 10374]  train_loss: 0.997  |  valid_loss: 0.877\n",
      "[epoch: 6, i: 10499]  train_loss: 1.019  |  valid_loss: 1.058\n",
      "[epoch: 6, i: 10624]  train_loss: 0.972  |  valid_loss: 1.100\n",
      "[epoch: 6, i: 10749]  train_loss: 0.909  |  valid_loss: 0.925\n",
      "[epoch: 6, i: 10874]  train_loss: 0.920  |  valid_loss: 1.027\n",
      "[epoch: 6, i: 10999]  train_loss: 1.071  |  valid_loss: 1.061\n",
      "[epoch: 6, i: 11124]  train_loss: 0.927  |  valid_loss: 0.895\n",
      "[epoch: 6, i: 11249]  train_loss: 0.964  |  valid_loss: 0.983\n",
      "[epoch: 6, i: 11374]  train_loss: 0.865  |  valid_loss: 0.918\n",
      "[epoch: 6, i: 11499]  train_loss: 0.984  |  valid_loss: 0.818\n",
      "[epoch: 6, i: 11624]  train_loss: 0.933  |  valid_loss: 0.899\n",
      "[epoch: 6, i: 11749]  train_loss: 0.941  |  valid_loss: 0.880\n",
      "[epoch: 6, i: 11874]  train_loss: 0.922  |  valid_loss: 0.877\n",
      "[epoch: 6, i: 11999]  train_loss: 0.952  |  valid_loss: 0.822\n",
      "[epoch: 6, i: 12124]  train_loss: 0.917  |  valid_loss: 0.888\n",
      "[epoch: 6, i: 12249]  train_loss: 0.904  |  valid_loss: 1.186\n",
      "[epoch: 6, i: 12374]  train_loss: 1.028  |  valid_loss: 0.990\n",
      "[epoch: 6, i: 12499]  train_loss: 0.965  |  valid_loss: 0.944\n",
      "--> [End of epoch 6] train_accuracy: 66.08%  |  valid_accuracy: 66.49%\n",
      "--> [Start of epoch 7]  lr: 0.000419\n",
      "[epoch: 7, i:   124]  train_loss: 0.964  |  valid_loss: 0.843\n",
      "[epoch: 7, i:   249]  train_loss: 0.912  |  valid_loss: 0.839\n",
      "[epoch: 7, i:   374]  train_loss: 1.021  |  valid_loss: 1.046\n",
      "[epoch: 7, i:   499]  train_loss: 1.018  |  valid_loss: 0.897\n",
      "[epoch: 7, i:   624]  train_loss: 0.970  |  valid_loss: 0.970\n",
      "[epoch: 7, i:   749]  train_loss: 0.930  |  valid_loss: 0.664\n",
      "[epoch: 7, i:   874]  train_loss: 0.955  |  valid_loss: 0.919\n",
      "[epoch: 7, i:   999]  train_loss: 0.904  |  valid_loss: 1.048\n",
      "[epoch: 7, i:  1124]  train_loss: 0.887  |  valid_loss: 0.970\n",
      "[epoch: 7, i:  1249]  train_loss: 0.984  |  valid_loss: 0.873\n",
      "[epoch: 7, i:  1374]  train_loss: 0.944  |  valid_loss: 0.938\n",
      "[epoch: 7, i:  1499]  train_loss: 0.950  |  valid_loss: 0.877\n",
      "[epoch: 7, i:  1624]  train_loss: 0.880  |  valid_loss: 0.783\n",
      "[epoch: 7, i:  1749]  train_loss: 0.820  |  valid_loss: 1.258\n",
      "[epoch: 7, i:  1874]  train_loss: 0.802  |  valid_loss: 0.827\n",
      "[epoch: 7, i:  1999]  train_loss: 0.949  |  valid_loss: 1.088\n",
      "[epoch: 7, i:  2124]  train_loss: 0.956  |  valid_loss: 0.835\n",
      "[epoch: 7, i:  2249]  train_loss: 0.943  |  valid_loss: 0.950\n",
      "[epoch: 7, i:  2374]  train_loss: 0.872  |  valid_loss: 0.792\n",
      "[epoch: 7, i:  2499]  train_loss: 0.976  |  valid_loss: 1.033\n",
      "[epoch: 7, i:  2624]  train_loss: 0.925  |  valid_loss: 1.022\n",
      "[epoch: 7, i:  2749]  train_loss: 0.838  |  valid_loss: 1.021\n",
      "[epoch: 7, i:  2874]  train_loss: 0.920  |  valid_loss: 1.194\n",
      "[epoch: 7, i:  2999]  train_loss: 0.964  |  valid_loss: 0.963\n",
      "[epoch: 7, i:  3124]  train_loss: 0.937  |  valid_loss: 0.967\n",
      "[epoch: 7, i:  3249]  train_loss: 0.948  |  valid_loss: 1.145\n",
      "[epoch: 7, i:  3374]  train_loss: 0.922  |  valid_loss: 0.872\n",
      "[epoch: 7, i:  3499]  train_loss: 0.967  |  valid_loss: 0.781\n",
      "[epoch: 7, i:  3624]  train_loss: 0.877  |  valid_loss: 0.928\n",
      "[epoch: 7, i:  3749]  train_loss: 0.934  |  valid_loss: 0.746\n",
      "[epoch: 7, i:  3874]  train_loss: 0.931  |  valid_loss: 0.896\n",
      "[epoch: 7, i:  3999]  train_loss: 0.917  |  valid_loss: 0.714\n",
      "[epoch: 7, i:  4124]  train_loss: 0.877  |  valid_loss: 0.929\n",
      "[epoch: 7, i:  4249]  train_loss: 0.911  |  valid_loss: 1.065\n",
      "[epoch: 7, i:  4374]  train_loss: 0.943  |  valid_loss: 0.990\n",
      "[epoch: 7, i:  4499]  train_loss: 0.832  |  valid_loss: 0.830\n",
      "[epoch: 7, i:  4624]  train_loss: 1.012  |  valid_loss: 1.031\n",
      "[epoch: 7, i:  4749]  train_loss: 0.957  |  valid_loss: 0.959\n",
      "[epoch: 7, i:  4874]  train_loss: 0.945  |  valid_loss: 0.876\n",
      "[epoch: 7, i:  4999]  train_loss: 0.946  |  valid_loss: 0.852\n",
      "[epoch: 7, i:  5124]  train_loss: 0.850  |  valid_loss: 0.857\n",
      "[epoch: 7, i:  5249]  train_loss: 0.906  |  valid_loss: 0.981\n",
      "[epoch: 7, i:  5374]  train_loss: 0.845  |  valid_loss: 0.714\n",
      "[epoch: 7, i:  5499]  train_loss: 0.957  |  valid_loss: 0.794\n",
      "[epoch: 7, i:  5624]  train_loss: 0.914  |  valid_loss: 1.000\n",
      "[epoch: 7, i:  5749]  train_loss: 0.927  |  valid_loss: 0.891\n",
      "[epoch: 7, i:  5874]  train_loss: 0.950  |  valid_loss: 0.836\n",
      "[epoch: 7, i:  5999]  train_loss: 0.962  |  valid_loss: 0.924\n",
      "[epoch: 7, i:  6124]  train_loss: 0.881  |  valid_loss: 0.841\n",
      "[epoch: 7, i:  6249]  train_loss: 0.904  |  valid_loss: 0.996\n",
      "[epoch: 7, i:  6374]  train_loss: 0.976  |  valid_loss: 0.843\n",
      "[epoch: 7, i:  6499]  train_loss: 0.903  |  valid_loss: 0.863\n",
      "[epoch: 7, i:  6624]  train_loss: 0.914  |  valid_loss: 0.730\n",
      "[epoch: 7, i:  6749]  train_loss: 0.961  |  valid_loss: 0.964\n",
      "[epoch: 7, i:  6874]  train_loss: 0.877  |  valid_loss: 0.808\n",
      "[epoch: 7, i:  6999]  train_loss: 0.880  |  valid_loss: 1.025\n",
      "[epoch: 7, i:  7124]  train_loss: 0.909  |  valid_loss: 1.081\n",
      "[epoch: 7, i:  7249]  train_loss: 0.891  |  valid_loss: 0.711\n",
      "[epoch: 7, i:  7374]  train_loss: 0.858  |  valid_loss: 1.132\n",
      "[epoch: 7, i:  7499]  train_loss: 0.971  |  valid_loss: 1.060\n",
      "[epoch: 7, i:  7624]  train_loss: 0.901  |  valid_loss: 0.998\n",
      "[epoch: 7, i:  7749]  train_loss: 0.938  |  valid_loss: 0.816\n",
      "[epoch: 7, i:  7874]  train_loss: 0.910  |  valid_loss: 0.908\n",
      "[epoch: 7, i:  7999]  train_loss: 0.921  |  valid_loss: 0.860\n",
      "[epoch: 7, i:  8124]  train_loss: 0.933  |  valid_loss: 1.023\n",
      "[epoch: 7, i:  8249]  train_loss: 0.894  |  valid_loss: 0.966\n",
      "[epoch: 7, i:  8374]  train_loss: 0.859  |  valid_loss: 0.971\n",
      "[epoch: 7, i:  8499]  train_loss: 0.910  |  valid_loss: 0.992\n",
      "[epoch: 7, i:  8624]  train_loss: 0.960  |  valid_loss: 0.907\n",
      "[epoch: 7, i:  8749]  train_loss: 0.959  |  valid_loss: 1.074\n",
      "[epoch: 7, i:  8874]  train_loss: 0.936  |  valid_loss: 0.954\n",
      "[epoch: 7, i:  8999]  train_loss: 0.976  |  valid_loss: 0.749\n",
      "[epoch: 7, i:  9124]  train_loss: 0.964  |  valid_loss: 0.811\n",
      "[epoch: 7, i:  9249]  train_loss: 0.947  |  valid_loss: 0.730\n",
      "[epoch: 7, i:  9374]  train_loss: 0.905  |  valid_loss: 0.993\n",
      "[epoch: 7, i:  9499]  train_loss: 1.022  |  valid_loss: 0.818\n",
      "[epoch: 7, i:  9624]  train_loss: 0.968  |  valid_loss: 0.990\n",
      "[epoch: 7, i:  9749]  train_loss: 0.847  |  valid_loss: 0.982\n",
      "[epoch: 7, i:  9874]  train_loss: 0.929  |  valid_loss: 1.021\n",
      "[epoch: 7, i:  9999]  train_loss: 0.907  |  valid_loss: 0.916\n",
      "[epoch: 7, i: 10124]  train_loss: 0.835  |  valid_loss: 0.809\n",
      "[epoch: 7, i: 10249]  train_loss: 0.993  |  valid_loss: 0.934\n",
      "[epoch: 7, i: 10374]  train_loss: 0.920  |  valid_loss: 0.813\n",
      "[epoch: 7, i: 10499]  train_loss: 0.897  |  valid_loss: 1.059\n",
      "[epoch: 7, i: 10624]  train_loss: 0.910  |  valid_loss: 0.990\n",
      "[epoch: 7, i: 10749]  train_loss: 0.935  |  valid_loss: 0.899\n",
      "[epoch: 7, i: 10874]  train_loss: 0.861  |  valid_loss: 1.054\n",
      "[epoch: 7, i: 10999]  train_loss: 0.891  |  valid_loss: 0.981\n",
      "[epoch: 7, i: 11124]  train_loss: 0.886  |  valid_loss: 0.884\n",
      "[epoch: 7, i: 11249]  train_loss: 0.869  |  valid_loss: 0.911\n",
      "[epoch: 7, i: 11374]  train_loss: 0.862  |  valid_loss: 0.926\n",
      "[epoch: 7, i: 11499]  train_loss: 0.846  |  valid_loss: 0.738\n",
      "[epoch: 7, i: 11624]  train_loss: 0.989  |  valid_loss: 0.823\n",
      "[epoch: 7, i: 11749]  train_loss: 0.978  |  valid_loss: 0.897\n",
      "[epoch: 7, i: 11874]  train_loss: 0.927  |  valid_loss: 0.861\n",
      "[epoch: 7, i: 11999]  train_loss: 0.919  |  valid_loss: 0.759\n",
      "[epoch: 7, i: 12124]  train_loss: 0.942  |  valid_loss: 0.810\n",
      "[epoch: 7, i: 12249]  train_loss: 0.946  |  valid_loss: 1.158\n",
      "[epoch: 7, i: 12374]  train_loss: 0.894  |  valid_loss: 0.977\n",
      "[epoch: 7, i: 12499]  train_loss: 0.944  |  valid_loss: 0.914\n",
      "--> [End of epoch 7] train_accuracy: 67.23%  |  valid_accuracy: 67.77%\n",
      "--> [Start of epoch 8]  lr: 0.000336\n",
      "[epoch: 8, i:   124]  train_loss: 0.839  |  valid_loss: 0.790\n",
      "[epoch: 8, i:   249]  train_loss: 0.880  |  valid_loss: 0.834\n",
      "[epoch: 8, i:   374]  train_loss: 0.881  |  valid_loss: 0.963\n",
      "[epoch: 8, i:   499]  train_loss: 0.977  |  valid_loss: 0.899\n",
      "[epoch: 8, i:   624]  train_loss: 0.832  |  valid_loss: 0.970\n",
      "[epoch: 8, i:   749]  train_loss: 0.924  |  valid_loss: 0.693\n",
      "[epoch: 8, i:   874]  train_loss: 0.890  |  valid_loss: 0.940\n",
      "[epoch: 8, i:   999]  train_loss: 0.870  |  valid_loss: 1.027\n",
      "[epoch: 8, i:  1124]  train_loss: 0.919  |  valid_loss: 0.993\n",
      "[epoch: 8, i:  1249]  train_loss: 0.967  |  valid_loss: 0.827\n",
      "[epoch: 8, i:  1374]  train_loss: 0.964  |  valid_loss: 0.874\n",
      "[epoch: 8, i:  1499]  train_loss: 0.883  |  valid_loss: 0.875\n",
      "[epoch: 8, i:  1624]  train_loss: 0.849  |  valid_loss: 0.812\n",
      "[epoch: 8, i:  1749]  train_loss: 0.868  |  valid_loss: 1.156\n",
      "[epoch: 8, i:  1874]  train_loss: 0.910  |  valid_loss: 0.857\n",
      "[epoch: 8, i:  1999]  train_loss: 0.934  |  valid_loss: 1.121\n",
      "[epoch: 8, i:  2124]  train_loss: 0.968  |  valid_loss: 0.794\n",
      "[epoch: 8, i:  2249]  train_loss: 1.007  |  valid_loss: 0.887\n",
      "[epoch: 8, i:  2374]  train_loss: 0.843  |  valid_loss: 0.800\n",
      "[epoch: 8, i:  2499]  train_loss: 0.844  |  valid_loss: 1.036\n",
      "[epoch: 8, i:  2624]  train_loss: 0.938  |  valid_loss: 1.003\n",
      "[epoch: 8, i:  2749]  train_loss: 0.910  |  valid_loss: 1.016\n",
      "[epoch: 8, i:  2874]  train_loss: 0.971  |  valid_loss: 1.164\n",
      "[epoch: 8, i:  2999]  train_loss: 0.880  |  valid_loss: 0.914\n",
      "[epoch: 8, i:  3124]  train_loss: 0.892  |  valid_loss: 0.909\n",
      "[epoch: 8, i:  3249]  train_loss: 0.880  |  valid_loss: 1.128\n",
      "[epoch: 8, i:  3374]  train_loss: 0.898  |  valid_loss: 0.848\n",
      "[epoch: 8, i:  3499]  train_loss: 0.895  |  valid_loss: 0.784\n",
      "[epoch: 8, i:  3624]  train_loss: 0.930  |  valid_loss: 0.897\n",
      "[epoch: 8, i:  3749]  train_loss: 1.017  |  valid_loss: 0.743\n",
      "[epoch: 8, i:  3874]  train_loss: 0.975  |  valid_loss: 0.882\n",
      "[epoch: 8, i:  3999]  train_loss: 0.957  |  valid_loss: 0.694\n",
      "[epoch: 8, i:  4124]  train_loss: 0.939  |  valid_loss: 0.914\n",
      "[epoch: 8, i:  4249]  train_loss: 0.946  |  valid_loss: 0.982\n",
      "[epoch: 8, i:  4374]  train_loss: 0.905  |  valid_loss: 0.977\n",
      "[epoch: 8, i:  4499]  train_loss: 0.924  |  valid_loss: 0.872\n",
      "[epoch: 8, i:  4624]  train_loss: 0.870  |  valid_loss: 1.035\n",
      "[epoch: 8, i:  4749]  train_loss: 1.032  |  valid_loss: 0.928\n",
      "[epoch: 8, i:  4874]  train_loss: 0.872  |  valid_loss: 0.815\n",
      "[epoch: 8, i:  4999]  train_loss: 0.819  |  valid_loss: 0.786\n",
      "[epoch: 8, i:  5124]  train_loss: 0.864  |  valid_loss: 0.857\n",
      "[epoch: 8, i:  5249]  train_loss: 0.935  |  valid_loss: 0.896\n",
      "[epoch: 8, i:  5374]  train_loss: 0.904  |  valid_loss: 0.714\n",
      "[epoch: 8, i:  5499]  train_loss: 0.920  |  valid_loss: 0.756\n",
      "[epoch: 8, i:  5624]  train_loss: 0.918  |  valid_loss: 0.901\n",
      "[epoch: 8, i:  5749]  train_loss: 0.865  |  valid_loss: 0.898\n",
      "[epoch: 8, i:  5874]  train_loss: 0.859  |  valid_loss: 0.792\n",
      "[epoch: 8, i:  5999]  train_loss: 0.906  |  valid_loss: 0.883\n",
      "[epoch: 8, i:  6124]  train_loss: 0.955  |  valid_loss: 0.783\n",
      "[epoch: 8, i:  6249]  train_loss: 0.893  |  valid_loss: 1.014\n",
      "[epoch: 8, i:  6374]  train_loss: 0.878  |  valid_loss: 0.907\n",
      "[epoch: 8, i:  6499]  train_loss: 0.963  |  valid_loss: 0.840\n",
      "[epoch: 8, i:  6624]  train_loss: 0.826  |  valid_loss: 0.758\n",
      "[epoch: 8, i:  6749]  train_loss: 0.910  |  valid_loss: 0.915\n",
      "[epoch: 8, i:  6874]  train_loss: 0.878  |  valid_loss: 0.847\n",
      "[epoch: 8, i:  6999]  train_loss: 0.865  |  valid_loss: 1.114\n",
      "[epoch: 8, i:  7124]  train_loss: 0.872  |  valid_loss: 0.966\n",
      "[epoch: 8, i:  7249]  train_loss: 0.928  |  valid_loss: 0.739\n",
      "[epoch: 8, i:  7374]  train_loss: 0.921  |  valid_loss: 1.043\n",
      "[epoch: 8, i:  7499]  train_loss: 0.873  |  valid_loss: 0.971\n",
      "[epoch: 8, i:  7624]  train_loss: 0.860  |  valid_loss: 0.987\n",
      "[epoch: 8, i:  7749]  train_loss: 0.849  |  valid_loss: 0.878\n",
      "[epoch: 8, i:  7874]  train_loss: 0.876  |  valid_loss: 0.862\n",
      "[epoch: 8, i:  7999]  train_loss: 0.957  |  valid_loss: 0.812\n",
      "[epoch: 8, i:  8124]  train_loss: 0.884  |  valid_loss: 0.943\n",
      "[epoch: 8, i:  8249]  train_loss: 0.820  |  valid_loss: 0.964\n",
      "[epoch: 8, i:  8374]  train_loss: 0.943  |  valid_loss: 0.911\n",
      "[epoch: 8, i:  8499]  train_loss: 0.951  |  valid_loss: 0.954\n",
      "[epoch: 8, i:  8624]  train_loss: 0.966  |  valid_loss: 0.881\n",
      "[epoch: 8, i:  8749]  train_loss: 0.896  |  valid_loss: 0.929\n",
      "[epoch: 8, i:  8874]  train_loss: 0.988  |  valid_loss: 0.897\n",
      "[epoch: 8, i:  8999]  train_loss: 0.913  |  valid_loss: 0.745\n",
      "[epoch: 8, i:  9124]  train_loss: 0.909  |  valid_loss: 0.841\n",
      "[epoch: 8, i:  9249]  train_loss: 0.806  |  valid_loss: 0.698\n",
      "[epoch: 8, i:  9374]  train_loss: 0.814  |  valid_loss: 0.980\n",
      "[epoch: 8, i:  9499]  train_loss: 0.927  |  valid_loss: 0.916\n",
      "[epoch: 8, i:  9624]  train_loss: 0.934  |  valid_loss: 0.913\n",
      "[epoch: 8, i:  9749]  train_loss: 0.937  |  valid_loss: 0.930\n",
      "[epoch: 8, i:  9874]  train_loss: 0.929  |  valid_loss: 0.960\n",
      "[epoch: 8, i:  9999]  train_loss: 0.920  |  valid_loss: 0.913\n",
      "[epoch: 8, i: 10124]  train_loss: 0.874  |  valid_loss: 0.833\n",
      "[epoch: 8, i: 10249]  train_loss: 0.845  |  valid_loss: 0.931\n",
      "[epoch: 8, i: 10374]  train_loss: 0.875  |  valid_loss: 0.822\n",
      "[epoch: 8, i: 10499]  train_loss: 0.939  |  valid_loss: 0.994\n",
      "[epoch: 8, i: 10624]  train_loss: 0.946  |  valid_loss: 1.023\n",
      "[epoch: 8, i: 10749]  train_loss: 0.877  |  valid_loss: 0.896\n",
      "[epoch: 8, i: 10874]  train_loss: 0.876  |  valid_loss: 0.997\n",
      "[epoch: 8, i: 10999]  train_loss: 0.815  |  valid_loss: 1.010\n",
      "[epoch: 8, i: 11124]  train_loss: 0.867  |  valid_loss: 0.905\n",
      "[epoch: 8, i: 11249]  train_loss: 0.959  |  valid_loss: 0.896\n",
      "[epoch: 8, i: 11374]  train_loss: 0.994  |  valid_loss: 0.789\n",
      "[epoch: 8, i: 11499]  train_loss: 0.866  |  valid_loss: 0.672\n",
      "[epoch: 8, i: 11624]  train_loss: 0.903  |  valid_loss: 0.821\n",
      "[epoch: 8, i: 11749]  train_loss: 0.867  |  valid_loss: 0.833\n",
      "[epoch: 8, i: 11874]  train_loss: 0.894  |  valid_loss: 0.822\n",
      "[epoch: 8, i: 11999]  train_loss: 0.855  |  valid_loss: 0.769\n",
      "[epoch: 8, i: 12124]  train_loss: 0.944  |  valid_loss: 0.741\n",
      "[epoch: 8, i: 12249]  train_loss: 0.931  |  valid_loss: 1.140\n",
      "[epoch: 8, i: 12374]  train_loss: 0.861  |  valid_loss: 0.917\n",
      "[epoch: 8, i: 12499]  train_loss: 0.909  |  valid_loss: 0.874\n",
      "--> [End of epoch 8] train_accuracy: 68.07%  |  valid_accuracy: 68.79%\n",
      "--> [Start of epoch 9]  lr: 0.000268\n",
      "[epoch: 9, i:   124]  train_loss: 0.910  |  valid_loss: 0.744\n",
      "[epoch: 9, i:   249]  train_loss: 0.828  |  valid_loss: 0.808\n",
      "[epoch: 9, i:   374]  train_loss: 0.930  |  valid_loss: 0.919\n",
      "[epoch: 9, i:   499]  train_loss: 0.914  |  valid_loss: 0.917\n",
      "[epoch: 9, i:   624]  train_loss: 0.887  |  valid_loss: 1.002\n",
      "[epoch: 9, i:   749]  train_loss: 0.838  |  valid_loss: 0.665\n",
      "[epoch: 9, i:   874]  train_loss: 0.891  |  valid_loss: 0.910\n",
      "[epoch: 9, i:   999]  train_loss: 0.817  |  valid_loss: 1.036\n",
      "[epoch: 9, i:  1124]  train_loss: 0.930  |  valid_loss: 1.002\n",
      "[epoch: 9, i:  1249]  train_loss: 0.862  |  valid_loss: 0.777\n",
      "[epoch: 9, i:  1374]  train_loss: 0.921  |  valid_loss: 0.807\n",
      "[epoch: 9, i:  1499]  train_loss: 0.927  |  valid_loss: 0.819\n",
      "[epoch: 9, i:  1624]  train_loss: 0.901  |  valid_loss: 0.730\n",
      "[epoch: 9, i:  1749]  train_loss: 0.876  |  valid_loss: 1.089\n",
      "[epoch: 9, i:  1874]  train_loss: 0.922  |  valid_loss: 0.781\n",
      "[epoch: 9, i:  1999]  train_loss: 0.843  |  valid_loss: 1.047\n",
      "[epoch: 9, i:  2124]  train_loss: 0.884  |  valid_loss: 0.762\n",
      "[epoch: 9, i:  2249]  train_loss: 0.853  |  valid_loss: 0.906\n",
      "[epoch: 9, i:  2374]  train_loss: 0.886  |  valid_loss: 0.782\n",
      "[epoch: 9, i:  2499]  train_loss: 0.819  |  valid_loss: 1.061\n",
      "[epoch: 9, i:  2624]  train_loss: 0.918  |  valid_loss: 1.028\n",
      "[epoch: 9, i:  2749]  train_loss: 0.868  |  valid_loss: 1.054\n",
      "[epoch: 9, i:  2874]  train_loss: 0.943  |  valid_loss: 1.166\n",
      "[epoch: 9, i:  2999]  train_loss: 0.829  |  valid_loss: 0.919\n",
      "[epoch: 9, i:  3124]  train_loss: 0.910  |  valid_loss: 0.949\n",
      "[epoch: 9, i:  3249]  train_loss: 0.827  |  valid_loss: 1.131\n",
      "[epoch: 9, i:  3374]  train_loss: 0.906  |  valid_loss: 0.934\n",
      "[epoch: 9, i:  3499]  train_loss: 0.900  |  valid_loss: 0.796\n",
      "[epoch: 9, i:  3624]  train_loss: 0.945  |  valid_loss: 0.894\n",
      "[epoch: 9, i:  3749]  train_loss: 0.928  |  valid_loss: 0.767\n",
      "[epoch: 9, i:  3874]  train_loss: 0.881  |  valid_loss: 0.869\n",
      "[epoch: 9, i:  3999]  train_loss: 0.920  |  valid_loss: 0.657\n",
      "[epoch: 9, i:  4124]  train_loss: 0.907  |  valid_loss: 0.947\n",
      "[epoch: 9, i:  4249]  train_loss: 0.894  |  valid_loss: 0.987\n",
      "[epoch: 9, i:  4374]  train_loss: 0.918  |  valid_loss: 0.960\n",
      "[epoch: 9, i:  4499]  train_loss: 0.891  |  valid_loss: 0.849\n",
      "[epoch: 9, i:  4624]  train_loss: 0.835  |  valid_loss: 0.968\n",
      "[epoch: 9, i:  4749]  train_loss: 0.937  |  valid_loss: 0.955\n",
      "[epoch: 9, i:  4874]  train_loss: 0.934  |  valid_loss: 0.765\n",
      "[epoch: 9, i:  4999]  train_loss: 0.883  |  valid_loss: 0.838\n",
      "[epoch: 9, i:  5124]  train_loss: 0.928  |  valid_loss: 0.856\n",
      "[epoch: 9, i:  5249]  train_loss: 0.844  |  valid_loss: 0.945\n",
      "[epoch: 9, i:  5374]  train_loss: 0.957  |  valid_loss: 0.677\n",
      "[epoch: 9, i:  5499]  train_loss: 0.838  |  valid_loss: 0.720\n",
      "[epoch: 9, i:  5624]  train_loss: 0.846  |  valid_loss: 1.002\n",
      "[epoch: 9, i:  5749]  train_loss: 0.861  |  valid_loss: 0.938\n",
      "[epoch: 9, i:  5874]  train_loss: 0.861  |  valid_loss: 0.792\n",
      "[epoch: 9, i:  5999]  train_loss: 0.862  |  valid_loss: 0.949\n",
      "[epoch: 9, i:  6124]  train_loss: 0.907  |  valid_loss: 0.819\n",
      "[epoch: 9, i:  6249]  train_loss: 0.843  |  valid_loss: 0.901\n",
      "[epoch: 9, i:  6374]  train_loss: 0.821  |  valid_loss: 0.766\n",
      "[epoch: 9, i:  6499]  train_loss: 0.855  |  valid_loss: 0.780\n",
      "[epoch: 9, i:  6624]  train_loss: 0.828  |  valid_loss: 0.747\n",
      "[epoch: 9, i:  6749]  train_loss: 0.944  |  valid_loss: 0.852\n",
      "[epoch: 9, i:  6874]  train_loss: 0.825  |  valid_loss: 0.841\n",
      "[epoch: 9, i:  6999]  train_loss: 0.883  |  valid_loss: 1.020\n",
      "[epoch: 9, i:  7124]  train_loss: 0.833  |  valid_loss: 1.017\n",
      "[epoch: 9, i:  7249]  train_loss: 0.892  |  valid_loss: 0.705\n",
      "[epoch: 9, i:  7374]  train_loss: 0.935  |  valid_loss: 1.104\n",
      "[epoch: 9, i:  7499]  train_loss: 0.886  |  valid_loss: 0.979\n",
      "[epoch: 9, i:  7624]  train_loss: 0.896  |  valid_loss: 0.992\n",
      "[epoch: 9, i:  7749]  train_loss: 0.900  |  valid_loss: 0.891\n",
      "[epoch: 9, i:  7874]  train_loss: 0.923  |  valid_loss: 0.917\n",
      "[epoch: 9, i:  7999]  train_loss: 0.921  |  valid_loss: 0.760\n",
      "[epoch: 9, i:  8124]  train_loss: 0.870  |  valid_loss: 0.926\n",
      "[epoch: 9, i:  8249]  train_loss: 0.876  |  valid_loss: 0.920\n",
      "[epoch: 9, i:  8374]  train_loss: 0.857  |  valid_loss: 0.927\n",
      "[epoch: 9, i:  8499]  train_loss: 0.845  |  valid_loss: 0.971\n",
      "[epoch: 9, i:  8624]  train_loss: 0.861  |  valid_loss: 0.875\n",
      "[epoch: 9, i:  8749]  train_loss: 0.884  |  valid_loss: 0.957\n",
      "[epoch: 9, i:  8874]  train_loss: 0.916  |  valid_loss: 0.910\n",
      "[epoch: 9, i:  8999]  train_loss: 0.880  |  valid_loss: 0.769\n",
      "[epoch: 9, i:  9124]  train_loss: 0.840  |  valid_loss: 0.878\n",
      "[epoch: 9, i:  9249]  train_loss: 0.888  |  valid_loss: 0.652\n",
      "[epoch: 9, i:  9374]  train_loss: 0.911  |  valid_loss: 0.969\n",
      "[epoch: 9, i:  9499]  train_loss: 0.840  |  valid_loss: 0.817\n",
      "[epoch: 9, i:  9624]  train_loss: 0.837  |  valid_loss: 0.968\n",
      "[epoch: 9, i:  9749]  train_loss: 0.890  |  valid_loss: 0.983\n",
      "[epoch: 9, i:  9874]  train_loss: 0.948  |  valid_loss: 0.963\n",
      "[epoch: 9, i:  9999]  train_loss: 0.914  |  valid_loss: 0.896\n",
      "[epoch: 9, i: 10124]  train_loss: 0.734  |  valid_loss: 0.834\n",
      "[epoch: 9, i: 10249]  train_loss: 0.917  |  valid_loss: 0.983\n",
      "[epoch: 9, i: 10374]  train_loss: 0.876  |  valid_loss: 0.787\n",
      "[epoch: 9, i: 10499]  train_loss: 0.919  |  valid_loss: 1.003\n",
      "[epoch: 9, i: 10624]  train_loss: 0.962  |  valid_loss: 0.990\n",
      "[epoch: 9, i: 10749]  train_loss: 0.879  |  valid_loss: 0.863\n",
      "[epoch: 9, i: 10874]  train_loss: 0.935  |  valid_loss: 1.005\n",
      "[epoch: 9, i: 10999]  train_loss: 0.841  |  valid_loss: 0.978\n",
      "[epoch: 9, i: 11124]  train_loss: 0.858  |  valid_loss: 0.890\n",
      "[epoch: 9, i: 11249]  train_loss: 0.931  |  valid_loss: 0.883\n",
      "[epoch: 9, i: 11374]  train_loss: 0.815  |  valid_loss: 0.860\n",
      "[epoch: 9, i: 11499]  train_loss: 0.856  |  valid_loss: 0.720\n",
      "[epoch: 9, i: 11624]  train_loss: 0.887  |  valid_loss: 0.855\n",
      "[epoch: 9, i: 11749]  train_loss: 0.818  |  valid_loss: 0.842\n",
      "[epoch: 9, i: 11874]  train_loss: 0.927  |  valid_loss: 0.907\n",
      "[epoch: 9, i: 11999]  train_loss: 0.930  |  valid_loss: 0.725\n",
      "[epoch: 9, i: 12124]  train_loss: 0.884  |  valid_loss: 0.777\n",
      "[epoch: 9, i: 12249]  train_loss: 0.869  |  valid_loss: 1.068\n",
      "[epoch: 9, i: 12374]  train_loss: 0.783  |  valid_loss: 0.934\n",
      "[epoch: 9, i: 12499]  train_loss: 0.891  |  valid_loss: 0.911\n",
      "--> [End of epoch 9] train_accuracy: 68.62%  |  valid_accuracy: 68.96%\n",
      "--> [Start of epoch 10]  lr: 0.000215\n",
      "[epoch: 10, i:   124]  train_loss: 0.903  |  valid_loss: 0.778\n",
      "[epoch: 10, i:   249]  train_loss: 0.873  |  valid_loss: 0.753\n",
      "[epoch: 10, i:   374]  train_loss: 0.856  |  valid_loss: 0.930\n",
      "[epoch: 10, i:   499]  train_loss: 0.875  |  valid_loss: 0.826\n",
      "[epoch: 10, i:   624]  train_loss: 0.856  |  valid_loss: 0.936\n",
      "[epoch: 10, i:   749]  train_loss: 0.878  |  valid_loss: 0.643\n",
      "[epoch: 10, i:   874]  train_loss: 0.884  |  valid_loss: 0.874\n",
      "[epoch: 10, i:   999]  train_loss: 0.832  |  valid_loss: 1.024\n",
      "[epoch: 10, i:  1124]  train_loss: 0.858  |  valid_loss: 0.960\n",
      "[epoch: 10, i:  1249]  train_loss: 0.988  |  valid_loss: 0.754\n",
      "[epoch: 10, i:  1374]  train_loss: 0.845  |  valid_loss: 0.764\n",
      "[epoch: 10, i:  1499]  train_loss: 0.881  |  valid_loss: 0.848\n",
      "[epoch: 10, i:  1624]  train_loss: 0.891  |  valid_loss: 0.722\n",
      "[epoch: 10, i:  1749]  train_loss: 0.846  |  valid_loss: 1.135\n",
      "[epoch: 10, i:  1874]  train_loss: 0.897  |  valid_loss: 0.780\n",
      "[epoch: 10, i:  1999]  train_loss: 0.867  |  valid_loss: 1.053\n",
      "[epoch: 10, i:  2124]  train_loss: 0.974  |  valid_loss: 0.814\n",
      "[epoch: 10, i:  2249]  train_loss: 0.872  |  valid_loss: 0.882\n",
      "[epoch: 10, i:  2374]  train_loss: 0.904  |  valid_loss: 0.779\n",
      "[epoch: 10, i:  2499]  train_loss: 0.776  |  valid_loss: 1.044\n",
      "[epoch: 10, i:  2624]  train_loss: 0.805  |  valid_loss: 0.908\n",
      "[epoch: 10, i:  2749]  train_loss: 0.847  |  valid_loss: 0.978\n",
      "[epoch: 10, i:  2874]  train_loss: 0.903  |  valid_loss: 1.153\n",
      "[epoch: 10, i:  2999]  train_loss: 0.901  |  valid_loss: 0.889\n",
      "[epoch: 10, i:  3124]  train_loss: 0.807  |  valid_loss: 0.923\n",
      "[epoch: 10, i:  3249]  train_loss: 0.917  |  valid_loss: 1.083\n",
      "[epoch: 10, i:  3374]  train_loss: 0.906  |  valid_loss: 0.841\n",
      "[epoch: 10, i:  3499]  train_loss: 0.856  |  valid_loss: 0.799\n",
      "[epoch: 10, i:  3624]  train_loss: 0.885  |  valid_loss: 0.908\n",
      "[epoch: 10, i:  3749]  train_loss: 0.850  |  valid_loss: 0.711\n",
      "[epoch: 10, i:  3874]  train_loss: 0.914  |  valid_loss: 0.861\n",
      "[epoch: 10, i:  3999]  train_loss: 0.835  |  valid_loss: 0.687\n",
      "[epoch: 10, i:  4124]  train_loss: 0.775  |  valid_loss: 0.898\n",
      "[epoch: 10, i:  4249]  train_loss: 0.846  |  valid_loss: 0.971\n",
      "[epoch: 10, i:  4374]  train_loss: 0.867  |  valid_loss: 0.963\n",
      "[epoch: 10, i:  4499]  train_loss: 0.818  |  valid_loss: 0.829\n",
      "[epoch: 10, i:  4624]  train_loss: 0.859  |  valid_loss: 1.002\n",
      "[epoch: 10, i:  4749]  train_loss: 0.890  |  valid_loss: 0.941\n",
      "[epoch: 10, i:  4874]  train_loss: 0.831  |  valid_loss: 0.801\n",
      "[epoch: 10, i:  4999]  train_loss: 0.924  |  valid_loss: 0.815\n",
      "[epoch: 10, i:  5124]  train_loss: 0.884  |  valid_loss: 0.834\n",
      "[epoch: 10, i:  5249]  train_loss: 0.866  |  valid_loss: 0.925\n",
      "[epoch: 10, i:  5374]  train_loss: 0.872  |  valid_loss: 0.714\n",
      "[epoch: 10, i:  5499]  train_loss: 0.849  |  valid_loss: 0.736\n",
      "[epoch: 10, i:  5624]  train_loss: 0.957  |  valid_loss: 0.947\n",
      "[epoch: 10, i:  5749]  train_loss: 0.856  |  valid_loss: 0.846\n",
      "[epoch: 10, i:  5874]  train_loss: 0.886  |  valid_loss: 0.767\n",
      "[epoch: 10, i:  5999]  train_loss: 0.909  |  valid_loss: 0.867\n",
      "[epoch: 10, i:  6124]  train_loss: 0.927  |  valid_loss: 0.804\n",
      "[epoch: 10, i:  6249]  train_loss: 0.852  |  valid_loss: 0.899\n",
      "[epoch: 10, i:  6374]  train_loss: 0.823  |  valid_loss: 0.812\n",
      "[epoch: 10, i:  6499]  train_loss: 0.872  |  valid_loss: 0.824\n",
      "[epoch: 10, i:  6624]  train_loss: 0.834  |  valid_loss: 0.749\n",
      "[epoch: 10, i:  6749]  train_loss: 0.791  |  valid_loss: 0.876\n",
      "[epoch: 10, i:  6874]  train_loss: 0.871  |  valid_loss: 0.803\n",
      "[epoch: 10, i:  6999]  train_loss: 0.789  |  valid_loss: 1.045\n",
      "[epoch: 10, i:  7124]  train_loss: 0.848  |  valid_loss: 0.988\n",
      "[epoch: 10, i:  7249]  train_loss: 0.825  |  valid_loss: 0.697\n",
      "[epoch: 10, i:  7374]  train_loss: 0.852  |  valid_loss: 1.065\n",
      "[epoch: 10, i:  7499]  train_loss: 0.908  |  valid_loss: 0.954\n",
      "[epoch: 10, i:  7624]  train_loss: 0.890  |  valid_loss: 0.954\n",
      "[epoch: 10, i:  7749]  train_loss: 0.823  |  valid_loss: 0.856\n",
      "[epoch: 10, i:  7874]  train_loss: 0.916  |  valid_loss: 0.853\n",
      "[epoch: 10, i:  7999]  train_loss: 0.878  |  valid_loss: 0.760\n",
      "[epoch: 10, i:  8124]  train_loss: 0.901  |  valid_loss: 0.999\n",
      "[epoch: 10, i:  8249]  train_loss: 0.862  |  valid_loss: 0.939\n",
      "[epoch: 10, i:  8374]  train_loss: 0.818  |  valid_loss: 0.885\n",
      "[epoch: 10, i:  8499]  train_loss: 0.818  |  valid_loss: 0.974\n",
      "[epoch: 10, i:  8624]  train_loss: 0.898  |  valid_loss: 0.865\n",
      "[epoch: 10, i:  8749]  train_loss: 0.911  |  valid_loss: 0.940\n",
      "[epoch: 10, i:  8874]  train_loss: 0.882  |  valid_loss: 0.909\n",
      "[epoch: 10, i:  8999]  train_loss: 0.792  |  valid_loss: 0.699\n",
      "[epoch: 10, i:  9124]  train_loss: 0.941  |  valid_loss: 0.744\n",
      "[epoch: 10, i:  9249]  train_loss: 0.832  |  valid_loss: 0.628\n",
      "[epoch: 10, i:  9374]  train_loss: 0.926  |  valid_loss: 0.897\n",
      "[epoch: 10, i:  9499]  train_loss: 0.842  |  valid_loss: 0.838\n",
      "[epoch: 10, i:  9624]  train_loss: 0.884  |  valid_loss: 0.907\n",
      "[epoch: 10, i:  9749]  train_loss: 0.845  |  valid_loss: 0.957\n",
      "[epoch: 10, i:  9874]  train_loss: 0.852  |  valid_loss: 0.972\n",
      "[epoch: 10, i:  9999]  train_loss: 0.786  |  valid_loss: 0.842\n",
      "[epoch: 10, i: 10124]  train_loss: 0.859  |  valid_loss: 0.806\n",
      "[epoch: 10, i: 10249]  train_loss: 0.935  |  valid_loss: 0.910\n",
      "[epoch: 10, i: 10374]  train_loss: 0.877  |  valid_loss: 0.791\n",
      "[epoch: 10, i: 10499]  train_loss: 0.831  |  valid_loss: 0.987\n",
      "[epoch: 10, i: 10624]  train_loss: 0.911  |  valid_loss: 1.001\n",
      "[epoch: 10, i: 10749]  train_loss: 0.864  |  valid_loss: 0.867\n",
      "[epoch: 10, i: 10874]  train_loss: 0.838  |  valid_loss: 0.986\n",
      "[epoch: 10, i: 10999]  train_loss: 0.920  |  valid_loss: 1.023\n",
      "[epoch: 10, i: 11124]  train_loss: 0.765  |  valid_loss: 0.825\n",
      "[epoch: 10, i: 11249]  train_loss: 0.913  |  valid_loss: 0.886\n",
      "[epoch: 10, i: 11374]  train_loss: 0.866  |  valid_loss: 0.832\n",
      "[epoch: 10, i: 11499]  train_loss: 0.857  |  valid_loss: 0.704\n",
      "[epoch: 10, i: 11624]  train_loss: 0.951  |  valid_loss: 0.812\n",
      "[epoch: 10, i: 11749]  train_loss: 0.842  |  valid_loss: 0.788\n",
      "[epoch: 10, i: 11874]  train_loss: 0.909  |  valid_loss: 0.812\n",
      "[epoch: 10, i: 11999]  train_loss: 0.876  |  valid_loss: 0.748\n",
      "[epoch: 10, i: 12124]  train_loss: 0.840  |  valid_loss: 0.765\n",
      "[epoch: 10, i: 12249]  train_loss: 0.932  |  valid_loss: 1.083\n",
      "[epoch: 10, i: 12374]  train_loss: 0.817  |  valid_loss: 0.905\n",
      "[epoch: 10, i: 12499]  train_loss: 0.910  |  valid_loss: 0.884\n",
      "--> [End of epoch 10] train_accuracy: 69.33%  |  valid_accuracy: 69.45%\n",
      "--> [Start of epoch 11]  lr: 0.000172\n",
      "[epoch: 11, i:   124]  train_loss: 0.821  |  valid_loss: 0.735\n",
      "[epoch: 11, i:   249]  train_loss: 0.860  |  valid_loss: 0.734\n",
      "[epoch: 11, i:   374]  train_loss: 0.861  |  valid_loss: 0.949\n",
      "[epoch: 11, i:   499]  train_loss: 0.953  |  valid_loss: 0.835\n",
      "[epoch: 11, i:   624]  train_loss: 0.870  |  valid_loss: 0.967\n",
      "[epoch: 11, i:   749]  train_loss: 0.824  |  valid_loss: 0.634\n",
      "[epoch: 11, i:   874]  train_loss: 0.836  |  valid_loss: 0.898\n",
      "[epoch: 11, i:   999]  train_loss: 0.847  |  valid_loss: 0.956\n",
      "[epoch: 11, i:  1124]  train_loss: 0.888  |  valid_loss: 0.972\n",
      "[epoch: 11, i:  1249]  train_loss: 0.872  |  valid_loss: 0.755\n",
      "[epoch: 11, i:  1374]  train_loss: 0.848  |  valid_loss: 0.765\n",
      "[epoch: 11, i:  1499]  train_loss: 0.904  |  valid_loss: 0.856\n",
      "[epoch: 11, i:  1624]  train_loss: 0.825  |  valid_loss: 0.698\n",
      "[epoch: 11, i:  1749]  train_loss: 0.839  |  valid_loss: 0.999\n",
      "[epoch: 11, i:  1874]  train_loss: 0.842  |  valid_loss: 0.797\n",
      "[epoch: 11, i:  1999]  train_loss: 0.853  |  valid_loss: 1.032\n",
      "[epoch: 11, i:  2124]  train_loss: 0.831  |  valid_loss: 0.745\n",
      "[epoch: 11, i:  2249]  train_loss: 0.760  |  valid_loss: 0.879\n",
      "[epoch: 11, i:  2374]  train_loss: 0.892  |  valid_loss: 0.753\n",
      "[epoch: 11, i:  2499]  train_loss: 0.918  |  valid_loss: 1.010\n",
      "[epoch: 11, i:  2624]  train_loss: 0.842  |  valid_loss: 0.957\n",
      "[epoch: 11, i:  2749]  train_loss: 0.785  |  valid_loss: 0.943\n",
      "[epoch: 11, i:  2874]  train_loss: 0.845  |  valid_loss: 1.114\n",
      "[epoch: 11, i:  2999]  train_loss: 0.815  |  valid_loss: 0.851\n",
      "[epoch: 11, i:  3124]  train_loss: 0.857  |  valid_loss: 0.939\n",
      "[epoch: 11, i:  3249]  train_loss: 0.892  |  valid_loss: 1.095\n",
      "[epoch: 11, i:  3374]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 11, i:  3499]  train_loss: 0.834  |  valid_loss: 0.778\n",
      "[epoch: 11, i:  3624]  train_loss: 0.807  |  valid_loss: 0.840\n",
      "[epoch: 11, i:  3749]  train_loss: 0.885  |  valid_loss: 0.719\n",
      "[epoch: 11, i:  3874]  train_loss: 0.873  |  valid_loss: 0.802\n",
      "[epoch: 11, i:  3999]  train_loss: 0.835  |  valid_loss: 0.677\n",
      "[epoch: 11, i:  4124]  train_loss: 0.874  |  valid_loss: 0.917\n",
      "[epoch: 11, i:  4249]  train_loss: 0.894  |  valid_loss: 0.942\n",
      "[epoch: 11, i:  4374]  train_loss: 0.838  |  valid_loss: 0.906\n",
      "[epoch: 11, i:  4499]  train_loss: 0.879  |  valid_loss: 0.809\n",
      "[epoch: 11, i:  4624]  train_loss: 0.801  |  valid_loss: 0.969\n",
      "[epoch: 11, i:  4749]  train_loss: 0.834  |  valid_loss: 0.913\n",
      "[epoch: 11, i:  4874]  train_loss: 0.915  |  valid_loss: 0.843\n",
      "[epoch: 11, i:  4999]  train_loss: 0.892  |  valid_loss: 0.817\n",
      "[epoch: 11, i:  5124]  train_loss: 0.878  |  valid_loss: 0.810\n",
      "[epoch: 11, i:  5249]  train_loss: 0.921  |  valid_loss: 0.903\n",
      "[epoch: 11, i:  5374]  train_loss: 0.829  |  valid_loss: 0.661\n",
      "[epoch: 11, i:  5499]  train_loss: 0.802  |  valid_loss: 0.720\n",
      "[epoch: 11, i:  5624]  train_loss: 0.881  |  valid_loss: 1.015\n",
      "[epoch: 11, i:  5749]  train_loss: 0.744  |  valid_loss: 0.839\n",
      "[epoch: 11, i:  5874]  train_loss: 0.886  |  valid_loss: 0.772\n",
      "[epoch: 11, i:  5999]  train_loss: 0.895  |  valid_loss: 0.878\n",
      "[epoch: 11, i:  6124]  train_loss: 0.808  |  valid_loss: 0.816\n",
      "[epoch: 11, i:  6249]  train_loss: 0.884  |  valid_loss: 0.898\n",
      "[epoch: 11, i:  6374]  train_loss: 0.805  |  valid_loss: 0.785\n",
      "[epoch: 11, i:  6499]  train_loss: 0.887  |  valid_loss: 0.827\n",
      "[epoch: 11, i:  6624]  train_loss: 0.946  |  valid_loss: 0.746\n",
      "[epoch: 11, i:  6749]  train_loss: 0.851  |  valid_loss: 0.857\n",
      "[epoch: 11, i:  6874]  train_loss: 0.837  |  valid_loss: 0.801\n",
      "[epoch: 11, i:  6999]  train_loss: 0.816  |  valid_loss: 1.004\n",
      "[epoch: 11, i:  7124]  train_loss: 0.908  |  valid_loss: 0.978\n",
      "[epoch: 11, i:  7249]  train_loss: 0.877  |  valid_loss: 0.700\n",
      "[epoch: 11, i:  7374]  train_loss: 0.792  |  valid_loss: 1.084\n",
      "[epoch: 11, i:  7499]  train_loss: 0.877  |  valid_loss: 0.959\n",
      "[epoch: 11, i:  7624]  train_loss: 0.929  |  valid_loss: 0.946\n",
      "[epoch: 11, i:  7749]  train_loss: 0.914  |  valid_loss: 0.831\n",
      "[epoch: 11, i:  7874]  train_loss: 0.913  |  valid_loss: 0.884\n",
      "[epoch: 11, i:  7999]  train_loss: 0.904  |  valid_loss: 0.730\n",
      "[epoch: 11, i:  8124]  train_loss: 0.857  |  valid_loss: 0.937\n",
      "[epoch: 11, i:  8249]  train_loss: 0.890  |  valid_loss: 0.926\n",
      "[epoch: 11, i:  8374]  train_loss: 0.915  |  valid_loss: 0.870\n",
      "[epoch: 11, i:  8499]  train_loss: 0.865  |  valid_loss: 0.935\n",
      "[epoch: 11, i:  8624]  train_loss: 0.822  |  valid_loss: 0.867\n",
      "[epoch: 11, i:  8749]  train_loss: 0.831  |  valid_loss: 0.963\n",
      "[epoch: 11, i:  8874]  train_loss: 0.775  |  valid_loss: 0.897\n",
      "[epoch: 11, i:  8999]  train_loss: 0.837  |  valid_loss: 0.700\n",
      "[epoch: 11, i:  9124]  train_loss: 0.866  |  valid_loss: 0.825\n",
      "[epoch: 11, i:  9249]  train_loss: 0.876  |  valid_loss: 0.651\n",
      "[epoch: 11, i:  9374]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 11, i:  9499]  train_loss: 0.798  |  valid_loss: 0.817\n",
      "[epoch: 11, i:  9624]  train_loss: 0.866  |  valid_loss: 0.943\n",
      "[epoch: 11, i:  9749]  train_loss: 0.899  |  valid_loss: 0.909\n",
      "[epoch: 11, i:  9874]  train_loss: 0.839  |  valid_loss: 0.939\n",
      "[epoch: 11, i:  9999]  train_loss: 0.903  |  valid_loss: 0.849\n",
      "[epoch: 11, i: 10124]  train_loss: 0.829  |  valid_loss: 0.777\n",
      "[epoch: 11, i: 10249]  train_loss: 0.782  |  valid_loss: 0.914\n",
      "[epoch: 11, i: 10374]  train_loss: 0.885  |  valid_loss: 0.795\n",
      "[epoch: 11, i: 10499]  train_loss: 0.826  |  valid_loss: 1.030\n",
      "[epoch: 11, i: 10624]  train_loss: 0.816  |  valid_loss: 0.957\n",
      "[epoch: 11, i: 10749]  train_loss: 0.886  |  valid_loss: 0.877\n",
      "[epoch: 11, i: 10874]  train_loss: 0.855  |  valid_loss: 0.973\n",
      "[epoch: 11, i: 10999]  train_loss: 0.873  |  valid_loss: 0.955\n",
      "[epoch: 11, i: 11124]  train_loss: 0.856  |  valid_loss: 0.820\n",
      "[epoch: 11, i: 11249]  train_loss: 0.797  |  valid_loss: 0.964\n",
      "[epoch: 11, i: 11374]  train_loss: 0.915  |  valid_loss: 0.847\n",
      "[epoch: 11, i: 11499]  train_loss: 0.932  |  valid_loss: 0.665\n",
      "[epoch: 11, i: 11624]  train_loss: 0.796  |  valid_loss: 0.822\n",
      "[epoch: 11, i: 11749]  train_loss: 0.873  |  valid_loss: 0.810\n",
      "[epoch: 11, i: 11874]  train_loss: 0.916  |  valid_loss: 0.822\n",
      "[epoch: 11, i: 11999]  train_loss: 0.843  |  valid_loss: 0.745\n",
      "[epoch: 11, i: 12124]  train_loss: 0.853  |  valid_loss: 0.759\n",
      "[epoch: 11, i: 12249]  train_loss: 0.898  |  valid_loss: 1.021\n",
      "[epoch: 11, i: 12374]  train_loss: 0.848  |  valid_loss: 0.903\n",
      "[epoch: 11, i: 12499]  train_loss: 0.788  |  valid_loss: 0.872\n",
      "--> [End of epoch 11] train_accuracy: 69.77%  |  valid_accuracy: 69.80%\n",
      "--> [Start of epoch 12]  lr: 0.000137\n",
      "[epoch: 12, i:   124]  train_loss: 0.876  |  valid_loss: 0.742\n",
      "[epoch: 12, i:   249]  train_loss: 0.856  |  valid_loss: 0.732\n",
      "[epoch: 12, i:   374]  train_loss: 0.894  |  valid_loss: 0.928\n",
      "[epoch: 12, i:   499]  train_loss: 0.808  |  valid_loss: 0.822\n",
      "[epoch: 12, i:   624]  train_loss: 0.803  |  valid_loss: 0.943\n",
      "[epoch: 12, i:   749]  train_loss: 0.827  |  valid_loss: 0.605\n",
      "[epoch: 12, i:   874]  train_loss: 0.840  |  valid_loss: 0.882\n",
      "[epoch: 12, i:   999]  train_loss: 0.784  |  valid_loss: 0.922\n",
      "[epoch: 12, i:  1124]  train_loss: 0.788  |  valid_loss: 0.932\n",
      "[epoch: 12, i:  1249]  train_loss: 0.870  |  valid_loss: 0.753\n",
      "[epoch: 12, i:  1374]  train_loss: 0.773  |  valid_loss: 0.794\n",
      "[epoch: 12, i:  1499]  train_loss: 0.784  |  valid_loss: 0.819\n",
      "[epoch: 12, i:  1624]  train_loss: 0.844  |  valid_loss: 0.795\n",
      "[epoch: 12, i:  1749]  train_loss: 0.914  |  valid_loss: 1.072\n",
      "[epoch: 12, i:  1874]  train_loss: 0.882  |  valid_loss: 0.775\n",
      "[epoch: 12, i:  1999]  train_loss: 0.880  |  valid_loss: 1.018\n",
      "[epoch: 12, i:  2124]  train_loss: 0.880  |  valid_loss: 0.781\n",
      "[epoch: 12, i:  2249]  train_loss: 0.823  |  valid_loss: 0.901\n",
      "[epoch: 12, i:  2374]  train_loss: 0.864  |  valid_loss: 0.767\n",
      "[epoch: 12, i:  2499]  train_loss: 0.763  |  valid_loss: 1.055\n",
      "[epoch: 12, i:  2624]  train_loss: 0.813  |  valid_loss: 0.924\n",
      "[epoch: 12, i:  2749]  train_loss: 0.904  |  valid_loss: 0.943\n",
      "[epoch: 12, i:  2874]  train_loss: 0.879  |  valid_loss: 1.113\n",
      "[epoch: 12, i:  2999]  train_loss: 0.831  |  valid_loss: 0.873\n",
      "[epoch: 12, i:  3124]  train_loss: 0.808  |  valid_loss: 0.909\n",
      "[epoch: 12, i:  3249]  train_loss: 0.918  |  valid_loss: 1.086\n",
      "[epoch: 12, i:  3374]  train_loss: 0.889  |  valid_loss: 0.852\n",
      "[epoch: 12, i:  3499]  train_loss: 0.821  |  valid_loss: 0.775\n",
      "[epoch: 12, i:  3624]  train_loss: 0.804  |  valid_loss: 0.862\n",
      "[epoch: 12, i:  3749]  train_loss: 0.834  |  valid_loss: 0.697\n",
      "[epoch: 12, i:  3874]  train_loss: 0.969  |  valid_loss: 0.822\n",
      "[epoch: 12, i:  3999]  train_loss: 0.829  |  valid_loss: 0.636\n",
      "[epoch: 12, i:  4124]  train_loss: 0.895  |  valid_loss: 0.877\n",
      "[epoch: 12, i:  4249]  train_loss: 0.769  |  valid_loss: 0.929\n",
      "[epoch: 12, i:  4374]  train_loss: 0.843  |  valid_loss: 0.912\n",
      "[epoch: 12, i:  4499]  train_loss: 0.857  |  valid_loss: 0.820\n",
      "[epoch: 12, i:  4624]  train_loss: 0.858  |  valid_loss: 0.936\n",
      "[epoch: 12, i:  4749]  train_loss: 0.795  |  valid_loss: 0.926\n",
      "[epoch: 12, i:  4874]  train_loss: 0.827  |  valid_loss: 0.758\n",
      "[epoch: 12, i:  4999]  train_loss: 0.819  |  valid_loss: 0.780\n",
      "[epoch: 12, i:  5124]  train_loss: 0.798  |  valid_loss: 0.856\n",
      "[epoch: 12, i:  5249]  train_loss: 0.829  |  valid_loss: 0.914\n",
      "[epoch: 12, i:  5374]  train_loss: 0.876  |  valid_loss: 0.646\n",
      "[epoch: 12, i:  5499]  train_loss: 0.877  |  valid_loss: 0.731\n",
      "[epoch: 12, i:  5624]  train_loss: 0.874  |  valid_loss: 1.063\n",
      "[epoch: 12, i:  5749]  train_loss: 0.820  |  valid_loss: 0.837\n",
      "[epoch: 12, i:  5874]  train_loss: 0.814  |  valid_loss: 0.763\n",
      "[epoch: 12, i:  5999]  train_loss: 0.935  |  valid_loss: 0.903\n",
      "[epoch: 12, i:  6124]  train_loss: 0.778  |  valid_loss: 0.808\n",
      "[epoch: 12, i:  6249]  train_loss: 0.831  |  valid_loss: 0.900\n",
      "[epoch: 12, i:  6374]  train_loss: 0.818  |  valid_loss: 0.802\n",
      "[epoch: 12, i:  6499]  train_loss: 0.897  |  valid_loss: 0.828\n",
      "[epoch: 12, i:  6624]  train_loss: 0.865  |  valid_loss: 0.758\n",
      "[epoch: 12, i:  6749]  train_loss: 0.797  |  valid_loss: 0.835\n",
      "[epoch: 12, i:  6874]  train_loss: 0.846  |  valid_loss: 0.804\n",
      "[epoch: 12, i:  6999]  train_loss: 0.818  |  valid_loss: 0.975\n",
      "[epoch: 12, i:  7124]  train_loss: 0.885  |  valid_loss: 1.013\n",
      "[epoch: 12, i:  7249]  train_loss: 0.826  |  valid_loss: 0.695\n",
      "[epoch: 12, i:  7374]  train_loss: 0.880  |  valid_loss: 1.050\n",
      "[epoch: 12, i:  7499]  train_loss: 0.862  |  valid_loss: 0.916\n",
      "[epoch: 12, i:  7624]  train_loss: 0.825  |  valid_loss: 0.952\n",
      "[epoch: 12, i:  7749]  train_loss: 0.865  |  valid_loss: 0.852\n",
      "[epoch: 12, i:  7874]  train_loss: 0.860  |  valid_loss: 0.892\n",
      "[epoch: 12, i:  7999]  train_loss: 0.797  |  valid_loss: 0.769\n",
      "[epoch: 12, i:  8124]  train_loss: 0.798  |  valid_loss: 0.926\n",
      "[epoch: 12, i:  8249]  train_loss: 0.758  |  valid_loss: 0.907\n",
      "[epoch: 12, i:  8374]  train_loss: 0.883  |  valid_loss: 0.855\n",
      "[epoch: 12, i:  8499]  train_loss: 0.891  |  valid_loss: 0.963\n",
      "[epoch: 12, i:  8624]  train_loss: 0.827  |  valid_loss: 0.874\n",
      "[epoch: 12, i:  8749]  train_loss: 0.774  |  valid_loss: 0.905\n",
      "[epoch: 12, i:  8874]  train_loss: 0.785  |  valid_loss: 0.890\n",
      "[epoch: 12, i:  8999]  train_loss: 0.806  |  valid_loss: 0.683\n",
      "[epoch: 12, i:  9124]  train_loss: 0.843  |  valid_loss: 0.767\n",
      "[epoch: 12, i:  9249]  train_loss: 0.872  |  valid_loss: 0.652\n",
      "[epoch: 12, i:  9374]  train_loss: 0.783  |  valid_loss: 0.923\n",
      "[epoch: 12, i:  9499]  train_loss: 0.923  |  valid_loss: 0.765\n",
      "[epoch: 12, i:  9624]  train_loss: 0.838  |  valid_loss: 0.921\n",
      "[epoch: 12, i:  9749]  train_loss: 0.912  |  valid_loss: 0.943\n",
      "[epoch: 12, i:  9874]  train_loss: 0.836  |  valid_loss: 0.947\n",
      "[epoch: 12, i:  9999]  train_loss: 0.883  |  valid_loss: 0.850\n",
      "[epoch: 12, i: 10124]  train_loss: 0.833  |  valid_loss: 0.843\n",
      "[epoch: 12, i: 10249]  train_loss: 0.846  |  valid_loss: 0.884\n",
      "[epoch: 12, i: 10374]  train_loss: 0.801  |  valid_loss: 0.765\n",
      "[epoch: 12, i: 10499]  train_loss: 0.837  |  valid_loss: 1.010\n",
      "[epoch: 12, i: 10624]  train_loss: 0.849  |  valid_loss: 1.023\n",
      "[epoch: 12, i: 10749]  train_loss: 0.882  |  valid_loss: 0.941\n",
      "[epoch: 12, i: 10874]  train_loss: 0.849  |  valid_loss: 1.011\n",
      "[epoch: 12, i: 10999]  train_loss: 0.865  |  valid_loss: 0.950\n",
      "[epoch: 12, i: 11124]  train_loss: 0.873  |  valid_loss: 0.829\n",
      "[epoch: 12, i: 11249]  train_loss: 0.799  |  valid_loss: 0.889\n",
      "[epoch: 12, i: 11374]  train_loss: 0.825  |  valid_loss: 0.783\n",
      "[epoch: 12, i: 11499]  train_loss: 0.899  |  valid_loss: 0.622\n",
      "[epoch: 12, i: 11624]  train_loss: 0.897  |  valid_loss: 0.818\n",
      "[epoch: 12, i: 11749]  train_loss: 0.919  |  valid_loss: 0.801\n",
      "[epoch: 12, i: 11874]  train_loss: 0.818  |  valid_loss: 0.772\n",
      "[epoch: 12, i: 11999]  train_loss: 0.922  |  valid_loss: 0.679\n",
      "[epoch: 12, i: 12124]  train_loss: 0.877  |  valid_loss: 0.767\n",
      "[epoch: 12, i: 12249]  train_loss: 0.915  |  valid_loss: 1.071\n",
      "[epoch: 12, i: 12374]  train_loss: 0.863  |  valid_loss: 0.910\n",
      "[epoch: 12, i: 12499]  train_loss: 0.887  |  valid_loss: 0.891\n",
      "--> [End of epoch 12] train_accuracy: 70.12%  |  valid_accuracy: 70.22%\n",
      "--> [Start of epoch 13]  lr: 0.000110\n",
      "[epoch: 13, i:   124]  train_loss: 0.846  |  valid_loss: 0.739\n",
      "[epoch: 13, i:   249]  train_loss: 0.822  |  valid_loss: 0.748\n",
      "[epoch: 13, i:   374]  train_loss: 0.792  |  valid_loss: 0.917\n",
      "[epoch: 13, i:   499]  train_loss: 0.831  |  valid_loss: 0.846\n",
      "[epoch: 13, i:   624]  train_loss: 0.866  |  valid_loss: 0.952\n",
      "[epoch: 13, i:   749]  train_loss: 0.921  |  valid_loss: 0.605\n",
      "[epoch: 13, i:   874]  train_loss: 0.825  |  valid_loss: 0.916\n",
      "[epoch: 13, i:   999]  train_loss: 0.832  |  valid_loss: 0.953\n",
      "[epoch: 13, i:  1124]  train_loss: 0.804  |  valid_loss: 0.984\n",
      "[epoch: 13, i:  1249]  train_loss: 0.873  |  valid_loss: 0.741\n",
      "[epoch: 13, i:  1374]  train_loss: 0.912  |  valid_loss: 0.776\n",
      "[epoch: 13, i:  1499]  train_loss: 0.847  |  valid_loss: 0.793\n",
      "[epoch: 13, i:  1624]  train_loss: 0.831  |  valid_loss: 0.701\n",
      "[epoch: 13, i:  1749]  train_loss: 0.893  |  valid_loss: 1.012\n",
      "[epoch: 13, i:  1874]  train_loss: 0.827  |  valid_loss: 0.814\n",
      "[epoch: 13, i:  1999]  train_loss: 0.780  |  valid_loss: 0.994\n",
      "[epoch: 13, i:  2124]  train_loss: 0.778  |  valid_loss: 0.704\n",
      "[epoch: 13, i:  2249]  train_loss: 0.818  |  valid_loss: 0.893\n",
      "[epoch: 13, i:  2374]  train_loss: 0.808  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  2499]  train_loss: 0.807  |  valid_loss: 1.024\n",
      "[epoch: 13, i:  2624]  train_loss: 0.803  |  valid_loss: 0.903\n",
      "[epoch: 13, i:  2749]  train_loss: 0.829  |  valid_loss: 0.949\n",
      "[epoch: 13, i:  2874]  train_loss: 0.834  |  valid_loss: 1.098\n",
      "[epoch: 13, i:  2999]  train_loss: 0.855  |  valid_loss: 0.892\n",
      "[epoch: 13, i:  3124]  train_loss: 0.896  |  valid_loss: 0.892\n",
      "[epoch: 13, i:  3249]  train_loss: 0.882  |  valid_loss: 1.126\n",
      "[epoch: 13, i:  3374]  train_loss: 0.840  |  valid_loss: 0.811\n",
      "[epoch: 13, i:  3499]  train_loss: 0.828  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  3624]  train_loss: 0.865  |  valid_loss: 0.829\n",
      "[epoch: 13, i:  3749]  train_loss: 0.809  |  valid_loss: 0.686\n",
      "[epoch: 13, i:  3874]  train_loss: 0.842  |  valid_loss: 0.852\n",
      "[epoch: 13, i:  3999]  train_loss: 0.759  |  valid_loss: 0.645\n",
      "[epoch: 13, i:  4124]  train_loss: 0.837  |  valid_loss: 0.900\n",
      "[epoch: 13, i:  4249]  train_loss: 0.849  |  valid_loss: 0.904\n",
      "[epoch: 13, i:  4374]  train_loss: 0.781  |  valid_loss: 0.923\n",
      "[epoch: 13, i:  4499]  train_loss: 0.864  |  valid_loss: 0.812\n",
      "[epoch: 13, i:  4624]  train_loss: 0.896  |  valid_loss: 0.988\n",
      "[epoch: 13, i:  4749]  train_loss: 0.923  |  valid_loss: 0.937\n",
      "[epoch: 13, i:  4874]  train_loss: 0.843  |  valid_loss: 0.748\n",
      "[epoch: 13, i:  4999]  train_loss: 0.755  |  valid_loss: 0.800\n",
      "[epoch: 13, i:  5124]  train_loss: 0.830  |  valid_loss: 0.858\n",
      "[epoch: 13, i:  5249]  train_loss: 0.919  |  valid_loss: 0.907\n",
      "[epoch: 13, i:  5374]  train_loss: 0.821  |  valid_loss: 0.658\n",
      "[epoch: 13, i:  5499]  train_loss: 0.722  |  valid_loss: 0.713\n",
      "[epoch: 13, i:  5624]  train_loss: 0.889  |  valid_loss: 0.991\n",
      "[epoch: 13, i:  5749]  train_loss: 0.807  |  valid_loss: 0.845\n",
      "[epoch: 13, i:  5874]  train_loss: 0.846  |  valid_loss: 0.740\n",
      "[epoch: 13, i:  5999]  train_loss: 0.874  |  valid_loss: 0.852\n",
      "[epoch: 13, i:  6124]  train_loss: 0.804  |  valid_loss: 0.772\n",
      "[epoch: 13, i:  6249]  train_loss: 0.858  |  valid_loss: 0.920\n",
      "[epoch: 13, i:  6374]  train_loss: 0.780  |  valid_loss: 0.781\n",
      "[epoch: 13, i:  6499]  train_loss: 0.889  |  valid_loss: 0.787\n",
      "[epoch: 13, i:  6624]  train_loss: 0.889  |  valid_loss: 0.655\n",
      "[epoch: 13, i:  6749]  train_loss: 0.830  |  valid_loss: 0.864\n",
      "[epoch: 13, i:  6874]  train_loss: 0.902  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  6999]  train_loss: 0.848  |  valid_loss: 0.944\n",
      "[epoch: 13, i:  7124]  train_loss: 0.843  |  valid_loss: 1.021\n",
      "[epoch: 13, i:  7249]  train_loss: 0.958  |  valid_loss: 0.677\n",
      "[epoch: 13, i:  7374]  train_loss: 0.846  |  valid_loss: 1.080\n",
      "[epoch: 13, i:  7499]  train_loss: 0.851  |  valid_loss: 0.923\n",
      "[epoch: 13, i:  7624]  train_loss: 0.858  |  valid_loss: 0.943\n",
      "[epoch: 13, i:  7749]  train_loss: 0.888  |  valid_loss: 0.834\n",
      "[epoch: 13, i:  7874]  train_loss: 0.828  |  valid_loss: 0.899\n",
      "[epoch: 13, i:  7999]  train_loss: 0.873  |  valid_loss: 0.750\n",
      "[epoch: 13, i:  8124]  train_loss: 0.862  |  valid_loss: 0.927\n",
      "[epoch: 13, i:  8249]  train_loss: 0.843  |  valid_loss: 0.922\n",
      "[epoch: 13, i:  8374]  train_loss: 0.735  |  valid_loss: 0.861\n",
      "[epoch: 13, i:  8499]  train_loss: 0.899  |  valid_loss: 0.921\n",
      "[epoch: 13, i:  8624]  train_loss: 0.891  |  valid_loss: 0.887\n",
      "[epoch: 13, i:  8749]  train_loss: 0.867  |  valid_loss: 0.932\n",
      "[epoch: 13, i:  8874]  train_loss: 0.855  |  valid_loss: 0.913\n",
      "[epoch: 13, i:  8999]  train_loss: 0.843  |  valid_loss: 0.722\n",
      "[epoch: 13, i:  9124]  train_loss: 0.861  |  valid_loss: 0.766\n",
      "[epoch: 13, i:  9249]  train_loss: 0.851  |  valid_loss: 0.659\n",
      "[epoch: 13, i:  9374]  train_loss: 0.800  |  valid_loss: 0.978\n",
      "[epoch: 13, i:  9499]  train_loss: 0.856  |  valid_loss: 0.766\n",
      "[epoch: 13, i:  9624]  train_loss: 0.834  |  valid_loss: 0.891\n",
      "[epoch: 13, i:  9749]  train_loss: 0.795  |  valid_loss: 0.919\n",
      "[epoch: 13, i:  9874]  train_loss: 0.871  |  valid_loss: 0.909\n",
      "[epoch: 13, i:  9999]  train_loss: 0.824  |  valid_loss: 0.839\n",
      "[epoch: 13, i: 10124]  train_loss: 0.893  |  valid_loss: 0.789\n",
      "[epoch: 13, i: 10249]  train_loss: 0.807  |  valid_loss: 0.923\n",
      "[epoch: 13, i: 10374]  train_loss: 0.815  |  valid_loss: 0.750\n",
      "[epoch: 13, i: 10499]  train_loss: 0.876  |  valid_loss: 1.038\n",
      "[epoch: 13, i: 10624]  train_loss: 0.888  |  valid_loss: 0.979\n",
      "[epoch: 13, i: 10749]  train_loss: 0.867  |  valid_loss: 0.835\n",
      "[epoch: 13, i: 10874]  train_loss: 0.854  |  valid_loss: 0.993\n",
      "[epoch: 13, i: 10999]  train_loss: 0.810  |  valid_loss: 0.965\n",
      "[epoch: 13, i: 11124]  train_loss: 0.751  |  valid_loss: 0.857\n",
      "[epoch: 13, i: 11249]  train_loss: 0.809  |  valid_loss: 0.889\n",
      "[epoch: 13, i: 11374]  train_loss: 0.826  |  valid_loss: 0.790\n",
      "[epoch: 13, i: 11499]  train_loss: 0.835  |  valid_loss: 0.663\n",
      "[epoch: 13, i: 11624]  train_loss: 0.803  |  valid_loss: 0.825\n",
      "[epoch: 13, i: 11749]  train_loss: 0.815  |  valid_loss: 0.771\n",
      "[epoch: 13, i: 11874]  train_loss: 0.889  |  valid_loss: 0.801\n",
      "[epoch: 13, i: 11999]  train_loss: 0.842  |  valid_loss: 0.728\n",
      "[epoch: 13, i: 12124]  train_loss: 0.848  |  valid_loss: 0.762\n",
      "[epoch: 13, i: 12249]  train_loss: 0.860  |  valid_loss: 1.030\n",
      "[epoch: 13, i: 12374]  train_loss: 0.836  |  valid_loss: 0.946\n",
      "[epoch: 13, i: 12499]  train_loss: 0.913  |  valid_loss: 0.871\n",
      "--> [End of epoch 13] train_accuracy: 70.15%  |  valid_accuracy: 70.33%\n",
      "--> [Start of epoch 14]  lr: 0.000088\n",
      "[epoch: 14, i:   124]  train_loss: 0.795  |  valid_loss: 0.765\n",
      "[epoch: 14, i:   249]  train_loss: 0.864  |  valid_loss: 0.763\n",
      "[epoch: 14, i:   374]  train_loss: 0.778  |  valid_loss: 0.887\n",
      "[epoch: 14, i:   499]  train_loss: 0.762  |  valid_loss: 0.825\n",
      "[epoch: 14, i:   624]  train_loss: 0.838  |  valid_loss: 0.949\n",
      "[epoch: 14, i:   749]  train_loss: 0.787  |  valid_loss: 0.602\n",
      "[epoch: 14, i:   874]  train_loss: 0.839  |  valid_loss: 0.859\n",
      "[epoch: 14, i:   999]  train_loss: 0.820  |  valid_loss: 0.981\n",
      "[epoch: 14, i:  1124]  train_loss: 0.837  |  valid_loss: 0.997\n",
      "[epoch: 14, i:  1249]  train_loss: 0.839  |  valid_loss: 0.767\n",
      "[epoch: 14, i:  1374]  train_loss: 0.814  |  valid_loss: 0.772\n",
      "[epoch: 14, i:  1499]  train_loss: 0.785  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  1624]  train_loss: 0.905  |  valid_loss: 0.720\n",
      "[epoch: 14, i:  1749]  train_loss: 0.947  |  valid_loss: 0.965\n",
      "[epoch: 14, i:  1874]  train_loss: 0.765  |  valid_loss: 0.787\n",
      "[epoch: 14, i:  1999]  train_loss: 0.853  |  valid_loss: 1.105\n",
      "[epoch: 14, i:  2124]  train_loss: 0.843  |  valid_loss: 0.752\n",
      "[epoch: 14, i:  2249]  train_loss: 0.880  |  valid_loss: 0.866\n",
      "[epoch: 14, i:  2374]  train_loss: 0.816  |  valid_loss: 0.802\n",
      "[epoch: 14, i:  2499]  train_loss: 0.829  |  valid_loss: 1.013\n",
      "[epoch: 14, i:  2624]  train_loss: 0.837  |  valid_loss: 0.961\n",
      "[epoch: 14, i:  2749]  train_loss: 0.775  |  valid_loss: 1.031\n",
      "[epoch: 14, i:  2874]  train_loss: 0.822  |  valid_loss: 1.063\n",
      "[epoch: 14, i:  2999]  train_loss: 0.803  |  valid_loss: 0.901\n",
      "[epoch: 14, i:  3124]  train_loss: 0.831  |  valid_loss: 0.935\n",
      "[epoch: 14, i:  3249]  train_loss: 0.865  |  valid_loss: 1.069\n",
      "[epoch: 14, i:  3374]  train_loss: 0.928  |  valid_loss: 0.868\n",
      "[epoch: 14, i:  3499]  train_loss: 0.813  |  valid_loss: 0.785\n",
      "[epoch: 14, i:  3624]  train_loss: 0.854  |  valid_loss: 0.803\n",
      "[epoch: 14, i:  3749]  train_loss: 0.854  |  valid_loss: 0.719\n",
      "[epoch: 14, i:  3874]  train_loss: 0.826  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  3999]  train_loss: 0.818  |  valid_loss: 0.661\n",
      "[epoch: 14, i:  4124]  train_loss: 0.784  |  valid_loss: 0.858\n",
      "[epoch: 14, i:  4249]  train_loss: 0.843  |  valid_loss: 0.908\n",
      "[epoch: 14, i:  4374]  train_loss: 0.938  |  valid_loss: 0.924\n",
      "[epoch: 14, i:  4499]  train_loss: 0.889  |  valid_loss: 0.791\n",
      "[epoch: 14, i:  4624]  train_loss: 0.853  |  valid_loss: 0.983\n",
      "[epoch: 14, i:  4749]  train_loss: 0.825  |  valid_loss: 0.887\n",
      "[epoch: 14, i:  4874]  train_loss: 0.801  |  valid_loss: 0.768\n",
      "[epoch: 14, i:  4999]  train_loss: 0.780  |  valid_loss: 0.822\n",
      "[epoch: 14, i:  5124]  train_loss: 0.901  |  valid_loss: 0.858\n",
      "[epoch: 14, i:  5249]  train_loss: 0.818  |  valid_loss: 0.904\n",
      "[epoch: 14, i:  5374]  train_loss: 0.825  |  valid_loss: 0.646\n",
      "[epoch: 14, i:  5499]  train_loss: 0.802  |  valid_loss: 0.714\n",
      "[epoch: 14, i:  5624]  train_loss: 0.842  |  valid_loss: 0.967\n",
      "[epoch: 14, i:  5749]  train_loss: 0.807  |  valid_loss: 0.831\n",
      "[epoch: 14, i:  5874]  train_loss: 0.901  |  valid_loss: 0.716\n",
      "[epoch: 14, i:  5999]  train_loss: 0.801  |  valid_loss: 0.875\n",
      "[epoch: 14, i:  6124]  train_loss: 0.837  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  6249]  train_loss: 0.879  |  valid_loss: 0.847\n",
      "[epoch: 14, i:  6374]  train_loss: 0.881  |  valid_loss: 0.764\n",
      "[epoch: 14, i:  6499]  train_loss: 0.940  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  6624]  train_loss: 0.831  |  valid_loss: 0.649\n",
      "[epoch: 14, i:  6749]  train_loss: 0.814  |  valid_loss: 0.844\n",
      "[epoch: 14, i:  6874]  train_loss: 0.924  |  valid_loss: 0.808\n",
      "[epoch: 14, i:  6999]  train_loss: 0.881  |  valid_loss: 0.987\n",
      "[epoch: 14, i:  7124]  train_loss: 0.823  |  valid_loss: 1.003\n",
      "[epoch: 14, i:  7249]  train_loss: 0.865  |  valid_loss: 0.665\n",
      "[epoch: 14, i:  7374]  train_loss: 0.789  |  valid_loss: 1.049\n",
      "[epoch: 14, i:  7499]  train_loss: 0.827  |  valid_loss: 0.961\n",
      "[epoch: 14, i:  7624]  train_loss: 0.880  |  valid_loss: 0.919\n",
      "[epoch: 14, i:  7749]  train_loss: 0.836  |  valid_loss: 0.801\n",
      "[epoch: 14, i:  7874]  train_loss: 0.838  |  valid_loss: 0.886\n",
      "[epoch: 14, i:  7999]  train_loss: 0.761  |  valid_loss: 0.734\n",
      "[epoch: 14, i:  8124]  train_loss: 0.811  |  valid_loss: 0.927\n",
      "[epoch: 14, i:  8249]  train_loss: 0.914  |  valid_loss: 0.898\n",
      "[epoch: 14, i:  8374]  train_loss: 0.755  |  valid_loss: 0.855\n",
      "[epoch: 14, i:  8499]  train_loss: 0.814  |  valid_loss: 0.881\n",
      "[epoch: 14, i:  8624]  train_loss: 0.882  |  valid_loss: 0.865\n",
      "[epoch: 14, i:  8749]  train_loss: 0.811  |  valid_loss: 0.955\n",
      "[epoch: 14, i:  8874]  train_loss: 0.812  |  valid_loss: 0.853\n",
      "[epoch: 14, i:  8999]  train_loss: 0.823  |  valid_loss: 0.696\n",
      "[epoch: 14, i:  9124]  train_loss: 0.812  |  valid_loss: 0.743\n",
      "[epoch: 14, i:  9249]  train_loss: 0.848  |  valid_loss: 0.641\n",
      "[epoch: 14, i:  9374]  train_loss: 0.830  |  valid_loss: 0.932\n",
      "[epoch: 14, i:  9499]  train_loss: 0.825  |  valid_loss: 0.766\n",
      "[epoch: 14, i:  9624]  train_loss: 0.838  |  valid_loss: 0.866\n",
      "[epoch: 14, i:  9749]  train_loss: 0.801  |  valid_loss: 0.930\n",
      "[epoch: 14, i:  9874]  train_loss: 0.817  |  valid_loss: 0.875\n",
      "[epoch: 14, i:  9999]  train_loss: 0.836  |  valid_loss: 0.822\n",
      "[epoch: 14, i: 10124]  train_loss: 0.840  |  valid_loss: 0.783\n",
      "[epoch: 14, i: 10249]  train_loss: 0.846  |  valid_loss: 0.829\n",
      "[epoch: 14, i: 10374]  train_loss: 0.953  |  valid_loss: 0.757\n",
      "[epoch: 14, i: 10499]  train_loss: 0.833  |  valid_loss: 0.954\n",
      "[epoch: 14, i: 10624]  train_loss: 0.860  |  valid_loss: 0.992\n",
      "[epoch: 14, i: 10749]  train_loss: 0.815  |  valid_loss: 0.852\n",
      "[epoch: 14, i: 10874]  train_loss: 0.787  |  valid_loss: 0.969\n",
      "[epoch: 14, i: 10999]  train_loss: 0.744  |  valid_loss: 0.980\n",
      "[epoch: 14, i: 11124]  train_loss: 0.873  |  valid_loss: 0.851\n",
      "[epoch: 14, i: 11249]  train_loss: 0.806  |  valid_loss: 0.866\n",
      "[epoch: 14, i: 11374]  train_loss: 0.858  |  valid_loss: 0.791\n",
      "[epoch: 14, i: 11499]  train_loss: 0.808  |  valid_loss: 0.620\n",
      "[epoch: 14, i: 11624]  train_loss: 0.823  |  valid_loss: 0.807\n",
      "[epoch: 14, i: 11749]  train_loss: 0.878  |  valid_loss: 0.759\n",
      "[epoch: 14, i: 11874]  train_loss: 0.851  |  valid_loss: 0.811\n",
      "[epoch: 14, i: 11999]  train_loss: 0.859  |  valid_loss: 0.703\n",
      "[epoch: 14, i: 12124]  train_loss: 0.849  |  valid_loss: 0.789\n",
      "[epoch: 14, i: 12249]  train_loss: 0.913  |  valid_loss: 1.080\n",
      "[epoch: 14, i: 12374]  train_loss: 0.764  |  valid_loss: 0.899\n",
      "[epoch: 14, i: 12499]  train_loss: 0.829  |  valid_loss: 0.879\n",
      "--> [End of epoch 14] train_accuracy: 70.60%  |  valid_accuracy: 70.54%\n",
      "--> [Start of epoch 15]  lr: 0.000070\n",
      "[epoch: 15, i:   124]  train_loss: 0.894  |  valid_loss: 0.743\n",
      "[epoch: 15, i:   249]  train_loss: 0.837  |  valid_loss: 0.713\n",
      "[epoch: 15, i:   374]  train_loss: 0.794  |  valid_loss: 0.882\n",
      "[epoch: 15, i:   499]  train_loss: 0.868  |  valid_loss: 0.805\n",
      "[epoch: 15, i:   624]  train_loss: 0.860  |  valid_loss: 0.931\n",
      "[epoch: 15, i:   749]  train_loss: 0.792  |  valid_loss: 0.601\n",
      "[epoch: 15, i:   874]  train_loss: 0.767  |  valid_loss: 0.903\n",
      "[epoch: 15, i:   999]  train_loss: 0.782  |  valid_loss: 0.936\n",
      "[epoch: 15, i:  1124]  train_loss: 0.782  |  valid_loss: 0.965\n",
      "[epoch: 15, i:  1249]  train_loss: 0.875  |  valid_loss: 0.730\n",
      "[epoch: 15, i:  1374]  train_loss: 0.844  |  valid_loss: 0.766\n",
      "[epoch: 15, i:  1499]  train_loss: 0.808  |  valid_loss: 0.817\n",
      "[epoch: 15, i:  1624]  train_loss: 0.835  |  valid_loss: 0.686\n",
      "[epoch: 15, i:  1749]  train_loss: 0.774  |  valid_loss: 0.972\n",
      "[epoch: 15, i:  1874]  train_loss: 0.804  |  valid_loss: 0.784\n",
      "[epoch: 15, i:  1999]  train_loss: 0.864  |  valid_loss: 1.029\n",
      "[epoch: 15, i:  2124]  train_loss: 0.836  |  valid_loss: 0.740\n",
      "[epoch: 15, i:  2249]  train_loss: 0.833  |  valid_loss: 0.836\n",
      "[epoch: 15, i:  2374]  train_loss: 0.845  |  valid_loss: 0.788\n",
      "[epoch: 15, i:  2499]  train_loss: 0.841  |  valid_loss: 1.023\n",
      "[epoch: 15, i:  2624]  train_loss: 0.848  |  valid_loss: 0.926\n",
      "[epoch: 15, i:  2749]  train_loss: 0.906  |  valid_loss: 1.045\n",
      "[epoch: 15, i:  2874]  train_loss: 0.801  |  valid_loss: 1.048\n",
      "[epoch: 15, i:  2999]  train_loss: 0.842  |  valid_loss: 0.863\n",
      "[epoch: 15, i:  3124]  train_loss: 0.827  |  valid_loss: 0.918\n",
      "[epoch: 15, i:  3249]  train_loss: 0.864  |  valid_loss: 1.065\n",
      "[epoch: 15, i:  3374]  train_loss: 0.783  |  valid_loss: 0.788\n",
      "[epoch: 15, i:  3499]  train_loss: 0.863  |  valid_loss: 0.778\n",
      "[epoch: 15, i:  3624]  train_loss: 0.851  |  valid_loss: 0.805\n",
      "[epoch: 15, i:  3749]  train_loss: 0.839  |  valid_loss: 0.740\n",
      "[epoch: 15, i:  3874]  train_loss: 0.818  |  valid_loss: 0.807\n",
      "[epoch: 15, i:  3999]  train_loss: 0.831  |  valid_loss: 0.663\n",
      "[epoch: 15, i:  4124]  train_loss: 0.831  |  valid_loss: 0.852\n",
      "[epoch: 15, i:  4249]  train_loss: 0.841  |  valid_loss: 0.911\n",
      "[epoch: 15, i:  4374]  train_loss: 0.886  |  valid_loss: 0.914\n",
      "[epoch: 15, i:  4499]  train_loss: 0.819  |  valid_loss: 0.785\n",
      "[epoch: 15, i:  4624]  train_loss: 0.851  |  valid_loss: 0.957\n",
      "[epoch: 15, i:  4749]  train_loss: 0.765  |  valid_loss: 0.904\n",
      "[epoch: 15, i:  4874]  train_loss: 0.786  |  valid_loss: 0.815\n",
      "[epoch: 15, i:  4999]  train_loss: 0.836  |  valid_loss: 0.769\n",
      "[epoch: 15, i:  5124]  train_loss: 0.760  |  valid_loss: 0.833\n",
      "[epoch: 15, i:  5249]  train_loss: 0.846  |  valid_loss: 0.901\n",
      "[epoch: 15, i:  5374]  train_loss: 0.803  |  valid_loss: 0.646\n",
      "[epoch: 15, i:  5499]  train_loss: 0.767  |  valid_loss: 0.720\n",
      "[epoch: 15, i:  5624]  train_loss: 0.859  |  valid_loss: 0.917\n",
      "[epoch: 15, i:  5749]  train_loss: 0.871  |  valid_loss: 0.821\n",
      "[epoch: 15, i:  5874]  train_loss: 0.791  |  valid_loss: 0.738\n",
      "[epoch: 15, i:  5999]  train_loss: 0.809  |  valid_loss: 0.843\n",
      "[epoch: 15, i:  6124]  train_loss: 0.873  |  valid_loss: 0.780\n",
      "[epoch: 15, i:  6249]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 15, i:  6374]  train_loss: 0.798  |  valid_loss: 0.754\n",
      "[epoch: 15, i:  6499]  train_loss: 0.849  |  valid_loss: 0.810\n",
      "[epoch: 15, i:  6624]  train_loss: 0.872  |  valid_loss: 0.708\n",
      "[epoch: 15, i:  6749]  train_loss: 0.788  |  valid_loss: 0.842\n",
      "[epoch: 15, i:  6874]  train_loss: 0.851  |  valid_loss: 0.782\n",
      "[epoch: 15, i:  6999]  train_loss: 0.750  |  valid_loss: 0.951\n",
      "[epoch: 15, i:  7124]  train_loss: 0.872  |  valid_loss: 0.958\n",
      "[epoch: 15, i:  7249]  train_loss: 0.836  |  valid_loss: 0.654\n",
      "[epoch: 15, i:  7374]  train_loss: 0.781  |  valid_loss: 1.072\n",
      "[epoch: 15, i:  7499]  train_loss: 0.790  |  valid_loss: 0.937\n",
      "[epoch: 15, i:  7624]  train_loss: 0.777  |  valid_loss: 0.908\n",
      "[epoch: 15, i:  7749]  train_loss: 0.801  |  valid_loss: 0.811\n",
      "[epoch: 15, i:  7874]  train_loss: 0.772  |  valid_loss: 0.866\n",
      "[epoch: 15, i:  7999]  train_loss: 0.786  |  valid_loss: 0.730\n",
      "[epoch: 15, i:  8124]  train_loss: 0.844  |  valid_loss: 0.941\n",
      "[epoch: 15, i:  8249]  train_loss: 0.851  |  valid_loss: 0.891\n",
      "[epoch: 15, i:  8374]  train_loss: 0.833  |  valid_loss: 0.864\n",
      "[epoch: 15, i:  8499]  train_loss: 0.860  |  valid_loss: 0.920\n",
      "[epoch: 15, i:  8624]  train_loss: 0.828  |  valid_loss: 0.851\n",
      "[epoch: 15, i:  8749]  train_loss: 0.905  |  valid_loss: 0.871\n",
      "[epoch: 15, i:  8874]  train_loss: 0.876  |  valid_loss: 0.869\n",
      "[epoch: 15, i:  8999]  train_loss: 0.848  |  valid_loss: 0.721\n",
      "[epoch: 15, i:  9124]  train_loss: 0.820  |  valid_loss: 0.776\n",
      "[epoch: 15, i:  9249]  train_loss: 0.823  |  valid_loss: 0.626\n",
      "[epoch: 15, i:  9374]  train_loss: 0.751  |  valid_loss: 0.900\n",
      "[epoch: 15, i:  9499]  train_loss: 0.848  |  valid_loss: 0.799\n",
      "[epoch: 15, i:  9624]  train_loss: 0.799  |  valid_loss: 0.921\n",
      "[epoch: 15, i:  9749]  train_loss: 0.863  |  valid_loss: 0.945\n",
      "[epoch: 15, i:  9874]  train_loss: 0.803  |  valid_loss: 0.883\n",
      "[epoch: 15, i:  9999]  train_loss: 0.854  |  valid_loss: 0.806\n",
      "[epoch: 15, i: 10124]  train_loss: 0.906  |  valid_loss: 0.773\n",
      "[epoch: 15, i: 10249]  train_loss: 0.805  |  valid_loss: 0.876\n",
      "[epoch: 15, i: 10374]  train_loss: 0.866  |  valid_loss: 0.749\n",
      "[epoch: 15, i: 10499]  train_loss: 0.820  |  valid_loss: 0.992\n",
      "[epoch: 15, i: 10624]  train_loss: 0.840  |  valid_loss: 0.947\n",
      "[epoch: 15, i: 10749]  train_loss: 0.870  |  valid_loss: 0.850\n",
      "[epoch: 15, i: 10874]  train_loss: 0.858  |  valid_loss: 1.000\n",
      "[epoch: 15, i: 10999]  train_loss: 0.881  |  valid_loss: 0.960\n",
      "[epoch: 15, i: 11124]  train_loss: 0.805  |  valid_loss: 0.816\n",
      "[epoch: 15, i: 11249]  train_loss: 0.879  |  valid_loss: 0.880\n",
      "[epoch: 15, i: 11374]  train_loss: 0.808  |  valid_loss: 0.814\n",
      "[epoch: 15, i: 11499]  train_loss: 0.862  |  valid_loss: 0.613\n",
      "[epoch: 15, i: 11624]  train_loss: 0.823  |  valid_loss: 0.786\n",
      "[epoch: 15, i: 11749]  train_loss: 0.798  |  valid_loss: 0.766\n",
      "[epoch: 15, i: 11874]  train_loss: 0.814  |  valid_loss: 0.812\n",
      "[epoch: 15, i: 11999]  train_loss: 0.768  |  valid_loss: 0.724\n",
      "[epoch: 15, i: 12124]  train_loss: 0.789  |  valid_loss: 0.746\n",
      "[epoch: 15, i: 12249]  train_loss: 0.849  |  valid_loss: 1.043\n",
      "[epoch: 15, i: 12374]  train_loss: 0.888  |  valid_loss: 0.905\n",
      "[epoch: 15, i: 12499]  train_loss: 0.805  |  valid_loss: 0.875\n",
      "--> [End of epoch 15] train_accuracy: 70.76%  |  valid_accuracy: 70.46%\n",
      "--> [Start of epoch 16]  lr: 0.000056\n",
      "[epoch: 16, i:   124]  train_loss: 0.995  |  valid_loss: 0.772\n",
      "[epoch: 16, i:   249]  train_loss: 0.818  |  valid_loss: 0.740\n",
      "[epoch: 16, i:   374]  train_loss: 0.853  |  valid_loss: 0.844\n",
      "[epoch: 16, i:   499]  train_loss: 0.781  |  valid_loss: 0.785\n",
      "[epoch: 16, i:   624]  train_loss: 0.819  |  valid_loss: 0.931\n",
      "[epoch: 16, i:   749]  train_loss: 0.802  |  valid_loss: 0.606\n",
      "[epoch: 16, i:   874]  train_loss: 0.867  |  valid_loss: 0.882\n",
      "[epoch: 16, i:   999]  train_loss: 0.819  |  valid_loss: 0.956\n",
      "[epoch: 16, i:  1124]  train_loss: 0.896  |  valid_loss: 0.932\n",
      "[epoch: 16, i:  1249]  train_loss: 0.851  |  valid_loss: 0.728\n",
      "[epoch: 16, i:  1374]  train_loss: 0.803  |  valid_loss: 0.793\n",
      "[epoch: 16, i:  1499]  train_loss: 0.801  |  valid_loss: 0.843\n",
      "[epoch: 16, i:  1624]  train_loss: 0.860  |  valid_loss: 0.691\n",
      "[epoch: 16, i:  1749]  train_loss: 0.827  |  valid_loss: 0.944\n",
      "[epoch: 16, i:  1874]  train_loss: 0.835  |  valid_loss: 0.783\n",
      "[epoch: 16, i:  1999]  train_loss: 0.880  |  valid_loss: 1.011\n",
      "[epoch: 16, i:  2124]  train_loss: 0.824  |  valid_loss: 0.717\n",
      "[epoch: 16, i:  2249]  train_loss: 0.776  |  valid_loss: 0.863\n",
      "[epoch: 16, i:  2374]  train_loss: 0.832  |  valid_loss: 0.789\n",
      "[epoch: 16, i:  2499]  train_loss: 0.789  |  valid_loss: 1.010\n",
      "[epoch: 16, i:  2624]  train_loss: 0.816  |  valid_loss: 0.960\n",
      "[epoch: 16, i:  2749]  train_loss: 0.833  |  valid_loss: 0.938\n",
      "[epoch: 16, i:  2874]  train_loss: 0.827  |  valid_loss: 1.052\n",
      "[epoch: 16, i:  2999]  train_loss: 0.805  |  valid_loss: 0.845\n",
      "[epoch: 16, i:  3124]  train_loss: 0.867  |  valid_loss: 0.905\n",
      "[epoch: 16, i:  3249]  train_loss: 0.921  |  valid_loss: 1.120\n",
      "[epoch: 16, i:  3374]  train_loss: 0.876  |  valid_loss: 0.790\n",
      "[epoch: 16, i:  3499]  train_loss: 0.816  |  valid_loss: 0.753\n",
      "[epoch: 16, i:  3624]  train_loss: 0.787  |  valid_loss: 0.823\n",
      "[epoch: 16, i:  3749]  train_loss: 0.820  |  valid_loss: 0.692\n",
      "[epoch: 16, i:  3874]  train_loss: 0.893  |  valid_loss: 0.800\n",
      "[epoch: 16, i:  3999]  train_loss: 0.790  |  valid_loss: 0.645\n",
      "[epoch: 16, i:  4124]  train_loss: 0.751  |  valid_loss: 0.847\n",
      "[epoch: 16, i:  4249]  train_loss: 0.837  |  valid_loss: 0.890\n",
      "[epoch: 16, i:  4374]  train_loss: 0.779  |  valid_loss: 0.905\n",
      "[epoch: 16, i:  4499]  train_loss: 0.805  |  valid_loss: 0.770\n",
      "[epoch: 16, i:  4624]  train_loss: 0.779  |  valid_loss: 0.920\n",
      "[epoch: 16, i:  4749]  train_loss: 0.751  |  valid_loss: 0.893\n",
      "[epoch: 16, i:  4874]  train_loss: 0.852  |  valid_loss: 0.744\n",
      "[epoch: 16, i:  4999]  train_loss: 0.791  |  valid_loss: 0.743\n",
      "[epoch: 16, i:  5124]  train_loss: 0.728  |  valid_loss: 0.812\n",
      "[epoch: 16, i:  5249]  train_loss: 0.789  |  valid_loss: 0.899\n",
      "[epoch: 16, i:  5374]  train_loss: 0.823  |  valid_loss: 0.627\n",
      "[epoch: 16, i:  5499]  train_loss: 0.858  |  valid_loss: 0.732\n",
      "[epoch: 16, i:  5624]  train_loss: 0.814  |  valid_loss: 0.907\n",
      "[epoch: 16, i:  5749]  train_loss: 0.775  |  valid_loss: 0.831\n",
      "[epoch: 16, i:  5874]  train_loss: 0.845  |  valid_loss: 0.736\n",
      "[epoch: 16, i:  5999]  train_loss: 0.902  |  valid_loss: 0.896\n",
      "[epoch: 16, i:  6124]  train_loss: 0.762  |  valid_loss: 0.772\n",
      "[epoch: 16, i:  6249]  train_loss: 0.778  |  valid_loss: 0.894\n",
      "[epoch: 16, i:  6374]  train_loss: 0.821  |  valid_loss: 0.723\n",
      "[epoch: 16, i:  6499]  train_loss: 0.875  |  valid_loss: 0.768\n",
      "[epoch: 16, i:  6624]  train_loss: 0.867  |  valid_loss: 0.715\n",
      "[epoch: 16, i:  6749]  train_loss: 0.776  |  valid_loss: 0.857\n",
      "[epoch: 16, i:  6874]  train_loss: 0.905  |  valid_loss: 0.779\n",
      "[epoch: 16, i:  6999]  train_loss: 0.823  |  valid_loss: 0.976\n",
      "[epoch: 16, i:  7124]  train_loss: 0.762  |  valid_loss: 0.982\n",
      "[epoch: 16, i:  7249]  train_loss: 0.860  |  valid_loss: 0.648\n",
      "[epoch: 16, i:  7374]  train_loss: 0.799  |  valid_loss: 1.017\n",
      "[epoch: 16, i:  7499]  train_loss: 0.764  |  valid_loss: 0.962\n",
      "[epoch: 16, i:  7624]  train_loss: 0.838  |  valid_loss: 0.945\n",
      "[epoch: 16, i:  7749]  train_loss: 0.899  |  valid_loss: 0.799\n",
      "[epoch: 16, i:  7874]  train_loss: 0.836  |  valid_loss: 0.886\n",
      "[epoch: 16, i:  7999]  train_loss: 0.841  |  valid_loss: 0.710\n",
      "[epoch: 16, i:  8124]  train_loss: 0.809  |  valid_loss: 0.946\n",
      "[epoch: 16, i:  8249]  train_loss: 0.841  |  valid_loss: 0.903\n",
      "[epoch: 16, i:  8374]  train_loss: 0.850  |  valid_loss: 0.832\n",
      "[epoch: 16, i:  8499]  train_loss: 0.824  |  valid_loss: 0.910\n",
      "[epoch: 16, i:  8624]  train_loss: 0.827  |  valid_loss: 0.856\n",
      "[epoch: 16, i:  8749]  train_loss: 0.814  |  valid_loss: 0.921\n",
      "[epoch: 16, i:  8874]  train_loss: 0.777  |  valid_loss: 0.872\n",
      "[epoch: 16, i:  8999]  train_loss: 0.816  |  valid_loss: 0.688\n",
      "[epoch: 16, i:  9124]  train_loss: 0.830  |  valid_loss: 0.725\n",
      "[epoch: 16, i:  9249]  train_loss: 0.784  |  valid_loss: 0.624\n",
      "[epoch: 16, i:  9374]  train_loss: 0.828  |  valid_loss: 0.892\n",
      "[epoch: 16, i:  9499]  train_loss: 0.870  |  valid_loss: 0.752\n",
      "[epoch: 16, i:  9624]  train_loss: 0.835  |  valid_loss: 0.894\n",
      "[epoch: 16, i:  9749]  train_loss: 0.761  |  valid_loss: 0.925\n",
      "[epoch: 16, i:  9874]  train_loss: 0.869  |  valid_loss: 0.900\n",
      "[epoch: 16, i:  9999]  train_loss: 0.820  |  valid_loss: 0.840\n",
      "[epoch: 16, i: 10124]  train_loss: 0.785  |  valid_loss: 0.744\n",
      "[epoch: 16, i: 10249]  train_loss: 0.792  |  valid_loss: 0.877\n",
      "[epoch: 16, i: 10374]  train_loss: 0.853  |  valid_loss: 0.734\n",
      "[epoch: 16, i: 10499]  train_loss: 0.840  |  valid_loss: 0.935\n",
      "[epoch: 16, i: 10624]  train_loss: 0.853  |  valid_loss: 0.976\n",
      "[epoch: 16, i: 10749]  train_loss: 0.864  |  valid_loss: 0.847\n",
      "[epoch: 16, i: 10874]  train_loss: 0.895  |  valid_loss: 0.987\n",
      "[epoch: 16, i: 10999]  train_loss: 0.830  |  valid_loss: 0.985\n",
      "[epoch: 16, i: 11124]  train_loss: 0.825  |  valid_loss: 0.861\n",
      "[epoch: 16, i: 11249]  train_loss: 0.837  |  valid_loss: 0.895\n",
      "[epoch: 16, i: 11374]  train_loss: 0.855  |  valid_loss: 0.814\n",
      "[epoch: 16, i: 11499]  train_loss: 0.800  |  valid_loss: 0.639\n",
      "[epoch: 16, i: 11624]  train_loss: 0.805  |  valid_loss: 0.792\n",
      "[epoch: 16, i: 11749]  train_loss: 0.849  |  valid_loss: 0.767\n",
      "[epoch: 16, i: 11874]  train_loss: 0.783  |  valid_loss: 0.814\n",
      "[epoch: 16, i: 11999]  train_loss: 0.857  |  valid_loss: 0.719\n",
      "[epoch: 16, i: 12124]  train_loss: 0.795  |  valid_loss: 0.725\n",
      "[epoch: 16, i: 12249]  train_loss: 0.801  |  valid_loss: 1.039\n",
      "[epoch: 16, i: 12374]  train_loss: 0.865  |  valid_loss: 0.891\n",
      "[epoch: 16, i: 12499]  train_loss: 0.830  |  valid_loss: 0.881\n",
      "--> [End of epoch 16] train_accuracy: 70.86%  |  valid_accuracy: 70.93%\n",
      "--> [Start of epoch 17]  lr: 0.000045\n",
      "[epoch: 17, i:   124]  train_loss: 0.867  |  valid_loss: 0.769\n",
      "[epoch: 17, i:   249]  train_loss: 0.743  |  valid_loss: 0.725\n",
      "[epoch: 17, i:   374]  train_loss: 0.790  |  valid_loss: 0.891\n",
      "[epoch: 17, i:   499]  train_loss: 0.751  |  valid_loss: 0.801\n",
      "[epoch: 17, i:   624]  train_loss: 0.850  |  valid_loss: 0.927\n",
      "[epoch: 17, i:   749]  train_loss: 0.803  |  valid_loss: 0.588\n",
      "[epoch: 17, i:   874]  train_loss: 0.829  |  valid_loss: 0.879\n",
      "[epoch: 17, i:   999]  train_loss: 0.873  |  valid_loss: 0.933\n",
      "[epoch: 17, i:  1124]  train_loss: 0.800  |  valid_loss: 0.909\n",
      "[epoch: 17, i:  1249]  train_loss: 0.822  |  valid_loss: 0.751\n",
      "[epoch: 17, i:  1374]  train_loss: 0.824  |  valid_loss: 0.787\n",
      "[epoch: 17, i:  1499]  train_loss: 0.860  |  valid_loss: 0.819\n",
      "[epoch: 17, i:  1624]  train_loss: 0.765  |  valid_loss: 0.680\n",
      "[epoch: 17, i:  1749]  train_loss: 0.856  |  valid_loss: 0.995\n",
      "[epoch: 17, i:  1874]  train_loss: 0.803  |  valid_loss: 0.800\n",
      "[epoch: 17, i:  1999]  train_loss: 0.785  |  valid_loss: 1.021\n",
      "[epoch: 17, i:  2124]  train_loss: 0.782  |  valid_loss: 0.714\n",
      "[epoch: 17, i:  2249]  train_loss: 0.819  |  valid_loss: 0.845\n",
      "[epoch: 17, i:  2374]  train_loss: 0.887  |  valid_loss: 0.782\n",
      "[epoch: 17, i:  2499]  train_loss: 0.767  |  valid_loss: 1.012\n",
      "[epoch: 17, i:  2624]  train_loss: 0.818  |  valid_loss: 0.934\n",
      "[epoch: 17, i:  2749]  train_loss: 0.882  |  valid_loss: 0.982\n",
      "[epoch: 17, i:  2874]  train_loss: 0.786  |  valid_loss: 1.032\n",
      "[epoch: 17, i:  2999]  train_loss: 0.860  |  valid_loss: 0.847\n",
      "[epoch: 17, i:  3124]  train_loss: 0.824  |  valid_loss: 0.909\n",
      "[epoch: 17, i:  3249]  train_loss: 0.852  |  valid_loss: 1.058\n",
      "[epoch: 17, i:  3374]  train_loss: 0.800  |  valid_loss: 0.787\n",
      "[epoch: 17, i:  3499]  train_loss: 0.778  |  valid_loss: 0.767\n",
      "[epoch: 17, i:  3624]  train_loss: 0.869  |  valid_loss: 0.829\n",
      "[epoch: 17, i:  3749]  train_loss: 0.875  |  valid_loss: 0.706\n",
      "[epoch: 17, i:  3874]  train_loss: 0.841  |  valid_loss: 0.794\n",
      "[epoch: 17, i:  3999]  train_loss: 0.880  |  valid_loss: 0.655\n",
      "[epoch: 17, i:  4124]  train_loss: 0.750  |  valid_loss: 0.881\n",
      "[epoch: 17, i:  4249]  train_loss: 0.886  |  valid_loss: 0.896\n",
      "[epoch: 17, i:  4374]  train_loss: 0.861  |  valid_loss: 0.882\n",
      "[epoch: 17, i:  4499]  train_loss: 0.787  |  valid_loss: 0.803\n",
      "[epoch: 17, i:  4624]  train_loss: 0.772  |  valid_loss: 0.934\n",
      "[epoch: 17, i:  4749]  train_loss: 0.772  |  valid_loss: 0.890\n",
      "[epoch: 17, i:  4874]  train_loss: 0.892  |  valid_loss: 0.778\n",
      "[epoch: 17, i:  4999]  train_loss: 0.797  |  valid_loss: 0.780\n",
      "[epoch: 17, i:  5124]  train_loss: 0.831  |  valid_loss: 0.837\n",
      "[epoch: 17, i:  5249]  train_loss: 0.900  |  valid_loss: 0.906\n",
      "[epoch: 17, i:  5374]  train_loss: 0.821  |  valid_loss: 0.646\n",
      "[epoch: 17, i:  5499]  train_loss: 0.870  |  valid_loss: 0.725\n",
      "[epoch: 17, i:  5624]  train_loss: 0.821  |  valid_loss: 0.948\n",
      "[epoch: 17, i:  5749]  train_loss: 0.780  |  valid_loss: 0.848\n",
      "[epoch: 17, i:  5874]  train_loss: 0.800  |  valid_loss: 0.730\n",
      "[epoch: 17, i:  5999]  train_loss: 0.785  |  valid_loss: 0.815\n",
      "[epoch: 17, i:  6124]  train_loss: 0.800  |  valid_loss: 0.733\n",
      "[epoch: 17, i:  6249]  train_loss: 0.776  |  valid_loss: 0.883\n",
      "[epoch: 17, i:  6374]  train_loss: 0.826  |  valid_loss: 0.741\n",
      "[epoch: 17, i:  6499]  train_loss: 0.895  |  valid_loss: 0.785\n",
      "[epoch: 17, i:  6624]  train_loss: 0.861  |  valid_loss: 0.725\n",
      "[epoch: 17, i:  6749]  train_loss: 0.870  |  valid_loss: 0.776\n",
      "[epoch: 17, i:  6874]  train_loss: 0.819  |  valid_loss: 0.809\n",
      "[epoch: 17, i:  6999]  train_loss: 0.808  |  valid_loss: 0.997\n",
      "[epoch: 17, i:  7124]  train_loss: 0.831  |  valid_loss: 0.957\n",
      "[epoch: 17, i:  7249]  train_loss: 0.841  |  valid_loss: 0.657\n",
      "[epoch: 17, i:  7374]  train_loss: 0.746  |  valid_loss: 1.024\n",
      "[epoch: 17, i:  7499]  train_loss: 0.836  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  7624]  train_loss: 0.867  |  valid_loss: 0.949\n",
      "[epoch: 17, i:  7749]  train_loss: 0.836  |  valid_loss: 0.832\n",
      "[epoch: 17, i:  7874]  train_loss: 0.759  |  valid_loss: 0.879\n",
      "[epoch: 17, i:  7999]  train_loss: 0.924  |  valid_loss: 0.724\n",
      "[epoch: 17, i:  8124]  train_loss: 0.794  |  valid_loss: 0.944\n",
      "[epoch: 17, i:  8249]  train_loss: 0.787  |  valid_loss: 0.904\n",
      "[epoch: 17, i:  8374]  train_loss: 0.860  |  valid_loss: 0.845\n",
      "[epoch: 17, i:  8499]  train_loss: 0.798  |  valid_loss: 0.955\n",
      "[epoch: 17, i:  8624]  train_loss: 0.774  |  valid_loss: 0.851\n",
      "[epoch: 17, i:  8749]  train_loss: 0.748  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  8874]  train_loss: 0.860  |  valid_loss: 0.889\n",
      "[epoch: 17, i:  8999]  train_loss: 0.858  |  valid_loss: 0.697\n",
      "[epoch: 17, i:  9124]  train_loss: 0.695  |  valid_loss: 0.737\n",
      "[epoch: 17, i:  9249]  train_loss: 0.822  |  valid_loss: 0.608\n",
      "[epoch: 17, i:  9374]  train_loss: 0.845  |  valid_loss: 0.908\n",
      "[epoch: 17, i:  9499]  train_loss: 0.828  |  valid_loss: 0.782\n",
      "[epoch: 17, i:  9624]  train_loss: 0.764  |  valid_loss: 0.919\n",
      "[epoch: 17, i:  9749]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 17, i:  9874]  train_loss: 0.817  |  valid_loss: 0.898\n",
      "[epoch: 17, i:  9999]  train_loss: 0.871  |  valid_loss: 0.820\n",
      "[epoch: 17, i: 10124]  train_loss: 0.801  |  valid_loss: 0.762\n",
      "[epoch: 17, i: 10249]  train_loss: 0.846  |  valid_loss: 0.882\n",
      "[epoch: 17, i: 10374]  train_loss: 0.850  |  valid_loss: 0.745\n",
      "[epoch: 17, i: 10499]  train_loss: 0.833  |  valid_loss: 0.955\n",
      "[epoch: 17, i: 10624]  train_loss: 0.820  |  valid_loss: 0.964\n",
      "[epoch: 17, i: 10749]  train_loss: 0.829  |  valid_loss: 0.862\n",
      "[epoch: 17, i: 10874]  train_loss: 0.776  |  valid_loss: 0.959\n",
      "[epoch: 17, i: 10999]  train_loss: 0.886  |  valid_loss: 0.985\n",
      "[epoch: 17, i: 11124]  train_loss: 0.793  |  valid_loss: 0.837\n",
      "[epoch: 17, i: 11249]  train_loss: 0.770  |  valid_loss: 0.876\n",
      "[epoch: 17, i: 11374]  train_loss: 0.837  |  valid_loss: 0.796\n",
      "[epoch: 17, i: 11499]  train_loss: 0.785  |  valid_loss: 0.586\n",
      "[epoch: 17, i: 11624]  train_loss: 0.837  |  valid_loss: 0.806\n",
      "[epoch: 17, i: 11749]  train_loss: 0.861  |  valid_loss: 0.747\n",
      "[epoch: 17, i: 11874]  train_loss: 0.915  |  valid_loss: 0.768\n",
      "[epoch: 17, i: 11999]  train_loss: 0.819  |  valid_loss: 0.673\n",
      "[epoch: 17, i: 12124]  train_loss: 0.786  |  valid_loss: 0.716\n",
      "[epoch: 17, i: 12249]  train_loss: 0.870  |  valid_loss: 1.092\n",
      "[epoch: 17, i: 12374]  train_loss: 0.862  |  valid_loss: 0.886\n",
      "[epoch: 17, i: 12499]  train_loss: 0.823  |  valid_loss: 0.888\n",
      "--> [End of epoch 17] train_accuracy: 71.02%  |  valid_accuracy: 71.04%\n",
      "--> [Start of epoch 18]  lr: 0.000036\n",
      "[epoch: 18, i:   124]  train_loss: 0.880  |  valid_loss: 0.730\n",
      "[epoch: 18, i:   249]  train_loss: 0.792  |  valid_loss: 0.758\n",
      "[epoch: 18, i:   374]  train_loss: 0.871  |  valid_loss: 0.865\n",
      "[epoch: 18, i:   499]  train_loss: 0.834  |  valid_loss: 0.783\n",
      "[epoch: 18, i:   624]  train_loss: 0.808  |  valid_loss: 0.930\n",
      "[epoch: 18, i:   749]  train_loss: 0.816  |  valid_loss: 0.566\n",
      "[epoch: 18, i:   874]  train_loss: 0.863  |  valid_loss: 0.859\n",
      "[epoch: 18, i:   999]  train_loss: 0.830  |  valid_loss: 0.946\n",
      "[epoch: 18, i:  1124]  train_loss: 0.826  |  valid_loss: 0.891\n",
      "[epoch: 18, i:  1249]  train_loss: 0.769  |  valid_loss: 0.755\n",
      "[epoch: 18, i:  1374]  train_loss: 0.796  |  valid_loss: 0.836\n",
      "[epoch: 18, i:  1499]  train_loss: 0.724  |  valid_loss: 0.820\n",
      "[epoch: 18, i:  1624]  train_loss: 0.835  |  valid_loss: 0.697\n",
      "[epoch: 18, i:  1749]  train_loss: 0.784  |  valid_loss: 1.024\n",
      "[epoch: 18, i:  1874]  train_loss: 0.792  |  valid_loss: 0.795\n",
      "[epoch: 18, i:  1999]  train_loss: 0.741  |  valid_loss: 1.009\n",
      "[epoch: 18, i:  2124]  train_loss: 0.739  |  valid_loss: 0.726\n",
      "[epoch: 18, i:  2249]  train_loss: 0.821  |  valid_loss: 0.837\n",
      "[epoch: 18, i:  2374]  train_loss: 0.889  |  valid_loss: 0.766\n",
      "[epoch: 18, i:  2499]  train_loss: 0.852  |  valid_loss: 0.990\n",
      "[epoch: 18, i:  2624]  train_loss: 0.779  |  valid_loss: 0.894\n",
      "[epoch: 18, i:  2749]  train_loss: 0.802  |  valid_loss: 0.956\n",
      "[epoch: 18, i:  2874]  train_loss: 0.882  |  valid_loss: 0.998\n",
      "[epoch: 18, i:  2999]  train_loss: 0.766  |  valid_loss: 0.870\n",
      "[epoch: 18, i:  3124]  train_loss: 0.822  |  valid_loss: 0.910\n",
      "[epoch: 18, i:  3249]  train_loss: 0.812  |  valid_loss: 1.060\n",
      "[epoch: 18, i:  3374]  train_loss: 0.829  |  valid_loss: 0.809\n",
      "[epoch: 18, i:  3499]  train_loss: 0.772  |  valid_loss: 0.773\n",
      "[epoch: 18, i:  3624]  train_loss: 0.760  |  valid_loss: 0.791\n",
      "[epoch: 18, i:  3749]  train_loss: 0.778  |  valid_loss: 0.721\n",
      "[epoch: 18, i:  3874]  train_loss: 0.877  |  valid_loss: 0.819\n",
      "[epoch: 18, i:  3999]  train_loss: 0.847  |  valid_loss: 0.646\n",
      "[epoch: 18, i:  4124]  train_loss: 0.812  |  valid_loss: 0.864\n",
      "[epoch: 18, i:  4249]  train_loss: 0.779  |  valid_loss: 0.895\n",
      "[epoch: 18, i:  4374]  train_loss: 0.844  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  4499]  train_loss: 0.802  |  valid_loss: 0.787\n",
      "[epoch: 18, i:  4624]  train_loss: 0.803  |  valid_loss: 0.963\n",
      "[epoch: 18, i:  4749]  train_loss: 0.799  |  valid_loss: 0.903\n",
      "[epoch: 18, i:  4874]  train_loss: 0.828  |  valid_loss: 0.764\n",
      "[epoch: 18, i:  4999]  train_loss: 0.833  |  valid_loss: 0.760\n",
      "[epoch: 18, i:  5124]  train_loss: 0.820  |  valid_loss: 0.801\n",
      "[epoch: 18, i:  5249]  train_loss: 0.829  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  5374]  train_loss: 0.852  |  valid_loss: 0.630\n",
      "[epoch: 18, i:  5499]  train_loss: 0.816  |  valid_loss: 0.695\n",
      "[epoch: 18, i:  5624]  train_loss: 0.823  |  valid_loss: 0.979\n",
      "[epoch: 18, i:  5749]  train_loss: 0.896  |  valid_loss: 0.824\n",
      "[epoch: 18, i:  5874]  train_loss: 0.840  |  valid_loss: 0.732\n",
      "[epoch: 18, i:  5999]  train_loss: 0.822  |  valid_loss: 0.863\n",
      "[epoch: 18, i:  6124]  train_loss: 0.782  |  valid_loss: 0.759\n",
      "[epoch: 18, i:  6249]  train_loss: 0.759  |  valid_loss: 0.850\n",
      "[epoch: 18, i:  6374]  train_loss: 0.848  |  valid_loss: 0.739\n",
      "[epoch: 18, i:  6499]  train_loss: 0.813  |  valid_loss: 0.773\n",
      "[epoch: 18, i:  6624]  train_loss: 0.800  |  valid_loss: 0.667\n",
      "[epoch: 18, i:  6749]  train_loss: 0.838  |  valid_loss: 0.829\n",
      "[epoch: 18, i:  6874]  train_loss: 0.844  |  valid_loss: 0.767\n",
      "[epoch: 18, i:  6999]  train_loss: 0.779  |  valid_loss: 1.001\n",
      "[epoch: 18, i:  7124]  train_loss: 0.805  |  valid_loss: 0.995\n",
      "[epoch: 18, i:  7249]  train_loss: 0.828  |  valid_loss: 0.674\n",
      "[epoch: 18, i:  7374]  train_loss: 0.936  |  valid_loss: 1.016\n",
      "[epoch: 18, i:  7499]  train_loss: 0.855  |  valid_loss: 0.912\n",
      "[epoch: 18, i:  7624]  train_loss: 0.855  |  valid_loss: 0.911\n",
      "[epoch: 18, i:  7749]  train_loss: 0.856  |  valid_loss: 0.807\n",
      "[epoch: 18, i:  7874]  train_loss: 0.865  |  valid_loss: 0.876\n",
      "[epoch: 18, i:  7999]  train_loss: 0.856  |  valid_loss: 0.726\n",
      "[epoch: 18, i:  8124]  train_loss: 0.851  |  valid_loss: 0.952\n",
      "[epoch: 18, i:  8249]  train_loss: 0.773  |  valid_loss: 0.873\n",
      "[epoch: 18, i:  8374]  train_loss: 0.804  |  valid_loss: 0.867\n",
      "[epoch: 18, i:  8499]  train_loss: 0.774  |  valid_loss: 0.937\n",
      "[epoch: 18, i:  8624]  train_loss: 0.790  |  valid_loss: 0.885\n",
      "[epoch: 18, i:  8749]  train_loss: 0.800  |  valid_loss: 0.897\n",
      "[epoch: 18, i:  8874]  train_loss: 0.863  |  valid_loss: 0.889\n",
      "[epoch: 18, i:  8999]  train_loss: 0.845  |  valid_loss: 0.703\n",
      "[epoch: 18, i:  9124]  train_loss: 0.846  |  valid_loss: 0.766\n",
      "[epoch: 18, i:  9249]  train_loss: 0.814  |  valid_loss: 0.631\n",
      "[epoch: 18, i:  9374]  train_loss: 0.841  |  valid_loss: 0.902\n",
      "[epoch: 18, i:  9499]  train_loss: 0.829  |  valid_loss: 0.764\n",
      "[epoch: 18, i:  9624]  train_loss: 0.822  |  valid_loss: 0.886\n",
      "[epoch: 18, i:  9749]  train_loss: 0.822  |  valid_loss: 0.920\n",
      "[epoch: 18, i:  9874]  train_loss: 0.824  |  valid_loss: 0.911\n",
      "[epoch: 18, i:  9999]  train_loss: 0.779  |  valid_loss: 0.844\n",
      "[epoch: 18, i: 10124]  train_loss: 0.833  |  valid_loss: 0.774\n",
      "[epoch: 18, i: 10249]  train_loss: 0.834  |  valid_loss: 0.858\n",
      "[epoch: 18, i: 10374]  train_loss: 0.822  |  valid_loss: 0.748\n",
      "[epoch: 18, i: 10499]  train_loss: 0.825  |  valid_loss: 0.996\n",
      "[epoch: 18, i: 10624]  train_loss: 0.790  |  valid_loss: 0.928\n",
      "[epoch: 18, i: 10749]  train_loss: 0.801  |  valid_loss: 0.846\n",
      "[epoch: 18, i: 10874]  train_loss: 0.843  |  valid_loss: 0.971\n",
      "[epoch: 18, i: 10999]  train_loss: 0.804  |  valid_loss: 0.958\n",
      "[epoch: 18, i: 11124]  train_loss: 0.753  |  valid_loss: 0.816\n",
      "[epoch: 18, i: 11249]  train_loss: 0.866  |  valid_loss: 0.854\n",
      "[epoch: 18, i: 11374]  train_loss: 0.872  |  valid_loss: 0.805\n",
      "[epoch: 18, i: 11499]  train_loss: 0.791  |  valid_loss: 0.611\n",
      "[epoch: 18, i: 11624]  train_loss: 0.837  |  valid_loss: 0.815\n",
      "[epoch: 18, i: 11749]  train_loss: 0.767  |  valid_loss: 0.753\n",
      "[epoch: 18, i: 11874]  train_loss: 0.870  |  valid_loss: 0.813\n",
      "[epoch: 18, i: 11999]  train_loss: 0.875  |  valid_loss: 0.702\n",
      "[epoch: 18, i: 12124]  train_loss: 0.853  |  valid_loss: 0.765\n",
      "[epoch: 18, i: 12249]  train_loss: 0.824  |  valid_loss: 1.063\n",
      "[epoch: 18, i: 12374]  train_loss: 0.841  |  valid_loss: 0.898\n",
      "[epoch: 18, i: 12499]  train_loss: 0.871  |  valid_loss: 0.878\n",
      "--> [End of epoch 18] train_accuracy: 71.17%  |  valid_accuracy: 70.52%\n",
      "--> [Start of epoch 19]  lr: 0.000029\n",
      "[epoch: 19, i:   124]  train_loss: 0.884  |  valid_loss: 0.738\n",
      "[epoch: 19, i:   249]  train_loss: 0.837  |  valid_loss: 0.729\n",
      "[epoch: 19, i:   374]  train_loss: 0.802  |  valid_loss: 0.855\n",
      "[epoch: 19, i:   499]  train_loss: 0.850  |  valid_loss: 0.805\n",
      "[epoch: 19, i:   624]  train_loss: 0.786  |  valid_loss: 0.941\n",
      "[epoch: 19, i:   749]  train_loss: 0.844  |  valid_loss: 0.604\n",
      "[epoch: 19, i:   874]  train_loss: 0.807  |  valid_loss: 0.874\n",
      "[epoch: 19, i:   999]  train_loss: 0.829  |  valid_loss: 0.914\n",
      "[epoch: 19, i:  1124]  train_loss: 0.790  |  valid_loss: 0.944\n",
      "[epoch: 19, i:  1249]  train_loss: 0.822  |  valid_loss: 0.702\n",
      "[epoch: 19, i:  1374]  train_loss: 0.827  |  valid_loss: 0.819\n",
      "[epoch: 19, i:  1499]  train_loss: 0.821  |  valid_loss: 0.817\n",
      "[epoch: 19, i:  1624]  train_loss: 0.832  |  valid_loss: 0.692\n",
      "[epoch: 19, i:  1749]  train_loss: 0.807  |  valid_loss: 0.984\n",
      "[epoch: 19, i:  1874]  train_loss: 0.840  |  valid_loss: 0.780\n",
      "[epoch: 19, i:  1999]  train_loss: 0.774  |  valid_loss: 0.985\n",
      "[epoch: 19, i:  2124]  train_loss: 0.787  |  valid_loss: 0.734\n",
      "[epoch: 19, i:  2249]  train_loss: 0.822  |  valid_loss: 0.803\n",
      "[epoch: 19, i:  2374]  train_loss: 0.900  |  valid_loss: 0.792\n",
      "[epoch: 19, i:  2499]  train_loss: 0.811  |  valid_loss: 1.022\n",
      "[epoch: 19, i:  2624]  train_loss: 0.811  |  valid_loss: 0.934\n",
      "[epoch: 19, i:  2749]  train_loss: 0.717  |  valid_loss: 0.954\n",
      "[epoch: 19, i:  2874]  train_loss: 0.806  |  valid_loss: 0.997\n",
      "[epoch: 19, i:  2999]  train_loss: 0.818  |  valid_loss: 0.841\n",
      "[epoch: 19, i:  3124]  train_loss: 0.883  |  valid_loss: 0.911\n",
      "[epoch: 19, i:  3249]  train_loss: 0.736  |  valid_loss: 1.086\n",
      "[epoch: 19, i:  3374]  train_loss: 0.840  |  valid_loss: 0.784\n",
      "[epoch: 19, i:  3499]  train_loss: 0.794  |  valid_loss: 0.769\n",
      "[epoch: 19, i:  3624]  train_loss: 0.810  |  valid_loss: 0.800\n",
      "[epoch: 19, i:  3749]  train_loss: 0.732  |  valid_loss: 0.697\n",
      "[epoch: 19, i:  3874]  train_loss: 0.793  |  valid_loss: 0.826\n",
      "[epoch: 19, i:  3999]  train_loss: 0.953  |  valid_loss: 0.656\n",
      "[epoch: 19, i:  4124]  train_loss: 0.828  |  valid_loss: 0.870\n",
      "[epoch: 19, i:  4249]  train_loss: 0.870  |  valid_loss: 0.933\n",
      "[epoch: 19, i:  4374]  train_loss: 0.824  |  valid_loss: 0.872\n",
      "[epoch: 19, i:  4499]  train_loss: 0.882  |  valid_loss: 0.785\n",
      "[epoch: 19, i:  4624]  train_loss: 0.781  |  valid_loss: 0.963\n",
      "[epoch: 19, i:  4749]  train_loss: 0.840  |  valid_loss: 0.887\n",
      "[epoch: 19, i:  4874]  train_loss: 0.767  |  valid_loss: 0.737\n",
      "[epoch: 19, i:  4999]  train_loss: 0.833  |  valid_loss: 0.761\n",
      "[epoch: 19, i:  5124]  train_loss: 0.792  |  valid_loss: 0.799\n",
      "[epoch: 19, i:  5249]  train_loss: 0.811  |  valid_loss: 0.902\n",
      "[epoch: 19, i:  5374]  train_loss: 0.880  |  valid_loss: 0.646\n",
      "[epoch: 19, i:  5499]  train_loss: 0.798  |  valid_loss: 0.700\n",
      "[epoch: 19, i:  5624]  train_loss: 0.861  |  valid_loss: 1.028\n",
      "[epoch: 19, i:  5749]  train_loss: 0.748  |  valid_loss: 0.833\n",
      "[epoch: 19, i:  5874]  train_loss: 0.851  |  valid_loss: 0.741\n",
      "[epoch: 19, i:  5999]  train_loss: 0.914  |  valid_loss: 0.843\n",
      "[epoch: 19, i:  6124]  train_loss: 0.859  |  valid_loss: 0.755\n",
      "[epoch: 19, i:  6249]  train_loss: 0.786  |  valid_loss: 0.857\n",
      "[epoch: 19, i:  6374]  train_loss: 0.795  |  valid_loss: 0.749\n",
      "[epoch: 19, i:  6499]  train_loss: 0.773  |  valid_loss: 0.785\n",
      "[epoch: 19, i:  6624]  train_loss: 0.859  |  valid_loss: 0.675\n",
      "[epoch: 19, i:  6749]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 19, i:  6874]  train_loss: 0.778  |  valid_loss: 0.780\n",
      "[epoch: 19, i:  6999]  train_loss: 0.822  |  valid_loss: 0.972\n",
      "[epoch: 19, i:  7124]  train_loss: 0.803  |  valid_loss: 0.947\n",
      "[epoch: 19, i:  7249]  train_loss: 0.839  |  valid_loss: 0.665\n",
      "[epoch: 19, i:  7374]  train_loss: 0.778  |  valid_loss: 1.018\n",
      "[epoch: 19, i:  7499]  train_loss: 0.838  |  valid_loss: 0.920\n",
      "[epoch: 19, i:  7624]  train_loss: 0.804  |  valid_loss: 0.896\n",
      "[epoch: 19, i:  7749]  train_loss: 0.794  |  valid_loss: 0.833\n",
      "[epoch: 19, i:  7874]  train_loss: 0.816  |  valid_loss: 0.873\n",
      "[epoch: 19, i:  7999]  train_loss: 0.819  |  valid_loss: 0.722\n",
      "[epoch: 19, i:  8124]  train_loss: 0.809  |  valid_loss: 0.938\n",
      "[epoch: 19, i:  8249]  train_loss: 0.764  |  valid_loss: 0.894\n",
      "[epoch: 19, i:  8374]  train_loss: 0.767  |  valid_loss: 0.876\n",
      "[epoch: 19, i:  8499]  train_loss: 0.760  |  valid_loss: 0.920\n",
      "[epoch: 19, i:  8624]  train_loss: 0.776  |  valid_loss: 0.855\n",
      "[epoch: 19, i:  8749]  train_loss: 0.822  |  valid_loss: 0.892\n",
      "[epoch: 19, i:  8874]  train_loss: 0.858  |  valid_loss: 0.856\n",
      "[epoch: 19, i:  8999]  train_loss: 0.770  |  valid_loss: 0.684\n",
      "[epoch: 19, i:  9124]  train_loss: 0.816  |  valid_loss: 0.748\n",
      "[epoch: 19, i:  9249]  train_loss: 0.816  |  valid_loss: 0.609\n",
      "[epoch: 19, i:  9374]  train_loss: 0.898  |  valid_loss: 0.881\n",
      "[epoch: 19, i:  9499]  train_loss: 0.842  |  valid_loss: 0.755\n",
      "[epoch: 19, i:  9624]  train_loss: 0.832  |  valid_loss: 0.866\n",
      "[epoch: 19, i:  9749]  train_loss: 0.877  |  valid_loss: 0.938\n",
      "[epoch: 19, i:  9874]  train_loss: 0.803  |  valid_loss: 0.881\n",
      "[epoch: 19, i:  9999]  train_loss: 0.872  |  valid_loss: 0.810\n",
      "[epoch: 19, i: 10124]  train_loss: 0.863  |  valid_loss: 0.769\n",
      "[epoch: 19, i: 10249]  train_loss: 0.848  |  valid_loss: 0.863\n",
      "[epoch: 19, i: 10374]  train_loss: 0.808  |  valid_loss: 0.740\n",
      "[epoch: 19, i: 10499]  train_loss: 0.806  |  valid_loss: 0.949\n",
      "[epoch: 19, i: 10624]  train_loss: 0.853  |  valid_loss: 0.932\n",
      "[epoch: 19, i: 10749]  train_loss: 0.887  |  valid_loss: 0.858\n",
      "[epoch: 19, i: 10874]  train_loss: 0.787  |  valid_loss: 0.960\n",
      "[epoch: 19, i: 10999]  train_loss: 0.800  |  valid_loss: 0.969\n",
      "[epoch: 19, i: 11124]  train_loss: 0.708  |  valid_loss: 0.823\n",
      "[epoch: 19, i: 11249]  train_loss: 0.770  |  valid_loss: 0.868\n",
      "[epoch: 19, i: 11374]  train_loss: 0.831  |  valid_loss: 0.824\n",
      "[epoch: 19, i: 11499]  train_loss: 0.818  |  valid_loss: 0.641\n",
      "[epoch: 19, i: 11624]  train_loss: 0.829  |  valid_loss: 0.812\n",
      "[epoch: 19, i: 11749]  train_loss: 0.892  |  valid_loss: 0.756\n",
      "[epoch: 19, i: 11874]  train_loss: 0.791  |  valid_loss: 0.807\n",
      "[epoch: 19, i: 11999]  train_loss: 0.859  |  valid_loss: 0.687\n",
      "[epoch: 19, i: 12124]  train_loss: 0.864  |  valid_loss: 0.768\n",
      "[epoch: 19, i: 12249]  train_loss: 0.787  |  valid_loss: 1.088\n",
      "[epoch: 19, i: 12374]  train_loss: 0.838  |  valid_loss: 0.909\n",
      "[epoch: 19, i: 12499]  train_loss: 0.853  |  valid_loss: 0.860\n",
      "--> [End of epoch 19] train_accuracy: 71.12%  |  valid_accuracy: 70.64%\n",
      "--> [Start of epoch 20]  lr: 0.000023\n",
      "[epoch: 20, i:   124]  train_loss: 0.785  |  valid_loss: 0.742\n",
      "[epoch: 20, i:   249]  train_loss: 0.896  |  valid_loss: 0.742\n",
      "[epoch: 20, i:   374]  train_loss: 0.817  |  valid_loss: 0.862\n",
      "[epoch: 20, i:   499]  train_loss: 0.766  |  valid_loss: 0.793\n",
      "[epoch: 20, i:   624]  train_loss: 0.787  |  valid_loss: 0.924\n",
      "[epoch: 20, i:   749]  train_loss: 0.760  |  valid_loss: 0.568\n",
      "[epoch: 20, i:   874]  train_loss: 0.733  |  valid_loss: 0.874\n",
      "[epoch: 20, i:   999]  train_loss: 0.859  |  valid_loss: 0.951\n",
      "[epoch: 20, i:  1124]  train_loss: 0.819  |  valid_loss: 0.937\n",
      "[epoch: 20, i:  1249]  train_loss: 0.863  |  valid_loss: 0.770\n",
      "[epoch: 20, i:  1374]  train_loss: 0.903  |  valid_loss: 0.781\n",
      "[epoch: 20, i:  1499]  train_loss: 0.837  |  valid_loss: 0.817\n",
      "[epoch: 20, i:  1624]  train_loss: 0.829  |  valid_loss: 0.692\n",
      "[epoch: 20, i:  1749]  train_loss: 0.865  |  valid_loss: 0.997\n",
      "[epoch: 20, i:  1874]  train_loss: 0.820  |  valid_loss: 0.760\n",
      "[epoch: 20, i:  1999]  train_loss: 0.788  |  valid_loss: 1.017\n",
      "[epoch: 20, i:  2124]  train_loss: 0.809  |  valid_loss: 0.706\n",
      "[epoch: 20, i:  2249]  train_loss: 0.849  |  valid_loss: 0.847\n",
      "[epoch: 20, i:  2374]  train_loss: 0.856  |  valid_loss: 0.770\n",
      "[epoch: 20, i:  2499]  train_loss: 0.776  |  valid_loss: 1.029\n",
      "[epoch: 20, i:  2624]  train_loss: 0.855  |  valid_loss: 0.918\n",
      "[epoch: 20, i:  2749]  train_loss: 0.798  |  valid_loss: 0.949\n",
      "[epoch: 20, i:  2874]  train_loss: 0.777  |  valid_loss: 0.975\n",
      "[epoch: 20, i:  2999]  train_loss: 0.847  |  valid_loss: 0.852\n",
      "[epoch: 20, i:  3124]  train_loss: 0.813  |  valid_loss: 0.905\n",
      "[epoch: 20, i:  3249]  train_loss: 0.744  |  valid_loss: 1.070\n",
      "[epoch: 20, i:  3374]  train_loss: 0.843  |  valid_loss: 0.798\n",
      "[epoch: 20, i:  3499]  train_loss: 0.795  |  valid_loss: 0.754\n",
      "[epoch: 20, i:  3624]  train_loss: 0.757  |  valid_loss: 0.776\n",
      "[epoch: 20, i:  3749]  train_loss: 0.820  |  valid_loss: 0.700\n",
      "[epoch: 20, i:  3874]  train_loss: 0.804  |  valid_loss: 0.810\n",
      "[epoch: 20, i:  3999]  train_loss: 0.810  |  valid_loss: 0.645\n",
      "[epoch: 20, i:  4124]  train_loss: 0.756  |  valid_loss: 0.839\n",
      "[epoch: 20, i:  4249]  train_loss: 0.881  |  valid_loss: 0.891\n",
      "[epoch: 20, i:  4374]  train_loss: 0.768  |  valid_loss: 0.860\n",
      "[epoch: 20, i:  4499]  train_loss: 0.847  |  valid_loss: 0.818\n",
      "[epoch: 20, i:  4624]  train_loss: 0.791  |  valid_loss: 0.948\n",
      "[epoch: 20, i:  4749]  train_loss: 0.779  |  valid_loss: 0.866\n",
      "[epoch: 20, i:  4874]  train_loss: 0.779  |  valid_loss: 0.727\n",
      "[epoch: 20, i:  4999]  train_loss: 0.837  |  valid_loss: 0.753\n",
      "[epoch: 20, i:  5124]  train_loss: 0.795  |  valid_loss: 0.809\n",
      "[epoch: 20, i:  5249]  train_loss: 0.824  |  valid_loss: 0.884\n",
      "[epoch: 20, i:  5374]  train_loss: 0.866  |  valid_loss: 0.635\n",
      "[epoch: 20, i:  5499]  train_loss: 0.819  |  valid_loss: 0.724\n",
      "[epoch: 20, i:  5624]  train_loss: 0.788  |  valid_loss: 0.976\n",
      "[epoch: 20, i:  5749]  train_loss: 0.838  |  valid_loss: 0.812\n",
      "[epoch: 20, i:  5874]  train_loss: 0.836  |  valid_loss: 0.740\n",
      "[epoch: 20, i:  5999]  train_loss: 0.825  |  valid_loss: 0.856\n",
      "[epoch: 20, i:  6124]  train_loss: 0.800  |  valid_loss: 0.763\n",
      "[epoch: 20, i:  6249]  train_loss: 0.790  |  valid_loss: 0.861\n",
      "[epoch: 20, i:  6374]  train_loss: 0.862  |  valid_loss: 0.723\n",
      "[epoch: 20, i:  6499]  train_loss: 0.742  |  valid_loss: 0.786\n",
      "[epoch: 20, i:  6624]  train_loss: 0.740  |  valid_loss: 0.700\n",
      "[epoch: 20, i:  6749]  train_loss: 0.762  |  valid_loss: 0.803\n",
      "[epoch: 20, i:  6874]  train_loss: 0.858  |  valid_loss: 0.747\n",
      "[epoch: 20, i:  6999]  train_loss: 0.787  |  valid_loss: 0.982\n",
      "[epoch: 20, i:  7124]  train_loss: 0.784  |  valid_loss: 0.975\n",
      "[epoch: 20, i:  7249]  train_loss: 0.824  |  valid_loss: 0.667\n",
      "[epoch: 20, i:  7374]  train_loss: 0.850  |  valid_loss: 1.025\n",
      "[epoch: 20, i:  7499]  train_loss: 0.792  |  valid_loss: 0.896\n",
      "[epoch: 20, i:  7624]  train_loss: 0.842  |  valid_loss: 0.927\n",
      "[epoch: 20, i:  7749]  train_loss: 0.752  |  valid_loss: 0.812\n",
      "[epoch: 20, i:  7874]  train_loss: 0.738  |  valid_loss: 0.854\n",
      "[epoch: 20, i:  7999]  train_loss: 0.754  |  valid_loss: 0.713\n",
      "[epoch: 20, i:  8124]  train_loss: 0.750  |  valid_loss: 0.924\n",
      "[epoch: 20, i:  8249]  train_loss: 0.871  |  valid_loss: 0.888\n",
      "[epoch: 20, i:  8374]  train_loss: 0.806  |  valid_loss: 0.837\n",
      "[epoch: 20, i:  8499]  train_loss: 0.834  |  valid_loss: 0.919\n",
      "[epoch: 20, i:  8624]  train_loss: 0.783  |  valid_loss: 0.876\n",
      "[epoch: 20, i:  8749]  train_loss: 0.899  |  valid_loss: 0.899\n",
      "[epoch: 20, i:  8874]  train_loss: 0.790  |  valid_loss: 0.814\n",
      "[epoch: 20, i:  8999]  train_loss: 0.803  |  valid_loss: 0.686\n",
      "[epoch: 20, i:  9124]  train_loss: 0.797  |  valid_loss: 0.760\n",
      "[epoch: 20, i:  9249]  train_loss: 0.792  |  valid_loss: 0.616\n",
      "[epoch: 20, i:  9374]  train_loss: 0.842  |  valid_loss: 0.886\n",
      "[epoch: 20, i:  9499]  train_loss: 0.802  |  valid_loss: 0.768\n",
      "[epoch: 20, i:  9624]  train_loss: 0.849  |  valid_loss: 0.887\n",
      "[epoch: 20, i:  9749]  train_loss: 0.844  |  valid_loss: 0.894\n",
      "[epoch: 20, i:  9874]  train_loss: 0.836  |  valid_loss: 0.904\n",
      "[epoch: 20, i:  9999]  train_loss: 0.834  |  valid_loss: 0.815\n",
      "[epoch: 20, i: 10124]  train_loss: 0.814  |  valid_loss: 0.766\n",
      "[epoch: 20, i: 10249]  train_loss: 0.735  |  valid_loss: 0.827\n",
      "[epoch: 20, i: 10374]  train_loss: 0.790  |  valid_loss: 0.733\n",
      "[epoch: 20, i: 10499]  train_loss: 0.858  |  valid_loss: 0.977\n",
      "[epoch: 20, i: 10624]  train_loss: 0.793  |  valid_loss: 0.939\n",
      "[epoch: 20, i: 10749]  train_loss: 0.917  |  valid_loss: 0.848\n",
      "[epoch: 20, i: 10874]  train_loss: 0.847  |  valid_loss: 0.955\n",
      "[epoch: 20, i: 10999]  train_loss: 0.782  |  valid_loss: 0.983\n",
      "[epoch: 20, i: 11124]  train_loss: 0.811  |  valid_loss: 0.828\n",
      "[epoch: 20, i: 11249]  train_loss: 0.902  |  valid_loss: 0.876\n",
      "[epoch: 20, i: 11374]  train_loss: 0.855  |  valid_loss: 0.777\n",
      "[epoch: 20, i: 11499]  train_loss: 0.822  |  valid_loss: 0.648\n",
      "[epoch: 20, i: 11624]  train_loss: 0.763  |  valid_loss: 0.809\n",
      "[epoch: 20, i: 11749]  train_loss: 0.794  |  valid_loss: 0.731\n",
      "[epoch: 20, i: 11874]  train_loss: 0.817  |  valid_loss: 0.790\n",
      "[epoch: 20, i: 11999]  train_loss: 0.871  |  valid_loss: 0.706\n",
      "[epoch: 20, i: 12124]  train_loss: 0.841  |  valid_loss: 0.724\n",
      "[epoch: 20, i: 12249]  train_loss: 0.841  |  valid_loss: 1.048\n",
      "[epoch: 20, i: 12374]  train_loss: 0.819  |  valid_loss: 0.883\n",
      "[epoch: 20, i: 12499]  train_loss: 0.868  |  valid_loss: 0.855\n",
      "--> [End of epoch 20] train_accuracy: 71.26%  |  valid_accuracy: 70.68%\n",
      "--> [Start of epoch 21]  lr: 0.000018\n",
      "[epoch: 21, i:   124]  train_loss: 0.787  |  valid_loss: 0.743\n",
      "[epoch: 21, i:   249]  train_loss: 0.770  |  valid_loss: 0.706\n",
      "[epoch: 21, i:   374]  train_loss: 0.897  |  valid_loss: 0.852\n",
      "[epoch: 21, i:   499]  train_loss: 0.853  |  valid_loss: 0.811\n",
      "[epoch: 21, i:   624]  train_loss: 0.823  |  valid_loss: 0.905\n",
      "[epoch: 21, i:   749]  train_loss: 0.785  |  valid_loss: 0.597\n",
      "[epoch: 21, i:   874]  train_loss: 0.852  |  valid_loss: 0.840\n",
      "[epoch: 21, i:   999]  train_loss: 0.839  |  valid_loss: 0.958\n",
      "[epoch: 21, i:  1124]  train_loss: 0.852  |  valid_loss: 0.939\n",
      "[epoch: 21, i:  1249]  train_loss: 0.813  |  valid_loss: 0.757\n",
      "[epoch: 21, i:  1374]  train_loss: 0.821  |  valid_loss: 0.789\n",
      "[epoch: 21, i:  1499]  train_loss: 0.780  |  valid_loss: 0.809\n",
      "[epoch: 21, i:  1624]  train_loss: 0.805  |  valid_loss: 0.679\n",
      "[epoch: 21, i:  1749]  train_loss: 0.755  |  valid_loss: 1.025\n",
      "[epoch: 21, i:  1874]  train_loss: 0.827  |  valid_loss: 0.780\n",
      "[epoch: 21, i:  1999]  train_loss: 0.829  |  valid_loss: 1.023\n",
      "[epoch: 21, i:  2124]  train_loss: 0.913  |  valid_loss: 0.708\n",
      "[epoch: 21, i:  2249]  train_loss: 0.829  |  valid_loss: 0.821\n",
      "[epoch: 21, i:  2374]  train_loss: 0.824  |  valid_loss: 0.760\n",
      "[epoch: 21, i:  2499]  train_loss: 0.758  |  valid_loss: 0.992\n",
      "[epoch: 21, i:  2624]  train_loss: 0.830  |  valid_loss: 0.940\n",
      "[epoch: 21, i:  2749]  train_loss: 0.836  |  valid_loss: 0.932\n",
      "[epoch: 21, i:  2874]  train_loss: 0.785  |  valid_loss: 1.050\n",
      "[epoch: 21, i:  2999]  train_loss: 0.846  |  valid_loss: 0.855\n",
      "[epoch: 21, i:  3124]  train_loss: 0.839  |  valid_loss: 0.921\n",
      "[epoch: 21, i:  3249]  train_loss: 0.857  |  valid_loss: 1.061\n",
      "[epoch: 21, i:  3374]  train_loss: 0.823  |  valid_loss: 0.818\n",
      "[epoch: 21, i:  3499]  train_loss: 0.797  |  valid_loss: 0.765\n",
      "[epoch: 21, i:  3624]  train_loss: 0.790  |  valid_loss: 0.782\n",
      "[epoch: 21, i:  3749]  train_loss: 0.881  |  valid_loss: 0.690\n",
      "[epoch: 21, i:  3874]  train_loss: 0.850  |  valid_loss: 0.804\n",
      "[epoch: 21, i:  3999]  train_loss: 0.724  |  valid_loss: 0.644\n",
      "[epoch: 21, i:  4124]  train_loss: 0.700  |  valid_loss: 0.816\n",
      "[epoch: 21, i:  4249]  train_loss: 0.844  |  valid_loss: 0.878\n",
      "[epoch: 21, i:  4374]  train_loss: 0.789  |  valid_loss: 0.872\n",
      "[epoch: 21, i:  4499]  train_loss: 0.793  |  valid_loss: 0.789\n",
      "[epoch: 21, i:  4624]  train_loss: 0.792  |  valid_loss: 0.959\n",
      "[epoch: 21, i:  4749]  train_loss: 0.809  |  valid_loss: 0.926\n",
      "[epoch: 21, i:  4874]  train_loss: 0.794  |  valid_loss: 0.747\n",
      "[epoch: 21, i:  4999]  train_loss: 0.821  |  valid_loss: 0.766\n",
      "[epoch: 21, i:  5124]  train_loss: 0.806  |  valid_loss: 0.805\n",
      "[epoch: 21, i:  5249]  train_loss: 0.812  |  valid_loss: 0.879\n",
      "[epoch: 21, i:  5374]  train_loss: 0.807  |  valid_loss: 0.618\n",
      "[epoch: 21, i:  5499]  train_loss: 0.877  |  valid_loss: 0.695\n",
      "[epoch: 21, i:  5624]  train_loss: 0.806  |  valid_loss: 0.906\n",
      "[epoch: 21, i:  5749]  train_loss: 0.831  |  valid_loss: 0.841\n",
      "[epoch: 21, i:  5874]  train_loss: 0.786  |  valid_loss: 0.734\n",
      "[epoch: 21, i:  5999]  train_loss: 0.771  |  valid_loss: 0.843\n",
      "[epoch: 21, i:  6124]  train_loss: 0.788  |  valid_loss: 0.783\n",
      "[epoch: 21, i:  6249]  train_loss: 0.835  |  valid_loss: 0.898\n",
      "[epoch: 21, i:  6374]  train_loss: 0.816  |  valid_loss: 0.748\n",
      "[epoch: 21, i:  6499]  train_loss: 0.768  |  valid_loss: 0.791\n",
      "[epoch: 21, i:  6624]  train_loss: 0.832  |  valid_loss: 0.655\n",
      "[epoch: 21, i:  6749]  train_loss: 0.814  |  valid_loss: 0.808\n",
      "[epoch: 21, i:  6874]  train_loss: 0.819  |  valid_loss: 0.760\n",
      "[epoch: 21, i:  6999]  train_loss: 0.791  |  valid_loss: 1.013\n",
      "[epoch: 21, i:  7124]  train_loss: 0.840  |  valid_loss: 0.947\n",
      "[epoch: 21, i:  7249]  train_loss: 0.847  |  valid_loss: 0.650\n",
      "[epoch: 21, i:  7374]  train_loss: 0.792  |  valid_loss: 1.049\n",
      "[epoch: 21, i:  7499]  train_loss: 0.769  |  valid_loss: 0.910\n",
      "[epoch: 21, i:  7624]  train_loss: 0.809  |  valid_loss: 0.937\n",
      "[epoch: 21, i:  7749]  train_loss: 0.865  |  valid_loss: 0.793\n",
      "[epoch: 21, i:  7874]  train_loss: 0.869  |  valid_loss: 0.855\n",
      "[epoch: 21, i:  7999]  train_loss: 0.801  |  valid_loss: 0.725\n",
      "[epoch: 21, i:  8124]  train_loss: 0.865  |  valid_loss: 0.956\n",
      "[epoch: 21, i:  8249]  train_loss: 0.812  |  valid_loss: 0.920\n",
      "[epoch: 21, i:  8374]  train_loss: 0.835  |  valid_loss: 0.864\n",
      "[epoch: 21, i:  8499]  train_loss: 0.816  |  valid_loss: 0.913\n",
      "[epoch: 21, i:  8624]  train_loss: 0.786  |  valid_loss: 0.856\n",
      "[epoch: 21, i:  8749]  train_loss: 0.828  |  valid_loss: 0.889\n",
      "[epoch: 21, i:  8874]  train_loss: 0.729  |  valid_loss: 0.829\n",
      "[epoch: 21, i:  8999]  train_loss: 0.744  |  valid_loss: 0.675\n",
      "[epoch: 21, i:  9124]  train_loss: 0.772  |  valid_loss: 0.763\n",
      "[epoch: 21, i:  9249]  train_loss: 0.883  |  valid_loss: 0.628\n",
      "[epoch: 21, i:  9374]  train_loss: 0.829  |  valid_loss: 0.901\n",
      "[epoch: 21, i:  9499]  train_loss: 0.775  |  valid_loss: 0.779\n",
      "[epoch: 21, i:  9624]  train_loss: 0.839  |  valid_loss: 0.888\n",
      "[epoch: 21, i:  9749]  train_loss: 0.853  |  valid_loss: 0.946\n",
      "[epoch: 21, i:  9874]  train_loss: 0.791  |  valid_loss: 0.897\n",
      "[epoch: 21, i:  9999]  train_loss: 0.892  |  valid_loss: 0.814\n",
      "[epoch: 21, i: 10124]  train_loss: 0.758  |  valid_loss: 0.764\n",
      "[epoch: 21, i: 10249]  train_loss: 0.834  |  valid_loss: 0.911\n",
      "[epoch: 21, i: 10374]  train_loss: 0.830  |  valid_loss: 0.764\n",
      "[epoch: 21, i: 10499]  train_loss: 0.786  |  valid_loss: 0.970\n",
      "[epoch: 21, i: 10624]  train_loss: 0.862  |  valid_loss: 0.941\n",
      "[epoch: 21, i: 10749]  train_loss: 0.838  |  valid_loss: 0.839\n",
      "[epoch: 21, i: 10874]  train_loss: 0.800  |  valid_loss: 0.960\n",
      "[epoch: 21, i: 10999]  train_loss: 0.784  |  valid_loss: 0.959\n",
      "[epoch: 21, i: 11124]  train_loss: 0.742  |  valid_loss: 0.828\n",
      "[epoch: 21, i: 11249]  train_loss: 0.805  |  valid_loss: 0.878\n",
      "[epoch: 21, i: 11374]  train_loss: 0.822  |  valid_loss: 0.806\n",
      "[epoch: 21, i: 11499]  train_loss: 0.845  |  valid_loss: 0.619\n",
      "[epoch: 21, i: 11624]  train_loss: 0.732  |  valid_loss: 0.809\n",
      "[epoch: 21, i: 11749]  train_loss: 0.851  |  valid_loss: 0.760\n",
      "[epoch: 21, i: 11874]  train_loss: 0.858  |  valid_loss: 0.789\n",
      "[epoch: 21, i: 11999]  train_loss: 0.818  |  valid_loss: 0.685\n",
      "[epoch: 21, i: 12124]  train_loss: 0.878  |  valid_loss: 0.721\n",
      "[epoch: 21, i: 12249]  train_loss: 0.873  |  valid_loss: 1.057\n",
      "[epoch: 21, i: 12374]  train_loss: 0.857  |  valid_loss: 0.879\n",
      "[epoch: 21, i: 12499]  train_loss: 0.833  |  valid_loss: 0.877\n",
      "--> [End of epoch 21] train_accuracy: 71.34%  |  valid_accuracy: 71.14%\n",
      "--> [Start of epoch 22]  lr: 0.000015\n",
      "[epoch: 22, i:   124]  train_loss: 0.772  |  valid_loss: 0.753\n",
      "[epoch: 22, i:   249]  train_loss: 0.775  |  valid_loss: 0.735\n",
      "[epoch: 22, i:   374]  train_loss: 0.820  |  valid_loss: 0.894\n",
      "[epoch: 22, i:   499]  train_loss: 0.819  |  valid_loss: 0.788\n",
      "[epoch: 22, i:   624]  train_loss: 0.807  |  valid_loss: 0.911\n",
      "[epoch: 22, i:   749]  train_loss: 0.814  |  valid_loss: 0.592\n",
      "[epoch: 22, i:   874]  train_loss: 0.842  |  valid_loss: 0.903\n",
      "[epoch: 22, i:   999]  train_loss: 0.803  |  valid_loss: 0.911\n",
      "[epoch: 22, i:  1124]  train_loss: 0.826  |  valid_loss: 0.902\n",
      "[epoch: 22, i:  1249]  train_loss: 0.850  |  valid_loss: 0.716\n",
      "[epoch: 22, i:  1374]  train_loss: 0.853  |  valid_loss: 0.789\n",
      "[epoch: 22, i:  1499]  train_loss: 0.777  |  valid_loss: 0.834\n",
      "[epoch: 22, i:  1624]  train_loss: 0.779  |  valid_loss: 0.688\n",
      "[epoch: 22, i:  1749]  train_loss: 0.774  |  valid_loss: 1.007\n",
      "[epoch: 22, i:  1874]  train_loss: 0.718  |  valid_loss: 0.761\n",
      "[epoch: 22, i:  1999]  train_loss: 0.832  |  valid_loss: 0.993\n",
      "[epoch: 22, i:  2124]  train_loss: 0.760  |  valid_loss: 0.698\n",
      "[epoch: 22, i:  2249]  train_loss: 0.761  |  valid_loss: 0.829\n",
      "[epoch: 22, i:  2374]  train_loss: 0.761  |  valid_loss: 0.772\n",
      "[epoch: 22, i:  2499]  train_loss: 0.791  |  valid_loss: 0.996\n",
      "[epoch: 22, i:  2624]  train_loss: 0.829  |  valid_loss: 0.932\n",
      "[epoch: 22, i:  2749]  train_loss: 0.817  |  valid_loss: 0.938\n",
      "[epoch: 22, i:  2874]  train_loss: 0.835  |  valid_loss: 0.983\n",
      "[epoch: 22, i:  2999]  train_loss: 0.872  |  valid_loss: 0.843\n",
      "[epoch: 22, i:  3124]  train_loss: 0.808  |  valid_loss: 0.908\n",
      "[epoch: 22, i:  3249]  train_loss: 0.789  |  valid_loss: 1.072\n",
      "[epoch: 22, i:  3374]  train_loss: 0.816  |  valid_loss: 0.822\n",
      "[epoch: 22, i:  3499]  train_loss: 0.793  |  valid_loss: 0.769\n",
      "[epoch: 22, i:  3624]  train_loss: 0.826  |  valid_loss: 0.798\n",
      "[epoch: 22, i:  3749]  train_loss: 0.815  |  valid_loss: 0.680\n",
      "[epoch: 22, i:  3874]  train_loss: 0.835  |  valid_loss: 0.806\n",
      "[epoch: 22, i:  3999]  train_loss: 0.803  |  valid_loss: 0.624\n",
      "[epoch: 22, i:  4124]  train_loss: 0.800  |  valid_loss: 0.835\n",
      "[epoch: 22, i:  4249]  train_loss: 0.861  |  valid_loss: 0.883\n",
      "[epoch: 22, i:  4374]  train_loss: 0.844  |  valid_loss: 0.896\n",
      "[epoch: 22, i:  4499]  train_loss: 0.757  |  valid_loss: 0.792\n",
      "[epoch: 22, i:  4624]  train_loss: 0.847  |  valid_loss: 0.961\n",
      "[epoch: 22, i:  4749]  train_loss: 0.836  |  valid_loss: 0.873\n",
      "[epoch: 22, i:  4874]  train_loss: 0.795  |  valid_loss: 0.786\n",
      "[epoch: 22, i:  4999]  train_loss: 0.799  |  valid_loss: 0.757\n",
      "[epoch: 22, i:  5124]  train_loss: 0.813  |  valid_loss: 0.820\n",
      "[epoch: 22, i:  5249]  train_loss: 0.764  |  valid_loss: 0.885\n",
      "[epoch: 22, i:  5374]  train_loss: 0.795  |  valid_loss: 0.625\n",
      "[epoch: 22, i:  5499]  train_loss: 0.806  |  valid_loss: 0.697\n",
      "[epoch: 22, i:  5624]  train_loss: 0.861  |  valid_loss: 0.937\n",
      "[epoch: 22, i:  5749]  train_loss: 0.782  |  valid_loss: 0.824\n",
      "[epoch: 22, i:  5874]  train_loss: 0.833  |  valid_loss: 0.713\n",
      "[epoch: 22, i:  5999]  train_loss: 0.825  |  valid_loss: 0.875\n",
      "[epoch: 22, i:  6124]  train_loss: 0.806  |  valid_loss: 0.725\n",
      "[epoch: 22, i:  6249]  train_loss: 0.786  |  valid_loss: 0.888\n",
      "[epoch: 22, i:  6374]  train_loss: 0.802  |  valid_loss: 0.731\n",
      "[epoch: 22, i:  6499]  train_loss: 0.827  |  valid_loss: 0.763\n",
      "[epoch: 22, i:  6624]  train_loss: 0.887  |  valid_loss: 0.681\n",
      "[epoch: 22, i:  6749]  train_loss: 0.844  |  valid_loss: 0.812\n",
      "[epoch: 22, i:  6874]  train_loss: 0.816  |  valid_loss: 0.783\n",
      "[epoch: 22, i:  6999]  train_loss: 0.831  |  valid_loss: 0.955\n",
      "[epoch: 22, i:  7124]  train_loss: 0.848  |  valid_loss: 0.978\n",
      "[epoch: 22, i:  7249]  train_loss: 0.839  |  valid_loss: 0.676\n",
      "[epoch: 22, i:  7374]  train_loss: 0.767  |  valid_loss: 1.036\n",
      "[epoch: 22, i:  7499]  train_loss: 0.793  |  valid_loss: 0.898\n",
      "[epoch: 22, i:  7624]  train_loss: 0.844  |  valid_loss: 0.919\n",
      "[epoch: 22, i:  7749]  train_loss: 0.879  |  valid_loss: 0.788\n",
      "[epoch: 22, i:  7874]  train_loss: 0.728  |  valid_loss: 0.884\n",
      "[epoch: 22, i:  7999]  train_loss: 0.752  |  valid_loss: 0.715\n",
      "[epoch: 22, i:  8124]  train_loss: 0.820  |  valid_loss: 0.903\n",
      "[epoch: 22, i:  8249]  train_loss: 0.790  |  valid_loss: 0.865\n",
      "[epoch: 22, i:  8374]  train_loss: 0.814  |  valid_loss: 0.853\n",
      "[epoch: 22, i:  8499]  train_loss: 0.769  |  valid_loss: 0.895\n",
      "[epoch: 22, i:  8624]  train_loss: 0.727  |  valid_loss: 0.867\n",
      "[epoch: 22, i:  8749]  train_loss: 0.833  |  valid_loss: 0.899\n",
      "[epoch: 22, i:  8874]  train_loss: 0.806  |  valid_loss: 0.844\n",
      "[epoch: 22, i:  8999]  train_loss: 0.842  |  valid_loss: 0.688\n",
      "[epoch: 22, i:  9124]  train_loss: 0.822  |  valid_loss: 0.743\n",
      "[epoch: 22, i:  9249]  train_loss: 0.778  |  valid_loss: 0.620\n",
      "[epoch: 22, i:  9374]  train_loss: 0.822  |  valid_loss: 0.937\n",
      "[epoch: 22, i:  9499]  train_loss: 0.779  |  valid_loss: 0.763\n",
      "[epoch: 22, i:  9624]  train_loss: 0.849  |  valid_loss: 0.902\n",
      "[epoch: 22, i:  9749]  train_loss: 0.835  |  valid_loss: 0.918\n",
      "[epoch: 22, i:  9874]  train_loss: 0.848  |  valid_loss: 0.890\n",
      "[epoch: 22, i:  9999]  train_loss: 0.860  |  valid_loss: 0.811\n",
      "[epoch: 22, i: 10124]  train_loss: 0.788  |  valid_loss: 0.758\n",
      "[epoch: 22, i: 10249]  train_loss: 0.841  |  valid_loss: 0.867\n",
      "[epoch: 22, i: 10374]  train_loss: 0.853  |  valid_loss: 0.753\n",
      "[epoch: 22, i: 10499]  train_loss: 0.839  |  valid_loss: 0.978\n",
      "[epoch: 22, i: 10624]  train_loss: 0.810  |  valid_loss: 0.961\n",
      "[epoch: 22, i: 10749]  train_loss: 0.864  |  valid_loss: 0.871\n",
      "[epoch: 22, i: 10874]  train_loss: 0.842  |  valid_loss: 0.961\n",
      "[epoch: 22, i: 10999]  train_loss: 0.730  |  valid_loss: 0.963\n",
      "[epoch: 22, i: 11124]  train_loss: 0.806  |  valid_loss: 0.825\n",
      "[epoch: 22, i: 11249]  train_loss: 0.831  |  valid_loss: 0.857\n",
      "[epoch: 22, i: 11374]  train_loss: 0.802  |  valid_loss: 0.808\n",
      "[epoch: 22, i: 11499]  train_loss: 0.873  |  valid_loss: 0.603\n",
      "[epoch: 22, i: 11624]  train_loss: 0.746  |  valid_loss: 0.835\n",
      "[epoch: 22, i: 11749]  train_loss: 0.867  |  valid_loss: 0.762\n",
      "[epoch: 22, i: 11874]  train_loss: 0.927  |  valid_loss: 0.780\n",
      "[epoch: 22, i: 11999]  train_loss: 0.823  |  valid_loss: 0.712\n",
      "[epoch: 22, i: 12124]  train_loss: 0.866  |  valid_loss: 0.720\n",
      "[epoch: 22, i: 12249]  train_loss: 0.797  |  valid_loss: 1.079\n",
      "[epoch: 22, i: 12374]  train_loss: 0.837  |  valid_loss: 0.888\n",
      "[epoch: 22, i: 12499]  train_loss: 0.839  |  valid_loss: 0.865\n",
      "--> [End of epoch 22] train_accuracy: 71.11%  |  valid_accuracy: 70.87%\n",
      "--> [Start of epoch 23]  lr: 0.000012\n",
      "[epoch: 23, i:   124]  train_loss: 0.798  |  valid_loss: 0.739\n",
      "[epoch: 23, i:   249]  train_loss: 0.833  |  valid_loss: 0.735\n",
      "[epoch: 23, i:   374]  train_loss: 0.778  |  valid_loss: 0.848\n",
      "[epoch: 23, i:   499]  train_loss: 0.848  |  valid_loss: 0.781\n",
      "[epoch: 23, i:   624]  train_loss: 0.753  |  valid_loss: 0.898\n",
      "[epoch: 23, i:   749]  train_loss: 0.875  |  valid_loss: 0.570\n",
      "[epoch: 23, i:   874]  train_loss: 0.803  |  valid_loss: 0.859\n",
      "[epoch: 23, i:   999]  train_loss: 0.801  |  valid_loss: 0.952\n",
      "[epoch: 23, i:  1124]  train_loss: 0.854  |  valid_loss: 0.911\n",
      "[epoch: 23, i:  1249]  train_loss: 0.857  |  valid_loss: 0.734\n",
      "[epoch: 23, i:  1374]  train_loss: 0.820  |  valid_loss: 0.776\n",
      "[epoch: 23, i:  1499]  train_loss: 0.735  |  valid_loss: 0.811\n",
      "[epoch: 23, i:  1624]  train_loss: 0.727  |  valid_loss: 0.698\n",
      "[epoch: 23, i:  1749]  train_loss: 0.771  |  valid_loss: 0.967\n",
      "[epoch: 23, i:  1874]  train_loss: 0.759  |  valid_loss: 0.766\n",
      "[epoch: 23, i:  1999]  train_loss: 0.844  |  valid_loss: 1.028\n",
      "[epoch: 23, i:  2124]  train_loss: 0.788  |  valid_loss: 0.718\n",
      "[epoch: 23, i:  2249]  train_loss: 0.762  |  valid_loss: 0.810\n",
      "[epoch: 23, i:  2374]  train_loss: 0.803  |  valid_loss: 0.783\n",
      "[epoch: 23, i:  2499]  train_loss: 0.839  |  valid_loss: 0.997\n",
      "[epoch: 23, i:  2624]  train_loss: 0.851  |  valid_loss: 0.926\n",
      "[epoch: 23, i:  2749]  train_loss: 0.806  |  valid_loss: 0.915\n",
      "[epoch: 23, i:  2874]  train_loss: 0.808  |  valid_loss: 0.967\n",
      "[epoch: 23, i:  2999]  train_loss: 0.872  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  3124]  train_loss: 0.879  |  valid_loss: 0.893\n",
      "[epoch: 23, i:  3249]  train_loss: 0.895  |  valid_loss: 1.072\n",
      "[epoch: 23, i:  3374]  train_loss: 0.791  |  valid_loss: 0.794\n",
      "[epoch: 23, i:  3499]  train_loss: 0.843  |  valid_loss: 0.757\n",
      "[epoch: 23, i:  3624]  train_loss: 0.795  |  valid_loss: 0.801\n",
      "[epoch: 23, i:  3749]  train_loss: 0.748  |  valid_loss: 0.667\n",
      "[epoch: 23, i:  3874]  train_loss: 0.800  |  valid_loss: 0.781\n",
      "[epoch: 23, i:  3999]  train_loss: 0.810  |  valid_loss: 0.631\n",
      "[epoch: 23, i:  4124]  train_loss: 0.874  |  valid_loss: 0.869\n",
      "[epoch: 23, i:  4249]  train_loss: 0.922  |  valid_loss: 0.885\n",
      "[epoch: 23, i:  4374]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 23, i:  4499]  train_loss: 0.854  |  valid_loss: 0.772\n",
      "[epoch: 23, i:  4624]  train_loss: 0.806  |  valid_loss: 0.950\n",
      "[epoch: 23, i:  4749]  train_loss: 0.806  |  valid_loss: 0.893\n",
      "[epoch: 23, i:  4874]  train_loss: 0.821  |  valid_loss: 0.745\n",
      "[epoch: 23, i:  4999]  train_loss: 0.786  |  valid_loss: 0.765\n",
      "[epoch: 23, i:  5124]  train_loss: 0.729  |  valid_loss: 0.802\n",
      "[epoch: 23, i:  5249]  train_loss: 0.868  |  valid_loss: 0.885\n",
      "[epoch: 23, i:  5374]  train_loss: 0.784  |  valid_loss: 0.638\n",
      "[epoch: 23, i:  5499]  train_loss: 0.779  |  valid_loss: 0.703\n",
      "[epoch: 23, i:  5624]  train_loss: 0.737  |  valid_loss: 0.958\n",
      "[epoch: 23, i:  5749]  train_loss: 0.810  |  valid_loss: 0.828\n",
      "[epoch: 23, i:  5874]  train_loss: 0.806  |  valid_loss: 0.724\n",
      "[epoch: 23, i:  5999]  train_loss: 0.841  |  valid_loss: 0.854\n",
      "[epoch: 23, i:  6124]  train_loss: 0.804  |  valid_loss: 0.771\n",
      "[epoch: 23, i:  6249]  train_loss: 0.838  |  valid_loss: 0.889\n",
      "[epoch: 23, i:  6374]  train_loss: 0.763  |  valid_loss: 0.746\n",
      "[epoch: 23, i:  6499]  train_loss: 0.732  |  valid_loss: 0.760\n",
      "[epoch: 23, i:  6624]  train_loss: 0.813  |  valid_loss: 0.691\n",
      "[epoch: 23, i:  6749]  train_loss: 0.796  |  valid_loss: 0.797\n",
      "[epoch: 23, i:  6874]  train_loss: 0.784  |  valid_loss: 0.776\n",
      "[epoch: 23, i:  6999]  train_loss: 0.821  |  valid_loss: 0.955\n",
      "[epoch: 23, i:  7124]  train_loss: 0.749  |  valid_loss: 1.012\n",
      "[epoch: 23, i:  7249]  train_loss: 0.787  |  valid_loss: 0.637\n",
      "[epoch: 23, i:  7374]  train_loss: 0.937  |  valid_loss: 1.040\n",
      "[epoch: 23, i:  7499]  train_loss: 0.795  |  valid_loss: 0.909\n",
      "[epoch: 23, i:  7624]  train_loss: 0.787  |  valid_loss: 0.876\n",
      "[epoch: 23, i:  7749]  train_loss: 0.751  |  valid_loss: 0.819\n",
      "[epoch: 23, i:  7874]  train_loss: 0.805  |  valid_loss: 0.879\n",
      "[epoch: 23, i:  7999]  train_loss: 0.807  |  valid_loss: 0.729\n",
      "[epoch: 23, i:  8124]  train_loss: 0.832  |  valid_loss: 0.892\n",
      "[epoch: 23, i:  8249]  train_loss: 0.849  |  valid_loss: 0.892\n",
      "[epoch: 23, i:  8374]  train_loss: 0.920  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  8499]  train_loss: 0.871  |  valid_loss: 0.930\n",
      "[epoch: 23, i:  8624]  train_loss: 0.860  |  valid_loss: 0.836\n",
      "[epoch: 23, i:  8749]  train_loss: 0.772  |  valid_loss: 0.891\n",
      "[epoch: 23, i:  8874]  train_loss: 0.840  |  valid_loss: 0.825\n",
      "[epoch: 23, i:  8999]  train_loss: 0.799  |  valid_loss: 0.706\n",
      "[epoch: 23, i:  9124]  train_loss: 0.793  |  valid_loss: 0.736\n",
      "[epoch: 23, i:  9249]  train_loss: 0.795  |  valid_loss: 0.619\n",
      "[epoch: 23, i:  9374]  train_loss: 0.736  |  valid_loss: 0.907\n",
      "[epoch: 23, i:  9499]  train_loss: 0.787  |  valid_loss: 0.777\n",
      "[epoch: 23, i:  9624]  train_loss: 0.853  |  valid_loss: 0.877\n",
      "[epoch: 23, i:  9749]  train_loss: 0.806  |  valid_loss: 0.914\n",
      "[epoch: 23, i:  9874]  train_loss: 0.743  |  valid_loss: 0.922\n",
      "[epoch: 23, i:  9999]  train_loss: 0.857  |  valid_loss: 0.828\n",
      "[epoch: 23, i: 10124]  train_loss: 0.773  |  valid_loss: 0.758\n",
      "[epoch: 23, i: 10249]  train_loss: 0.836  |  valid_loss: 0.853\n",
      "[epoch: 23, i: 10374]  train_loss: 0.874  |  valid_loss: 0.766\n",
      "[epoch: 23, i: 10499]  train_loss: 0.882  |  valid_loss: 0.955\n",
      "[epoch: 23, i: 10624]  train_loss: 0.794  |  valid_loss: 0.921\n",
      "[epoch: 23, i: 10749]  train_loss: 0.875  |  valid_loss: 0.818\n",
      "[epoch: 23, i: 10874]  train_loss: 0.856  |  valid_loss: 0.946\n",
      "[epoch: 23, i: 10999]  train_loss: 0.779  |  valid_loss: 0.972\n",
      "[epoch: 23, i: 11124]  train_loss: 0.909  |  valid_loss: 0.845\n",
      "[epoch: 23, i: 11249]  train_loss: 0.832  |  valid_loss: 0.868\n",
      "[epoch: 23, i: 11374]  train_loss: 0.806  |  valid_loss: 0.796\n",
      "[epoch: 23, i: 11499]  train_loss: 0.790  |  valid_loss: 0.618\n",
      "[epoch: 23, i: 11624]  train_loss: 0.768  |  valid_loss: 0.812\n",
      "[epoch: 23, i: 11749]  train_loss: 0.789  |  valid_loss: 0.749\n",
      "[epoch: 23, i: 11874]  train_loss: 0.811  |  valid_loss: 0.792\n",
      "[epoch: 23, i: 11999]  train_loss: 0.866  |  valid_loss: 0.683\n",
      "[epoch: 23, i: 12124]  train_loss: 0.842  |  valid_loss: 0.716\n",
      "[epoch: 23, i: 12249]  train_loss: 0.809  |  valid_loss: 1.096\n",
      "[epoch: 23, i: 12374]  train_loss: 0.844  |  valid_loss: 0.886\n",
      "[epoch: 23, i: 12499]  train_loss: 0.785  |  valid_loss: 0.864\n",
      "--> [End of epoch 23] train_accuracy: 71.31%  |  valid_accuracy: 70.91%\n",
      "--> [Start of epoch 24]  lr: 0.000009\n",
      "[epoch: 24, i:   124]  train_loss: 0.802  |  valid_loss: 0.738\n",
      "[epoch: 24, i:   249]  train_loss: 0.802  |  valid_loss: 0.702\n",
      "[epoch: 24, i:   374]  train_loss: 0.784  |  valid_loss: 0.898\n",
      "[epoch: 24, i:   499]  train_loss: 0.818  |  valid_loss: 0.795\n",
      "[epoch: 24, i:   624]  train_loss: 0.754  |  valid_loss: 0.921\n",
      "[epoch: 24, i:   749]  train_loss: 0.830  |  valid_loss: 0.576\n",
      "[epoch: 24, i:   874]  train_loss: 0.819  |  valid_loss: 0.855\n",
      "[epoch: 24, i:   999]  train_loss: 0.857  |  valid_loss: 0.945\n",
      "[epoch: 24, i:  1124]  train_loss: 0.719  |  valid_loss: 0.949\n",
      "[epoch: 24, i:  1249]  train_loss: 0.788  |  valid_loss: 0.714\n",
      "[epoch: 24, i:  1374]  train_loss: 0.817  |  valid_loss: 0.791\n",
      "[epoch: 24, i:  1499]  train_loss: 0.798  |  valid_loss: 0.807\n",
      "[epoch: 24, i:  1624]  train_loss: 0.812  |  valid_loss: 0.689\n",
      "[epoch: 24, i:  1749]  train_loss: 0.863  |  valid_loss: 0.999\n",
      "[epoch: 24, i:  1874]  train_loss: 0.916  |  valid_loss: 0.790\n",
      "[epoch: 24, i:  1999]  train_loss: 0.849  |  valid_loss: 0.961\n",
      "[epoch: 24, i:  2124]  train_loss: 0.860  |  valid_loss: 0.726\n",
      "[epoch: 24, i:  2249]  train_loss: 0.851  |  valid_loss: 0.820\n",
      "[epoch: 24, i:  2374]  train_loss: 0.781  |  valid_loss: 0.757\n",
      "[epoch: 24, i:  2499]  train_loss: 0.796  |  valid_loss: 1.007\n",
      "[epoch: 24, i:  2624]  train_loss: 0.887  |  valid_loss: 0.923\n",
      "[epoch: 24, i:  2749]  train_loss: 0.774  |  valid_loss: 0.955\n",
      "[epoch: 24, i:  2874]  train_loss: 0.834  |  valid_loss: 1.007\n",
      "[epoch: 24, i:  2999]  train_loss: 0.854  |  valid_loss: 0.846\n",
      "[epoch: 24, i:  3124]  train_loss: 0.794  |  valid_loss: 0.896\n",
      "[epoch: 24, i:  3249]  train_loss: 0.763  |  valid_loss: 1.039\n",
      "[epoch: 24, i:  3374]  train_loss: 0.905  |  valid_loss: 0.786\n",
      "[epoch: 24, i:  3499]  train_loss: 0.840  |  valid_loss: 0.758\n",
      "[epoch: 24, i:  3624]  train_loss: 0.841  |  valid_loss: 0.802\n",
      "[epoch: 24, i:  3749]  train_loss: 0.786  |  valid_loss: 0.693\n",
      "[epoch: 24, i:  3874]  train_loss: 0.810  |  valid_loss: 0.829\n",
      "[epoch: 24, i:  3999]  train_loss: 0.841  |  valid_loss: 0.625\n",
      "[epoch: 24, i:  4124]  train_loss: 0.849  |  valid_loss: 0.851\n",
      "[epoch: 24, i:  4249]  train_loss: 0.814  |  valid_loss: 0.907\n",
      "[epoch: 24, i:  4374]  train_loss: 0.834  |  valid_loss: 0.890\n",
      "[epoch: 24, i:  4499]  train_loss: 0.818  |  valid_loss: 0.792\n",
      "[epoch: 24, i:  4624]  train_loss: 0.820  |  valid_loss: 0.931\n",
      "[epoch: 24, i:  4749]  train_loss: 0.726  |  valid_loss: 0.887\n",
      "[epoch: 24, i:  4874]  train_loss: 0.798  |  valid_loss: 0.750\n",
      "[epoch: 24, i:  4999]  train_loss: 0.751  |  valid_loss: 0.766\n",
      "[epoch: 24, i:  5124]  train_loss: 0.822  |  valid_loss: 0.840\n",
      "[epoch: 24, i:  5249]  train_loss: 0.807  |  valid_loss: 0.898\n",
      "[epoch: 24, i:  5374]  train_loss: 0.839  |  valid_loss: 0.628\n",
      "[epoch: 24, i:  5499]  train_loss: 0.783  |  valid_loss: 0.710\n",
      "[epoch: 24, i:  5624]  train_loss: 0.849  |  valid_loss: 0.885\n",
      "[epoch: 24, i:  5749]  train_loss: 0.713  |  valid_loss: 0.828\n",
      "[epoch: 24, i:  5874]  train_loss: 0.866  |  valid_loss: 0.719\n",
      "[epoch: 24, i:  5999]  train_loss: 0.859  |  valid_loss: 0.856\n",
      "[epoch: 24, i:  6124]  train_loss: 0.831  |  valid_loss: 0.769\n",
      "[epoch: 24, i:  6249]  train_loss: 0.715  |  valid_loss: 0.893\n",
      "[epoch: 24, i:  6374]  train_loss: 0.789  |  valid_loss: 0.749\n",
      "[epoch: 24, i:  6499]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 24, i:  6624]  train_loss: 0.804  |  valid_loss: 0.691\n",
      "[epoch: 24, i:  6749]  train_loss: 0.761  |  valid_loss: 0.807\n",
      "[epoch: 24, i:  6874]  train_loss: 0.724  |  valid_loss: 0.763\n",
      "[epoch: 24, i:  6999]  train_loss: 0.781  |  valid_loss: 0.954\n",
      "[epoch: 24, i:  7124]  train_loss: 0.833  |  valid_loss: 0.968\n",
      "[epoch: 24, i:  7249]  train_loss: 0.765  |  valid_loss: 0.665\n",
      "[epoch: 24, i:  7374]  train_loss: 0.780  |  valid_loss: 1.041\n",
      "[epoch: 24, i:  7499]  train_loss: 0.808  |  valid_loss: 0.926\n",
      "[epoch: 24, i:  7624]  train_loss: 0.815  |  valid_loss: 0.923\n",
      "[epoch: 24, i:  7749]  train_loss: 0.758  |  valid_loss: 0.798\n",
      "[epoch: 24, i:  7874]  train_loss: 0.726  |  valid_loss: 0.886\n",
      "[epoch: 24, i:  7999]  train_loss: 0.800  |  valid_loss: 0.710\n",
      "[epoch: 24, i:  8124]  train_loss: 0.824  |  valid_loss: 0.893\n",
      "[epoch: 24, i:  8249]  train_loss: 0.850  |  valid_loss: 0.899\n",
      "[epoch: 24, i:  8374]  train_loss: 0.796  |  valid_loss: 0.840\n",
      "[epoch: 24, i:  8499]  train_loss: 0.802  |  valid_loss: 0.934\n",
      "[epoch: 24, i:  8624]  train_loss: 0.794  |  valid_loss: 0.819\n",
      "[epoch: 24, i:  8749]  train_loss: 0.889  |  valid_loss: 0.891\n",
      "[epoch: 24, i:  8874]  train_loss: 0.837  |  valid_loss: 0.867\n",
      "[epoch: 24, i:  8999]  train_loss: 0.835  |  valid_loss: 0.688\n",
      "[epoch: 24, i:  9124]  train_loss: 0.765  |  valid_loss: 0.774\n",
      "[epoch: 24, i:  9249]  train_loss: 0.871  |  valid_loss: 0.623\n",
      "[epoch: 24, i:  9374]  train_loss: 0.766  |  valid_loss: 0.910\n",
      "[epoch: 24, i:  9499]  train_loss: 0.802  |  valid_loss: 0.747\n",
      "[epoch: 24, i:  9624]  train_loss: 0.821  |  valid_loss: 0.889\n",
      "[epoch: 24, i:  9749]  train_loss: 0.873  |  valid_loss: 0.913\n",
      "[epoch: 24, i:  9874]  train_loss: 0.785  |  valid_loss: 0.922\n",
      "[epoch: 24, i:  9999]  train_loss: 0.769  |  valid_loss: 0.817\n",
      "[epoch: 24, i: 10124]  train_loss: 0.816  |  valid_loss: 0.762\n",
      "[epoch: 24, i: 10249]  train_loss: 0.795  |  valid_loss: 0.882\n",
      "[epoch: 24, i: 10374]  train_loss: 0.734  |  valid_loss: 0.741\n",
      "[epoch: 24, i: 10499]  train_loss: 0.816  |  valid_loss: 0.953\n",
      "[epoch: 24, i: 10624]  train_loss: 0.856  |  valid_loss: 0.940\n",
      "[epoch: 24, i: 10749]  train_loss: 0.815  |  valid_loss: 0.860\n",
      "[epoch: 24, i: 10874]  train_loss: 0.890  |  valid_loss: 0.955\n",
      "[epoch: 24, i: 10999]  train_loss: 0.784  |  valid_loss: 0.971\n",
      "[epoch: 24, i: 11124]  train_loss: 0.819  |  valid_loss: 0.839\n",
      "[epoch: 24, i: 11249]  train_loss: 0.841  |  valid_loss: 0.866\n",
      "[epoch: 24, i: 11374]  train_loss: 0.801  |  valid_loss: 0.797\n",
      "[epoch: 24, i: 11499]  train_loss: 0.870  |  valid_loss: 0.605\n",
      "[epoch: 24, i: 11624]  train_loss: 0.827  |  valid_loss: 0.800\n",
      "[epoch: 24, i: 11749]  train_loss: 0.823  |  valid_loss: 0.733\n",
      "[epoch: 24, i: 11874]  train_loss: 0.813  |  valid_loss: 0.793\n",
      "[epoch: 24, i: 11999]  train_loss: 0.827  |  valid_loss: 0.687\n",
      "[epoch: 24, i: 12124]  train_loss: 0.802  |  valid_loss: 0.733\n",
      "[epoch: 24, i: 12249]  train_loss: 0.832  |  valid_loss: 1.057\n",
      "[epoch: 24, i: 12374]  train_loss: 0.780  |  valid_loss: 0.861\n",
      "[epoch: 24, i: 12499]  train_loss: 0.836  |  valid_loss: 0.876\n",
      "--> [End of epoch 24] train_accuracy: 71.46%  |  valid_accuracy: 71.06%\n",
      "--> [Start of epoch 25]  lr: 0.000008\n",
      "[epoch: 25, i:   124]  train_loss: 0.831  |  valid_loss: 0.751\n",
      "[epoch: 25, i:   249]  train_loss: 0.767  |  valid_loss: 0.719\n",
      "[epoch: 25, i:   374]  train_loss: 0.779  |  valid_loss: 0.876\n",
      "[epoch: 25, i:   499]  train_loss: 0.895  |  valid_loss: 0.793\n",
      "[epoch: 25, i:   624]  train_loss: 0.840  |  valid_loss: 0.915\n",
      "[epoch: 25, i:   749]  train_loss: 0.793  |  valid_loss: 0.576\n",
      "[epoch: 25, i:   874]  train_loss: 0.844  |  valid_loss: 0.873\n",
      "[epoch: 25, i:   999]  train_loss: 0.828  |  valid_loss: 0.931\n",
      "[epoch: 25, i:  1124]  train_loss: 0.851  |  valid_loss: 0.910\n",
      "[epoch: 25, i:  1249]  train_loss: 0.803  |  valid_loss: 0.737\n",
      "[epoch: 25, i:  1374]  train_loss: 0.787  |  valid_loss: 0.764\n",
      "[epoch: 25, i:  1499]  train_loss: 0.837  |  valid_loss: 0.840\n",
      "[epoch: 25, i:  1624]  train_loss: 0.838  |  valid_loss: 0.676\n",
      "[epoch: 25, i:  1749]  train_loss: 0.777  |  valid_loss: 0.947\n",
      "[epoch: 25, i:  1874]  train_loss: 0.829  |  valid_loss: 0.761\n",
      "[epoch: 25, i:  1999]  train_loss: 0.854  |  valid_loss: 1.008\n",
      "[epoch: 25, i:  2124]  train_loss: 0.860  |  valid_loss: 0.703\n",
      "[epoch: 25, i:  2249]  train_loss: 0.815  |  valid_loss: 0.821\n",
      "[epoch: 25, i:  2374]  train_loss: 0.794  |  valid_loss: 0.767\n",
      "[epoch: 25, i:  2499]  train_loss: 0.768  |  valid_loss: 0.995\n",
      "[epoch: 25, i:  2624]  train_loss: 0.749  |  valid_loss: 0.935\n",
      "[epoch: 25, i:  2749]  train_loss: 0.826  |  valid_loss: 0.955\n",
      "[epoch: 25, i:  2874]  train_loss: 0.854  |  valid_loss: 1.005\n",
      "[epoch: 25, i:  2999]  train_loss: 0.885  |  valid_loss: 0.834\n",
      "[epoch: 25, i:  3124]  train_loss: 0.834  |  valid_loss: 0.902\n",
      "[epoch: 25, i:  3249]  train_loss: 0.854  |  valid_loss: 1.066\n",
      "[epoch: 25, i:  3374]  train_loss: 0.843  |  valid_loss: 0.779\n",
      "[epoch: 25, i:  3499]  train_loss: 0.784  |  valid_loss: 0.748\n",
      "[epoch: 25, i:  3624]  train_loss: 0.819  |  valid_loss: 0.780\n",
      "[epoch: 25, i:  3749]  train_loss: 0.814  |  valid_loss: 0.685\n",
      "[epoch: 25, i:  3874]  train_loss: 0.742  |  valid_loss: 0.807\n",
      "[epoch: 25, i:  3999]  train_loss: 0.905  |  valid_loss: 0.660\n",
      "[epoch: 25, i:  4124]  train_loss: 0.793  |  valid_loss: 0.889\n",
      "[epoch: 25, i:  4249]  train_loss: 0.797  |  valid_loss: 0.874\n",
      "[epoch: 25, i:  4374]  train_loss: 0.849  |  valid_loss: 0.872\n",
      "[epoch: 25, i:  4499]  train_loss: 0.810  |  valid_loss: 0.794\n",
      "[epoch: 25, i:  4624]  train_loss: 0.794  |  valid_loss: 0.947\n",
      "[epoch: 25, i:  4749]  train_loss: 0.808  |  valid_loss: 0.893\n",
      "[epoch: 25, i:  4874]  train_loss: 0.805  |  valid_loss: 0.741\n",
      "[epoch: 25, i:  4999]  train_loss: 0.789  |  valid_loss: 0.725\n",
      "[epoch: 25, i:  5124]  train_loss: 0.814  |  valid_loss: 0.804\n",
      "[epoch: 25, i:  5249]  train_loss: 0.762  |  valid_loss: 0.880\n",
      "[epoch: 25, i:  5374]  train_loss: 0.807  |  valid_loss: 0.620\n",
      "[epoch: 25, i:  5499]  train_loss: 0.771  |  valid_loss: 0.714\n",
      "[epoch: 25, i:  5624]  train_loss: 0.801  |  valid_loss: 0.901\n",
      "[epoch: 25, i:  5749]  train_loss: 0.842  |  valid_loss: 0.820\n",
      "[epoch: 25, i:  5874]  train_loss: 0.778  |  valid_loss: 0.739\n",
      "[epoch: 25, i:  5999]  train_loss: 0.782  |  valid_loss: 0.873\n",
      "[epoch: 25, i:  6124]  train_loss: 0.827  |  valid_loss: 0.749\n",
      "[epoch: 25, i:  6249]  train_loss: 0.804  |  valid_loss: 0.851\n",
      "[epoch: 25, i:  6374]  train_loss: 0.850  |  valid_loss: 0.745\n",
      "[epoch: 25, i:  6499]  train_loss: 0.798  |  valid_loss: 0.801\n",
      "[epoch: 25, i:  6624]  train_loss: 0.850  |  valid_loss: 0.673\n",
      "[epoch: 25, i:  6749]  train_loss: 0.808  |  valid_loss: 0.820\n",
      "[epoch: 25, i:  6874]  train_loss: 0.753  |  valid_loss: 0.753\n",
      "[epoch: 25, i:  6999]  train_loss: 0.815  |  valid_loss: 0.964\n",
      "[epoch: 25, i:  7124]  train_loss: 0.829  |  valid_loss: 0.972\n",
      "[epoch: 25, i:  7249]  train_loss: 0.829  |  valid_loss: 0.659\n",
      "[epoch: 25, i:  7374]  train_loss: 0.902  |  valid_loss: 1.044\n",
      "[epoch: 25, i:  7499]  train_loss: 0.800  |  valid_loss: 0.903\n",
      "[epoch: 25, i:  7624]  train_loss: 0.781  |  valid_loss: 0.901\n",
      "[epoch: 25, i:  7749]  train_loss: 0.878  |  valid_loss: 0.798\n",
      "[epoch: 25, i:  7874]  train_loss: 0.809  |  valid_loss: 0.868\n",
      "[epoch: 25, i:  7999]  train_loss: 0.770  |  valid_loss: 0.710\n",
      "[epoch: 25, i:  8124]  train_loss: 0.821  |  valid_loss: 0.903\n",
      "[epoch: 25, i:  8249]  train_loss: 0.821  |  valid_loss: 0.888\n",
      "[epoch: 25, i:  8374]  train_loss: 0.834  |  valid_loss: 0.832\n",
      "[epoch: 25, i:  8499]  train_loss: 0.748  |  valid_loss: 0.919\n",
      "[epoch: 25, i:  8624]  train_loss: 0.857  |  valid_loss: 0.869\n",
      "[epoch: 25, i:  8749]  train_loss: 0.800  |  valid_loss: 0.888\n",
      "[epoch: 25, i:  8874]  train_loss: 0.798  |  valid_loss: 0.839\n",
      "[epoch: 25, i:  8999]  train_loss: 0.843  |  valid_loss: 0.704\n",
      "[epoch: 25, i:  9124]  train_loss: 0.841  |  valid_loss: 0.716\n",
      "[epoch: 25, i:  9249]  train_loss: 0.793  |  valid_loss: 0.618\n",
      "[epoch: 25, i:  9374]  train_loss: 0.795  |  valid_loss: 0.920\n",
      "[epoch: 25, i:  9499]  train_loss: 0.761  |  valid_loss: 0.747\n",
      "[epoch: 25, i:  9624]  train_loss: 0.863  |  valid_loss: 0.872\n",
      "[epoch: 25, i:  9749]  train_loss: 0.868  |  valid_loss: 0.931\n",
      "[epoch: 25, i:  9874]  train_loss: 0.683  |  valid_loss: 0.891\n",
      "[epoch: 25, i:  9999]  train_loss: 0.794  |  valid_loss: 0.822\n",
      "[epoch: 25, i: 10124]  train_loss: 0.867  |  valid_loss: 0.773\n",
      "[epoch: 25, i: 10249]  train_loss: 0.793  |  valid_loss: 0.856\n",
      "[epoch: 25, i: 10374]  train_loss: 0.910  |  valid_loss: 0.768\n",
      "[epoch: 25, i: 10499]  train_loss: 0.866  |  valid_loss: 0.991\n",
      "[epoch: 25, i: 10624]  train_loss: 0.738  |  valid_loss: 0.910\n",
      "[epoch: 25, i: 10749]  train_loss: 0.853  |  valid_loss: 0.825\n",
      "[epoch: 25, i: 10874]  train_loss: 0.817  |  valid_loss: 0.976\n",
      "[epoch: 25, i: 10999]  train_loss: 0.720  |  valid_loss: 0.975\n",
      "[epoch: 25, i: 11124]  train_loss: 0.819  |  valid_loss: 0.830\n",
      "[epoch: 25, i: 11249]  train_loss: 0.860  |  valid_loss: 0.867\n",
      "[epoch: 25, i: 11374]  train_loss: 0.775  |  valid_loss: 0.794\n",
      "[epoch: 25, i: 11499]  train_loss: 0.797  |  valid_loss: 0.594\n",
      "[epoch: 25, i: 11624]  train_loss: 0.740  |  valid_loss: 0.783\n",
      "[epoch: 25, i: 11749]  train_loss: 0.790  |  valid_loss: 0.739\n",
      "[epoch: 25, i: 11874]  train_loss: 0.768  |  valid_loss: 0.771\n",
      "[epoch: 25, i: 11999]  train_loss: 0.793  |  valid_loss: 0.691\n",
      "[epoch: 25, i: 12124]  train_loss: 0.810  |  valid_loss: 0.716\n",
      "[epoch: 25, i: 12249]  train_loss: 0.799  |  valid_loss: 1.079\n",
      "[epoch: 25, i: 12374]  train_loss: 0.821  |  valid_loss: 0.881\n",
      "[epoch: 25, i: 12499]  train_loss: 0.857  |  valid_loss: 0.860\n",
      "--> [End of epoch 25] train_accuracy: 71.20%  |  valid_accuracy: 71.08%\n",
      "--> [Start of epoch 26]  lr: 0.000006\n",
      "[epoch: 26, i:   124]  train_loss: 0.774  |  valid_loss: 0.719\n",
      "[epoch: 26, i:   249]  train_loss: 0.767  |  valid_loss: 0.731\n",
      "[epoch: 26, i:   374]  train_loss: 0.784  |  valid_loss: 0.874\n",
      "[epoch: 26, i:   499]  train_loss: 0.762  |  valid_loss: 0.793\n",
      "[epoch: 26, i:   624]  train_loss: 0.807  |  valid_loss: 0.924\n",
      "[epoch: 26, i:   749]  train_loss: 0.832  |  valid_loss: 0.571\n",
      "[epoch: 26, i:   874]  train_loss: 0.807  |  valid_loss: 0.862\n",
      "[epoch: 26, i:   999]  train_loss: 0.690  |  valid_loss: 0.901\n",
      "[epoch: 26, i:  1124]  train_loss: 0.811  |  valid_loss: 0.925\n",
      "[epoch: 26, i:  1249]  train_loss: 0.819  |  valid_loss: 0.757\n",
      "[epoch: 26, i:  1374]  train_loss: 0.756  |  valid_loss: 0.791\n",
      "[epoch: 26, i:  1499]  train_loss: 0.818  |  valid_loss: 0.826\n",
      "[epoch: 26, i:  1624]  train_loss: 0.893  |  valid_loss: 0.671\n",
      "[epoch: 26, i:  1749]  train_loss: 0.809  |  valid_loss: 0.957\n",
      "[epoch: 26, i:  1874]  train_loss: 0.812  |  valid_loss: 0.775\n",
      "[epoch: 26, i:  1999]  train_loss: 0.900  |  valid_loss: 1.002\n",
      "[epoch: 26, i:  2124]  train_loss: 0.810  |  valid_loss: 0.723\n",
      "[epoch: 26, i:  2249]  train_loss: 0.815  |  valid_loss: 0.813\n",
      "[epoch: 26, i:  2374]  train_loss: 0.859  |  valid_loss: 0.760\n",
      "[epoch: 26, i:  2499]  train_loss: 0.812  |  valid_loss: 1.008\n",
      "[epoch: 26, i:  2624]  train_loss: 0.831  |  valid_loss: 0.918\n",
      "[epoch: 26, i:  2749]  train_loss: 0.807  |  valid_loss: 0.922\n",
      "[epoch: 26, i:  2874]  train_loss: 0.818  |  valid_loss: 1.026\n",
      "[epoch: 26, i:  2999]  train_loss: 0.838  |  valid_loss: 0.824\n",
      "[epoch: 26, i:  3124]  train_loss: 0.849  |  valid_loss: 0.920\n",
      "[epoch: 26, i:  3249]  train_loss: 0.837  |  valid_loss: 1.039\n",
      "[epoch: 26, i:  3374]  train_loss: 0.843  |  valid_loss: 0.808\n",
      "[epoch: 26, i:  3499]  train_loss: 0.751  |  valid_loss: 0.744\n",
      "[epoch: 26, i:  3624]  train_loss: 0.741  |  valid_loss: 0.780\n",
      "[epoch: 26, i:  3749]  train_loss: 0.794  |  valid_loss: 0.691\n",
      "[epoch: 26, i:  3874]  train_loss: 0.763  |  valid_loss: 0.791\n",
      "[epoch: 26, i:  3999]  train_loss: 0.859  |  valid_loss: 0.642\n",
      "[epoch: 26, i:  4124]  train_loss: 0.886  |  valid_loss: 0.831\n",
      "[epoch: 26, i:  4249]  train_loss: 0.817  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  4374]  train_loss: 0.770  |  valid_loss: 0.859\n",
      "[epoch: 26, i:  4499]  train_loss: 0.760  |  valid_loss: 0.801\n",
      "[epoch: 26, i:  4624]  train_loss: 0.860  |  valid_loss: 0.929\n",
      "[epoch: 26, i:  4749]  train_loss: 0.800  |  valid_loss: 0.868\n",
      "[epoch: 26, i:  4874]  train_loss: 0.805  |  valid_loss: 0.749\n",
      "[epoch: 26, i:  4999]  train_loss: 0.881  |  valid_loss: 0.742\n",
      "[epoch: 26, i:  5124]  train_loss: 0.799  |  valid_loss: 0.832\n",
      "[epoch: 26, i:  5249]  train_loss: 0.781  |  valid_loss: 0.920\n",
      "[epoch: 26, i:  5374]  train_loss: 0.819  |  valid_loss: 0.607\n",
      "[epoch: 26, i:  5499]  train_loss: 0.731  |  valid_loss: 0.708\n",
      "[epoch: 26, i:  5624]  train_loss: 0.831  |  valid_loss: 0.994\n",
      "[epoch: 26, i:  5749]  train_loss: 0.822  |  valid_loss: 0.814\n",
      "[epoch: 26, i:  5874]  train_loss: 0.879  |  valid_loss: 0.710\n",
      "[epoch: 26, i:  5999]  train_loss: 0.829  |  valid_loss: 0.839\n",
      "[epoch: 26, i:  6124]  train_loss: 0.825  |  valid_loss: 0.733\n",
      "[epoch: 26, i:  6249]  train_loss: 0.874  |  valid_loss: 0.894\n",
      "[epoch: 26, i:  6374]  train_loss: 0.843  |  valid_loss: 0.707\n",
      "[epoch: 26, i:  6499]  train_loss: 0.811  |  valid_loss: 0.799\n",
      "[epoch: 26, i:  6624]  train_loss: 0.784  |  valid_loss: 0.673\n",
      "[epoch: 26, i:  6749]  train_loss: 0.765  |  valid_loss: 0.839\n",
      "[epoch: 26, i:  6874]  train_loss: 0.861  |  valid_loss: 0.762\n",
      "[epoch: 26, i:  6999]  train_loss: 0.801  |  valid_loss: 0.936\n",
      "[epoch: 26, i:  7124]  train_loss: 0.761  |  valid_loss: 0.981\n",
      "[epoch: 26, i:  7249]  train_loss: 0.836  |  valid_loss: 0.646\n",
      "[epoch: 26, i:  7374]  train_loss: 0.817  |  valid_loss: 1.057\n",
      "[epoch: 26, i:  7499]  train_loss: 0.862  |  valid_loss: 0.912\n",
      "[epoch: 26, i:  7624]  train_loss: 0.843  |  valid_loss: 0.909\n",
      "[epoch: 26, i:  7749]  train_loss: 0.843  |  valid_loss: 0.800\n",
      "[epoch: 26, i:  7874]  train_loss: 0.830  |  valid_loss: 0.868\n",
      "[epoch: 26, i:  7999]  train_loss: 0.850  |  valid_loss: 0.716\n",
      "[epoch: 26, i:  8124]  train_loss: 0.780  |  valid_loss: 0.912\n",
      "[epoch: 26, i:  8249]  train_loss: 0.745  |  valid_loss: 0.896\n",
      "[epoch: 26, i:  8374]  train_loss: 0.815  |  valid_loss: 0.842\n",
      "[epoch: 26, i:  8499]  train_loss: 0.830  |  valid_loss: 0.902\n",
      "[epoch: 26, i:  8624]  train_loss: 0.762  |  valid_loss: 0.863\n",
      "[epoch: 26, i:  8749]  train_loss: 0.790  |  valid_loss: 0.886\n",
      "[epoch: 26, i:  8874]  train_loss: 0.758  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  8999]  train_loss: 0.864  |  valid_loss: 0.694\n",
      "[epoch: 26, i:  9124]  train_loss: 0.842  |  valid_loss: 0.765\n",
      "[epoch: 26, i:  9249]  train_loss: 0.808  |  valid_loss: 0.601\n",
      "[epoch: 26, i:  9374]  train_loss: 0.829  |  valid_loss: 0.897\n",
      "[epoch: 26, i:  9499]  train_loss: 0.831  |  valid_loss: 0.731\n",
      "[epoch: 26, i:  9624]  train_loss: 0.833  |  valid_loss: 0.864\n",
      "[epoch: 26, i:  9749]  train_loss: 0.842  |  valid_loss: 0.953\n",
      "[epoch: 26, i:  9874]  train_loss: 0.771  |  valid_loss: 0.867\n",
      "[epoch: 26, i:  9999]  train_loss: 0.813  |  valid_loss: 0.807\n",
      "[epoch: 26, i: 10124]  train_loss: 0.879  |  valid_loss: 0.782\n",
      "[epoch: 26, i: 10249]  train_loss: 0.774  |  valid_loss: 0.890\n",
      "[epoch: 26, i: 10374]  train_loss: 0.783  |  valid_loss: 0.750\n",
      "[epoch: 26, i: 10499]  train_loss: 0.780  |  valid_loss: 0.953\n",
      "[epoch: 26, i: 10624]  train_loss: 0.848  |  valid_loss: 0.921\n",
      "[epoch: 26, i: 10749]  train_loss: 0.859  |  valid_loss: 0.835\n",
      "[epoch: 26, i: 10874]  train_loss: 0.849  |  valid_loss: 0.964\n",
      "[epoch: 26, i: 10999]  train_loss: 0.702  |  valid_loss: 0.935\n",
      "[epoch: 26, i: 11124]  train_loss: 0.711  |  valid_loss: 0.835\n",
      "[epoch: 26, i: 11249]  train_loss: 0.765  |  valid_loss: 0.867\n",
      "[epoch: 26, i: 11374]  train_loss: 0.796  |  valid_loss: 0.785\n",
      "[epoch: 26, i: 11499]  train_loss: 0.816  |  valid_loss: 0.629\n",
      "[epoch: 26, i: 11624]  train_loss: 0.856  |  valid_loss: 0.790\n",
      "[epoch: 26, i: 11749]  train_loss: 0.806  |  valid_loss: 0.739\n",
      "[epoch: 26, i: 11874]  train_loss: 0.836  |  valid_loss: 0.779\n",
      "[epoch: 26, i: 11999]  train_loss: 0.822  |  valid_loss: 0.711\n",
      "[epoch: 26, i: 12124]  train_loss: 0.895  |  valid_loss: 0.731\n",
      "[epoch: 26, i: 12249]  train_loss: 0.925  |  valid_loss: 1.053\n",
      "[epoch: 26, i: 12374]  train_loss: 0.816  |  valid_loss: 0.878\n",
      "[epoch: 26, i: 12499]  train_loss: 0.819  |  valid_loss: 0.844\n",
      "--> [End of epoch 26] train_accuracy: 71.37%  |  valid_accuracy: 70.93%\n",
      "--> [Start of epoch 27]  lr: 0.000005\n",
      "[epoch: 27, i:   124]  train_loss: 0.800  |  valid_loss: 0.731\n",
      "[epoch: 27, i:   249]  train_loss: 0.863  |  valid_loss: 0.730\n",
      "[epoch: 27, i:   374]  train_loss: 0.741  |  valid_loss: 0.857\n",
      "[epoch: 27, i:   499]  train_loss: 0.783  |  valid_loss: 0.781\n",
      "[epoch: 27, i:   624]  train_loss: 0.805  |  valid_loss: 0.942\n",
      "[epoch: 27, i:   749]  train_loss: 0.783  |  valid_loss: 0.600\n",
      "[epoch: 27, i:   874]  train_loss: 0.852  |  valid_loss: 0.864\n",
      "[epoch: 27, i:   999]  train_loss: 0.844  |  valid_loss: 0.949\n",
      "[epoch: 27, i:  1124]  train_loss: 0.789  |  valid_loss: 0.951\n",
      "[epoch: 27, i:  1249]  train_loss: 0.885  |  valid_loss: 0.728\n",
      "[epoch: 27, i:  1374]  train_loss: 0.740  |  valid_loss: 0.743\n",
      "[epoch: 27, i:  1499]  train_loss: 0.792  |  valid_loss: 0.820\n",
      "[epoch: 27, i:  1624]  train_loss: 0.842  |  valid_loss: 0.693\n",
      "[epoch: 27, i:  1749]  train_loss: 0.800  |  valid_loss: 0.967\n",
      "[epoch: 27, i:  1874]  train_loss: 0.863  |  valid_loss: 0.744\n",
      "[epoch: 27, i:  1999]  train_loss: 0.846  |  valid_loss: 0.977\n",
      "[epoch: 27, i:  2124]  train_loss: 0.787  |  valid_loss: 0.749\n",
      "[epoch: 27, i:  2249]  train_loss: 0.874  |  valid_loss: 0.827\n",
      "[epoch: 27, i:  2374]  train_loss: 0.794  |  valid_loss: 0.756\n",
      "[epoch: 27, i:  2499]  train_loss: 0.782  |  valid_loss: 0.994\n",
      "[epoch: 27, i:  2624]  train_loss: 0.782  |  valid_loss: 0.946\n",
      "[epoch: 27, i:  2749]  train_loss: 0.782  |  valid_loss: 0.895\n",
      "[epoch: 27, i:  2874]  train_loss: 0.808  |  valid_loss: 0.986\n",
      "[epoch: 27, i:  2999]  train_loss: 0.829  |  valid_loss: 0.846\n",
      "[epoch: 27, i:  3124]  train_loss: 0.848  |  valid_loss: 0.906\n",
      "[epoch: 27, i:  3249]  train_loss: 0.772  |  valid_loss: 1.047\n",
      "[epoch: 27, i:  3374]  train_loss: 0.804  |  valid_loss: 0.781\n",
      "[epoch: 27, i:  3499]  train_loss: 0.812  |  valid_loss: 0.757\n",
      "[epoch: 27, i:  3624]  train_loss: 0.828  |  valid_loss: 0.790\n",
      "[epoch: 27, i:  3749]  train_loss: 0.786  |  valid_loss: 0.675\n",
      "[epoch: 27, i:  3874]  train_loss: 0.846  |  valid_loss: 0.790\n",
      "[epoch: 27, i:  3999]  train_loss: 0.815  |  valid_loss: 0.659\n",
      "[epoch: 27, i:  4124]  train_loss: 0.865  |  valid_loss: 0.828\n",
      "[epoch: 27, i:  4249]  train_loss: 0.872  |  valid_loss: 0.876\n",
      "[epoch: 27, i:  4374]  train_loss: 0.762  |  valid_loss: 0.861\n",
      "[epoch: 27, i:  4499]  train_loss: 0.776  |  valid_loss: 0.778\n",
      "[epoch: 27, i:  4624]  train_loss: 0.837  |  valid_loss: 0.926\n",
      "[epoch: 27, i:  4749]  train_loss: 0.849  |  valid_loss: 0.865\n",
      "[epoch: 27, i:  4874]  train_loss: 0.767  |  valid_loss: 0.726\n",
      "[epoch: 27, i:  4999]  train_loss: 0.838  |  valid_loss: 0.760\n",
      "[epoch: 27, i:  5124]  train_loss: 0.829  |  valid_loss: 0.855\n",
      "[epoch: 27, i:  5249]  train_loss: 0.773  |  valid_loss: 0.891\n",
      "[epoch: 27, i:  5374]  train_loss: 0.793  |  valid_loss: 0.586\n",
      "[epoch: 27, i:  5499]  train_loss: 0.812  |  valid_loss: 0.722\n",
      "[epoch: 27, i:  5624]  train_loss: 0.803  |  valid_loss: 0.991\n",
      "[epoch: 27, i:  5749]  train_loss: 0.810  |  valid_loss: 0.818\n",
      "[epoch: 27, i:  5874]  train_loss: 0.874  |  valid_loss: 0.712\n",
      "[epoch: 27, i:  5999]  train_loss: 0.794  |  valid_loss: 0.835\n",
      "[epoch: 27, i:  6124]  train_loss: 0.764  |  valid_loss: 0.763\n",
      "[epoch: 27, i:  6249]  train_loss: 0.777  |  valid_loss: 0.898\n",
      "[epoch: 27, i:  6374]  train_loss: 0.819  |  valid_loss: 0.740\n",
      "[epoch: 27, i:  6499]  train_loss: 0.859  |  valid_loss: 0.800\n",
      "[epoch: 27, i:  6624]  train_loss: 0.808  |  valid_loss: 0.681\n",
      "[epoch: 27, i:  6749]  train_loss: 0.800  |  valid_loss: 0.816\n",
      "[epoch: 27, i:  6874]  train_loss: 0.833  |  valid_loss: 0.777\n",
      "[epoch: 27, i:  6999]  train_loss: 0.790  |  valid_loss: 0.981\n",
      "[epoch: 27, i:  7124]  train_loss: 0.799  |  valid_loss: 0.952\n",
      "[epoch: 27, i:  7249]  train_loss: 0.755  |  valid_loss: 0.672\n",
      "[epoch: 27, i:  7374]  train_loss: 0.870  |  valid_loss: 1.032\n",
      "[epoch: 27, i:  7499]  train_loss: 0.771  |  valid_loss: 0.888\n",
      "[epoch: 27, i:  7624]  train_loss: 0.773  |  valid_loss: 0.909\n",
      "[epoch: 27, i:  7749]  train_loss: 0.809  |  valid_loss: 0.787\n",
      "[epoch: 27, i:  7874]  train_loss: 0.752  |  valid_loss: 0.870\n",
      "[epoch: 27, i:  7999]  train_loss: 0.816  |  valid_loss: 0.722\n",
      "[epoch: 27, i:  8124]  train_loss: 0.762  |  valid_loss: 0.927\n",
      "[epoch: 27, i:  8249]  train_loss: 0.834  |  valid_loss: 0.894\n",
      "[epoch: 27, i:  8374]  train_loss: 0.776  |  valid_loss: 0.853\n",
      "[epoch: 27, i:  8499]  train_loss: 0.916  |  valid_loss: 0.917\n",
      "[epoch: 27, i:  8624]  train_loss: 0.738  |  valid_loss: 0.873\n",
      "[epoch: 27, i:  8749]  train_loss: 0.817  |  valid_loss: 0.900\n",
      "[epoch: 27, i:  8874]  train_loss: 0.817  |  valid_loss: 0.851\n",
      "[epoch: 27, i:  8999]  train_loss: 0.779  |  valid_loss: 0.705\n",
      "[epoch: 27, i:  9124]  train_loss: 0.784  |  valid_loss: 0.758\n",
      "[epoch: 27, i:  9249]  train_loss: 0.792  |  valid_loss: 0.600\n",
      "[epoch: 27, i:  9374]  train_loss: 0.808  |  valid_loss: 0.907\n",
      "[epoch: 27, i:  9499]  train_loss: 0.790  |  valid_loss: 0.779\n",
      "[epoch: 27, i:  9624]  train_loss: 0.808  |  valid_loss: 0.880\n",
      "[epoch: 27, i:  9749]  train_loss: 0.773  |  valid_loss: 0.894\n",
      "[epoch: 27, i:  9874]  train_loss: 0.809  |  valid_loss: 0.919\n",
      "[epoch: 27, i:  9999]  train_loss: 0.925  |  valid_loss: 0.834\n",
      "[epoch: 27, i: 10124]  train_loss: 0.772  |  valid_loss: 0.752\n",
      "[epoch: 27, i: 10249]  train_loss: 0.785  |  valid_loss: 0.873\n",
      "[epoch: 27, i: 10374]  train_loss: 0.773  |  valid_loss: 0.745\n",
      "[epoch: 27, i: 10499]  train_loss: 0.858  |  valid_loss: 0.984\n",
      "[epoch: 27, i: 10624]  train_loss: 0.817  |  valid_loss: 0.931\n",
      "[epoch: 27, i: 10749]  train_loss: 0.761  |  valid_loss: 0.856\n",
      "[epoch: 27, i: 10874]  train_loss: 0.807  |  valid_loss: 0.968\n",
      "[epoch: 27, i: 10999]  train_loss: 0.836  |  valid_loss: 0.969\n",
      "[epoch: 27, i: 11124]  train_loss: 0.834  |  valid_loss: 0.844\n",
      "[epoch: 27, i: 11249]  train_loss: 0.835  |  valid_loss: 0.866\n",
      "[epoch: 27, i: 11374]  train_loss: 0.811  |  valid_loss: 0.801\n",
      "[epoch: 27, i: 11499]  train_loss: 0.800  |  valid_loss: 0.620\n",
      "[epoch: 27, i: 11624]  train_loss: 0.864  |  valid_loss: 0.823\n",
      "[epoch: 27, i: 11749]  train_loss: 0.816  |  valid_loss: 0.764\n",
      "[epoch: 27, i: 11874]  train_loss: 0.797  |  valid_loss: 0.757\n",
      "[epoch: 27, i: 11999]  train_loss: 0.801  |  valid_loss: 0.702\n",
      "[epoch: 27, i: 12124]  train_loss: 0.814  |  valid_loss: 0.730\n",
      "[epoch: 27, i: 12249]  train_loss: 0.854  |  valid_loss: 1.055\n",
      "[epoch: 27, i: 12374]  train_loss: 0.815  |  valid_loss: 0.876\n",
      "[epoch: 27, i: 12499]  train_loss: 0.814  |  valid_loss: 0.866\n",
      "--> [End of epoch 27] train_accuracy: 71.41%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 28]  lr: 0.000004\n",
      "[epoch: 28, i:   124]  train_loss: 0.745  |  valid_loss: 0.733\n",
      "[epoch: 28, i:   249]  train_loss: 0.776  |  valid_loss: 0.724\n",
      "[epoch: 28, i:   374]  train_loss: 0.775  |  valid_loss: 0.856\n",
      "[epoch: 28, i:   499]  train_loss: 0.789  |  valid_loss: 0.783\n",
      "[epoch: 28, i:   624]  train_loss: 0.848  |  valid_loss: 0.931\n",
      "[epoch: 28, i:   749]  train_loss: 0.820  |  valid_loss: 0.574\n",
      "[epoch: 28, i:   874]  train_loss: 0.900  |  valid_loss: 0.862\n",
      "[epoch: 28, i:   999]  train_loss: 0.813  |  valid_loss: 0.948\n",
      "[epoch: 28, i:  1124]  train_loss: 0.752  |  valid_loss: 0.919\n",
      "[epoch: 28, i:  1249]  train_loss: 0.798  |  valid_loss: 0.738\n",
      "[epoch: 28, i:  1374]  train_loss: 0.847  |  valid_loss: 0.783\n",
      "[epoch: 28, i:  1499]  train_loss: 0.751  |  valid_loss: 0.827\n",
      "[epoch: 28, i:  1624]  train_loss: 0.785  |  valid_loss: 0.691\n",
      "[epoch: 28, i:  1749]  train_loss: 0.874  |  valid_loss: 0.954\n",
      "[epoch: 28, i:  1874]  train_loss: 0.733  |  valid_loss: 0.766\n",
      "[epoch: 28, i:  1999]  train_loss: 0.811  |  valid_loss: 1.012\n",
      "[epoch: 28, i:  2124]  train_loss: 0.823  |  valid_loss: 0.716\n",
      "[epoch: 28, i:  2249]  train_loss: 0.754  |  valid_loss: 0.852\n",
      "[epoch: 28, i:  2374]  train_loss: 0.782  |  valid_loss: 0.777\n",
      "[epoch: 28, i:  2499]  train_loss: 0.819  |  valid_loss: 1.001\n",
      "[epoch: 28, i:  2624]  train_loss: 0.782  |  valid_loss: 0.931\n",
      "[epoch: 28, i:  2749]  train_loss: 0.932  |  valid_loss: 0.918\n",
      "[epoch: 28, i:  2874]  train_loss: 0.813  |  valid_loss: 1.028\n",
      "[epoch: 28, i:  2999]  train_loss: 0.837  |  valid_loss: 0.819\n",
      "[epoch: 28, i:  3124]  train_loss: 0.823  |  valid_loss: 0.909\n",
      "[epoch: 28, i:  3249]  train_loss: 0.841  |  valid_loss: 1.050\n",
      "[epoch: 28, i:  3374]  train_loss: 0.804  |  valid_loss: 0.826\n",
      "[epoch: 28, i:  3499]  train_loss: 0.729  |  valid_loss: 0.789\n",
      "[epoch: 28, i:  3624]  train_loss: 0.816  |  valid_loss: 0.791\n",
      "[epoch: 28, i:  3749]  train_loss: 0.746  |  valid_loss: 0.672\n",
      "[epoch: 28, i:  3874]  train_loss: 0.779  |  valid_loss: 0.791\n",
      "[epoch: 28, i:  3999]  train_loss: 0.737  |  valid_loss: 0.633\n",
      "[epoch: 28, i:  4124]  train_loss: 0.816  |  valid_loss: 0.843\n",
      "[epoch: 28, i:  4249]  train_loss: 0.837  |  valid_loss: 0.867\n",
      "[epoch: 28, i:  4374]  train_loss: 0.838  |  valid_loss: 0.860\n",
      "[epoch: 28, i:  4499]  train_loss: 0.812  |  valid_loss: 0.778\n",
      "[epoch: 28, i:  4624]  train_loss: 0.831  |  valid_loss: 0.945\n",
      "[epoch: 28, i:  4749]  train_loss: 0.847  |  valid_loss: 0.868\n",
      "[epoch: 28, i:  4874]  train_loss: 0.889  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  4999]  train_loss: 0.856  |  valid_loss: 0.753\n",
      "[epoch: 28, i:  5124]  train_loss: 0.804  |  valid_loss: 0.836\n",
      "[epoch: 28, i:  5249]  train_loss: 0.813  |  valid_loss: 0.901\n",
      "[epoch: 28, i:  5374]  train_loss: 0.810  |  valid_loss: 0.630\n",
      "[epoch: 28, i:  5499]  train_loss: 0.808  |  valid_loss: 0.704\n",
      "[epoch: 28, i:  5624]  train_loss: 0.787  |  valid_loss: 0.930\n",
      "[epoch: 28, i:  5749]  train_loss: 0.901  |  valid_loss: 0.794\n",
      "[epoch: 28, i:  5874]  train_loss: 0.847  |  valid_loss: 0.728\n",
      "[epoch: 28, i:  5999]  train_loss: 0.777  |  valid_loss: 0.875\n",
      "[epoch: 28, i:  6124]  train_loss: 0.780  |  valid_loss: 0.748\n",
      "[epoch: 28, i:  6249]  train_loss: 0.875  |  valid_loss: 0.887\n",
      "[epoch: 28, i:  6374]  train_loss: 0.791  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  6499]  train_loss: 0.785  |  valid_loss: 0.766\n",
      "[epoch: 28, i:  6624]  train_loss: 0.715  |  valid_loss: 0.691\n",
      "[epoch: 28, i:  6749]  train_loss: 0.804  |  valid_loss: 0.820\n",
      "[epoch: 28, i:  6874]  train_loss: 0.794  |  valid_loss: 0.744\n",
      "[epoch: 28, i:  6999]  train_loss: 0.840  |  valid_loss: 0.964\n",
      "[epoch: 28, i:  7124]  train_loss: 0.809  |  valid_loss: 0.989\n",
      "[epoch: 28, i:  7249]  train_loss: 0.788  |  valid_loss: 0.647\n",
      "[epoch: 28, i:  7374]  train_loss: 0.800  |  valid_loss: 1.030\n",
      "[epoch: 28, i:  7499]  train_loss: 0.815  |  valid_loss: 0.910\n",
      "[epoch: 28, i:  7624]  train_loss: 0.832  |  valid_loss: 0.907\n",
      "[epoch: 28, i:  7749]  train_loss: 0.769  |  valid_loss: 0.797\n",
      "[epoch: 28, i:  7874]  train_loss: 0.795  |  valid_loss: 0.890\n",
      "[epoch: 28, i:  7999]  train_loss: 0.863  |  valid_loss: 0.733\n",
      "[epoch: 28, i:  8124]  train_loss: 0.907  |  valid_loss: 0.903\n",
      "[epoch: 28, i:  8249]  train_loss: 0.849  |  valid_loss: 0.909\n",
      "[epoch: 28, i:  8374]  train_loss: 0.808  |  valid_loss: 0.861\n",
      "[epoch: 28, i:  8499]  train_loss: 0.805  |  valid_loss: 0.917\n",
      "[epoch: 28, i:  8624]  train_loss: 0.747  |  valid_loss: 0.867\n",
      "[epoch: 28, i:  8749]  train_loss: 0.766  |  valid_loss: 0.887\n",
      "[epoch: 28, i:  8874]  train_loss: 0.868  |  valid_loss: 0.872\n",
      "[epoch: 28, i:  8999]  train_loss: 0.859  |  valid_loss: 0.690\n",
      "[epoch: 28, i:  9124]  train_loss: 0.792  |  valid_loss: 0.754\n",
      "[epoch: 28, i:  9249]  train_loss: 0.817  |  valid_loss: 0.611\n",
      "[epoch: 28, i:  9374]  train_loss: 0.840  |  valid_loss: 0.899\n",
      "[epoch: 28, i:  9499]  train_loss: 0.783  |  valid_loss: 0.773\n",
      "[epoch: 28, i:  9624]  train_loss: 0.791  |  valid_loss: 0.877\n",
      "[epoch: 28, i:  9749]  train_loss: 0.790  |  valid_loss: 0.900\n",
      "[epoch: 28, i:  9874]  train_loss: 0.874  |  valid_loss: 0.932\n",
      "[epoch: 28, i:  9999]  train_loss: 0.842  |  valid_loss: 0.822\n",
      "[epoch: 28, i: 10124]  train_loss: 0.827  |  valid_loss: 0.758\n",
      "[epoch: 28, i: 10249]  train_loss: 0.775  |  valid_loss: 0.852\n",
      "[epoch: 28, i: 10374]  train_loss: 0.852  |  valid_loss: 0.746\n",
      "[epoch: 28, i: 10499]  train_loss: 0.865  |  valid_loss: 0.958\n",
      "[epoch: 28, i: 10624]  train_loss: 0.856  |  valid_loss: 0.938\n",
      "[epoch: 28, i: 10749]  train_loss: 0.840  |  valid_loss: 0.832\n",
      "[epoch: 28, i: 10874]  train_loss: 0.834  |  valid_loss: 0.969\n",
      "[epoch: 28, i: 10999]  train_loss: 0.895  |  valid_loss: 0.947\n",
      "[epoch: 28, i: 11124]  train_loss: 0.813  |  valid_loss: 0.833\n",
      "[epoch: 28, i: 11249]  train_loss: 0.802  |  valid_loss: 0.873\n",
      "[epoch: 28, i: 11374]  train_loss: 0.786  |  valid_loss: 0.803\n",
      "[epoch: 28, i: 11499]  train_loss: 0.825  |  valid_loss: 0.630\n",
      "[epoch: 28, i: 11624]  train_loss: 0.747  |  valid_loss: 0.820\n",
      "[epoch: 28, i: 11749]  train_loss: 0.789  |  valid_loss: 0.756\n",
      "[epoch: 28, i: 11874]  train_loss: 0.851  |  valid_loss: 0.787\n",
      "[epoch: 28, i: 11999]  train_loss: 0.788  |  valid_loss: 0.679\n",
      "[epoch: 28, i: 12124]  train_loss: 0.805  |  valid_loss: 0.713\n",
      "[epoch: 28, i: 12249]  train_loss: 0.821  |  valid_loss: 1.044\n",
      "[epoch: 28, i: 12374]  train_loss: 0.817  |  valid_loss: 0.876\n",
      "[epoch: 28, i: 12499]  train_loss: 0.841  |  valid_loss: 0.849\n",
      "--> [End of epoch 28] train_accuracy: 71.17%  |  valid_accuracy: 71.14%\n",
      "--> [Start of epoch 29]  lr: 0.000003\n",
      "[epoch: 29, i:   124]  train_loss: 0.774  |  valid_loss: 0.743\n",
      "[epoch: 29, i:   249]  train_loss: 0.812  |  valid_loss: 0.724\n",
      "[epoch: 29, i:   374]  train_loss: 0.831  |  valid_loss: 0.850\n",
      "[epoch: 29, i:   499]  train_loss: 0.790  |  valid_loss: 0.785\n",
      "[epoch: 29, i:   624]  train_loss: 0.835  |  valid_loss: 0.935\n",
      "[epoch: 29, i:   749]  train_loss: 0.777  |  valid_loss: 0.586\n",
      "[epoch: 29, i:   874]  train_loss: 0.748  |  valid_loss: 0.862\n",
      "[epoch: 29, i:   999]  train_loss: 0.782  |  valid_loss: 0.932\n",
      "[epoch: 29, i:  1124]  train_loss: 0.844  |  valid_loss: 0.971\n",
      "[epoch: 29, i:  1249]  train_loss: 0.808  |  valid_loss: 0.724\n",
      "[epoch: 29, i:  1374]  train_loss: 0.903  |  valid_loss: 0.749\n",
      "[epoch: 29, i:  1499]  train_loss: 0.812  |  valid_loss: 0.824\n",
      "[epoch: 29, i:  1624]  train_loss: 0.837  |  valid_loss: 0.681\n",
      "[epoch: 29, i:  1749]  train_loss: 0.778  |  valid_loss: 0.975\n",
      "[epoch: 29, i:  1874]  train_loss: 0.818  |  valid_loss: 0.765\n",
      "[epoch: 29, i:  1999]  train_loss: 0.771  |  valid_loss: 0.989\n",
      "[epoch: 29, i:  2124]  train_loss: 0.858  |  valid_loss: 0.737\n",
      "[epoch: 29, i:  2249]  train_loss: 0.773  |  valid_loss: 0.838\n",
      "[epoch: 29, i:  2374]  train_loss: 0.754  |  valid_loss: 0.775\n",
      "[epoch: 29, i:  2499]  train_loss: 0.810  |  valid_loss: 0.989\n",
      "[epoch: 29, i:  2624]  train_loss: 0.889  |  valid_loss: 0.911\n",
      "[epoch: 29, i:  2749]  train_loss: 0.846  |  valid_loss: 0.904\n",
      "[epoch: 29, i:  2874]  train_loss: 0.826  |  valid_loss: 1.019\n",
      "[epoch: 29, i:  2999]  train_loss: 0.855  |  valid_loss: 0.827\n",
      "[epoch: 29, i:  3124]  train_loss: 0.858  |  valid_loss: 0.904\n",
      "[epoch: 29, i:  3249]  train_loss: 0.831  |  valid_loss: 1.040\n",
      "[epoch: 29, i:  3374]  train_loss: 0.880  |  valid_loss: 0.784\n",
      "[epoch: 29, i:  3499]  train_loss: 0.861  |  valid_loss: 0.774\n",
      "[epoch: 29, i:  3624]  train_loss: 0.735  |  valid_loss: 0.783\n",
      "[epoch: 29, i:  3749]  train_loss: 0.778  |  valid_loss: 0.684\n",
      "[epoch: 29, i:  3874]  train_loss: 0.792  |  valid_loss: 0.825\n",
      "[epoch: 29, i:  3999]  train_loss: 0.828  |  valid_loss: 0.643\n",
      "[epoch: 29, i:  4124]  train_loss: 0.758  |  valid_loss: 0.895\n",
      "[epoch: 29, i:  4249]  train_loss: 0.871  |  valid_loss: 0.876\n",
      "[epoch: 29, i:  4374]  train_loss: 0.847  |  valid_loss: 0.881\n",
      "[epoch: 29, i:  4499]  train_loss: 0.770  |  valid_loss: 0.781\n",
      "[epoch: 29, i:  4624]  train_loss: 0.845  |  valid_loss: 0.940\n",
      "[epoch: 29, i:  4749]  train_loss: 0.819  |  valid_loss: 0.880\n",
      "[epoch: 29, i:  4874]  train_loss: 0.758  |  valid_loss: 0.753\n",
      "[epoch: 29, i:  4999]  train_loss: 0.739  |  valid_loss: 0.758\n",
      "[epoch: 29, i:  5124]  train_loss: 0.874  |  valid_loss: 0.800\n",
      "[epoch: 29, i:  5249]  train_loss: 0.791  |  valid_loss: 0.900\n",
      "[epoch: 29, i:  5374]  train_loss: 0.846  |  valid_loss: 0.623\n",
      "[epoch: 29, i:  5499]  train_loss: 0.858  |  valid_loss: 0.691\n",
      "[epoch: 29, i:  5624]  train_loss: 0.853  |  valid_loss: 0.906\n",
      "[epoch: 29, i:  5749]  train_loss: 0.811  |  valid_loss: 0.820\n",
      "[epoch: 29, i:  5874]  train_loss: 0.758  |  valid_loss: 0.751\n",
      "[epoch: 29, i:  5999]  train_loss: 0.869  |  valid_loss: 0.867\n",
      "[epoch: 29, i:  6124]  train_loss: 0.777  |  valid_loss: 0.747\n",
      "[epoch: 29, i:  6249]  train_loss: 0.839  |  valid_loss: 0.879\n",
      "[epoch: 29, i:  6374]  train_loss: 0.757  |  valid_loss: 0.740\n",
      "[epoch: 29, i:  6499]  train_loss: 0.807  |  valid_loss: 0.759\n",
      "[epoch: 29, i:  6624]  train_loss: 0.839  |  valid_loss: 0.677\n",
      "[epoch: 29, i:  6749]  train_loss: 0.753  |  valid_loss: 0.829\n",
      "[epoch: 29, i:  6874]  train_loss: 0.850  |  valid_loss: 0.764\n",
      "[epoch: 29, i:  6999]  train_loss: 0.795  |  valid_loss: 0.934\n",
      "[epoch: 29, i:  7124]  train_loss: 0.863  |  valid_loss: 0.952\n",
      "[epoch: 29, i:  7249]  train_loss: 0.821  |  valid_loss: 0.651\n",
      "[epoch: 29, i:  7374]  train_loss: 0.865  |  valid_loss: 1.031\n",
      "[epoch: 29, i:  7499]  train_loss: 0.801  |  valid_loss: 0.900\n",
      "[epoch: 29, i:  7624]  train_loss: 0.825  |  valid_loss: 0.911\n",
      "[epoch: 29, i:  7749]  train_loss: 0.862  |  valid_loss: 0.793\n",
      "[epoch: 29, i:  7874]  train_loss: 0.818  |  valid_loss: 0.853\n",
      "[epoch: 29, i:  7999]  train_loss: 0.775  |  valid_loss: 0.709\n",
      "[epoch: 29, i:  8124]  train_loss: 0.805  |  valid_loss: 0.932\n",
      "[epoch: 29, i:  8249]  train_loss: 0.781  |  valid_loss: 0.918\n",
      "[epoch: 29, i:  8374]  train_loss: 0.759  |  valid_loss: 0.834\n",
      "[epoch: 29, i:  8499]  train_loss: 0.718  |  valid_loss: 0.906\n",
      "[epoch: 29, i:  8624]  train_loss: 0.785  |  valid_loss: 0.848\n",
      "[epoch: 29, i:  8749]  train_loss: 0.827  |  valid_loss: 0.902\n",
      "[epoch: 29, i:  8874]  train_loss: 0.825  |  valid_loss: 0.851\n",
      "[epoch: 29, i:  8999]  train_loss: 0.833  |  valid_loss: 0.682\n",
      "[epoch: 29, i:  9124]  train_loss: 0.743  |  valid_loss: 0.720\n",
      "[epoch: 29, i:  9249]  train_loss: 0.810  |  valid_loss: 0.613\n",
      "[epoch: 29, i:  9374]  train_loss: 0.912  |  valid_loss: 0.893\n",
      "[epoch: 29, i:  9499]  train_loss: 0.842  |  valid_loss: 0.751\n",
      "[epoch: 29, i:  9624]  train_loss: 0.769  |  valid_loss: 0.898\n",
      "[epoch: 29, i:  9749]  train_loss: 0.794  |  valid_loss: 0.926\n",
      "[epoch: 29, i:  9874]  train_loss: 0.800  |  valid_loss: 0.890\n",
      "[epoch: 29, i:  9999]  train_loss: 0.768  |  valid_loss: 0.814\n",
      "[epoch: 29, i: 10124]  train_loss: 0.800  |  valid_loss: 0.757\n",
      "[epoch: 29, i: 10249]  train_loss: 0.817  |  valid_loss: 0.854\n",
      "[epoch: 29, i: 10374]  train_loss: 0.818  |  valid_loss: 0.743\n",
      "[epoch: 29, i: 10499]  train_loss: 0.817  |  valid_loss: 0.930\n",
      "[epoch: 29, i: 10624]  train_loss: 0.826  |  valid_loss: 0.956\n",
      "[epoch: 29, i: 10749]  train_loss: 0.783  |  valid_loss: 0.837\n",
      "[epoch: 29, i: 10874]  train_loss: 0.845  |  valid_loss: 0.961\n",
      "[epoch: 29, i: 10999]  train_loss: 0.847  |  valid_loss: 0.963\n",
      "[epoch: 29, i: 11124]  train_loss: 0.840  |  valid_loss: 0.823\n",
      "[epoch: 29, i: 11249]  train_loss: 0.852  |  valid_loss: 0.886\n",
      "[epoch: 29, i: 11374]  train_loss: 0.825  |  valid_loss: 0.796\n",
      "[epoch: 29, i: 11499]  train_loss: 0.773  |  valid_loss: 0.630\n",
      "[epoch: 29, i: 11624]  train_loss: 0.780  |  valid_loss: 0.805\n",
      "[epoch: 29, i: 11749]  train_loss: 0.806  |  valid_loss: 0.750\n",
      "[epoch: 29, i: 11874]  train_loss: 0.875  |  valid_loss: 0.793\n",
      "[epoch: 29, i: 11999]  train_loss: 0.767  |  valid_loss: 0.697\n",
      "[epoch: 29, i: 12124]  train_loss: 0.823  |  valid_loss: 0.697\n",
      "[epoch: 29, i: 12249]  train_loss: 0.841  |  valid_loss: 1.042\n",
      "[epoch: 29, i: 12374]  train_loss: 0.774  |  valid_loss: 0.906\n",
      "[epoch: 29, i: 12499]  train_loss: 0.807  |  valid_loss: 0.862\n",
      "--> [End of epoch 29] train_accuracy: 71.37%  |  valid_accuracy: 71.11%\n",
      "--> [Start of epoch 30]  lr: 0.000002\n",
      "[epoch: 30, i:   124]  train_loss: 0.720  |  valid_loss: 0.717\n",
      "[epoch: 30, i:   249]  train_loss: 0.885  |  valid_loss: 0.731\n",
      "[epoch: 30, i:   374]  train_loss: 0.836  |  valid_loss: 0.874\n",
      "[epoch: 30, i:   499]  train_loss: 0.808  |  valid_loss: 0.818\n",
      "[epoch: 30, i:   624]  train_loss: 0.816  |  valid_loss: 0.923\n",
      "[epoch: 30, i:   749]  train_loss: 0.837  |  valid_loss: 0.579\n",
      "[epoch: 30, i:   874]  train_loss: 0.823  |  valid_loss: 0.841\n",
      "[epoch: 30, i:   999]  train_loss: 0.846  |  valid_loss: 0.911\n",
      "[epoch: 30, i:  1124]  train_loss: 0.835  |  valid_loss: 0.918\n",
      "[epoch: 30, i:  1249]  train_loss: 0.745  |  valid_loss: 0.711\n",
      "[epoch: 30, i:  1374]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  1499]  train_loss: 0.774  |  valid_loss: 0.822\n",
      "[epoch: 30, i:  1624]  train_loss: 0.787  |  valid_loss: 0.683\n",
      "[epoch: 30, i:  1749]  train_loss: 0.846  |  valid_loss: 0.938\n",
      "[epoch: 30, i:  1874]  train_loss: 0.839  |  valid_loss: 0.767\n",
      "[epoch: 30, i:  1999]  train_loss: 0.766  |  valid_loss: 0.964\n",
      "[epoch: 30, i:  2124]  train_loss: 0.740  |  valid_loss: 0.705\n",
      "[epoch: 30, i:  2249]  train_loss: 0.854  |  valid_loss: 0.807\n",
      "[epoch: 30, i:  2374]  train_loss: 0.846  |  valid_loss: 0.756\n",
      "[epoch: 30, i:  2499]  train_loss: 0.818  |  valid_loss: 0.996\n",
      "[epoch: 30, i:  2624]  train_loss: 0.839  |  valid_loss: 0.923\n",
      "[epoch: 30, i:  2749]  train_loss: 0.861  |  valid_loss: 0.919\n",
      "[epoch: 30, i:  2874]  train_loss: 0.738  |  valid_loss: 1.005\n",
      "[epoch: 30, i:  2999]  train_loss: 0.844  |  valid_loss: 0.846\n",
      "[epoch: 30, i:  3124]  train_loss: 0.783  |  valid_loss: 0.894\n",
      "[epoch: 30, i:  3249]  train_loss: 0.812  |  valid_loss: 1.043\n",
      "[epoch: 30, i:  3374]  train_loss: 0.801  |  valid_loss: 0.783\n",
      "[epoch: 30, i:  3499]  train_loss: 0.887  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  3624]  train_loss: 0.816  |  valid_loss: 0.818\n",
      "[epoch: 30, i:  3749]  train_loss: 0.733  |  valid_loss: 0.703\n",
      "[epoch: 30, i:  3874]  train_loss: 0.773  |  valid_loss: 0.822\n",
      "[epoch: 30, i:  3999]  train_loss: 0.870  |  valid_loss: 0.633\n",
      "[epoch: 30, i:  4124]  train_loss: 0.841  |  valid_loss: 0.915\n",
      "[epoch: 30, i:  4249]  train_loss: 0.808  |  valid_loss: 0.886\n",
      "[epoch: 30, i:  4374]  train_loss: 0.825  |  valid_loss: 0.871\n",
      "[epoch: 30, i:  4499]  train_loss: 0.768  |  valid_loss: 0.773\n",
      "[epoch: 30, i:  4624]  train_loss: 0.841  |  valid_loss: 0.955\n",
      "[epoch: 30, i:  4749]  train_loss: 0.794  |  valid_loss: 0.884\n",
      "[epoch: 30, i:  4874]  train_loss: 0.842  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  4999]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 30, i:  5124]  train_loss: 0.743  |  valid_loss: 0.824\n",
      "[epoch: 30, i:  5249]  train_loss: 0.831  |  valid_loss: 0.865\n",
      "[epoch: 30, i:  5374]  train_loss: 0.863  |  valid_loss: 0.647\n",
      "[epoch: 30, i:  5499]  train_loss: 0.821  |  valid_loss: 0.706\n",
      "[epoch: 30, i:  5624]  train_loss: 0.791  |  valid_loss: 1.014\n",
      "[epoch: 30, i:  5749]  train_loss: 0.841  |  valid_loss: 0.808\n",
      "[epoch: 30, i:  5874]  train_loss: 0.808  |  valid_loss: 0.723\n",
      "[epoch: 30, i:  5999]  train_loss: 0.782  |  valid_loss: 0.880\n",
      "[epoch: 30, i:  6124]  train_loss: 0.751  |  valid_loss: 0.743\n",
      "[epoch: 30, i:  6249]  train_loss: 0.768  |  valid_loss: 0.881\n",
      "[epoch: 30, i:  6374]  train_loss: 0.786  |  valid_loss: 0.726\n",
      "[epoch: 30, i:  6499]  train_loss: 0.789  |  valid_loss: 0.778\n",
      "[epoch: 30, i:  6624]  train_loss: 0.804  |  valid_loss: 0.655\n",
      "[epoch: 30, i:  6749]  train_loss: 0.811  |  valid_loss: 0.818\n",
      "[epoch: 30, i:  6874]  train_loss: 0.793  |  valid_loss: 0.774\n",
      "[epoch: 30, i:  6999]  train_loss: 0.763  |  valid_loss: 0.957\n",
      "[epoch: 30, i:  7124]  train_loss: 0.792  |  valid_loss: 0.994\n",
      "[epoch: 30, i:  7249]  train_loss: 0.816  |  valid_loss: 0.669\n",
      "[epoch: 30, i:  7374]  train_loss: 0.838  |  valid_loss: 1.019\n",
      "[epoch: 30, i:  7499]  train_loss: 0.797  |  valid_loss: 0.893\n",
      "[epoch: 30, i:  7624]  train_loss: 0.758  |  valid_loss: 0.910\n",
      "[epoch: 30, i:  7749]  train_loss: 0.871  |  valid_loss: 0.801\n",
      "[epoch: 30, i:  7874]  train_loss: 0.803  |  valid_loss: 0.863\n",
      "[epoch: 30, i:  7999]  train_loss: 0.827  |  valid_loss: 0.712\n",
      "[epoch: 30, i:  8124]  train_loss: 0.790  |  valid_loss: 0.906\n",
      "[epoch: 30, i:  8249]  train_loss: 0.804  |  valid_loss: 0.888\n",
      "[epoch: 30, i:  8374]  train_loss: 0.822  |  valid_loss: 0.824\n",
      "[epoch: 30, i:  8499]  train_loss: 0.847  |  valid_loss: 0.893\n",
      "[epoch: 30, i:  8624]  train_loss: 0.777  |  valid_loss: 0.854\n",
      "[epoch: 30, i:  8749]  train_loss: 0.774  |  valid_loss: 0.891\n",
      "[epoch: 30, i:  8874]  train_loss: 0.814  |  valid_loss: 0.865\n",
      "[epoch: 30, i:  8999]  train_loss: 0.845  |  valid_loss: 0.691\n",
      "[epoch: 30, i:  9124]  train_loss: 0.728  |  valid_loss: 0.747\n",
      "[epoch: 30, i:  9249]  train_loss: 0.876  |  valid_loss: 0.611\n",
      "[epoch: 30, i:  9374]  train_loss: 0.774  |  valid_loss: 0.900\n",
      "[epoch: 30, i:  9499]  train_loss: 0.789  |  valid_loss: 0.774\n",
      "[epoch: 30, i:  9624]  train_loss: 0.810  |  valid_loss: 0.869\n",
      "[epoch: 30, i:  9749]  train_loss: 0.936  |  valid_loss: 0.913\n",
      "[epoch: 30, i:  9874]  train_loss: 0.821  |  valid_loss: 0.898\n",
      "[epoch: 30, i:  9999]  train_loss: 0.881  |  valid_loss: 0.821\n",
      "[epoch: 30, i: 10124]  train_loss: 0.795  |  valid_loss: 0.751\n",
      "[epoch: 30, i: 10249]  train_loss: 0.756  |  valid_loss: 0.859\n",
      "[epoch: 30, i: 10374]  train_loss: 0.842  |  valid_loss: 0.736\n",
      "[epoch: 30, i: 10499]  train_loss: 0.738  |  valid_loss: 0.957\n",
      "[epoch: 30, i: 10624]  train_loss: 0.881  |  valid_loss: 0.938\n",
      "[epoch: 30, i: 10749]  train_loss: 0.784  |  valid_loss: 0.832\n",
      "[epoch: 30, i: 10874]  train_loss: 0.796  |  valid_loss: 0.932\n",
      "[epoch: 30, i: 10999]  train_loss: 0.877  |  valid_loss: 0.944\n",
      "[epoch: 30, i: 11124]  train_loss: 0.774  |  valid_loss: 0.820\n",
      "[epoch: 30, i: 11249]  train_loss: 0.827  |  valid_loss: 0.857\n",
      "[epoch: 30, i: 11374]  train_loss: 0.790  |  valid_loss: 0.796\n",
      "[epoch: 30, i: 11499]  train_loss: 0.748  |  valid_loss: 0.591\n",
      "[epoch: 30, i: 11624]  train_loss: 0.875  |  valid_loss: 0.792\n",
      "[epoch: 30, i: 11749]  train_loss: 0.873  |  valid_loss: 0.749\n",
      "[epoch: 30, i: 11874]  train_loss: 0.840  |  valid_loss: 0.788\n",
      "[epoch: 30, i: 11999]  train_loss: 0.752  |  valid_loss: 0.678\n",
      "[epoch: 30, i: 12124]  train_loss: 0.792  |  valid_loss: 0.716\n",
      "[epoch: 30, i: 12249]  train_loss: 0.752  |  valid_loss: 1.063\n",
      "[epoch: 30, i: 12374]  train_loss: 0.803  |  valid_loss: 0.868\n",
      "[epoch: 30, i: 12499]  train_loss: 0.792  |  valid_loss: 0.859\n",
      "--> [End of epoch 30] train_accuracy: 71.39%  |  valid_accuracy: 71.06%\n",
      "--> [Start of epoch 31]  lr: 0.000002\n",
      "[epoch: 31, i:   124]  train_loss: 0.792  |  valid_loss: 0.744\n",
      "[epoch: 31, i:   249]  train_loss: 0.792  |  valid_loss: 0.751\n",
      "[epoch: 31, i:   374]  train_loss: 0.840  |  valid_loss: 0.847\n",
      "[epoch: 31, i:   499]  train_loss: 0.742  |  valid_loss: 0.802\n",
      "[epoch: 31, i:   624]  train_loss: 0.839  |  valid_loss: 0.944\n",
      "[epoch: 31, i:   749]  train_loss: 0.881  |  valid_loss: 0.567\n",
      "[epoch: 31, i:   874]  train_loss: 0.875  |  valid_loss: 0.852\n",
      "[epoch: 31, i:   999]  train_loss: 0.776  |  valid_loss: 0.934\n",
      "[epoch: 31, i:  1124]  train_loss: 0.848  |  valid_loss: 0.888\n",
      "[epoch: 31, i:  1249]  train_loss: 0.853  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  1374]  train_loss: 0.829  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  1499]  train_loss: 0.803  |  valid_loss: 0.804\n",
      "[epoch: 31, i:  1624]  train_loss: 0.845  |  valid_loss: 0.683\n",
      "[epoch: 31, i:  1749]  train_loss: 0.840  |  valid_loss: 0.997\n",
      "[epoch: 31, i:  1874]  train_loss: 0.792  |  valid_loss: 0.738\n",
      "[epoch: 31, i:  1999]  train_loss: 0.807  |  valid_loss: 0.985\n",
      "[epoch: 31, i:  2124]  train_loss: 0.806  |  valid_loss: 0.721\n",
      "[epoch: 31, i:  2249]  train_loss: 0.794  |  valid_loss: 0.836\n",
      "[epoch: 31, i:  2374]  train_loss: 0.808  |  valid_loss: 0.771\n",
      "[epoch: 31, i:  2499]  train_loss: 0.848  |  valid_loss: 1.008\n",
      "[epoch: 31, i:  2624]  train_loss: 0.804  |  valid_loss: 0.909\n",
      "[epoch: 31, i:  2749]  train_loss: 0.748  |  valid_loss: 0.921\n",
      "[epoch: 31, i:  2874]  train_loss: 0.845  |  valid_loss: 1.024\n",
      "[epoch: 31, i:  2999]  train_loss: 0.837  |  valid_loss: 0.822\n",
      "[epoch: 31, i:  3124]  train_loss: 0.776  |  valid_loss: 0.913\n",
      "[epoch: 31, i:  3249]  train_loss: 0.782  |  valid_loss: 1.078\n",
      "[epoch: 31, i:  3374]  train_loss: 0.760  |  valid_loss: 0.806\n",
      "[epoch: 31, i:  3499]  train_loss: 0.772  |  valid_loss: 0.745\n",
      "[epoch: 31, i:  3624]  train_loss: 0.774  |  valid_loss: 0.788\n",
      "[epoch: 31, i:  3749]  train_loss: 0.760  |  valid_loss: 0.678\n",
      "[epoch: 31, i:  3874]  train_loss: 0.836  |  valid_loss: 0.790\n",
      "[epoch: 31, i:  3999]  train_loss: 0.782  |  valid_loss: 0.636\n",
      "[epoch: 31, i:  4124]  train_loss: 0.810  |  valid_loss: 0.850\n",
      "[epoch: 31, i:  4249]  train_loss: 0.813  |  valid_loss: 0.875\n",
      "[epoch: 31, i:  4374]  train_loss: 0.793  |  valid_loss: 0.860\n",
      "[epoch: 31, i:  4499]  train_loss: 0.818  |  valid_loss: 0.786\n",
      "[epoch: 31, i:  4624]  train_loss: 0.832  |  valid_loss: 0.934\n",
      "[epoch: 31, i:  4749]  train_loss: 0.809  |  valid_loss: 0.880\n",
      "[epoch: 31, i:  4874]  train_loss: 0.762  |  valid_loss: 0.735\n",
      "[epoch: 31, i:  4999]  train_loss: 0.789  |  valid_loss: 0.751\n",
      "[epoch: 31, i:  5124]  train_loss: 0.756  |  valid_loss: 0.812\n",
      "[epoch: 31, i:  5249]  train_loss: 0.847  |  valid_loss: 0.898\n",
      "[epoch: 31, i:  5374]  train_loss: 0.768  |  valid_loss: 0.641\n",
      "[epoch: 31, i:  5499]  train_loss: 0.817  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  5624]  train_loss: 0.815  |  valid_loss: 0.956\n",
      "[epoch: 31, i:  5749]  train_loss: 0.853  |  valid_loss: 0.818\n",
      "[epoch: 31, i:  5874]  train_loss: 0.825  |  valid_loss: 0.724\n",
      "[epoch: 31, i:  5999]  train_loss: 0.847  |  valid_loss: 0.855\n",
      "[epoch: 31, i:  6124]  train_loss: 0.875  |  valid_loss: 0.730\n",
      "[epoch: 31, i:  6249]  train_loss: 0.846  |  valid_loss: 0.858\n",
      "[epoch: 31, i:  6374]  train_loss: 0.834  |  valid_loss: 0.747\n",
      "[epoch: 31, i:  6499]  train_loss: 0.794  |  valid_loss: 0.775\n",
      "[epoch: 31, i:  6624]  train_loss: 0.751  |  valid_loss: 0.685\n",
      "[epoch: 31, i:  6749]  train_loss: 0.712  |  valid_loss: 0.831\n",
      "[epoch: 31, i:  6874]  train_loss: 0.805  |  valid_loss: 0.772\n",
      "[epoch: 31, i:  6999]  train_loss: 0.855  |  valid_loss: 0.961\n",
      "[epoch: 31, i:  7124]  train_loss: 0.808  |  valid_loss: 0.975\n",
      "[epoch: 31, i:  7249]  train_loss: 0.786  |  valid_loss: 0.633\n",
      "[epoch: 31, i:  7374]  train_loss: 0.772  |  valid_loss: 1.031\n",
      "[epoch: 31, i:  7499]  train_loss: 0.834  |  valid_loss: 0.916\n",
      "[epoch: 31, i:  7624]  train_loss: 0.831  |  valid_loss: 0.928\n",
      "[epoch: 31, i:  7749]  train_loss: 0.812  |  valid_loss: 0.815\n",
      "[epoch: 31, i:  7874]  train_loss: 0.820  |  valid_loss: 0.884\n",
      "[epoch: 31, i:  7999]  train_loss: 0.804  |  valid_loss: 0.706\n",
      "[epoch: 31, i:  8124]  train_loss: 0.836  |  valid_loss: 0.915\n",
      "[epoch: 31, i:  8249]  train_loss: 0.815  |  valid_loss: 0.914\n",
      "[epoch: 31, i:  8374]  train_loss: 0.805  |  valid_loss: 0.825\n",
      "[epoch: 31, i:  8499]  train_loss: 0.804  |  valid_loss: 0.889\n",
      "[epoch: 31, i:  8624]  train_loss: 0.802  |  valid_loss: 0.851\n",
      "[epoch: 31, i:  8749]  train_loss: 0.844  |  valid_loss: 0.904\n",
      "[epoch: 31, i:  8874]  train_loss: 0.789  |  valid_loss: 0.867\n",
      "[epoch: 31, i:  8999]  train_loss: 0.847  |  valid_loss: 0.687\n",
      "[epoch: 31, i:  9124]  train_loss: 0.812  |  valid_loss: 0.745\n",
      "[epoch: 31, i:  9249]  train_loss: 0.808  |  valid_loss: 0.626\n",
      "[epoch: 31, i:  9374]  train_loss: 0.806  |  valid_loss: 0.909\n",
      "[epoch: 31, i:  9499]  train_loss: 0.783  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  9624]  train_loss: 0.816  |  valid_loss: 0.876\n",
      "[epoch: 31, i:  9749]  train_loss: 0.790  |  valid_loss: 0.908\n",
      "[epoch: 31, i:  9874]  train_loss: 0.792  |  valid_loss: 0.891\n",
      "[epoch: 31, i:  9999]  train_loss: 0.804  |  valid_loss: 0.813\n",
      "[epoch: 31, i: 10124]  train_loss: 0.779  |  valid_loss: 0.751\n",
      "[epoch: 31, i: 10249]  train_loss: 0.833  |  valid_loss: 0.841\n",
      "[epoch: 31, i: 10374]  train_loss: 0.799  |  valid_loss: 0.778\n",
      "[epoch: 31, i: 10499]  train_loss: 0.832  |  valid_loss: 0.983\n",
      "[epoch: 31, i: 10624]  train_loss: 0.745  |  valid_loss: 0.975\n",
      "[epoch: 31, i: 10749]  train_loss: 0.861  |  valid_loss: 0.872\n",
      "[epoch: 31, i: 10874]  train_loss: 0.863  |  valid_loss: 0.935\n",
      "[epoch: 31, i: 10999]  train_loss: 0.871  |  valid_loss: 0.948\n",
      "[epoch: 31, i: 11124]  train_loss: 0.864  |  valid_loss: 0.831\n",
      "[epoch: 31, i: 11249]  train_loss: 0.807  |  valid_loss: 0.874\n",
      "[epoch: 31, i: 11374]  train_loss: 0.840  |  valid_loss: 0.792\n",
      "[epoch: 31, i: 11499]  train_loss: 0.769  |  valid_loss: 0.608\n",
      "[epoch: 31, i: 11624]  train_loss: 0.831  |  valid_loss: 0.783\n",
      "[epoch: 31, i: 11749]  train_loss: 0.826  |  valid_loss: 0.749\n",
      "[epoch: 31, i: 11874]  train_loss: 0.798  |  valid_loss: 0.779\n",
      "[epoch: 31, i: 11999]  train_loss: 0.803  |  valid_loss: 0.676\n",
      "[epoch: 31, i: 12124]  train_loss: 0.843  |  valid_loss: 0.742\n",
      "[epoch: 31, i: 12249]  train_loss: 0.828  |  valid_loss: 1.038\n",
      "[epoch: 31, i: 12374]  train_loss: 0.816  |  valid_loss: 0.901\n",
      "[epoch: 31, i: 12499]  train_loss: 0.773  |  valid_loss: 0.875\n",
      "--> [End of epoch 31] train_accuracy: 71.47%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 32]  lr: 0.000002\n",
      "[epoch: 32, i:   124]  train_loss: 0.837  |  valid_loss: 0.731\n",
      "[epoch: 32, i:   249]  train_loss: 0.810  |  valid_loss: 0.729\n",
      "[epoch: 32, i:   374]  train_loss: 0.862  |  valid_loss: 0.865\n",
      "[epoch: 32, i:   499]  train_loss: 0.797  |  valid_loss: 0.801\n",
      "[epoch: 32, i:   624]  train_loss: 0.800  |  valid_loss: 0.909\n",
      "[epoch: 32, i:   749]  train_loss: 0.910  |  valid_loss: 0.589\n",
      "[epoch: 32, i:   874]  train_loss: 0.844  |  valid_loss: 0.872\n",
      "[epoch: 32, i:   999]  train_loss: 0.840  |  valid_loss: 0.914\n",
      "[epoch: 32, i:  1124]  train_loss: 0.710  |  valid_loss: 0.918\n",
      "[epoch: 32, i:  1249]  train_loss: 0.864  |  valid_loss: 0.727\n",
      "[epoch: 32, i:  1374]  train_loss: 0.803  |  valid_loss: 0.794\n",
      "[epoch: 32, i:  1499]  train_loss: 0.821  |  valid_loss: 0.828\n",
      "[epoch: 32, i:  1624]  train_loss: 0.817  |  valid_loss: 0.668\n",
      "[epoch: 32, i:  1749]  train_loss: 0.802  |  valid_loss: 0.980\n",
      "[epoch: 32, i:  1874]  train_loss: 0.852  |  valid_loss: 0.774\n",
      "[epoch: 32, i:  1999]  train_loss: 0.820  |  valid_loss: 0.978\n",
      "[epoch: 32, i:  2124]  train_loss: 0.763  |  valid_loss: 0.734\n",
      "[epoch: 32, i:  2249]  train_loss: 0.769  |  valid_loss: 0.823\n",
      "[epoch: 32, i:  2374]  train_loss: 0.771  |  valid_loss: 0.764\n",
      "[epoch: 32, i:  2499]  train_loss: 0.811  |  valid_loss: 1.000\n",
      "[epoch: 32, i:  2624]  train_loss: 0.740  |  valid_loss: 0.902\n",
      "[epoch: 32, i:  2749]  train_loss: 0.811  |  valid_loss: 0.979\n",
      "[epoch: 32, i:  2874]  train_loss: 0.887  |  valid_loss: 1.049\n",
      "[epoch: 32, i:  2999]  train_loss: 0.831  |  valid_loss: 0.841\n",
      "[epoch: 32, i:  3124]  train_loss: 0.802  |  valid_loss: 0.897\n",
      "[epoch: 32, i:  3249]  train_loss: 0.817  |  valid_loss: 1.058\n",
      "[epoch: 32, i:  3374]  train_loss: 0.876  |  valid_loss: 0.829\n",
      "[epoch: 32, i:  3499]  train_loss: 0.782  |  valid_loss: 0.753\n",
      "[epoch: 32, i:  3624]  train_loss: 0.712  |  valid_loss: 0.771\n",
      "[epoch: 32, i:  3749]  train_loss: 0.798  |  valid_loss: 0.699\n",
      "[epoch: 32, i:  3874]  train_loss: 0.825  |  valid_loss: 0.812\n",
      "[epoch: 32, i:  3999]  train_loss: 0.786  |  valid_loss: 0.628\n",
      "[epoch: 32, i:  4124]  train_loss: 0.777  |  valid_loss: 0.827\n",
      "[epoch: 32, i:  4249]  train_loss: 0.785  |  valid_loss: 0.877\n",
      "[epoch: 32, i:  4374]  train_loss: 0.824  |  valid_loss: 0.879\n",
      "[epoch: 32, i:  4499]  train_loss: 0.733  |  valid_loss: 0.776\n",
      "[epoch: 32, i:  4624]  train_loss: 0.841  |  valid_loss: 0.937\n",
      "[epoch: 32, i:  4749]  train_loss: 0.816  |  valid_loss: 0.896\n",
      "[epoch: 32, i:  4874]  train_loss: 0.810  |  valid_loss: 0.744\n",
      "[epoch: 32, i:  4999]  train_loss: 0.860  |  valid_loss: 0.727\n",
      "[epoch: 32, i:  5124]  train_loss: 0.845  |  valid_loss: 0.824\n",
      "[epoch: 32, i:  5249]  train_loss: 0.782  |  valid_loss: 0.884\n",
      "[epoch: 32, i:  5374]  train_loss: 0.841  |  valid_loss: 0.640\n",
      "[epoch: 32, i:  5499]  train_loss: 0.819  |  valid_loss: 0.699\n",
      "[epoch: 32, i:  5624]  train_loss: 0.848  |  valid_loss: 0.870\n",
      "[epoch: 32, i:  5749]  train_loss: 0.741  |  valid_loss: 0.833\n",
      "[epoch: 32, i:  5874]  train_loss: 0.744  |  valid_loss: 0.744\n",
      "[epoch: 32, i:  5999]  train_loss: 0.837  |  valid_loss: 0.850\n",
      "[epoch: 32, i:  6124]  train_loss: 0.851  |  valid_loss: 0.768\n",
      "[epoch: 32, i:  6249]  train_loss: 0.837  |  valid_loss: 0.864\n",
      "[epoch: 32, i:  6374]  train_loss: 0.745  |  valid_loss: 0.734\n",
      "[epoch: 32, i:  6499]  train_loss: 0.795  |  valid_loss: 0.767\n",
      "[epoch: 32, i:  6624]  train_loss: 0.830  |  valid_loss: 0.671\n",
      "[epoch: 32, i:  6749]  train_loss: 0.843  |  valid_loss: 0.793\n",
      "[epoch: 32, i:  6874]  train_loss: 0.819  |  valid_loss: 0.749\n",
      "[epoch: 32, i:  6999]  train_loss: 0.830  |  valid_loss: 0.962\n",
      "[epoch: 32, i:  7124]  train_loss: 0.884  |  valid_loss: 0.961\n",
      "[epoch: 32, i:  7249]  train_loss: 0.880  |  valid_loss: 0.673\n",
      "[epoch: 32, i:  7374]  train_loss: 0.850  |  valid_loss: 1.050\n",
      "[epoch: 32, i:  7499]  train_loss: 0.782  |  valid_loss: 0.898\n",
      "[epoch: 32, i:  7624]  train_loss: 0.800  |  valid_loss: 0.895\n",
      "[epoch: 32, i:  7749]  train_loss: 0.799  |  valid_loss: 0.800\n",
      "[epoch: 32, i:  7874]  train_loss: 0.743  |  valid_loss: 0.865\n",
      "[epoch: 32, i:  7999]  train_loss: 0.861  |  valid_loss: 0.716\n",
      "[epoch: 32, i:  8124]  train_loss: 0.755  |  valid_loss: 0.934\n",
      "[epoch: 32, i:  8249]  train_loss: 0.839  |  valid_loss: 0.903\n",
      "[epoch: 32, i:  8374]  train_loss: 0.896  |  valid_loss: 0.846\n",
      "[epoch: 32, i:  8499]  train_loss: 0.826  |  valid_loss: 0.906\n",
      "[epoch: 32, i:  8624]  train_loss: 0.796  |  valid_loss: 0.884\n",
      "[epoch: 32, i:  8749]  train_loss: 0.858  |  valid_loss: 0.895\n",
      "[epoch: 32, i:  8874]  train_loss: 0.821  |  valid_loss: 0.830\n",
      "[epoch: 32, i:  8999]  train_loss: 0.814  |  valid_loss: 0.695\n",
      "[epoch: 32, i:  9124]  train_loss: 0.813  |  valid_loss: 0.750\n",
      "[epoch: 32, i:  9249]  train_loss: 0.796  |  valid_loss: 0.613\n",
      "[epoch: 32, i:  9374]  train_loss: 0.802  |  valid_loss: 0.879\n",
      "[epoch: 32, i:  9499]  train_loss: 0.777  |  valid_loss: 0.765\n",
      "[epoch: 32, i:  9624]  train_loss: 0.816  |  valid_loss: 0.860\n",
      "[epoch: 32, i:  9749]  train_loss: 0.799  |  valid_loss: 0.903\n",
      "[epoch: 32, i:  9874]  train_loss: 0.810  |  valid_loss: 0.890\n",
      "[epoch: 32, i:  9999]  train_loss: 0.825  |  valid_loss: 0.828\n",
      "[epoch: 32, i: 10124]  train_loss: 0.759  |  valid_loss: 0.770\n",
      "[epoch: 32, i: 10249]  train_loss: 0.795  |  valid_loss: 0.860\n",
      "[epoch: 32, i: 10374]  train_loss: 0.860  |  valid_loss: 0.757\n",
      "[epoch: 32, i: 10499]  train_loss: 0.741  |  valid_loss: 0.945\n",
      "[epoch: 32, i: 10624]  train_loss: 0.803  |  valid_loss: 0.962\n",
      "[epoch: 32, i: 10749]  train_loss: 0.845  |  valid_loss: 0.839\n",
      "[epoch: 32, i: 10874]  train_loss: 0.848  |  valid_loss: 0.961\n",
      "[epoch: 32, i: 10999]  train_loss: 0.791  |  valid_loss: 0.966\n",
      "[epoch: 32, i: 11124]  train_loss: 0.844  |  valid_loss: 0.818\n",
      "[epoch: 32, i: 11249]  train_loss: 0.800  |  valid_loss: 0.860\n",
      "[epoch: 32, i: 11374]  train_loss: 0.730  |  valid_loss: 0.789\n",
      "[epoch: 32, i: 11499]  train_loss: 0.849  |  valid_loss: 0.622\n",
      "[epoch: 32, i: 11624]  train_loss: 0.835  |  valid_loss: 0.797\n",
      "[epoch: 32, i: 11749]  train_loss: 0.765  |  valid_loss: 0.734\n",
      "[epoch: 32, i: 11874]  train_loss: 0.786  |  valid_loss: 0.776\n",
      "[epoch: 32, i: 11999]  train_loss: 0.759  |  valid_loss: 0.693\n",
      "[epoch: 32, i: 12124]  train_loss: 0.763  |  valid_loss: 0.725\n",
      "[epoch: 32, i: 12249]  train_loss: 0.864  |  valid_loss: 1.085\n",
      "[epoch: 32, i: 12374]  train_loss: 0.825  |  valid_loss: 0.923\n",
      "[epoch: 32, i: 12499]  train_loss: 0.848  |  valid_loss: 0.853\n",
      "--> [End of epoch 32] train_accuracy: 71.32%  |  valid_accuracy: 71.04%\n",
      "--> [Start of epoch 33]  lr: 0.000001\n",
      "[epoch: 33, i:   124]  train_loss: 0.879  |  valid_loss: 0.734\n",
      "[epoch: 33, i:   249]  train_loss: 0.751  |  valid_loss: 0.743\n",
      "[epoch: 33, i:   374]  train_loss: 0.818  |  valid_loss: 0.854\n",
      "[epoch: 33, i:   499]  train_loss: 0.803  |  valid_loss: 0.775\n",
      "[epoch: 33, i:   624]  train_loss: 0.855  |  valid_loss: 0.951\n",
      "[epoch: 33, i:   749]  train_loss: 0.899  |  valid_loss: 0.565\n",
      "[epoch: 33, i:   874]  train_loss: 0.823  |  valid_loss: 0.849\n",
      "[epoch: 33, i:   999]  train_loss: 0.782  |  valid_loss: 0.939\n",
      "[epoch: 33, i:  1124]  train_loss: 0.793  |  valid_loss: 0.922\n",
      "[epoch: 33, i:  1249]  train_loss: 0.856  |  valid_loss: 0.727\n",
      "[epoch: 33, i:  1374]  train_loss: 0.712  |  valid_loss: 0.762\n",
      "[epoch: 33, i:  1499]  train_loss: 0.808  |  valid_loss: 0.804\n",
      "[epoch: 33, i:  1624]  train_loss: 0.807  |  valid_loss: 0.695\n",
      "[epoch: 33, i:  1749]  train_loss: 0.839  |  valid_loss: 0.963\n",
      "[epoch: 33, i:  1874]  train_loss: 0.758  |  valid_loss: 0.779\n",
      "[epoch: 33, i:  1999]  train_loss: 0.842  |  valid_loss: 1.010\n",
      "[epoch: 33, i:  2124]  train_loss: 0.836  |  valid_loss: 0.705\n",
      "[epoch: 33, i:  2249]  train_loss: 0.799  |  valid_loss: 0.817\n",
      "[epoch: 33, i:  2374]  train_loss: 0.825  |  valid_loss: 0.781\n",
      "[epoch: 33, i:  2499]  train_loss: 0.835  |  valid_loss: 0.997\n",
      "[epoch: 33, i:  2624]  train_loss: 0.843  |  valid_loss: 0.940\n",
      "[epoch: 33, i:  2749]  train_loss: 0.814  |  valid_loss: 0.917\n",
      "[epoch: 33, i:  2874]  train_loss: 0.774  |  valid_loss: 0.962\n",
      "[epoch: 33, i:  2999]  train_loss: 0.777  |  valid_loss: 0.830\n",
      "[epoch: 33, i:  3124]  train_loss: 0.810  |  valid_loss: 0.897\n",
      "[epoch: 33, i:  3249]  train_loss: 0.727  |  valid_loss: 1.039\n",
      "[epoch: 33, i:  3374]  train_loss: 0.862  |  valid_loss: 0.799\n",
      "[epoch: 33, i:  3499]  train_loss: 0.776  |  valid_loss: 0.744\n",
      "[epoch: 33, i:  3624]  train_loss: 0.810  |  valid_loss: 0.780\n",
      "[epoch: 33, i:  3749]  train_loss: 0.849  |  valid_loss: 0.697\n",
      "[epoch: 33, i:  3874]  train_loss: 0.813  |  valid_loss: 0.797\n",
      "[epoch: 33, i:  3999]  train_loss: 0.870  |  valid_loss: 0.636\n",
      "[epoch: 33, i:  4124]  train_loss: 0.895  |  valid_loss: 0.855\n",
      "[epoch: 33, i:  4249]  train_loss: 0.887  |  valid_loss: 0.897\n",
      "[epoch: 33, i:  4374]  train_loss: 0.890  |  valid_loss: 0.881\n",
      "[epoch: 33, i:  4499]  train_loss: 0.712  |  valid_loss: 0.805\n",
      "[epoch: 33, i:  4624]  train_loss: 0.769  |  valid_loss: 0.930\n",
      "[epoch: 33, i:  4749]  train_loss: 0.897  |  valid_loss: 0.871\n",
      "[epoch: 33, i:  4874]  train_loss: 0.814  |  valid_loss: 0.748\n",
      "[epoch: 33, i:  4999]  train_loss: 0.875  |  valid_loss: 0.735\n",
      "[epoch: 33, i:  5124]  train_loss: 0.775  |  valid_loss: 0.839\n",
      "[epoch: 33, i:  5249]  train_loss: 0.790  |  valid_loss: 0.896\n",
      "[epoch: 33, i:  5374]  train_loss: 0.818  |  valid_loss: 0.632\n",
      "[epoch: 33, i:  5499]  train_loss: 0.827  |  valid_loss: 0.691\n",
      "[epoch: 33, i:  5624]  train_loss: 0.812  |  valid_loss: 0.929\n",
      "[epoch: 33, i:  5749]  train_loss: 0.872  |  valid_loss: 0.805\n",
      "[epoch: 33, i:  5874]  train_loss: 0.909  |  valid_loss: 0.718\n",
      "[epoch: 33, i:  5999]  train_loss: 0.886  |  valid_loss: 0.833\n",
      "[epoch: 33, i:  6124]  train_loss: 0.853  |  valid_loss: 0.774\n",
      "[epoch: 33, i:  6249]  train_loss: 0.842  |  valid_loss: 0.849\n",
      "[epoch: 33, i:  6374]  train_loss: 0.738  |  valid_loss: 0.744\n",
      "[epoch: 33, i:  6499]  train_loss: 0.823  |  valid_loss: 0.758\n",
      "[epoch: 33, i:  6624]  train_loss: 0.820  |  valid_loss: 0.656\n",
      "[epoch: 33, i:  6749]  train_loss: 0.700  |  valid_loss: 0.841\n",
      "[epoch: 33, i:  6874]  train_loss: 0.823  |  valid_loss: 0.760\n",
      "[epoch: 33, i:  6999]  train_loss: 0.821  |  valid_loss: 0.987\n",
      "[epoch: 33, i:  7124]  train_loss: 0.814  |  valid_loss: 0.961\n",
      "[epoch: 33, i:  7249]  train_loss: 0.826  |  valid_loss: 0.663\n",
      "[epoch: 33, i:  7374]  train_loss: 0.786  |  valid_loss: 1.014\n",
      "[epoch: 33, i:  7499]  train_loss: 0.858  |  valid_loss: 0.912\n",
      "[epoch: 33, i:  7624]  train_loss: 0.836  |  valid_loss: 0.906\n",
      "[epoch: 33, i:  7749]  train_loss: 0.753  |  valid_loss: 0.798\n",
      "[epoch: 33, i:  7874]  train_loss: 0.822  |  valid_loss: 0.872\n",
      "[epoch: 33, i:  7999]  train_loss: 0.781  |  valid_loss: 0.722\n",
      "[epoch: 33, i:  8124]  train_loss: 0.800  |  valid_loss: 0.899\n",
      "[epoch: 33, i:  8249]  train_loss: 0.768  |  valid_loss: 0.889\n",
      "[epoch: 33, i:  8374]  train_loss: 0.849  |  valid_loss: 0.830\n",
      "[epoch: 33, i:  8499]  train_loss: 0.702  |  valid_loss: 0.906\n",
      "[epoch: 33, i:  8624]  train_loss: 0.876  |  valid_loss: 0.859\n",
      "[epoch: 33, i:  8749]  train_loss: 0.813  |  valid_loss: 0.882\n",
      "[epoch: 33, i:  8874]  train_loss: 0.733  |  valid_loss: 0.853\n",
      "[epoch: 33, i:  8999]  train_loss: 0.891  |  valid_loss: 0.692\n",
      "[epoch: 33, i:  9124]  train_loss: 0.791  |  valid_loss: 0.754\n",
      "[epoch: 33, i:  9249]  train_loss: 0.729  |  valid_loss: 0.614\n",
      "[epoch: 33, i:  9374]  train_loss: 0.832  |  valid_loss: 0.893\n",
      "[epoch: 33, i:  9499]  train_loss: 0.780  |  valid_loss: 0.743\n",
      "[epoch: 33, i:  9624]  train_loss: 0.892  |  valid_loss: 0.873\n",
      "[epoch: 33, i:  9749]  train_loss: 0.799  |  valid_loss: 0.923\n",
      "[epoch: 33, i:  9874]  train_loss: 0.785  |  valid_loss: 0.902\n",
      "[epoch: 33, i:  9999]  train_loss: 0.844  |  valid_loss: 0.811\n",
      "[epoch: 33, i: 10124]  train_loss: 0.770  |  valid_loss: 0.724\n",
      "[epoch: 33, i: 10249]  train_loss: 0.723  |  valid_loss: 0.881\n",
      "[epoch: 33, i: 10374]  train_loss: 0.748  |  valid_loss: 0.737\n",
      "[epoch: 33, i: 10499]  train_loss: 0.842  |  valid_loss: 0.980\n",
      "[epoch: 33, i: 10624]  train_loss: 0.784  |  valid_loss: 0.944\n",
      "[epoch: 33, i: 10749]  train_loss: 0.866  |  valid_loss: 0.833\n",
      "[epoch: 33, i: 10874]  train_loss: 0.754  |  valid_loss: 0.961\n",
      "[epoch: 33, i: 10999]  train_loss: 0.840  |  valid_loss: 0.963\n",
      "[epoch: 33, i: 11124]  train_loss: 0.855  |  valid_loss: 0.847\n",
      "[epoch: 33, i: 11249]  train_loss: 0.745  |  valid_loss: 0.860\n",
      "[epoch: 33, i: 11374]  train_loss: 0.817  |  valid_loss: 0.805\n",
      "[epoch: 33, i: 11499]  train_loss: 0.818  |  valid_loss: 0.605\n",
      "[epoch: 33, i: 11624]  train_loss: 0.833  |  valid_loss: 0.788\n",
      "[epoch: 33, i: 11749]  train_loss: 0.764  |  valid_loss: 0.751\n",
      "[epoch: 33, i: 11874]  train_loss: 0.793  |  valid_loss: 0.761\n",
      "[epoch: 33, i: 11999]  train_loss: 0.838  |  valid_loss: 0.709\n",
      "[epoch: 33, i: 12124]  train_loss: 0.763  |  valid_loss: 0.726\n",
      "[epoch: 33, i: 12249]  train_loss: 0.712  |  valid_loss: 1.054\n",
      "[epoch: 33, i: 12374]  train_loss: 0.856  |  valid_loss: 0.887\n",
      "[epoch: 33, i: 12499]  train_loss: 0.873  |  valid_loss: 0.863\n",
      "--> [End of epoch 33] train_accuracy: 71.34%  |  valid_accuracy: 71.05%\n",
      "--> [Start of epoch 34]  lr: 0.000001\n",
      "[epoch: 34, i:   124]  train_loss: 0.786  |  valid_loss: 0.730\n",
      "[epoch: 34, i:   249]  train_loss: 0.842  |  valid_loss: 0.716\n",
      "[epoch: 34, i:   374]  train_loss: 0.819  |  valid_loss: 0.846\n",
      "[epoch: 34, i:   499]  train_loss: 0.895  |  valid_loss: 0.804\n",
      "[epoch: 34, i:   624]  train_loss: 0.761  |  valid_loss: 0.897\n",
      "[epoch: 34, i:   749]  train_loss: 0.832  |  valid_loss: 0.577\n",
      "[epoch: 34, i:   874]  train_loss: 0.746  |  valid_loss: 0.875\n",
      "[epoch: 34, i:   999]  train_loss: 0.830  |  valid_loss: 0.924\n",
      "[epoch: 34, i:  1124]  train_loss: 0.815  |  valid_loss: 0.912\n",
      "[epoch: 34, i:  1249]  train_loss: 0.830  |  valid_loss: 0.722\n",
      "[epoch: 34, i:  1374]  train_loss: 0.771  |  valid_loss: 0.751\n",
      "[epoch: 34, i:  1499]  train_loss: 0.812  |  valid_loss: 0.820\n",
      "[epoch: 34, i:  1624]  train_loss: 0.799  |  valid_loss: 0.713\n",
      "[epoch: 34, i:  1749]  train_loss: 0.865  |  valid_loss: 0.941\n",
      "[epoch: 34, i:  1874]  train_loss: 0.865  |  valid_loss: 0.760\n",
      "[epoch: 34, i:  1999]  train_loss: 0.772  |  valid_loss: 0.988\n",
      "[epoch: 34, i:  2124]  train_loss: 0.779  |  valid_loss: 0.719\n",
      "[epoch: 34, i:  2249]  train_loss: 0.876  |  valid_loss: 0.821\n",
      "[epoch: 34, i:  2374]  train_loss: 0.828  |  valid_loss: 0.787\n",
      "[epoch: 34, i:  2499]  train_loss: 0.804  |  valid_loss: 0.995\n",
      "[epoch: 34, i:  2624]  train_loss: 0.727  |  valid_loss: 0.928\n",
      "[epoch: 34, i:  2749]  train_loss: 0.881  |  valid_loss: 0.931\n",
      "[epoch: 34, i:  2874]  train_loss: 0.810  |  valid_loss: 1.003\n",
      "[epoch: 34, i:  2999]  train_loss: 0.721  |  valid_loss: 0.824\n",
      "[epoch: 34, i:  3124]  train_loss: 0.872  |  valid_loss: 0.886\n",
      "[epoch: 34, i:  3249]  train_loss: 0.832  |  valid_loss: 1.033\n",
      "[epoch: 34, i:  3374]  train_loss: 0.815  |  valid_loss: 0.799\n",
      "[epoch: 34, i:  3499]  train_loss: 0.831  |  valid_loss: 0.804\n",
      "[epoch: 34, i:  3624]  train_loss: 0.752  |  valid_loss: 0.795\n",
      "[epoch: 34, i:  3749]  train_loss: 0.852  |  valid_loss: 0.682\n",
      "[epoch: 34, i:  3874]  train_loss: 0.808  |  valid_loss: 0.830\n",
      "[epoch: 34, i:  3999]  train_loss: 0.791  |  valid_loss: 0.642\n",
      "[epoch: 34, i:  4124]  train_loss: 0.835  |  valid_loss: 0.851\n",
      "[epoch: 34, i:  4249]  train_loss: 0.813  |  valid_loss: 0.882\n",
      "[epoch: 34, i:  4374]  train_loss: 0.810  |  valid_loss: 0.858\n",
      "[epoch: 34, i:  4499]  train_loss: 0.783  |  valid_loss: 0.763\n",
      "[epoch: 34, i:  4624]  train_loss: 0.769  |  valid_loss: 0.937\n",
      "[epoch: 34, i:  4749]  train_loss: 0.791  |  valid_loss: 0.900\n",
      "[epoch: 34, i:  4874]  train_loss: 0.960  |  valid_loss: 0.757\n",
      "[epoch: 34, i:  4999]  train_loss: 0.837  |  valid_loss: 0.740\n",
      "[epoch: 34, i:  5124]  train_loss: 0.793  |  valid_loss: 0.842\n",
      "[epoch: 34, i:  5249]  train_loss: 0.837  |  valid_loss: 0.881\n",
      "[epoch: 34, i:  5374]  train_loss: 0.851  |  valid_loss: 0.643\n",
      "[epoch: 34, i:  5499]  train_loss: 0.810  |  valid_loss: 0.712\n",
      "[epoch: 34, i:  5624]  train_loss: 0.811  |  valid_loss: 0.928\n",
      "[epoch: 34, i:  5749]  train_loss: 0.815  |  valid_loss: 0.814\n",
      "[epoch: 34, i:  5874]  train_loss: 0.853  |  valid_loss: 0.736\n",
      "[epoch: 34, i:  5999]  train_loss: 0.803  |  valid_loss: 0.832\n",
      "[epoch: 34, i:  6124]  train_loss: 0.798  |  valid_loss: 0.751\n",
      "[epoch: 34, i:  6249]  train_loss: 0.788  |  valid_loss: 0.854\n",
      "[epoch: 34, i:  6374]  train_loss: 0.793  |  valid_loss: 0.737\n",
      "[epoch: 34, i:  6499]  train_loss: 0.789  |  valid_loss: 0.760\n",
      "[epoch: 34, i:  6624]  train_loss: 0.774  |  valid_loss: 0.658\n",
      "[epoch: 34, i:  6749]  train_loss: 0.770  |  valid_loss: 0.823\n",
      "[epoch: 34, i:  6874]  train_loss: 0.845  |  valid_loss: 0.769\n",
      "[epoch: 34, i:  6999]  train_loss: 0.878  |  valid_loss: 0.956\n",
      "[epoch: 34, i:  7124]  train_loss: 0.770  |  valid_loss: 0.964\n",
      "[epoch: 34, i:  7249]  train_loss: 0.801  |  valid_loss: 0.637\n",
      "[epoch: 34, i:  7374]  train_loss: 0.798  |  valid_loss: 1.012\n",
      "[epoch: 34, i:  7499]  train_loss: 0.779  |  valid_loss: 0.903\n",
      "[epoch: 34, i:  7624]  train_loss: 0.866  |  valid_loss: 0.887\n",
      "[epoch: 34, i:  7749]  train_loss: 0.755  |  valid_loss: 0.802\n",
      "[epoch: 34, i:  7874]  train_loss: 0.819  |  valid_loss: 0.863\n",
      "[epoch: 34, i:  7999]  train_loss: 0.735  |  valid_loss: 0.714\n",
      "[epoch: 34, i:  8124]  train_loss: 0.753  |  valid_loss: 0.940\n",
      "[epoch: 34, i:  8249]  train_loss: 0.855  |  valid_loss: 0.871\n",
      "[epoch: 34, i:  8374]  train_loss: 0.842  |  valid_loss: 0.853\n",
      "[epoch: 34, i:  8499]  train_loss: 0.770  |  valid_loss: 0.901\n",
      "[epoch: 34, i:  8624]  train_loss: 0.777  |  valid_loss: 0.868\n",
      "[epoch: 34, i:  8749]  train_loss: 0.813  |  valid_loss: 0.901\n",
      "[epoch: 34, i:  8874]  train_loss: 0.790  |  valid_loss: 0.839\n",
      "[epoch: 34, i:  8999]  train_loss: 0.804  |  valid_loss: 0.683\n",
      "[epoch: 34, i:  9124]  train_loss: 0.764  |  valid_loss: 0.743\n",
      "[epoch: 34, i:  9249]  train_loss: 0.702  |  valid_loss: 0.634\n",
      "[epoch: 34, i:  9374]  train_loss: 0.806  |  valid_loss: 0.890\n",
      "[epoch: 34, i:  9499]  train_loss: 0.865  |  valid_loss: 0.753\n",
      "[epoch: 34, i:  9624]  train_loss: 0.760  |  valid_loss: 0.876\n",
      "[epoch: 34, i:  9749]  train_loss: 0.893  |  valid_loss: 0.900\n",
      "[epoch: 34, i:  9874]  train_loss: 0.843  |  valid_loss: 0.912\n",
      "[epoch: 34, i:  9999]  train_loss: 0.876  |  valid_loss: 0.813\n",
      "[epoch: 34, i: 10124]  train_loss: 0.783  |  valid_loss: 0.739\n",
      "[epoch: 34, i: 10249]  train_loss: 0.766  |  valid_loss: 0.859\n",
      "[epoch: 34, i: 10374]  train_loss: 0.779  |  valid_loss: 0.748\n",
      "[epoch: 34, i: 10499]  train_loss: 0.856  |  valid_loss: 0.908\n",
      "[epoch: 34, i: 10624]  train_loss: 0.835  |  valid_loss: 0.948\n",
      "[epoch: 34, i: 10749]  train_loss: 0.761  |  valid_loss: 0.857\n",
      "[epoch: 34, i: 10874]  train_loss: 0.759  |  valid_loss: 0.985\n",
      "[epoch: 34, i: 10999]  train_loss: 0.768  |  valid_loss: 0.991\n",
      "[epoch: 34, i: 11124]  train_loss: 0.831  |  valid_loss: 0.823\n",
      "[epoch: 34, i: 11249]  train_loss: 0.849  |  valid_loss: 0.860\n",
      "[epoch: 34, i: 11374]  train_loss: 0.878  |  valid_loss: 0.805\n",
      "[epoch: 34, i: 11499]  train_loss: 0.842  |  valid_loss: 0.605\n",
      "[epoch: 34, i: 11624]  train_loss: 0.803  |  valid_loss: 0.794\n",
      "[epoch: 34, i: 11749]  train_loss: 0.756  |  valid_loss: 0.749\n",
      "[epoch: 34, i: 11874]  train_loss: 0.833  |  valid_loss: 0.767\n",
      "[epoch: 34, i: 11999]  train_loss: 0.786  |  valid_loss: 0.700\n",
      "[epoch: 34, i: 12124]  train_loss: 0.774  |  valid_loss: 0.736\n",
      "[epoch: 34, i: 12249]  train_loss: 0.744  |  valid_loss: 1.034\n",
      "[epoch: 34, i: 12374]  train_loss: 0.849  |  valid_loss: 0.902\n",
      "[epoch: 34, i: 12499]  train_loss: 0.922  |  valid_loss: 0.852\n",
      "--> [End of epoch 34] train_accuracy: 71.65%  |  valid_accuracy: 71.03%\n",
      "--> [Start of epoch 35]  lr: 0.000001\n",
      "[epoch: 35, i:   124]  train_loss: 0.735  |  valid_loss: 0.740\n",
      "[epoch: 35, i:   249]  train_loss: 0.806  |  valid_loss: 0.708\n",
      "[epoch: 35, i:   374]  train_loss: 0.757  |  valid_loss: 0.861\n",
      "[epoch: 35, i:   499]  train_loss: 0.901  |  valid_loss: 0.814\n",
      "[epoch: 35, i:   624]  train_loss: 0.797  |  valid_loss: 0.920\n",
      "[epoch: 35, i:   749]  train_loss: 0.812  |  valid_loss: 0.585\n",
      "[epoch: 35, i:   874]  train_loss: 0.750  |  valid_loss: 0.853\n",
      "[epoch: 35, i:   999]  train_loss: 0.764  |  valid_loss: 0.931\n",
      "[epoch: 35, i:  1124]  train_loss: 0.773  |  valid_loss: 0.920\n",
      "[epoch: 35, i:  1249]  train_loss: 0.914  |  valid_loss: 0.739\n",
      "[epoch: 35, i:  1374]  train_loss: 0.809  |  valid_loss: 0.754\n",
      "[epoch: 35, i:  1499]  train_loss: 0.749  |  valid_loss: 0.815\n",
      "[epoch: 35, i:  1624]  train_loss: 0.792  |  valid_loss: 0.690\n",
      "[epoch: 35, i:  1749]  train_loss: 0.841  |  valid_loss: 1.005\n",
      "[epoch: 35, i:  1874]  train_loss: 0.889  |  valid_loss: 0.766\n",
      "[epoch: 35, i:  1999]  train_loss: 0.816  |  valid_loss: 1.023\n",
      "[epoch: 35, i:  2124]  train_loss: 0.821  |  valid_loss: 0.722\n",
      "[epoch: 35, i:  2249]  train_loss: 0.809  |  valid_loss: 0.863\n",
      "[epoch: 35, i:  2374]  train_loss: 0.838  |  valid_loss: 0.768\n",
      "[epoch: 35, i:  2499]  train_loss: 0.795  |  valid_loss: 0.993\n",
      "[epoch: 35, i:  2624]  train_loss: 0.807  |  valid_loss: 0.927\n",
      "[epoch: 35, i:  2749]  train_loss: 0.816  |  valid_loss: 0.901\n",
      "[epoch: 35, i:  2874]  train_loss: 0.869  |  valid_loss: 1.031\n",
      "[epoch: 35, i:  2999]  train_loss: 0.783  |  valid_loss: 0.849\n",
      "[epoch: 35, i:  3124]  train_loss: 0.878  |  valid_loss: 0.903\n",
      "[epoch: 35, i:  3249]  train_loss: 0.755  |  valid_loss: 1.058\n",
      "[epoch: 35, i:  3374]  train_loss: 0.813  |  valid_loss: 0.779\n",
      "[epoch: 35, i:  3499]  train_loss: 0.865  |  valid_loss: 0.764\n",
      "[epoch: 35, i:  3624]  train_loss: 0.777  |  valid_loss: 0.801\n",
      "[epoch: 35, i:  3749]  train_loss: 0.752  |  valid_loss: 0.700\n",
      "[epoch: 35, i:  3874]  train_loss: 0.797  |  valid_loss: 0.792\n",
      "[epoch: 35, i:  3999]  train_loss: 0.806  |  valid_loss: 0.638\n",
      "[epoch: 35, i:  4124]  train_loss: 0.762  |  valid_loss: 0.876\n",
      "[epoch: 35, i:  4249]  train_loss: 0.794  |  valid_loss: 0.852\n",
      "[epoch: 35, i:  4374]  train_loss: 0.801  |  valid_loss: 0.857\n",
      "[epoch: 35, i:  4499]  train_loss: 0.837  |  valid_loss: 0.780\n",
      "[epoch: 35, i:  4624]  train_loss: 0.817  |  valid_loss: 0.944\n",
      "[epoch: 35, i:  4749]  train_loss: 0.814  |  valid_loss: 0.902\n",
      "[epoch: 35, i:  4874]  train_loss: 0.818  |  valid_loss: 0.749\n",
      "[epoch: 35, i:  4999]  train_loss: 0.845  |  valid_loss: 0.746\n",
      "[epoch: 35, i:  5124]  train_loss: 0.799  |  valid_loss: 0.845\n",
      "[epoch: 35, i:  5249]  train_loss: 0.800  |  valid_loss: 0.871\n",
      "[epoch: 35, i:  5374]  train_loss: 0.802  |  valid_loss: 0.631\n",
      "[epoch: 35, i:  5499]  train_loss: 0.783  |  valid_loss: 0.691\n",
      "[epoch: 35, i:  5624]  train_loss: 0.862  |  valid_loss: 0.919\n",
      "[epoch: 35, i:  5749]  train_loss: 0.794  |  valid_loss: 0.829\n",
      "[epoch: 35, i:  5874]  train_loss: 0.822  |  valid_loss: 0.720\n",
      "[epoch: 35, i:  5999]  train_loss: 0.727  |  valid_loss: 0.849\n",
      "[epoch: 35, i:  6124]  train_loss: 0.808  |  valid_loss: 0.751\n",
      "[epoch: 35, i:  6249]  train_loss: 0.787  |  valid_loss: 0.862\n",
      "[epoch: 35, i:  6374]  train_loss: 0.864  |  valid_loss: 0.740\n",
      "[epoch: 35, i:  6499]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 35, i:  6624]  train_loss: 0.863  |  valid_loss: 0.680\n",
      "[epoch: 35, i:  6749]  train_loss: 0.798  |  valid_loss: 0.771\n",
      "[epoch: 35, i:  6874]  train_loss: 0.816  |  valid_loss: 0.761\n",
      "[epoch: 35, i:  6999]  train_loss: 0.872  |  valid_loss: 0.945\n",
      "[epoch: 35, i:  7124]  train_loss: 0.789  |  valid_loss: 0.967\n",
      "[epoch: 35, i:  7249]  train_loss: 0.793  |  valid_loss: 0.627\n",
      "[epoch: 35, i:  7374]  train_loss: 0.824  |  valid_loss: 1.032\n",
      "[epoch: 35, i:  7499]  train_loss: 0.793  |  valid_loss: 0.919\n",
      "[epoch: 35, i:  7624]  train_loss: 0.750  |  valid_loss: 0.914\n",
      "[epoch: 35, i:  7749]  train_loss: 0.784  |  valid_loss: 0.811\n",
      "[epoch: 35, i:  7874]  train_loss: 0.791  |  valid_loss: 0.888\n",
      "[epoch: 35, i:  7999]  train_loss: 0.804  |  valid_loss: 0.720\n",
      "[epoch: 35, i:  8124]  train_loss: 0.828  |  valid_loss: 0.908\n",
      "[epoch: 35, i:  8249]  train_loss: 0.732  |  valid_loss: 0.884\n",
      "[epoch: 35, i:  8374]  train_loss: 0.855  |  valid_loss: 0.818\n",
      "[epoch: 35, i:  8499]  train_loss: 0.722  |  valid_loss: 0.909\n",
      "[epoch: 35, i:  8624]  train_loss: 0.821  |  valid_loss: 0.874\n",
      "[epoch: 35, i:  8749]  train_loss: 0.836  |  valid_loss: 0.907\n",
      "[epoch: 35, i:  8874]  train_loss: 0.794  |  valid_loss: 0.857\n",
      "[epoch: 35, i:  8999]  train_loss: 0.824  |  valid_loss: 0.697\n",
      "[epoch: 35, i:  9124]  train_loss: 0.810  |  valid_loss: 0.716\n",
      "[epoch: 35, i:  9249]  train_loss: 0.858  |  valid_loss: 0.599\n",
      "[epoch: 35, i:  9374]  train_loss: 0.864  |  valid_loss: 0.910\n",
      "[epoch: 35, i:  9499]  train_loss: 0.743  |  valid_loss: 0.754\n",
      "[epoch: 35, i:  9624]  train_loss: 0.826  |  valid_loss: 0.908\n",
      "[epoch: 35, i:  9749]  train_loss: 0.764  |  valid_loss: 0.909\n",
      "[epoch: 35, i:  9874]  train_loss: 0.803  |  valid_loss: 0.884\n",
      "[epoch: 35, i:  9999]  train_loss: 0.723  |  valid_loss: 0.810\n",
      "[epoch: 35, i: 10124]  train_loss: 0.797  |  valid_loss: 0.741\n",
      "[epoch: 35, i: 10249]  train_loss: 0.837  |  valid_loss: 0.868\n",
      "[epoch: 35, i: 10374]  train_loss: 0.840  |  valid_loss: 0.748\n",
      "[epoch: 35, i: 10499]  train_loss: 0.806  |  valid_loss: 0.986\n",
      "[epoch: 35, i: 10624]  train_loss: 0.772  |  valid_loss: 0.934\n",
      "[epoch: 35, i: 10749]  train_loss: 0.858  |  valid_loss: 0.858\n",
      "[epoch: 35, i: 10874]  train_loss: 0.843  |  valid_loss: 0.948\n",
      "[epoch: 35, i: 10999]  train_loss: 0.813  |  valid_loss: 0.972\n",
      "[epoch: 35, i: 11124]  train_loss: 0.798  |  valid_loss: 0.811\n",
      "[epoch: 35, i: 11249]  train_loss: 0.816  |  valid_loss: 0.868\n",
      "[epoch: 35, i: 11374]  train_loss: 0.755  |  valid_loss: 0.789\n",
      "[epoch: 35, i: 11499]  train_loss: 0.851  |  valid_loss: 0.617\n",
      "[epoch: 35, i: 11624]  train_loss: 0.866  |  valid_loss: 0.807\n",
      "[epoch: 35, i: 11749]  train_loss: 0.836  |  valid_loss: 0.775\n",
      "[epoch: 35, i: 11874]  train_loss: 0.835  |  valid_loss: 0.767\n",
      "[epoch: 35, i: 11999]  train_loss: 0.824  |  valid_loss: 0.700\n",
      "[epoch: 35, i: 12124]  train_loss: 0.864  |  valid_loss: 0.696\n",
      "[epoch: 35, i: 12249]  train_loss: 0.834  |  valid_loss: 1.049\n",
      "[epoch: 35, i: 12374]  train_loss: 0.815  |  valid_loss: 0.870\n",
      "[epoch: 35, i: 12499]  train_loss: 0.762  |  valid_loss: 0.862\n",
      "--> [End of epoch 35] train_accuracy: 71.62%  |  valid_accuracy: 71.05%\n",
      "--> [Start of epoch 36]  lr: 0.000001\n",
      "[epoch: 36, i:   124]  train_loss: 0.757  |  valid_loss: 0.733\n",
      "[epoch: 36, i:   249]  train_loss: 0.745  |  valid_loss: 0.742\n",
      "[epoch: 36, i:   374]  train_loss: 0.843  |  valid_loss: 0.841\n",
      "[epoch: 36, i:   499]  train_loss: 0.740  |  valid_loss: 0.792\n",
      "[epoch: 36, i:   624]  train_loss: 0.766  |  valid_loss: 0.937\n",
      "[epoch: 36, i:   749]  train_loss: 0.823  |  valid_loss: 0.572\n",
      "[epoch: 36, i:   874]  train_loss: 0.806  |  valid_loss: 0.843\n",
      "[epoch: 36, i:   999]  train_loss: 0.805  |  valid_loss: 0.920\n",
      "[epoch: 36, i:  1124]  train_loss: 0.804  |  valid_loss: 0.957\n",
      "[epoch: 36, i:  1249]  train_loss: 0.850  |  valid_loss: 0.715\n",
      "[epoch: 36, i:  1374]  train_loss: 0.866  |  valid_loss: 0.795\n",
      "[epoch: 36, i:  1499]  train_loss: 0.826  |  valid_loss: 0.803\n",
      "[epoch: 36, i:  1624]  train_loss: 0.794  |  valid_loss: 0.671\n",
      "[epoch: 36, i:  1749]  train_loss: 0.810  |  valid_loss: 0.955\n",
      "[epoch: 36, i:  1874]  train_loss: 0.829  |  valid_loss: 0.778\n",
      "[epoch: 36, i:  1999]  train_loss: 0.868  |  valid_loss: 0.988\n",
      "[epoch: 36, i:  2124]  train_loss: 0.841  |  valid_loss: 0.717\n",
      "[epoch: 36, i:  2249]  train_loss: 0.803  |  valid_loss: 0.833\n",
      "[epoch: 36, i:  2374]  train_loss: 0.789  |  valid_loss: 0.764\n",
      "[epoch: 36, i:  2499]  train_loss: 0.891  |  valid_loss: 1.004\n",
      "[epoch: 36, i:  2624]  train_loss: 0.739  |  valid_loss: 0.934\n",
      "[epoch: 36, i:  2749]  train_loss: 0.810  |  valid_loss: 0.948\n",
      "[epoch: 36, i:  2874]  train_loss: 0.809  |  valid_loss: 1.013\n",
      "[epoch: 36, i:  2999]  train_loss: 0.777  |  valid_loss: 0.823\n",
      "[epoch: 36, i:  3124]  train_loss: 0.856  |  valid_loss: 0.884\n",
      "[epoch: 36, i:  3249]  train_loss: 0.790  |  valid_loss: 1.066\n",
      "[epoch: 36, i:  3374]  train_loss: 0.773  |  valid_loss: 0.785\n",
      "[epoch: 36, i:  3499]  train_loss: 0.775  |  valid_loss: 0.765\n",
      "[epoch: 36, i:  3624]  train_loss: 0.795  |  valid_loss: 0.803\n",
      "[epoch: 36, i:  3749]  train_loss: 0.780  |  valid_loss: 0.688\n",
      "[epoch: 36, i:  3874]  train_loss: 0.804  |  valid_loss: 0.794\n",
      "[epoch: 36, i:  3999]  train_loss: 0.783  |  valid_loss: 0.652\n",
      "[epoch: 36, i:  4124]  train_loss: 0.842  |  valid_loss: 0.886\n",
      "[epoch: 36, i:  4249]  train_loss: 0.807  |  valid_loss: 0.857\n",
      "[epoch: 36, i:  4374]  train_loss: 0.849  |  valid_loss: 0.878\n",
      "[epoch: 36, i:  4499]  train_loss: 0.816  |  valid_loss: 0.784\n",
      "[epoch: 36, i:  4624]  train_loss: 0.837  |  valid_loss: 0.946\n",
      "[epoch: 36, i:  4749]  train_loss: 0.687  |  valid_loss: 0.871\n",
      "[epoch: 36, i:  4874]  train_loss: 0.868  |  valid_loss: 0.759\n",
      "[epoch: 36, i:  4999]  train_loss: 0.748  |  valid_loss: 0.732\n",
      "[epoch: 36, i:  5124]  train_loss: 0.784  |  valid_loss: 0.819\n",
      "[epoch: 36, i:  5249]  train_loss: 0.848  |  valid_loss: 0.881\n",
      "[epoch: 36, i:  5374]  train_loss: 0.826  |  valid_loss: 0.630\n",
      "[epoch: 36, i:  5499]  train_loss: 0.755  |  valid_loss: 0.724\n",
      "[epoch: 36, i:  5624]  train_loss: 0.799  |  valid_loss: 0.970\n",
      "[epoch: 36, i:  5749]  train_loss: 0.760  |  valid_loss: 0.818\n",
      "[epoch: 36, i:  5874]  train_loss: 0.766  |  valid_loss: 0.731\n",
      "[epoch: 36, i:  5999]  train_loss: 0.795  |  valid_loss: 0.844\n",
      "[epoch: 36, i:  6124]  train_loss: 0.855  |  valid_loss: 0.736\n",
      "[epoch: 36, i:  6249]  train_loss: 0.819  |  valid_loss: 0.877\n",
      "[epoch: 36, i:  6374]  train_loss: 0.813  |  valid_loss: 0.740\n",
      "[epoch: 36, i:  6499]  train_loss: 0.812  |  valid_loss: 0.784\n",
      "[epoch: 36, i:  6624]  train_loss: 0.845  |  valid_loss: 0.688\n",
      "[epoch: 36, i:  6749]  train_loss: 0.795  |  valid_loss: 0.821\n",
      "[epoch: 36, i:  6874]  train_loss: 0.845  |  valid_loss: 0.744\n",
      "[epoch: 36, i:  6999]  train_loss: 0.760  |  valid_loss: 0.954\n",
      "[epoch: 36, i:  7124]  train_loss: 0.827  |  valid_loss: 0.964\n",
      "[epoch: 36, i:  7249]  train_loss: 0.868  |  valid_loss: 0.665\n",
      "[epoch: 36, i:  7374]  train_loss: 0.815  |  valid_loss: 1.040\n",
      "[epoch: 36, i:  7499]  train_loss: 0.887  |  valid_loss: 0.880\n",
      "[epoch: 36, i:  7624]  train_loss: 0.791  |  valid_loss: 0.885\n",
      "[epoch: 36, i:  7749]  train_loss: 0.874  |  valid_loss: 0.811\n",
      "[epoch: 36, i:  7874]  train_loss: 0.808  |  valid_loss: 0.882\n",
      "[epoch: 36, i:  7999]  train_loss: 0.863  |  valid_loss: 0.710\n",
      "[epoch: 36, i:  8124]  train_loss: 0.852  |  valid_loss: 0.905\n",
      "[epoch: 36, i:  8249]  train_loss: 0.864  |  valid_loss: 0.924\n",
      "[epoch: 36, i:  8374]  train_loss: 0.809  |  valid_loss: 0.836\n",
      "[epoch: 36, i:  8499]  train_loss: 0.776  |  valid_loss: 0.934\n",
      "[epoch: 36, i:  8624]  train_loss: 0.807  |  valid_loss: 0.854\n",
      "[epoch: 36, i:  8749]  train_loss: 0.823  |  valid_loss: 0.898\n",
      "[epoch: 36, i:  8874]  train_loss: 0.816  |  valid_loss: 0.857\n",
      "[epoch: 36, i:  8999]  train_loss: 0.870  |  valid_loss: 0.691\n",
      "[epoch: 36, i:  9124]  train_loss: 0.835  |  valid_loss: 0.714\n",
      "[epoch: 36, i:  9249]  train_loss: 0.833  |  valid_loss: 0.614\n",
      "[epoch: 36, i:  9374]  train_loss: 0.802  |  valid_loss: 0.893\n",
      "[epoch: 36, i:  9499]  train_loss: 0.882  |  valid_loss: 0.752\n",
      "[epoch: 36, i:  9624]  train_loss: 0.790  |  valid_loss: 0.896\n",
      "[epoch: 36, i:  9749]  train_loss: 0.847  |  valid_loss: 0.911\n",
      "[epoch: 36, i:  9874]  train_loss: 0.823  |  valid_loss: 0.881\n",
      "[epoch: 36, i:  9999]  train_loss: 0.813  |  valid_loss: 0.813\n",
      "[epoch: 36, i: 10124]  train_loss: 0.812  |  valid_loss: 0.745\n",
      "[epoch: 36, i: 10249]  train_loss: 0.855  |  valid_loss: 0.845\n",
      "[epoch: 36, i: 10374]  train_loss: 0.795  |  valid_loss: 0.739\n",
      "[epoch: 36, i: 10499]  train_loss: 0.784  |  valid_loss: 0.949\n",
      "[epoch: 36, i: 10624]  train_loss: 0.735  |  valid_loss: 0.937\n",
      "[epoch: 36, i: 10749]  train_loss: 0.799  |  valid_loss: 0.846\n",
      "[epoch: 36, i: 10874]  train_loss: 0.800  |  valid_loss: 0.976\n",
      "[epoch: 36, i: 10999]  train_loss: 0.837  |  valid_loss: 0.937\n",
      "[epoch: 36, i: 11124]  train_loss: 0.779  |  valid_loss: 0.812\n",
      "[epoch: 36, i: 11249]  train_loss: 0.895  |  valid_loss: 0.856\n",
      "[epoch: 36, i: 11374]  train_loss: 0.761  |  valid_loss: 0.821\n",
      "[epoch: 36, i: 11499]  train_loss: 0.826  |  valid_loss: 0.597\n",
      "[epoch: 36, i: 11624]  train_loss: 0.790  |  valid_loss: 0.792\n",
      "[epoch: 36, i: 11749]  train_loss: 0.792  |  valid_loss: 0.735\n",
      "[epoch: 36, i: 11874]  train_loss: 0.892  |  valid_loss: 0.782\n",
      "[epoch: 36, i: 11999]  train_loss: 0.796  |  valid_loss: 0.695\n",
      "[epoch: 36, i: 12124]  train_loss: 0.765  |  valid_loss: 0.730\n",
      "[epoch: 36, i: 12249]  train_loss: 0.870  |  valid_loss: 1.040\n",
      "[epoch: 36, i: 12374]  train_loss: 0.828  |  valid_loss: 0.883\n",
      "[epoch: 36, i: 12499]  train_loss: 0.776  |  valid_loss: 0.876\n",
      "--> [End of epoch 36] train_accuracy: 71.46%  |  valid_accuracy: 71.19%\n",
      "--> [Start of epoch 37]  lr: 0.000001\n",
      "[epoch: 37, i:   124]  train_loss: 0.835  |  valid_loss: 0.763\n",
      "[epoch: 37, i:   249]  train_loss: 0.867  |  valid_loss: 0.729\n",
      "[epoch: 37, i:   374]  train_loss: 0.876  |  valid_loss: 0.834\n",
      "[epoch: 37, i:   499]  train_loss: 0.824  |  valid_loss: 0.796\n",
      "[epoch: 37, i:   624]  train_loss: 0.862  |  valid_loss: 0.904\n",
      "[epoch: 37, i:   749]  train_loss: 0.813  |  valid_loss: 0.569\n",
      "[epoch: 37, i:   874]  train_loss: 0.824  |  valid_loss: 0.864\n",
      "[epoch: 37, i:   999]  train_loss: 0.771  |  valid_loss: 0.931\n",
      "[epoch: 37, i:  1124]  train_loss: 0.841  |  valid_loss: 0.939\n",
      "[epoch: 37, i:  1249]  train_loss: 0.821  |  valid_loss: 0.711\n",
      "[epoch: 37, i:  1374]  train_loss: 0.810  |  valid_loss: 0.802\n",
      "[epoch: 37, i:  1499]  train_loss: 0.771  |  valid_loss: 0.819\n",
      "[epoch: 37, i:  1624]  train_loss: 0.695  |  valid_loss: 0.697\n",
      "[epoch: 37, i:  1749]  train_loss: 0.802  |  valid_loss: 0.940\n",
      "[epoch: 37, i:  1874]  train_loss: 0.813  |  valid_loss: 0.773\n",
      "[epoch: 37, i:  1999]  train_loss: 0.766  |  valid_loss: 0.998\n",
      "[epoch: 37, i:  2124]  train_loss: 0.713  |  valid_loss: 0.703\n",
      "[epoch: 37, i:  2249]  train_loss: 0.750  |  valid_loss: 0.831\n",
      "[epoch: 37, i:  2374]  train_loss: 0.798  |  valid_loss: 0.776\n",
      "[epoch: 37, i:  2499]  train_loss: 0.875  |  valid_loss: 1.008\n",
      "[epoch: 37, i:  2624]  train_loss: 0.813  |  valid_loss: 0.926\n",
      "[epoch: 37, i:  2749]  train_loss: 0.850  |  valid_loss: 0.994\n",
      "[epoch: 37, i:  2874]  train_loss: 0.773  |  valid_loss: 0.967\n",
      "[epoch: 37, i:  2999]  train_loss: 0.738  |  valid_loss: 0.842\n",
      "[epoch: 37, i:  3124]  train_loss: 0.766  |  valid_loss: 0.876\n",
      "[epoch: 37, i:  3249]  train_loss: 0.860  |  valid_loss: 1.050\n",
      "[epoch: 37, i:  3374]  train_loss: 0.857  |  valid_loss: 0.790\n",
      "[epoch: 37, i:  3499]  train_loss: 0.796  |  valid_loss: 0.749\n",
      "[epoch: 37, i:  3624]  train_loss: 0.845  |  valid_loss: 0.803\n",
      "[epoch: 37, i:  3749]  train_loss: 0.779  |  valid_loss: 0.700\n",
      "[epoch: 37, i:  3874]  train_loss: 0.885  |  valid_loss: 0.811\n",
      "[epoch: 37, i:  3999]  train_loss: 0.793  |  valid_loss: 0.643\n",
      "[epoch: 37, i:  4124]  train_loss: 0.872  |  valid_loss: 0.880\n",
      "[epoch: 37, i:  4249]  train_loss: 0.755  |  valid_loss: 0.874\n",
      "[epoch: 37, i:  4374]  train_loss: 0.783  |  valid_loss: 0.868\n",
      "[epoch: 37, i:  4499]  train_loss: 0.814  |  valid_loss: 0.780\n",
      "[epoch: 37, i:  4624]  train_loss: 0.840  |  valid_loss: 0.951\n",
      "[epoch: 37, i:  4749]  train_loss: 0.905  |  valid_loss: 0.888\n",
      "[epoch: 37, i:  4874]  train_loss: 0.771  |  valid_loss: 0.751\n",
      "[epoch: 37, i:  4999]  train_loss: 0.818  |  valid_loss: 0.727\n",
      "[epoch: 37, i:  5124]  train_loss: 0.834  |  valid_loss: 0.816\n",
      "[epoch: 37, i:  5249]  train_loss: 0.829  |  valid_loss: 0.881\n",
      "[epoch: 37, i:  5374]  train_loss: 0.824  |  valid_loss: 0.628\n",
      "[epoch: 37, i:  5499]  train_loss: 0.830  |  valid_loss: 0.702\n",
      "[epoch: 37, i:  5624]  train_loss: 0.772  |  valid_loss: 1.009\n",
      "[epoch: 37, i:  5749]  train_loss: 0.831  |  valid_loss: 0.812\n",
      "[epoch: 37, i:  5874]  train_loss: 0.764  |  valid_loss: 0.758\n",
      "[epoch: 37, i:  5999]  train_loss: 0.823  |  valid_loss: 0.858\n",
      "[epoch: 37, i:  6124]  train_loss: 0.756  |  valid_loss: 0.750\n",
      "[epoch: 37, i:  6249]  train_loss: 0.786  |  valid_loss: 0.870\n",
      "[epoch: 37, i:  6374]  train_loss: 0.765  |  valid_loss: 0.726\n",
      "[epoch: 37, i:  6499]  train_loss: 0.818  |  valid_loss: 0.769\n",
      "[epoch: 37, i:  6624]  train_loss: 0.731  |  valid_loss: 0.715\n",
      "[epoch: 37, i:  6749]  train_loss: 0.788  |  valid_loss: 0.796\n",
      "[epoch: 37, i:  6874]  train_loss: 0.817  |  valid_loss: 0.766\n",
      "[epoch: 37, i:  6999]  train_loss: 0.843  |  valid_loss: 0.925\n",
      "[epoch: 37, i:  7124]  train_loss: 0.823  |  valid_loss: 0.961\n",
      "[epoch: 37, i:  7249]  train_loss: 0.804  |  valid_loss: 0.642\n",
      "[epoch: 37, i:  7374]  train_loss: 0.884  |  valid_loss: 1.042\n",
      "[epoch: 37, i:  7499]  train_loss: 0.812  |  valid_loss: 0.907\n",
      "[epoch: 37, i:  7624]  train_loss: 0.815  |  valid_loss: 0.902\n",
      "[epoch: 37, i:  7749]  train_loss: 0.769  |  valid_loss: 0.790\n",
      "[epoch: 37, i:  7874]  train_loss: 0.840  |  valid_loss: 0.861\n",
      "[epoch: 37, i:  7999]  train_loss: 0.734  |  valid_loss: 0.732\n",
      "[epoch: 37, i:  8124]  train_loss: 0.838  |  valid_loss: 0.895\n",
      "[epoch: 37, i:  8249]  train_loss: 0.790  |  valid_loss: 0.899\n",
      "[epoch: 37, i:  8374]  train_loss: 0.795  |  valid_loss: 0.844\n",
      "[epoch: 37, i:  8499]  train_loss: 0.783  |  valid_loss: 0.923\n",
      "[epoch: 37, i:  8624]  train_loss: 0.822  |  valid_loss: 0.858\n",
      "[epoch: 37, i:  8749]  train_loss: 0.831  |  valid_loss: 0.919\n",
      "[epoch: 37, i:  8874]  train_loss: 0.784  |  valid_loss: 0.846\n",
      "[epoch: 37, i:  8999]  train_loss: 0.861  |  valid_loss: 0.692\n",
      "[epoch: 37, i:  9124]  train_loss: 0.742  |  valid_loss: 0.724\n",
      "[epoch: 37, i:  9249]  train_loss: 0.824  |  valid_loss: 0.622\n",
      "[epoch: 37, i:  9374]  train_loss: 0.854  |  valid_loss: 0.886\n",
      "[epoch: 37, i:  9499]  train_loss: 0.773  |  valid_loss: 0.761\n",
      "[epoch: 37, i:  9624]  train_loss: 0.807  |  valid_loss: 0.879\n",
      "[epoch: 37, i:  9749]  train_loss: 0.815  |  valid_loss: 0.916\n",
      "[epoch: 37, i:  9874]  train_loss: 0.748  |  valid_loss: 0.891\n",
      "[epoch: 37, i:  9999]  train_loss: 0.794  |  valid_loss: 0.814\n",
      "[epoch: 37, i: 10124]  train_loss: 0.838  |  valid_loss: 0.766\n",
      "[epoch: 37, i: 10249]  train_loss: 0.892  |  valid_loss: 0.866\n",
      "[epoch: 37, i: 10374]  train_loss: 0.800  |  valid_loss: 0.756\n",
      "[epoch: 37, i: 10499]  train_loss: 0.749  |  valid_loss: 0.967\n",
      "[epoch: 37, i: 10624]  train_loss: 0.775  |  valid_loss: 0.934\n",
      "[epoch: 37, i: 10749]  train_loss: 0.799  |  valid_loss: 0.835\n",
      "[epoch: 37, i: 10874]  train_loss: 0.868  |  valid_loss: 0.947\n",
      "[epoch: 37, i: 10999]  train_loss: 0.833  |  valid_loss: 0.997\n",
      "[epoch: 37, i: 11124]  train_loss: 0.810  |  valid_loss: 0.816\n",
      "[epoch: 37, i: 11249]  train_loss: 0.828  |  valid_loss: 0.856\n",
      "[epoch: 37, i: 11374]  train_loss: 0.862  |  valid_loss: 0.772\n",
      "[epoch: 37, i: 11499]  train_loss: 0.822  |  valid_loss: 0.601\n",
      "[epoch: 37, i: 11624]  train_loss: 0.732  |  valid_loss: 0.791\n",
      "[epoch: 37, i: 11749]  train_loss: 0.877  |  valid_loss: 0.763\n",
      "[epoch: 37, i: 11874]  train_loss: 0.801  |  valid_loss: 0.790\n",
      "[epoch: 37, i: 11999]  train_loss: 0.904  |  valid_loss: 0.694\n",
      "[epoch: 37, i: 12124]  train_loss: 0.768  |  valid_loss: 0.723\n",
      "[epoch: 37, i: 12249]  train_loss: 0.844  |  valid_loss: 1.056\n",
      "[epoch: 37, i: 12374]  train_loss: 0.829  |  valid_loss: 0.881\n",
      "[epoch: 37, i: 12499]  train_loss: 0.752  |  valid_loss: 0.869\n",
      "--> [End of epoch 37] train_accuracy: 71.48%  |  valid_accuracy: 71.01%\n",
      "--> [Start of epoch 38]  lr: 0.000000\n",
      "[epoch: 38, i:   124]  train_loss: 0.801  |  valid_loss: 0.742\n",
      "[epoch: 38, i:   249]  train_loss: 0.817  |  valid_loss: 0.715\n",
      "[epoch: 38, i:   374]  train_loss: 0.826  |  valid_loss: 0.863\n",
      "[epoch: 38, i:   499]  train_loss: 0.765  |  valid_loss: 0.783\n",
      "[epoch: 38, i:   624]  train_loss: 0.838  |  valid_loss: 0.912\n",
      "[epoch: 38, i:   749]  train_loss: 0.837  |  valid_loss: 0.572\n",
      "[epoch: 38, i:   874]  train_loss: 0.827  |  valid_loss: 0.862\n",
      "[epoch: 38, i:   999]  train_loss: 0.811  |  valid_loss: 0.937\n",
      "[epoch: 38, i:  1124]  train_loss: 0.785  |  valid_loss: 0.917\n",
      "[epoch: 38, i:  1249]  train_loss: 0.860  |  valid_loss: 0.731\n",
      "[epoch: 38, i:  1374]  train_loss: 0.790  |  valid_loss: 0.773\n",
      "[epoch: 38, i:  1499]  train_loss: 0.856  |  valid_loss: 0.817\n",
      "[epoch: 38, i:  1624]  train_loss: 0.761  |  valid_loss: 0.670\n",
      "[epoch: 38, i:  1749]  train_loss: 0.734  |  valid_loss: 0.974\n",
      "[epoch: 38, i:  1874]  train_loss: 0.870  |  valid_loss: 0.771\n",
      "[epoch: 38, i:  1999]  train_loss: 0.859  |  valid_loss: 0.989\n",
      "[epoch: 38, i:  2124]  train_loss: 0.804  |  valid_loss: 0.700\n",
      "[epoch: 38, i:  2249]  train_loss: 0.820  |  valid_loss: 0.815\n",
      "[epoch: 38, i:  2374]  train_loss: 0.819  |  valid_loss: 0.754\n",
      "[epoch: 38, i:  2499]  train_loss: 0.830  |  valid_loss: 0.989\n",
      "[epoch: 38, i:  2624]  train_loss: 0.806  |  valid_loss: 0.911\n",
      "[epoch: 38, i:  2749]  train_loss: 0.850  |  valid_loss: 0.909\n",
      "[epoch: 38, i:  2874]  train_loss: 0.842  |  valid_loss: 0.964\n",
      "[epoch: 38, i:  2999]  train_loss: 0.808  |  valid_loss: 0.839\n",
      "[epoch: 38, i:  3124]  train_loss: 0.774  |  valid_loss: 0.890\n",
      "[epoch: 38, i:  3249]  train_loss: 0.826  |  valid_loss: 1.048\n",
      "[epoch: 38, i:  3374]  train_loss: 0.813  |  valid_loss: 0.839\n",
      "[epoch: 38, i:  3499]  train_loss: 0.733  |  valid_loss: 0.782\n",
      "[epoch: 38, i:  3624]  train_loss: 0.808  |  valid_loss: 0.806\n",
      "[epoch: 38, i:  3749]  train_loss: 0.772  |  valid_loss: 0.684\n",
      "[epoch: 38, i:  3874]  train_loss: 0.801  |  valid_loss: 0.790\n",
      "[epoch: 38, i:  3999]  train_loss: 0.858  |  valid_loss: 0.656\n",
      "[epoch: 38, i:  4124]  train_loss: 0.761  |  valid_loss: 0.913\n",
      "[epoch: 38, i:  4249]  train_loss: 0.805  |  valid_loss: 0.879\n",
      "[epoch: 38, i:  4374]  train_loss: 0.839  |  valid_loss: 0.878\n",
      "[epoch: 38, i:  4499]  train_loss: 0.762  |  valid_loss: 0.805\n",
      "[epoch: 38, i:  4624]  train_loss: 0.840  |  valid_loss: 0.927\n",
      "[epoch: 38, i:  4749]  train_loss: 0.747  |  valid_loss: 0.872\n",
      "[epoch: 38, i:  4874]  train_loss: 0.775  |  valid_loss: 0.743\n",
      "[epoch: 38, i:  4999]  train_loss: 0.831  |  valid_loss: 0.740\n",
      "[epoch: 38, i:  5124]  train_loss: 0.769  |  valid_loss: 0.811\n",
      "[epoch: 38, i:  5249]  train_loss: 0.833  |  valid_loss: 0.894\n",
      "[epoch: 38, i:  5374]  train_loss: 0.860  |  valid_loss: 0.612\n",
      "[epoch: 38, i:  5499]  train_loss: 0.829  |  valid_loss: 0.708\n",
      "[epoch: 38, i:  5624]  train_loss: 0.792  |  valid_loss: 0.798\n",
      "[epoch: 38, i:  5749]  train_loss: 0.802  |  valid_loss: 0.797\n",
      "[epoch: 38, i:  5874]  train_loss: 0.853  |  valid_loss: 0.741\n",
      "[epoch: 38, i:  5999]  train_loss: 0.810  |  valid_loss: 0.827\n",
      "[epoch: 38, i:  6124]  train_loss: 0.864  |  valid_loss: 0.755\n",
      "[epoch: 38, i:  6249]  train_loss: 0.794  |  valid_loss: 0.853\n",
      "[epoch: 38, i:  6374]  train_loss: 0.844  |  valid_loss: 0.741\n",
      "[epoch: 38, i:  6499]  train_loss: 0.855  |  valid_loss: 0.797\n",
      "[epoch: 38, i:  6624]  train_loss: 0.823  |  valid_loss: 0.723\n",
      "[epoch: 38, i:  6749]  train_loss: 0.826  |  valid_loss: 0.818\n",
      "[epoch: 38, i:  6874]  train_loss: 0.849  |  valid_loss: 0.760\n",
      "[epoch: 38, i:  6999]  train_loss: 0.792  |  valid_loss: 0.947\n",
      "[epoch: 38, i:  7124]  train_loss: 0.803  |  valid_loss: 0.960\n",
      "[epoch: 38, i:  7249]  train_loss: 0.757  |  valid_loss: 0.665\n",
      "[epoch: 38, i:  7374]  train_loss: 0.756  |  valid_loss: 1.038\n",
      "[epoch: 38, i:  7499]  train_loss: 0.832  |  valid_loss: 0.901\n",
      "[epoch: 38, i:  7624]  train_loss: 0.812  |  valid_loss: 0.912\n",
      "[epoch: 38, i:  7749]  train_loss: 0.876  |  valid_loss: 0.801\n",
      "[epoch: 38, i:  7874]  train_loss: 0.778  |  valid_loss: 0.851\n",
      "[epoch: 38, i:  7999]  train_loss: 0.855  |  valid_loss: 0.736\n",
      "[epoch: 38, i:  8124]  train_loss: 0.794  |  valid_loss: 0.939\n",
      "[epoch: 38, i:  8249]  train_loss: 0.810  |  valid_loss: 0.870\n",
      "[epoch: 38, i:  8374]  train_loss: 0.740  |  valid_loss: 0.855\n",
      "[epoch: 38, i:  8499]  train_loss: 0.771  |  valid_loss: 0.901\n",
      "[epoch: 38, i:  8624]  train_loss: 0.773  |  valid_loss: 0.873\n",
      "[epoch: 38, i:  8749]  train_loss: 0.819  |  valid_loss: 0.912\n",
      "[epoch: 38, i:  8874]  train_loss: 0.891  |  valid_loss: 0.879\n",
      "[epoch: 38, i:  8999]  train_loss: 0.877  |  valid_loss: 0.703\n",
      "[epoch: 38, i:  9124]  train_loss: 0.805  |  valid_loss: 0.753\n",
      "[epoch: 38, i:  9249]  train_loss: 0.808  |  valid_loss: 0.617\n",
      "[epoch: 38, i:  9374]  train_loss: 0.824  |  valid_loss: 0.880\n",
      "[epoch: 38, i:  9499]  train_loss: 0.799  |  valid_loss: 0.759\n",
      "[epoch: 38, i:  9624]  train_loss: 0.788  |  valid_loss: 0.895\n",
      "[epoch: 38, i:  9749]  train_loss: 0.825  |  valid_loss: 0.898\n",
      "[epoch: 38, i:  9874]  train_loss: 0.800  |  valid_loss: 0.905\n",
      "[epoch: 38, i:  9999]  train_loss: 0.841  |  valid_loss: 0.820\n",
      "[epoch: 38, i: 10124]  train_loss: 0.788  |  valid_loss: 0.751\n",
      "[epoch: 38, i: 10249]  train_loss: 0.813  |  valid_loss: 0.851\n",
      "[epoch: 38, i: 10374]  train_loss: 0.840  |  valid_loss: 0.748\n",
      "[epoch: 38, i: 10499]  train_loss: 0.779  |  valid_loss: 0.968\n",
      "[epoch: 38, i: 10624]  train_loss: 0.862  |  valid_loss: 0.916\n",
      "[epoch: 38, i: 10749]  train_loss: 0.787  |  valid_loss: 0.841\n",
      "[epoch: 38, i: 10874]  train_loss: 0.821  |  valid_loss: 0.980\n",
      "[epoch: 38, i: 10999]  train_loss: 0.930  |  valid_loss: 0.969\n",
      "[epoch: 38, i: 11124]  train_loss: 0.794  |  valid_loss: 0.834\n",
      "[epoch: 38, i: 11249]  train_loss: 0.864  |  valid_loss: 0.891\n",
      "[epoch: 38, i: 11374]  train_loss: 0.832  |  valid_loss: 0.795\n",
      "[epoch: 38, i: 11499]  train_loss: 0.796  |  valid_loss: 0.613\n",
      "[epoch: 38, i: 11624]  train_loss: 0.863  |  valid_loss: 0.822\n",
      "[epoch: 38, i: 11749]  train_loss: 0.785  |  valid_loss: 0.745\n",
      "[epoch: 38, i: 11874]  train_loss: 0.784  |  valid_loss: 0.782\n",
      "[epoch: 38, i: 11999]  train_loss: 0.691  |  valid_loss: 0.719\n",
      "[epoch: 38, i: 12124]  train_loss: 0.786  |  valid_loss: 0.705\n",
      "[epoch: 38, i: 12249]  train_loss: 0.833  |  valid_loss: 1.046\n",
      "[epoch: 38, i: 12374]  train_loss: 0.793  |  valid_loss: 0.865\n",
      "[epoch: 38, i: 12499]  train_loss: 0.876  |  valid_loss: 0.875\n",
      "--> [End of epoch 38] train_accuracy: 71.52%  |  valid_accuracy: 70.86%\n",
      "--> [Start of epoch 39]  lr: 0.000000\n",
      "[epoch: 39, i:   124]  train_loss: 0.858  |  valid_loss: 0.727\n",
      "[epoch: 39, i:   249]  train_loss: 0.857  |  valid_loss: 0.734\n",
      "[epoch: 39, i:   374]  train_loss: 0.779  |  valid_loss: 0.856\n",
      "[epoch: 39, i:   499]  train_loss: 0.745  |  valid_loss: 0.805\n",
      "[epoch: 39, i:   624]  train_loss: 0.811  |  valid_loss: 0.935\n",
      "[epoch: 39, i:   749]  train_loss: 0.802  |  valid_loss: 0.582\n",
      "[epoch: 39, i:   874]  train_loss: 0.743  |  valid_loss: 0.874\n",
      "[epoch: 39, i:   999]  train_loss: 0.807  |  valid_loss: 0.952\n",
      "[epoch: 39, i:  1124]  train_loss: 0.861  |  valid_loss: 0.943\n",
      "[epoch: 39, i:  1249]  train_loss: 0.827  |  valid_loss: 0.734\n",
      "[epoch: 39, i:  1374]  train_loss: 0.845  |  valid_loss: 0.760\n",
      "[epoch: 39, i:  1499]  train_loss: 0.747  |  valid_loss: 0.814\n",
      "[epoch: 39, i:  1624]  train_loss: 0.850  |  valid_loss: 0.689\n",
      "[epoch: 39, i:  1749]  train_loss: 0.783  |  valid_loss: 0.931\n",
      "[epoch: 39, i:  1874]  train_loss: 0.778  |  valid_loss: 0.779\n",
      "[epoch: 39, i:  1999]  train_loss: 0.721  |  valid_loss: 1.002\n",
      "[epoch: 39, i:  2124]  train_loss: 0.834  |  valid_loss: 0.731\n",
      "[epoch: 39, i:  2249]  train_loss: 0.781  |  valid_loss: 0.831\n",
      "[epoch: 39, i:  2374]  train_loss: 0.823  |  valid_loss: 0.767\n",
      "[epoch: 39, i:  2499]  train_loss: 0.830  |  valid_loss: 1.008\n",
      "[epoch: 39, i:  2624]  train_loss: 0.869  |  valid_loss: 0.939\n",
      "[epoch: 39, i:  2749]  train_loss: 0.770  |  valid_loss: 0.934\n",
      "[epoch: 39, i:  2874]  train_loss: 0.816  |  valid_loss: 0.951\n",
      "[epoch: 39, i:  2999]  train_loss: 0.794  |  valid_loss: 0.833\n",
      "[epoch: 39, i:  3124]  train_loss: 0.803  |  valid_loss: 0.907\n",
      "[epoch: 39, i:  3249]  train_loss: 0.858  |  valid_loss: 1.053\n",
      "[epoch: 39, i:  3374]  train_loss: 0.843  |  valid_loss: 0.809\n",
      "[epoch: 39, i:  3499]  train_loss: 0.896  |  valid_loss: 0.757\n",
      "[epoch: 39, i:  3624]  train_loss: 0.781  |  valid_loss: 0.777\n",
      "[epoch: 39, i:  3749]  train_loss: 0.809  |  valid_loss: 0.659\n",
      "[epoch: 39, i:  3874]  train_loss: 0.799  |  valid_loss: 0.812\n",
      "[epoch: 39, i:  3999]  train_loss: 0.799  |  valid_loss: 0.652\n",
      "[epoch: 39, i:  4124]  train_loss: 0.830  |  valid_loss: 0.864\n",
      "[epoch: 39, i:  4249]  train_loss: 0.819  |  valid_loss: 0.895\n",
      "[epoch: 39, i:  4374]  train_loss: 0.820  |  valid_loss: 0.873\n",
      "[epoch: 39, i:  4499]  train_loss: 0.778  |  valid_loss: 0.792\n",
      "[epoch: 39, i:  4624]  train_loss: 0.810  |  valid_loss: 0.961\n",
      "[epoch: 39, i:  4749]  train_loss: 0.799  |  valid_loss: 0.883\n",
      "[epoch: 39, i:  4874]  train_loss: 0.812  |  valid_loss: 0.759\n",
      "[epoch: 39, i:  4999]  train_loss: 0.743  |  valid_loss: 0.746\n",
      "[epoch: 39, i:  5124]  train_loss: 0.835  |  valid_loss: 0.818\n",
      "[epoch: 39, i:  5249]  train_loss: 0.775  |  valid_loss: 0.869\n",
      "[epoch: 39, i:  5374]  train_loss: 0.749  |  valid_loss: 0.599\n",
      "[epoch: 39, i:  5499]  train_loss: 0.837  |  valid_loss: 0.708\n",
      "[epoch: 39, i:  5624]  train_loss: 0.788  |  valid_loss: 0.956\n",
      "[epoch: 39, i:  5749]  train_loss: 0.833  |  valid_loss: 0.810\n",
      "[epoch: 39, i:  5874]  train_loss: 0.775  |  valid_loss: 0.722\n",
      "[epoch: 39, i:  5999]  train_loss: 0.853  |  valid_loss: 0.862\n",
      "[epoch: 39, i:  6124]  train_loss: 0.823  |  valid_loss: 0.747\n",
      "[epoch: 39, i:  6249]  train_loss: 0.847  |  valid_loss: 0.838\n",
      "[epoch: 39, i:  6374]  train_loss: 0.791  |  valid_loss: 0.708\n",
      "[epoch: 39, i:  6499]  train_loss: 0.860  |  valid_loss: 0.772\n",
      "[epoch: 39, i:  6624]  train_loss: 0.892  |  valid_loss: 0.672\n",
      "[epoch: 39, i:  6749]  train_loss: 0.838  |  valid_loss: 0.818\n",
      "[epoch: 39, i:  6874]  train_loss: 0.784  |  valid_loss: 0.741\n",
      "[epoch: 39, i:  6999]  train_loss: 0.785  |  valid_loss: 0.939\n",
      "[epoch: 39, i:  7124]  train_loss: 0.855  |  valid_loss: 0.982\n",
      "[epoch: 39, i:  7249]  train_loss: 0.809  |  valid_loss: 0.673\n",
      "[epoch: 39, i:  7374]  train_loss: 0.758  |  valid_loss: 1.020\n",
      "[epoch: 39, i:  7499]  train_loss: 0.839  |  valid_loss: 0.909\n",
      "[epoch: 39, i:  7624]  train_loss: 0.765  |  valid_loss: 0.912\n",
      "[epoch: 39, i:  7749]  train_loss: 0.785  |  valid_loss: 0.778\n",
      "[epoch: 39, i:  7874]  train_loss: 0.834  |  valid_loss: 0.887\n",
      "[epoch: 39, i:  7999]  train_loss: 0.858  |  valid_loss: 0.703\n",
      "[epoch: 39, i:  8124]  train_loss: 0.837  |  valid_loss: 0.910\n",
      "[epoch: 39, i:  8249]  train_loss: 0.760  |  valid_loss: 0.922\n",
      "[epoch: 39, i:  8374]  train_loss: 0.779  |  valid_loss: 0.856\n",
      "[epoch: 39, i:  8499]  train_loss: 0.810  |  valid_loss: 0.917\n",
      "[epoch: 39, i:  8624]  train_loss: 0.807  |  valid_loss: 0.863\n",
      "[epoch: 39, i:  8749]  train_loss: 0.794  |  valid_loss: 0.880\n",
      "[epoch: 39, i:  8874]  train_loss: 0.859  |  valid_loss: 0.849\n",
      "[epoch: 39, i:  8999]  train_loss: 0.771  |  valid_loss: 0.683\n",
      "[epoch: 39, i:  9124]  train_loss: 0.783  |  valid_loss: 0.750\n",
      "[epoch: 39, i:  9249]  train_loss: 0.769  |  valid_loss: 0.622\n",
      "[epoch: 39, i:  9374]  train_loss: 0.804  |  valid_loss: 0.910\n",
      "[epoch: 39, i:  9499]  train_loss: 0.782  |  valid_loss: 0.764\n",
      "[epoch: 39, i:  9624]  train_loss: 0.877  |  valid_loss: 0.861\n",
      "[epoch: 39, i:  9749]  train_loss: 0.813  |  valid_loss: 0.948\n",
      "[epoch: 39, i:  9874]  train_loss: 0.839  |  valid_loss: 0.895\n",
      "[epoch: 39, i:  9999]  train_loss: 0.840  |  valid_loss: 0.836\n",
      "[epoch: 39, i: 10124]  train_loss: 0.793  |  valid_loss: 0.742\n",
      "[epoch: 39, i: 10249]  train_loss: 0.794  |  valid_loss: 0.861\n",
      "[epoch: 39, i: 10374]  train_loss: 0.881  |  valid_loss: 0.735\n",
      "[epoch: 39, i: 10499]  train_loss: 0.756  |  valid_loss: 0.925\n",
      "[epoch: 39, i: 10624]  train_loss: 0.834  |  valid_loss: 0.931\n",
      "[epoch: 39, i: 10749]  train_loss: 0.826  |  valid_loss: 0.812\n",
      "[epoch: 39, i: 10874]  train_loss: 0.817  |  valid_loss: 0.958\n",
      "[epoch: 39, i: 10999]  train_loss: 0.862  |  valid_loss: 0.961\n",
      "[epoch: 39, i: 11124]  train_loss: 0.830  |  valid_loss: 0.848\n",
      "[epoch: 39, i: 11249]  train_loss: 0.909  |  valid_loss: 0.877\n",
      "[epoch: 39, i: 11374]  train_loss: 0.826  |  valid_loss: 0.796\n",
      "[epoch: 39, i: 11499]  train_loss: 0.695  |  valid_loss: 0.613\n",
      "[epoch: 39, i: 11624]  train_loss: 0.772  |  valid_loss: 0.801\n",
      "[epoch: 39, i: 11749]  train_loss: 0.798  |  valid_loss: 0.763\n",
      "[epoch: 39, i: 11874]  train_loss: 0.828  |  valid_loss: 0.801\n",
      "[epoch: 39, i: 11999]  train_loss: 0.800  |  valid_loss: 0.687\n",
      "[epoch: 39, i: 12124]  train_loss: 0.839  |  valid_loss: 0.743\n",
      "[epoch: 39, i: 12249]  train_loss: 0.845  |  valid_loss: 1.046\n",
      "[epoch: 39, i: 12374]  train_loss: 0.769  |  valid_loss: 0.873\n",
      "[epoch: 39, i: 12499]  train_loss: 0.770  |  valid_loss: 0.859\n",
      "--> [End of epoch 39] train_accuracy: 71.57%  |  valid_accuracy: 70.93%\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "PATH = './models/cifar_dense_avgpool_exp_adam.pth'\n",
    "\n",
    "epochs = 40             # Total epochs.\n",
    "iter_n = len(trainloader)  # number of iteration per epoch\n",
    "record_freq = iter_n // 100  # Record frequency.\n",
    "print_freq = iter_n // 100 # Print frequency, need to be multiple of record_freq.\n",
    "\n",
    "# a number for computing running validation loss in each iteration.\n",
    "train_per_valid = len(trainloader) / len(validloader)  \n",
    "\n",
    "avg_train_losses, avg_valid_losses = [], []   # Avg. losses.\n",
    "train_accuracies, valid_accuracies = [], []  # Train and test accuracies.\n",
    "\n",
    "dense_net = DenseNet()     # Create the network instance.\n",
    "dense_net.to(device)  # Move the network parameters to the specified device.\n",
    "\n",
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "\n",
    "# We use Adam as optimizer.\n",
    "opt = optim.Adam(dense_net.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# add plateau scheduling\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.8)\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_train_loss = 0.0       # Initialize running train_loss for training set.\n",
    "    running_val_loss = 0.0     # Initialize running train_loss for validation set.\n",
    "    \n",
    "    # Initialize running total and correct for computing train accuracy.\n",
    "    train_correct = 0 \n",
    "    train_total = 0\n",
    "    # Initialize running total and correct for computing test accuracy.\n",
    "    valid_total = 0   \n",
    "    valid_correct = 0\n",
    "    \n",
    "    print('--> [Start of epoch {}]'.format(epoch) +\n",
    "          '  lr: {:.6f}'.format(scheduler.get_last_lr()[0]))\n",
    "    \n",
    "    \n",
    "    validiter = iter(validloader)\n",
    "    for i, data in enumerate(iterable=trainloader, start=0):\n",
    "        \n",
    "        # Set the model to training mode.\n",
    "        dense_net.train()\n",
    "        \n",
    "        # Get the train_inputs.\n",
    "        train_inputs, train_labels = data\n",
    "        \n",
    "        # Move the train_inputs to the specified device.\n",
    "        train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step\n",
    "        train_output = dense_net(train_inputs)\n",
    "        train_loss = loss_func(train_output, train_labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "        \n",
    "        dense_net.eval()\n",
    "        # record statistics.\n",
    "        running_train_loss += train_loss.item()\n",
    "        \n",
    "        # only iterate through validation set according to train_per_valid ratio\n",
    "        if i % train_per_valid == train_per_valid - 1:\n",
    "            # Set the model to evaluation mode.\n",
    "            with torch.no_grad():\n",
    "                valid_inputs, valid_labels = next(validiter)\n",
    "                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)\n",
    "                valid_output = dense_net(valid_inputs)\n",
    "                valid_loss = loss_func(valid_output, valid_labels)\n",
    "                running_val_loss += valid_loss.item()\n",
    "                \n",
    "                _, valid_predicted = torch.max(valid_output.data, 1)\n",
    "                valid_total += valid_labels.size(0)\n",
    "                valid_correct += (valid_predicted == valid_labels).sum().item()\n",
    "        \n",
    "        # record training accuracy in every mini-batch\n",
    "        _, train_predicted = torch.max(train_output.data, 1)\n",
    "        train_total += train_labels.size(0)\n",
    "        train_correct += (train_predicted == train_labels).sum().item()\n",
    "        \n",
    "        # Record training/validation loss every several mini-batches.\n",
    "        if i % record_freq == record_freq - 1: \n",
    "            avg_train_loss = running_train_loss / record_freq\n",
    "            avg_train_losses.append(avg_train_loss)\n",
    "            avg_valid_loss = train_per_valid * running_val_loss / record_freq\n",
    "            avg_valid_losses.append(avg_valid_loss)\n",
    "            running_train_loss, running_val_loss = 0.0, 0.0\n",
    "            \n",
    "            # only print traing training loss once in a while to avoid cluster output\n",
    "            if i % print_freq == print_freq - 1:\n",
    "                print('[epoch: {}, i: {:5d}]'.format(epoch, i) +\n",
    "                      '  train_loss: {:.3f}'.format(avg_train_loss) + \n",
    "                      '  |  valid_loss: {:.3f}'.format(avg_valid_loss))\n",
    "\n",
    "    # do exponential scheduling\n",
    "    scheduler.step()\n",
    "                   \n",
    "    # calculating train accuracy\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    # Store the networks after each epochs.\n",
    "    # in case we want to do simple average or weighted average\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': dense_net.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'train_loss': avg_train_losses[epoch],\n",
    "            'train_accuracy': train_accuracies[epoch],\n",
    "            'valid_accuracy': valid_accuracies[epoch],\n",
    "            }, PATH)\n",
    "    \n",
    "    print('--> [End of epoch {}]'.format(epoch) +\n",
    "                      ' train_accuracy: {:.2f}%'.format(train_accuracy) + \n",
    "                      '  |  valid_accuracy: {:.2f}%'.format(valid_accuracy))\n",
    "              \n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_train_losses)\n",
    "# plt.plot(avg_valid_losses)\n",
    "plt.title('DenseNet loss curve + pleateau scheduling')\n",
    "plt.xlabel('mini-batch index / {}'.format(record_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute the moving average of the data using a specified window size.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Calculate moving averages\n",
    "window_size = 100  # Adjust this based on your preference\n",
    "train_losses_ma = moving_average(avg_train_losses, window_size)\n",
    "valid_losses_ma = moving_average(avg_valid_losses, window_size)\n",
    "\n",
    "# Plot original loss curves\n",
    "# plt.plot(avg_train_losses, label='Training Loss', alpha=0.5)\n",
    "# plt.plot(avg_valid_losses, label='Validation Loss', alpha=0.5)\n",
    "\n",
    "# Plot trend lines (moving averages)\n",
    "plt.plot(np.arange(window_size - 1, len(train_losses_ma) + window_size - 1), train_losses_ma, label='Training Loss Trend', linewidth=2)\n",
    "plt.plot(np.arange(window_size - 1, len(valid_losses_ma) + window_size - 1), valid_losses_ma, label='Validation Loss Trend', linewidth=2)\n",
    "\n",
    "plt.title('Loss Curve with Trend Lines')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies)\n",
    "plt.plot(valid_accuracies)\n",
    "plt.title('Accuracy Curve for DenseNet + ReduceLROnPlateau + Adam')\n",
    "plt.xlabel('epoch / {}'.format(record_freq))\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train accuracy', 'test accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to show an image.\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # Unnormalize.\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n",
      "Predicted:    cat  ship  ship plane\n"
     ]
    }
   ],
   "source": [
    "# Check several images.\n",
    "dataiter = iter(validloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "outputs = dense_net(images.to(device))\n",
    "\n",
    "# max compare along the row, return the index of the max value, which is the predicted class\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 73.13 %\n"
     ]
    }
   ],
   "source": [
    "dense_net.eval()\n",
    "# Get test accuracy.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 70 %\n",
      "Accuracy of   car : 84 %\n",
      "Accuracy of  bird : 59 %\n",
      "Accuracy of   cat : 48 %\n",
      "Accuracy of  deer : 69 %\n",
      "Accuracy of   dog : 65 %\n",
      "Accuracy of  frog : 84 %\n",
      "Accuracy of horse : 74 %\n",
      "Accuracy of  ship : 91 %\n",
      "Accuracy of truck : 83 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs181",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
