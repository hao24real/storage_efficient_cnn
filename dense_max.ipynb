{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haotu/anaconda3/envs/cogs181/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "mps\n",
      "50000\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "std_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "std_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=std_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(std_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=std_transforms)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# the order for cifar10 10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(device)\n",
    "print(len(std_trainset))\n",
    "print(len(trainloader))  \n",
    "# If 'mps or cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Apply data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "70000\n",
      "17500\n"
     ]
    }
   ],
   "source": [
    "from custom_nn.datasets import CustomCIFAR10\n",
    "\n",
    "# Additional focused augmentations\n",
    "focused_augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "])\n",
    "\n",
    "# using custom dataset class to add focused augmentations capability\n",
    "aug_trainset = CustomCIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=std_transforms,\n",
    "    focused_transform=focused_augmentations\n",
    ")\n",
    "\n",
    "# the class we want the augmentation for birds, cats, deer, and dogs\n",
    "desired_classes = [1, 2, 3, 4]\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_indices = [i for i, (_, label) in enumerate(aug_trainset) if label in desired_classes]\n",
    "\n",
    "filtered_dataset = torch.utils.data.Subset(aug_trainset, filtered_indices)\n",
    "\n",
    "# Combine the two datasets\n",
    "combined_trainset = torch.utils.data.ConcatDataset([std_trainset, filtered_dataset])\n",
    "trainloader = torch.utils.data.DataLoader(combined_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(len(combined_trainset))\n",
    "print(len(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_per_valid: 7.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from custom_nn.models import DenseMax\n",
    "\n",
    "epoch_start = 0     # Start from epoch 0 or last checkpoint epoch\n",
    "iter_n = len(trainloader)  # number of iteration per epoch\n",
    "record_freq = 100 # how many times to record the loss and accuracy per epoch\n",
    "print_freq = 100 # Print frequency, need to be multiple of record_freq.\n",
    "iter_n_per_record = iter_n // record_freq  # Record frequency.\n",
    "iter_n_per_print = iter_n // print_freq # Print frequency, need to be multiple of record_freq.\n",
    "\n",
    "# a number for computing running validation loss in each iteration.\n",
    "train_per_valid = len(trainloader) / len(validloader)  \n",
    "\n",
    "avg_train_losses, avg_valid_losses = [], []   # Avg. losses.\n",
    "train_accuracies, valid_accuracies = [], []  # Train and test accuracies.\n",
    "\n",
    "dense_max = DenseMax()     # Create the network instance.\n",
    "# Move the network parameters to the specified device\n",
    "# need to be done before passing to optimizer.\n",
    "dense_max.to(device)  \n",
    "\n",
    "assert dense_max(torch.randn(4, 3, 32, 32).to(device)).shape == torch.Size([4, 10])\n",
    "\n",
    "# We use Adam as optimizer.\n",
    "opt = optim.Adam(dense_max.parameters(), lr=5e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss() \n",
    "\n",
    "# add scheduling\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.8)\n",
    "# scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.8)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.1, patience=4, threshold=1e-4)\n",
    "\n",
    "# directory to save the model\n",
    "directory_path = Path(\"./checkpoints/dense_max_batch\" + str(batch_size))\n",
    "# Make the directory if it doesn't exist\n",
    "directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"train_per_valid:\", train_per_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'avg_train_losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m epoch_files:\n\u001b[1;32m     23\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m     avg_train_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_train_losses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m     avg_valid_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_valid_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m     train_accuracies\u001b[38;5;241m.\u001b[39mappend(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'avg_train_losses'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Pattern to match the files and extract epoch numbers\n",
    "pattern = r'epoch_(\\d+).pth'\n",
    "\n",
    "# Use Path.glob to list all files matching the pattern\n",
    "files = list(directory_path.glob('epoch_*.pth'))\n",
    "\n",
    "# Extract epochs and files into a list of tuples\n",
    "epoch_files = []\n",
    "for file_path in files:\n",
    "    match = re.search(pattern, file_path.name)\n",
    "    if match:\n",
    "        epoch_num = int(match.group(1))\n",
    "        epoch_files.append((epoch_num, file_path))\n",
    "\n",
    "# Sort the list by epoch number in descending order\n",
    "epoch_files.sort(key=lambda x: x[0], reverse=False)\n",
    "\n",
    "if epoch_files:\n",
    "    # populating history for loss and accuracy for plotting\n",
    "    for file in epoch_files:\n",
    "        checkpoint = torch.load(file[1])\n",
    "        avg_train_losses += checkpoint['avg_train_losses']\n",
    "        avg_valid_losses += checkpoint['avg_valid_losses']\n",
    "        train_accuracies.append(checkpoint['train_accuracy'])\n",
    "        valid_accuracies.append(checkpoint['valid_accuracy'])\n",
    "    \n",
    "    # load the model from the latest checkpoint file\n",
    "    latest_file_path = epoch_files[-1][1]\n",
    "    print(f\"The latest checkpoint file is: {latest_file_path}\")\n",
    "    checkpoint = torch.load(latest_file_path)\n",
    "    dense_max.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # start training from the next epoch\n",
    "    epoch_start = checkpoint['epoch'] + 1\n",
    "else:\n",
    "    print(\"No checkpoint files found.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> [Start of epoch 0]  lr: 0.000500\n",
      "[epoch: 0, i:    86]  train_loss: 2.080  |  valid_loss: 2.222\n",
      "[epoch: 0, i:   173]  train_loss: 1.845  |  valid_loss: 1.943\n",
      "[epoch: 0, i:   260]  train_loss: 1.770  |  valid_loss: 1.908\n",
      "[epoch: 0, i:   347]  train_loss: 1.724  |  valid_loss: 1.694\n",
      "[epoch: 0, i:   434]  train_loss: 1.635  |  valid_loss: 1.817\n",
      "[epoch: 0, i:   521]  train_loss: 1.591  |  valid_loss: 1.664\n",
      "[epoch: 0, i:   608]  train_loss: 1.565  |  valid_loss: 1.710\n",
      "[epoch: 0, i:   695]  train_loss: 1.588  |  valid_loss: 1.540\n",
      "[epoch: 0, i:   782]  train_loss: 1.556  |  valid_loss: 1.440\n",
      "[epoch: 0, i:   869]  train_loss: 1.532  |  valid_loss: 1.606\n",
      "[epoch: 0, i:   956]  train_loss: 1.499  |  valid_loss: 1.413\n",
      "[epoch: 0, i:  1043]  train_loss: 1.530  |  valid_loss: 1.555\n",
      "[epoch: 0, i:  1130]  train_loss: 1.431  |  valid_loss: 1.480\n",
      "[epoch: 0, i:  1217]  train_loss: 1.434  |  valid_loss: 1.514\n",
      "[epoch: 0, i:  1304]  train_loss: 1.489  |  valid_loss: 1.303\n",
      "[epoch: 0, i:  1391]  train_loss: 1.430  |  valid_loss: 1.355\n",
      "[epoch: 0, i:  1478]  train_loss: 1.455  |  valid_loss: 1.448\n",
      "[epoch: 0, i:  1565]  train_loss: 1.384  |  valid_loss: 1.446\n",
      "[epoch: 0, i:  1652]  train_loss: 1.457  |  valid_loss: 1.426\n",
      "[epoch: 0, i:  1739]  train_loss: 1.362  |  valid_loss: 1.362\n",
      "[epoch: 0, i:  1826]  train_loss: 1.445  |  valid_loss: 1.529\n",
      "[epoch: 0, i:  1913]  train_loss: 1.372  |  valid_loss: 1.314\n",
      "[epoch: 0, i:  2000]  train_loss: 1.423  |  valid_loss: 1.289\n",
      "[epoch: 0, i:  2087]  train_loss: 1.450  |  valid_loss: 1.503\n",
      "[epoch: 0, i:  2174]  train_loss: 1.275  |  valid_loss: 1.276\n",
      "[epoch: 0, i:  2261]  train_loss: 1.355  |  valid_loss: 1.616\n",
      "[epoch: 0, i:  2348]  train_loss: 1.411  |  valid_loss: 1.240\n",
      "[epoch: 0, i:  2435]  train_loss: 1.342  |  valid_loss: 1.426\n",
      "[epoch: 0, i:  2522]  train_loss: 1.308  |  valid_loss: 1.255\n",
      "[epoch: 0, i:  2609]  train_loss: 1.321  |  valid_loss: 1.168\n",
      "[epoch: 0, i:  2696]  train_loss: 1.293  |  valid_loss: 1.386\n",
      "[epoch: 0, i:  2783]  train_loss: 1.233  |  valid_loss: 1.107\n",
      "[epoch: 0, i:  2870]  train_loss: 1.300  |  valid_loss: 1.337\n",
      "[epoch: 0, i:  2957]  train_loss: 1.275  |  valid_loss: 1.185\n",
      "[epoch: 0, i:  3044]  train_loss: 1.326  |  valid_loss: 1.319\n",
      "[epoch: 0, i:  3131]  train_loss: 1.290  |  valid_loss: 1.229\n",
      "[epoch: 0, i:  3218]  train_loss: 1.209  |  valid_loss: 1.166\n",
      "[epoch: 0, i:  3305]  train_loss: 1.328  |  valid_loss: 1.328\n",
      "[epoch: 0, i:  3392]  train_loss: 1.284  |  valid_loss: 1.135\n",
      "[epoch: 0, i:  3479]  train_loss: 1.236  |  valid_loss: 1.263\n",
      "[epoch: 0, i:  3566]  train_loss: 1.308  |  valid_loss: 1.194\n",
      "[epoch: 0, i:  3653]  train_loss: 1.260  |  valid_loss: 1.173\n",
      "[epoch: 0, i:  3740]  train_loss: 1.168  |  valid_loss: 1.021\n",
      "[epoch: 0, i:  3827]  train_loss: 1.232  |  valid_loss: 1.036\n",
      "[epoch: 0, i:  3914]  train_loss: 1.275  |  valid_loss: 1.168\n",
      "[epoch: 0, i:  4001]  train_loss: 1.231  |  valid_loss: 1.230\n",
      "[epoch: 0, i:  4088]  train_loss: 1.193  |  valid_loss: 1.260\n",
      "[epoch: 0, i:  4175]  train_loss: 1.217  |  valid_loss: 1.124\n",
      "[epoch: 0, i:  4262]  train_loss: 1.205  |  valid_loss: 1.075\n",
      "[epoch: 0, i:  4349]  train_loss: 1.176  |  valid_loss: 0.971\n",
      "[epoch: 0, i:  4436]  train_loss: 1.292  |  valid_loss: 1.130\n",
      "[epoch: 0, i:  4523]  train_loss: 1.189  |  valid_loss: 1.299\n",
      "[epoch: 0, i:  4610]  train_loss: 1.235  |  valid_loss: 1.076\n",
      "[epoch: 0, i:  4697]  train_loss: 1.142  |  valid_loss: 1.165\n",
      "[epoch: 0, i:  4784]  train_loss: 1.209  |  valid_loss: 1.149\n",
      "[epoch: 0, i:  4871]  train_loss: 1.194  |  valid_loss: 1.281\n",
      "[epoch: 0, i:  4958]  train_loss: 1.172  |  valid_loss: 1.335\n",
      "[epoch: 0, i:  5045]  train_loss: 1.175  |  valid_loss: 0.854\n",
      "[epoch: 0, i:  5132]  train_loss: 1.141  |  valid_loss: 1.256\n",
      "[epoch: 0, i:  5219]  train_loss: 1.150  |  valid_loss: 1.213\n",
      "[epoch: 0, i:  5306]  train_loss: 1.158  |  valid_loss: 1.160\n",
      "[epoch: 0, i:  5393]  train_loss: 1.174  |  valid_loss: 1.070\n",
      "[epoch: 0, i:  5480]  train_loss: 1.144  |  valid_loss: 1.324\n",
      "[epoch: 0, i:  5567]  train_loss: 1.211  |  valid_loss: 0.869\n",
      "[epoch: 0, i:  5654]  train_loss: 1.131  |  valid_loss: 1.165\n",
      "[epoch: 0, i:  5741]  train_loss: 1.203  |  valid_loss: 1.044\n",
      "[epoch: 0, i:  5828]  train_loss: 1.202  |  valid_loss: 1.142\n",
      "[epoch: 0, i:  5915]  train_loss: 1.167  |  valid_loss: 1.273\n",
      "[epoch: 0, i:  6002]  train_loss: 1.193  |  valid_loss: 0.996\n",
      "[epoch: 0, i:  6089]  train_loss: 1.152  |  valid_loss: 1.328\n",
      "[epoch: 0, i:  6176]  train_loss: 1.161  |  valid_loss: 1.268\n",
      "[epoch: 0, i:  6263]  train_loss: 1.243  |  valid_loss: 0.981\n",
      "[epoch: 0, i:  6350]  train_loss: 1.081  |  valid_loss: 1.083\n",
      "[epoch: 0, i:  6437]  train_loss: 1.152  |  valid_loss: 0.926\n",
      "[epoch: 0, i:  6524]  train_loss: 1.143  |  valid_loss: 0.993\n",
      "[epoch: 0, i:  6611]  train_loss: 1.102  |  valid_loss: 0.879\n",
      "[epoch: 0, i:  6698]  train_loss: 1.184  |  valid_loss: 1.228\n",
      "[epoch: 0, i:  6785]  train_loss: 1.116  |  valid_loss: 1.025\n",
      "[epoch: 0, i:  6872]  train_loss: 1.138  |  valid_loss: 1.012\n",
      "[epoch: 0, i:  6959]  train_loss: 1.086  |  valid_loss: 1.197\n",
      "[epoch: 0, i:  7046]  train_loss: 1.124  |  valid_loss: 1.012\n",
      "[epoch: 0, i:  7133]  train_loss: 1.078  |  valid_loss: 1.016\n",
      "[epoch: 0, i:  7220]  train_loss: 1.123  |  valid_loss: 0.986\n",
      "[epoch: 0, i:  7307]  train_loss: 1.135  |  valid_loss: 1.224\n",
      "[epoch: 0, i:  7394]  train_loss: 1.095  |  valid_loss: 0.876\n",
      "[epoch: 0, i:  7481]  train_loss: 1.132  |  valid_loss: 1.096\n",
      "[epoch: 0, i:  7568]  train_loss: 1.098  |  valid_loss: 1.081\n",
      "[epoch: 0, i:  7655]  train_loss: 1.130  |  valid_loss: 1.184\n",
      "[epoch: 0, i:  7742]  train_loss: 1.109  |  valid_loss: 1.005\n",
      "[epoch: 0, i:  7829]  train_loss: 1.110  |  valid_loss: 1.090\n",
      "[epoch: 0, i:  7916]  train_loss: 0.982  |  valid_loss: 1.264\n",
      "[epoch: 0, i:  8003]  train_loss: 1.053  |  valid_loss: 1.013\n",
      "[epoch: 0, i:  8090]  train_loss: 1.050  |  valid_loss: 0.838\n",
      "[epoch: 0, i:  8177]  train_loss: 1.009  |  valid_loss: 1.054\n",
      "[epoch: 0, i:  8264]  train_loss: 1.110  |  valid_loss: 1.120\n",
      "[epoch: 0, i:  8351]  train_loss: 1.060  |  valid_loss: 1.264\n",
      "[epoch: 0, i:  8438]  train_loss: 1.030  |  valid_loss: 0.855\n",
      "[epoch: 0, i:  8525]  train_loss: 1.005  |  valid_loss: 1.098\n",
      "[epoch: 0, i:  8612]  train_loss: 1.103  |  valid_loss: 1.056\n",
      "[epoch: 0, i:  8699]  train_loss: 1.053  |  valid_loss: 1.092\n",
      "--> [End of epoch 0] train_accuracy: 54.62%  |  valid_accuracy: 55.59%\n",
      "--> [Start of epoch 1]  lr: 0.000500\n",
      "[epoch: 1, i:    86]  train_loss: 0.988  |  valid_loss: 0.813\n",
      "[epoch: 1, i:   173]  train_loss: 1.030  |  valid_loss: 1.046\n",
      "[epoch: 1, i:   260]  train_loss: 1.056  |  valid_loss: 1.137\n",
      "[epoch: 1, i:   347]  train_loss: 1.007  |  valid_loss: 0.869\n",
      "[epoch: 1, i:   434]  train_loss: 1.000  |  valid_loss: 1.097\n",
      "[epoch: 1, i:   521]  train_loss: 1.028  |  valid_loss: 0.851\n",
      "[epoch: 1, i:   608]  train_loss: 1.053  |  valid_loss: 1.033\n",
      "[epoch: 1, i:   695]  train_loss: 1.062  |  valid_loss: 0.865\n",
      "[epoch: 1, i:   782]  train_loss: 1.087  |  valid_loss: 1.034\n",
      "[epoch: 1, i:   869]  train_loss: 0.979  |  valid_loss: 0.969\n",
      "[epoch: 1, i:   956]  train_loss: 1.038  |  valid_loss: 0.830\n",
      "[epoch: 1, i:  1043]  train_loss: 1.077  |  valid_loss: 0.958\n",
      "[epoch: 1, i:  1130]  train_loss: 1.095  |  valid_loss: 0.928\n",
      "[epoch: 1, i:  1217]  train_loss: 1.048  |  valid_loss: 0.987\n",
      "[epoch: 1, i:  1304]  train_loss: 0.998  |  valid_loss: 0.749\n",
      "[epoch: 1, i:  1391]  train_loss: 0.951  |  valid_loss: 0.986\n",
      "[epoch: 1, i:  1478]  train_loss: 1.013  |  valid_loss: 0.951\n",
      "[epoch: 1, i:  1565]  train_loss: 1.005  |  valid_loss: 0.965\n",
      "[epoch: 1, i:  1652]  train_loss: 1.046  |  valid_loss: 0.907\n",
      "[epoch: 1, i:  1739]  train_loss: 1.049  |  valid_loss: 1.084\n",
      "[epoch: 1, i:  1826]  train_loss: 0.997  |  valid_loss: 0.978\n",
      "[epoch: 1, i:  1913]  train_loss: 0.975  |  valid_loss: 0.936\n",
      "[epoch: 1, i:  2000]  train_loss: 0.975  |  valid_loss: 0.897\n",
      "[epoch: 1, i:  2087]  train_loss: 1.022  |  valid_loss: 1.018\n",
      "[epoch: 1, i:  2174]  train_loss: 0.973  |  valid_loss: 0.798\n",
      "[epoch: 1, i:  2261]  train_loss: 0.966  |  valid_loss: 1.271\n",
      "[epoch: 1, i:  2348]  train_loss: 0.973  |  valid_loss: 0.889\n",
      "[epoch: 1, i:  2435]  train_loss: 1.039  |  valid_loss: 1.141\n",
      "[epoch: 1, i:  2522]  train_loss: 1.013  |  valid_loss: 0.880\n",
      "[epoch: 1, i:  2609]  train_loss: 0.995  |  valid_loss: 0.732\n",
      "[epoch: 1, i:  2696]  train_loss: 1.012  |  valid_loss: 0.980\n",
      "[epoch: 1, i:  2783]  train_loss: 0.976  |  valid_loss: 0.850\n",
      "[epoch: 1, i:  2870]  train_loss: 0.999  |  valid_loss: 0.945\n",
      "[epoch: 1, i:  2957]  train_loss: 1.008  |  valid_loss: 0.971\n",
      "[epoch: 1, i:  3044]  train_loss: 0.931  |  valid_loss: 1.088\n",
      "[epoch: 1, i:  3131]  train_loss: 1.128  |  valid_loss: 0.990\n",
      "[epoch: 1, i:  3218]  train_loss: 1.023  |  valid_loss: 0.957\n",
      "[epoch: 1, i:  3305]  train_loss: 1.032  |  valid_loss: 1.033\n",
      "[epoch: 1, i:  3392]  train_loss: 1.080  |  valid_loss: 0.906\n",
      "[epoch: 1, i:  3479]  train_loss: 1.014  |  valid_loss: 0.905\n",
      "[epoch: 1, i:  3566]  train_loss: 0.984  |  valid_loss: 0.818\n",
      "[epoch: 1, i:  3653]  train_loss: 0.963  |  valid_loss: 0.922\n",
      "[epoch: 1, i:  3740]  train_loss: 1.001  |  valid_loss: 0.640\n",
      "[epoch: 1, i:  3827]  train_loss: 0.972  |  valid_loss: 0.928\n",
      "[epoch: 1, i:  3914]  train_loss: 0.922  |  valid_loss: 0.878\n",
      "[epoch: 1, i:  4001]  train_loss: 1.034  |  valid_loss: 0.915\n",
      "[epoch: 1, i:  4088]  train_loss: 0.981  |  valid_loss: 0.982\n",
      "[epoch: 1, i:  4175]  train_loss: 1.065  |  valid_loss: 0.926\n",
      "[epoch: 1, i:  4262]  train_loss: 0.939  |  valid_loss: 0.729\n",
      "[epoch: 1, i:  4349]  train_loss: 0.988  |  valid_loss: 0.863\n",
      "[epoch: 1, i:  4436]  train_loss: 1.001  |  valid_loss: 0.872\n",
      "[epoch: 1, i:  4523]  train_loss: 1.091  |  valid_loss: 0.965\n",
      "[epoch: 1, i:  4610]  train_loss: 0.949  |  valid_loss: 0.790\n",
      "[epoch: 1, i:  4697]  train_loss: 0.984  |  valid_loss: 0.871\n",
      "[epoch: 1, i:  4784]  train_loss: 0.953  |  valid_loss: 0.923\n",
      "[epoch: 1, i:  4871]  train_loss: 0.954  |  valid_loss: 1.025\n",
      "[epoch: 1, i:  4958]  train_loss: 0.940  |  valid_loss: 1.092\n",
      "[epoch: 1, i:  5045]  train_loss: 0.899  |  valid_loss: 0.687\n",
      "[epoch: 1, i:  5132]  train_loss: 0.946  |  valid_loss: 0.980\n",
      "[epoch: 1, i:  5219]  train_loss: 0.992  |  valid_loss: 1.049\n",
      "[epoch: 1, i:  5306]  train_loss: 1.037  |  valid_loss: 1.042\n",
      "[epoch: 1, i:  5393]  train_loss: 0.955  |  valid_loss: 0.828\n",
      "[epoch: 1, i:  5480]  train_loss: 0.972  |  valid_loss: 0.888\n",
      "[epoch: 1, i:  5567]  train_loss: 0.919  |  valid_loss: 0.693\n",
      "[epoch: 1, i:  5654]  train_loss: 0.982  |  valid_loss: 1.008\n",
      "[epoch: 1, i:  5741]  train_loss: 0.997  |  valid_loss: 0.875\n",
      "[epoch: 1, i:  5828]  train_loss: 0.996  |  valid_loss: 0.945\n",
      "[epoch: 1, i:  5915]  train_loss: 0.948  |  valid_loss: 1.061\n",
      "[epoch: 1, i:  6002]  train_loss: 0.971  |  valid_loss: 0.780\n",
      "[epoch: 1, i:  6089]  train_loss: 0.881  |  valid_loss: 1.095\n",
      "[epoch: 1, i:  6176]  train_loss: 0.972  |  valid_loss: 1.006\n",
      "[epoch: 1, i:  6263]  train_loss: 1.050  |  valid_loss: 0.792\n",
      "[epoch: 1, i:  6350]  train_loss: 0.912  |  valid_loss: 0.862\n",
      "[epoch: 1, i:  6437]  train_loss: 1.028  |  valid_loss: 0.652\n",
      "[epoch: 1, i:  6524]  train_loss: 0.978  |  valid_loss: 0.988\n",
      "[epoch: 1, i:  6611]  train_loss: 0.898  |  valid_loss: 0.710\n",
      "[epoch: 1, i:  6698]  train_loss: 0.902  |  valid_loss: 0.970\n",
      "[epoch: 1, i:  6785]  train_loss: 0.995  |  valid_loss: 0.704\n",
      "[epoch: 1, i:  6872]  train_loss: 0.908  |  valid_loss: 0.914\n",
      "[epoch: 1, i:  6959]  train_loss: 0.989  |  valid_loss: 1.020\n",
      "[epoch: 1, i:  7046]  train_loss: 0.897  |  valid_loss: 0.894\n",
      "[epoch: 1, i:  7133]  train_loss: 0.944  |  valid_loss: 0.796\n",
      "[epoch: 1, i:  7220]  train_loss: 0.948  |  valid_loss: 0.844\n",
      "[epoch: 1, i:  7307]  train_loss: 0.906  |  valid_loss: 0.952\n",
      "[epoch: 1, i:  7394]  train_loss: 0.952  |  valid_loss: 0.769\n",
      "[epoch: 1, i:  7481]  train_loss: 0.936  |  valid_loss: 0.913\n",
      "[epoch: 1, i:  7568]  train_loss: 0.965  |  valid_loss: 0.888\n",
      "[epoch: 1, i:  7655]  train_loss: 0.957  |  valid_loss: 0.940\n",
      "[epoch: 1, i:  7742]  train_loss: 0.889  |  valid_loss: 0.938\n",
      "[epoch: 1, i:  7829]  train_loss: 0.984  |  valid_loss: 0.760\n",
      "[epoch: 1, i:  7916]  train_loss: 0.995  |  valid_loss: 0.944\n",
      "[epoch: 1, i:  8003]  train_loss: 0.868  |  valid_loss: 0.760\n",
      "[epoch: 1, i:  8090]  train_loss: 0.916  |  valid_loss: 0.602\n",
      "[epoch: 1, i:  8177]  train_loss: 0.912  |  valid_loss: 0.913\n",
      "[epoch: 1, i:  8264]  train_loss: 0.888  |  valid_loss: 0.953\n",
      "[epoch: 1, i:  8351]  train_loss: 0.982  |  valid_loss: 1.043\n",
      "[epoch: 1, i:  8438]  train_loss: 0.882  |  valid_loss: 0.736\n",
      "[epoch: 1, i:  8525]  train_loss: 0.949  |  valid_loss: 0.909\n",
      "[epoch: 1, i:  8612]  train_loss: 0.932  |  valid_loss: 0.976\n",
      "[epoch: 1, i:  8699]  train_loss: 0.974  |  valid_loss: 0.986\n",
      "--> [End of epoch 1] train_accuracy: 65.23%  |  valid_accuracy: 68.46%\n",
      "--> [Start of epoch 2]  lr: 0.000500\n",
      "[epoch: 2, i:    86]  train_loss: 0.923  |  valid_loss: 0.674\n",
      "[epoch: 2, i:   173]  train_loss: 0.957  |  valid_loss: 0.881\n",
      "[epoch: 2, i:   260]  train_loss: 0.793  |  valid_loss: 0.956\n",
      "[epoch: 2, i:   347]  train_loss: 0.928  |  valid_loss: 0.836\n",
      "[epoch: 2, i:   434]  train_loss: 0.875  |  valid_loss: 0.871\n",
      "[epoch: 2, i:   521]  train_loss: 0.860  |  valid_loss: 0.711\n",
      "[epoch: 2, i:   608]  train_loss: 0.872  |  valid_loss: 0.895\n",
      "[epoch: 2, i:   695]  train_loss: 0.784  |  valid_loss: 0.818\n",
      "[epoch: 2, i:   782]  train_loss: 0.855  |  valid_loss: 0.857\n",
      "[epoch: 2, i:   869]  train_loss: 0.868  |  valid_loss: 0.827\n",
      "[epoch: 2, i:   956]  train_loss: 0.916  |  valid_loss: 0.739\n",
      "[epoch: 2, i:  1043]  train_loss: 0.901  |  valid_loss: 0.901\n",
      "[epoch: 2, i:  1130]  train_loss: 0.908  |  valid_loss: 0.849\n",
      "[epoch: 2, i:  1217]  train_loss: 0.947  |  valid_loss: 0.924\n",
      "[epoch: 2, i:  1304]  train_loss: 0.953  |  valid_loss: 0.705\n",
      "[epoch: 2, i:  1391]  train_loss: 0.928  |  valid_loss: 0.913\n",
      "[epoch: 2, i:  1478]  train_loss: 0.913  |  valid_loss: 0.851\n",
      "[epoch: 2, i:  1565]  train_loss: 0.956  |  valid_loss: 0.801\n",
      "[epoch: 2, i:  1652]  train_loss: 0.812  |  valid_loss: 0.849\n",
      "[epoch: 2, i:  1739]  train_loss: 0.868  |  valid_loss: 1.006\n",
      "[epoch: 2, i:  1826]  train_loss: 0.855  |  valid_loss: 0.988\n",
      "[epoch: 2, i:  1913]  train_loss: 0.891  |  valid_loss: 0.867\n",
      "[epoch: 2, i:  2000]  train_loss: 0.906  |  valid_loss: 0.829\n",
      "[epoch: 2, i:  2087]  train_loss: 0.847  |  valid_loss: 0.953\n",
      "[epoch: 2, i:  2174]  train_loss: 0.869  |  valid_loss: 0.722\n",
      "[epoch: 2, i:  2261]  train_loss: 0.870  |  valid_loss: 1.041\n",
      "[epoch: 2, i:  2348]  train_loss: 0.934  |  valid_loss: 0.809\n",
      "[epoch: 2, i:  2435]  train_loss: 0.900  |  valid_loss: 1.007\n",
      "[epoch: 2, i:  2522]  train_loss: 0.914  |  valid_loss: 0.793\n",
      "[epoch: 2, i:  2609]  train_loss: 0.868  |  valid_loss: 0.704\n",
      "[epoch: 2, i:  2696]  train_loss: 0.899  |  valid_loss: 0.823\n",
      "[epoch: 2, i:  2783]  train_loss: 0.918  |  valid_loss: 0.693\n",
      "[epoch: 2, i:  2870]  train_loss: 0.810  |  valid_loss: 0.891\n",
      "[epoch: 2, i:  2957]  train_loss: 0.925  |  valid_loss: 0.968\n",
      "[epoch: 2, i:  3044]  train_loss: 0.815  |  valid_loss: 0.917\n",
      "[epoch: 2, i:  3131]  train_loss: 0.909  |  valid_loss: 0.886\n",
      "[epoch: 2, i:  3218]  train_loss: 0.961  |  valid_loss: 0.890\n",
      "[epoch: 2, i:  3305]  train_loss: 0.906  |  valid_loss: 1.056\n",
      "[epoch: 2, i:  3392]  train_loss: 0.917  |  valid_loss: 0.738\n",
      "[epoch: 2, i:  3479]  train_loss: 0.918  |  valid_loss: 0.716\n",
      "[epoch: 2, i:  3566]  train_loss: 0.890  |  valid_loss: 0.789\n",
      "[epoch: 2, i:  3653]  train_loss: 0.903  |  valid_loss: 0.815\n",
      "[epoch: 2, i:  3740]  train_loss: 0.837  |  valid_loss: 0.589\n",
      "[epoch: 2, i:  3827]  train_loss: 0.807  |  valid_loss: 0.939\n",
      "[epoch: 2, i:  3914]  train_loss: 0.953  |  valid_loss: 0.761\n",
      "[epoch: 2, i:  4001]  train_loss: 0.944  |  valid_loss: 0.801\n",
      "[epoch: 2, i:  4088]  train_loss: 0.863  |  valid_loss: 0.904\n",
      "[epoch: 2, i:  4175]  train_loss: 0.899  |  valid_loss: 0.832\n",
      "[epoch: 2, i:  4262]  train_loss: 0.859  |  valid_loss: 0.752\n",
      "[epoch: 2, i:  4349]  train_loss: 0.792  |  valid_loss: 0.762\n",
      "[epoch: 2, i:  4436]  train_loss: 0.841  |  valid_loss: 0.683\n",
      "[epoch: 2, i:  4523]  train_loss: 0.871  |  valid_loss: 0.887\n",
      "[epoch: 2, i:  4610]  train_loss: 0.870  |  valid_loss: 0.737\n",
      "[epoch: 2, i:  4697]  train_loss: 0.929  |  valid_loss: 0.768\n",
      "[epoch: 2, i:  4784]  train_loss: 0.908  |  valid_loss: 0.857\n",
      "[epoch: 2, i:  4871]  train_loss: 0.760  |  valid_loss: 0.958\n",
      "[epoch: 2, i:  4958]  train_loss: 0.830  |  valid_loss: 0.937\n",
      "[epoch: 2, i:  5045]  train_loss: 0.935  |  valid_loss: 0.554\n",
      "[epoch: 2, i:  5132]  train_loss: 0.883  |  valid_loss: 0.840\n",
      "[epoch: 2, i:  5219]  train_loss: 0.835  |  valid_loss: 0.987\n",
      "[epoch: 2, i:  5306]  train_loss: 0.896  |  valid_loss: 0.856\n",
      "[epoch: 2, i:  5393]  train_loss: 0.991  |  valid_loss: 0.745\n",
      "[epoch: 2, i:  5480]  train_loss: 0.912  |  valid_loss: 0.818\n",
      "[epoch: 2, i:  5567]  train_loss: 0.890  |  valid_loss: 0.651\n",
      "[epoch: 2, i:  5654]  train_loss: 0.941  |  valid_loss: 0.855\n",
      "[epoch: 2, i:  5741]  train_loss: 0.809  |  valid_loss: 0.760\n",
      "[epoch: 2, i:  5828]  train_loss: 0.911  |  valid_loss: 0.810\n",
      "[epoch: 2, i:  5915]  train_loss: 0.854  |  valid_loss: 0.969\n",
      "[epoch: 2, i:  6002]  train_loss: 0.881  |  valid_loss: 0.730\n",
      "[epoch: 2, i:  6089]  train_loss: 0.837  |  valid_loss: 1.008\n",
      "[epoch: 2, i:  6176]  train_loss: 0.868  |  valid_loss: 1.004\n",
      "[epoch: 2, i:  6263]  train_loss: 0.824  |  valid_loss: 0.751\n",
      "[epoch: 2, i:  6350]  train_loss: 0.834  |  valid_loss: 0.795\n",
      "[epoch: 2, i:  6437]  train_loss: 0.915  |  valid_loss: 0.656\n",
      "[epoch: 2, i:  6524]  train_loss: 0.941  |  valid_loss: 0.923\n",
      "[epoch: 2, i:  6611]  train_loss: 0.877  |  valid_loss: 0.558\n",
      "[epoch: 2, i:  6698]  train_loss: 0.856  |  valid_loss: 0.865\n",
      "[epoch: 2, i:  6785]  train_loss: 0.865  |  valid_loss: 0.648\n",
      "[epoch: 2, i:  6872]  train_loss: 0.849  |  valid_loss: 0.972\n",
      "[epoch: 2, i:  6959]  train_loss: 0.877  |  valid_loss: 0.928\n",
      "[epoch: 2, i:  7046]  train_loss: 0.806  |  valid_loss: 0.846\n",
      "[epoch: 2, i:  7133]  train_loss: 0.830  |  valid_loss: 0.712\n",
      "[epoch: 2, i:  7220]  train_loss: 0.894  |  valid_loss: 0.774\n",
      "[epoch: 2, i:  7307]  train_loss: 0.823  |  valid_loss: 0.965\n",
      "[epoch: 2, i:  7394]  train_loss: 0.829  |  valid_loss: 0.803\n",
      "[epoch: 2, i:  7481]  train_loss: 0.890  |  valid_loss: 0.829\n",
      "[epoch: 2, i:  7568]  train_loss: 0.795  |  valid_loss: 0.867\n",
      "[epoch: 2, i:  7655]  train_loss: 0.866  |  valid_loss: 0.816\n",
      "[epoch: 2, i:  7742]  train_loss: 0.832  |  valid_loss: 0.883\n",
      "[epoch: 2, i:  7829]  train_loss: 0.848  |  valid_loss: 0.689\n",
      "[epoch: 2, i:  7916]  train_loss: 0.890  |  valid_loss: 0.838\n",
      "[epoch: 2, i:  8003]  train_loss: 0.926  |  valid_loss: 0.740\n",
      "[epoch: 2, i:  8090]  train_loss: 0.901  |  valid_loss: 0.591\n",
      "[epoch: 2, i:  8177]  train_loss: 0.831  |  valid_loss: 0.816\n",
      "[epoch: 2, i:  8264]  train_loss: 0.869  |  valid_loss: 0.896\n",
      "[epoch: 2, i:  8351]  train_loss: 0.803  |  valid_loss: 0.935\n",
      "[epoch: 2, i:  8438]  train_loss: 0.862  |  valid_loss: 0.739\n",
      "[epoch: 2, i:  8525]  train_loss: 0.807  |  valid_loss: 0.809\n",
      "[epoch: 2, i:  8612]  train_loss: 0.826  |  valid_loss: 0.989\n",
      "[epoch: 2, i:  8699]  train_loss: 0.852  |  valid_loss: 0.831\n",
      "--> [End of epoch 2] train_accuracy: 69.27%  |  valid_accuracy: 71.20%\n",
      "--> [Start of epoch 3]  lr: 0.000500\n",
      "[epoch: 3, i:    86]  train_loss: 0.844  |  valid_loss: 0.678\n",
      "[epoch: 3, i:   173]  train_loss: 0.813  |  valid_loss: 0.669\n",
      "[epoch: 3, i:   260]  train_loss: 0.824  |  valid_loss: 0.974\n",
      "[epoch: 3, i:   347]  train_loss: 0.839  |  valid_loss: 0.672\n",
      "[epoch: 3, i:   434]  train_loss: 0.773  |  valid_loss: 0.848\n",
      "[epoch: 3, i:   521]  train_loss: 0.832  |  valid_loss: 0.570\n",
      "[epoch: 3, i:   608]  train_loss: 0.841  |  valid_loss: 0.840\n",
      "[epoch: 3, i:   695]  train_loss: 0.769  |  valid_loss: 0.773\n",
      "[epoch: 3, i:   782]  train_loss: 0.878  |  valid_loss: 0.756\n",
      "[epoch: 3, i:   869]  train_loss: 0.864  |  valid_loss: 0.753\n",
      "[epoch: 3, i:   956]  train_loss: 0.929  |  valid_loss: 0.602\n",
      "[epoch: 3, i:  1043]  train_loss: 0.854  |  valid_loss: 0.777\n",
      "[epoch: 3, i:  1130]  train_loss: 0.854  |  valid_loss: 0.740\n",
      "[epoch: 3, i:  1217]  train_loss: 0.767  |  valid_loss: 0.715\n",
      "[epoch: 3, i:  1304]  train_loss: 0.744  |  valid_loss: 0.601\n",
      "[epoch: 3, i:  1391]  train_loss: 0.813  |  valid_loss: 0.915\n",
      "[epoch: 3, i:  1478]  train_loss: 0.764  |  valid_loss: 0.723\n",
      "[epoch: 3, i:  1565]  train_loss: 0.825  |  valid_loss: 0.777\n",
      "[epoch: 3, i:  1652]  train_loss: 0.825  |  valid_loss: 0.798\n",
      "[epoch: 3, i:  1739]  train_loss: 0.768  |  valid_loss: 0.989\n",
      "[epoch: 3, i:  1826]  train_loss: 0.873  |  valid_loss: 0.920\n",
      "[epoch: 3, i:  1913]  train_loss: 0.849  |  valid_loss: 0.866\n",
      "[epoch: 3, i:  2000]  train_loss: 0.841  |  valid_loss: 0.704\n",
      "[epoch: 3, i:  2087]  train_loss: 0.831  |  valid_loss: 0.893\n",
      "[epoch: 3, i:  2174]  train_loss: 0.861  |  valid_loss: 0.644\n",
      "[epoch: 3, i:  2261]  train_loss: 0.834  |  valid_loss: 1.034\n",
      "[epoch: 3, i:  2348]  train_loss: 0.762  |  valid_loss: 0.755\n",
      "[epoch: 3, i:  2435]  train_loss: 0.871  |  valid_loss: 0.755\n",
      "[epoch: 3, i:  2522]  train_loss: 0.835  |  valid_loss: 0.697\n",
      "[epoch: 3, i:  2609]  train_loss: 0.878  |  valid_loss: 0.726\n",
      "[epoch: 3, i:  2696]  train_loss: 0.848  |  valid_loss: 0.884\n",
      "[epoch: 3, i:  2783]  train_loss: 0.908  |  valid_loss: 0.569\n",
      "[epoch: 3, i:  2870]  train_loss: 0.844  |  valid_loss: 0.833\n",
      "[epoch: 3, i:  2957]  train_loss: 0.807  |  valid_loss: 0.973\n",
      "[epoch: 3, i:  3044]  train_loss: 0.889  |  valid_loss: 0.875\n",
      "[epoch: 3, i:  3131]  train_loss: 0.793  |  valid_loss: 0.821\n",
      "[epoch: 3, i:  3218]  train_loss: 0.841  |  valid_loss: 0.778\n",
      "[epoch: 3, i:  3305]  train_loss: 0.822  |  valid_loss: 0.904\n",
      "[epoch: 3, i:  3392]  train_loss: 0.839  |  valid_loss: 0.721\n",
      "[epoch: 3, i:  3479]  train_loss: 0.875  |  valid_loss: 0.755\n",
      "[epoch: 3, i:  3566]  train_loss: 0.791  |  valid_loss: 0.689\n",
      "[epoch: 3, i:  3653]  train_loss: 0.823  |  valid_loss: 0.836\n",
      "[epoch: 3, i:  3740]  train_loss: 0.796  |  valid_loss: 0.536\n",
      "[epoch: 3, i:  3827]  train_loss: 0.802  |  valid_loss: 0.811\n",
      "[epoch: 3, i:  3914]  train_loss: 0.867  |  valid_loss: 0.732\n",
      "[epoch: 3, i:  4001]  train_loss: 0.796  |  valid_loss: 0.731\n",
      "[epoch: 3, i:  4088]  train_loss: 0.801  |  valid_loss: 0.804\n",
      "[epoch: 3, i:  4175]  train_loss: 0.849  |  valid_loss: 0.809\n",
      "[epoch: 3, i:  4262]  train_loss: 0.734  |  valid_loss: 0.665\n",
      "[epoch: 3, i:  4349]  train_loss: 0.804  |  valid_loss: 0.732\n",
      "[epoch: 3, i:  4436]  train_loss: 0.779  |  valid_loss: 0.725\n",
      "[epoch: 3, i:  4523]  train_loss: 0.801  |  valid_loss: 0.923\n",
      "[epoch: 3, i:  4610]  train_loss: 0.823  |  valid_loss: 0.613\n",
      "[epoch: 3, i:  4697]  train_loss: 0.934  |  valid_loss: 0.822\n",
      "[epoch: 3, i:  4784]  train_loss: 0.883  |  valid_loss: 0.739\n",
      "[epoch: 3, i:  4871]  train_loss: 0.862  |  valid_loss: 0.877\n",
      "[epoch: 3, i:  4958]  train_loss: 0.850  |  valid_loss: 0.930\n",
      "[epoch: 3, i:  5045]  train_loss: 0.816  |  valid_loss: 0.563\n",
      "[epoch: 3, i:  5132]  train_loss: 0.796  |  valid_loss: 0.783\n",
      "[epoch: 3, i:  5219]  train_loss: 0.826  |  valid_loss: 0.840\n",
      "[epoch: 3, i:  5306]  train_loss: 0.797  |  valid_loss: 0.754\n",
      "[epoch: 3, i:  5393]  train_loss: 0.808  |  valid_loss: 0.711\n",
      "[epoch: 3, i:  5480]  train_loss: 0.849  |  valid_loss: 0.752\n",
      "[epoch: 3, i:  5567]  train_loss: 0.840  |  valid_loss: 0.574\n",
      "[epoch: 3, i:  5654]  train_loss: 0.814  |  valid_loss: 0.860\n",
      "[epoch: 3, i:  5741]  train_loss: 0.750  |  valid_loss: 0.775\n",
      "[epoch: 3, i:  5828]  train_loss: 0.782  |  valid_loss: 0.808\n",
      "[epoch: 3, i:  5915]  train_loss: 0.828  |  valid_loss: 0.894\n",
      "[epoch: 3, i:  6002]  train_loss: 0.849  |  valid_loss: 0.735\n",
      "[epoch: 3, i:  6089]  train_loss: 0.881  |  valid_loss: 0.925\n",
      "[epoch: 3, i:  6176]  train_loss: 0.801  |  valid_loss: 0.849\n",
      "[epoch: 3, i:  6263]  train_loss: 0.880  |  valid_loss: 0.703\n",
      "[epoch: 3, i:  6350]  train_loss: 0.837  |  valid_loss: 0.784\n",
      "[epoch: 3, i:  6437]  train_loss: 0.841  |  valid_loss: 0.598\n",
      "[epoch: 3, i:  6524]  train_loss: 0.741  |  valid_loss: 0.935\n",
      "[epoch: 3, i:  6611]  train_loss: 0.812  |  valid_loss: 0.576\n",
      "[epoch: 3, i:  6698]  train_loss: 0.794  |  valid_loss: 0.809\n",
      "[epoch: 3, i:  6785]  train_loss: 0.770  |  valid_loss: 0.636\n",
      "[epoch: 3, i:  6872]  train_loss: 0.836  |  valid_loss: 0.856\n",
      "[epoch: 3, i:  6959]  train_loss: 0.774  |  valid_loss: 0.912\n",
      "[epoch: 3, i:  7046]  train_loss: 0.834  |  valid_loss: 0.727\n",
      "[epoch: 3, i:  7133]  train_loss: 0.778  |  valid_loss: 0.725\n",
      "[epoch: 3, i:  7220]  train_loss: 0.808  |  valid_loss: 0.724\n",
      "[epoch: 3, i:  7307]  train_loss: 0.903  |  valid_loss: 0.862\n",
      "[epoch: 3, i:  7394]  train_loss: 0.790  |  valid_loss: 0.688\n",
      "[epoch: 3, i:  7481]  train_loss: 0.787  |  valid_loss: 0.723\n",
      "[epoch: 3, i:  7568]  train_loss: 0.815  |  valid_loss: 0.862\n",
      "[epoch: 3, i:  7655]  train_loss: 0.773  |  valid_loss: 0.757\n",
      "[epoch: 3, i:  7742]  train_loss: 0.873  |  valid_loss: 0.805\n",
      "[epoch: 3, i:  7829]  train_loss: 0.883  |  valid_loss: 0.642\n",
      "[epoch: 3, i:  7916]  train_loss: 0.833  |  valid_loss: 0.896\n",
      "[epoch: 3, i:  8003]  train_loss: 0.874  |  valid_loss: 0.658\n",
      "[epoch: 3, i:  8090]  train_loss: 0.800  |  valid_loss: 0.534\n",
      "[epoch: 3, i:  8177]  train_loss: 0.826  |  valid_loss: 0.852\n",
      "[epoch: 3, i:  8264]  train_loss: 0.860  |  valid_loss: 0.834\n",
      "[epoch: 3, i:  8351]  train_loss: 0.802  |  valid_loss: 0.879\n",
      "[epoch: 3, i:  8438]  train_loss: 0.678  |  valid_loss: 0.637\n",
      "[epoch: 3, i:  8525]  train_loss: 0.858  |  valid_loss: 0.751\n",
      "[epoch: 3, i:  8612]  train_loss: 0.828  |  valid_loss: 0.892\n",
      "[epoch: 3, i:  8699]  train_loss: 0.872  |  valid_loss: 0.786\n",
      "--> [End of epoch 3] train_accuracy: 71.08%  |  valid_accuracy: 73.34%\n",
      "--> [Start of epoch 4]  lr: 0.000500\n",
      "[epoch: 4, i:    86]  train_loss: 0.767  |  valid_loss: 0.675\n",
      "[epoch: 4, i:   173]  train_loss: 0.761  |  valid_loss: 0.643\n",
      "[epoch: 4, i:   260]  train_loss: 0.766  |  valid_loss: 0.891\n",
      "[epoch: 4, i:   347]  train_loss: 0.701  |  valid_loss: 0.669\n",
      "[epoch: 4, i:   434]  train_loss: 0.780  |  valid_loss: 0.804\n",
      "[epoch: 4, i:   521]  train_loss: 0.856  |  valid_loss: 0.567\n",
      "[epoch: 4, i:   608]  train_loss: 0.763  |  valid_loss: 0.728\n",
      "[epoch: 4, i:   695]  train_loss: 0.715  |  valid_loss: 0.693\n",
      "[epoch: 4, i:   782]  train_loss: 0.777  |  valid_loss: 0.765\n",
      "[epoch: 4, i:   869]  train_loss: 0.799  |  valid_loss: 0.774\n",
      "[epoch: 4, i:   956]  train_loss: 0.830  |  valid_loss: 0.625\n",
      "[epoch: 4, i:  1043]  train_loss: 0.786  |  valid_loss: 0.762\n",
      "[epoch: 4, i:  1130]  train_loss: 0.758  |  valid_loss: 0.725\n",
      "[epoch: 4, i:  1217]  train_loss: 0.896  |  valid_loss: 0.732\n",
      "[epoch: 4, i:  1304]  train_loss: 0.689  |  valid_loss: 0.614\n",
      "[epoch: 4, i:  1391]  train_loss: 0.750  |  valid_loss: 0.818\n",
      "[epoch: 4, i:  1478]  train_loss: 0.802  |  valid_loss: 0.807\n",
      "[epoch: 4, i:  1565]  train_loss: 0.784  |  valid_loss: 0.774\n",
      "[epoch: 4, i:  1652]  train_loss: 0.749  |  valid_loss: 0.724\n",
      "[epoch: 4, i:  1739]  train_loss: 0.804  |  valid_loss: 0.936\n",
      "[epoch: 4, i:  1826]  train_loss: 0.796  |  valid_loss: 0.933\n",
      "[epoch: 4, i:  1913]  train_loss: 0.847  |  valid_loss: 0.797\n",
      "[epoch: 4, i:  2000]  train_loss: 0.731  |  valid_loss: 0.688\n",
      "[epoch: 4, i:  2087]  train_loss: 0.816  |  valid_loss: 0.829\n",
      "[epoch: 4, i:  2174]  train_loss: 0.759  |  valid_loss: 0.694\n",
      "[epoch: 4, i:  2261]  train_loss: 0.721  |  valid_loss: 1.006\n",
      "[epoch: 4, i:  2348]  train_loss: 0.710  |  valid_loss: 0.783\n",
      "[epoch: 4, i:  2435]  train_loss: 0.785  |  valid_loss: 0.788\n",
      "[epoch: 4, i:  2522]  train_loss: 0.766  |  valid_loss: 0.661\n",
      "[epoch: 4, i:  2609]  train_loss: 0.795  |  valid_loss: 0.649\n",
      "[epoch: 4, i:  2696]  train_loss: 0.702  |  valid_loss: 0.786\n",
      "[epoch: 4, i:  2783]  train_loss: 0.811  |  valid_loss: 0.578\n",
      "[epoch: 4, i:  2870]  train_loss: 0.858  |  valid_loss: 0.823\n",
      "[epoch: 4, i:  2957]  train_loss: 0.815  |  valid_loss: 0.844\n",
      "[epoch: 4, i:  3044]  train_loss: 0.829  |  valid_loss: 0.926\n",
      "[epoch: 4, i:  3131]  train_loss: 0.814  |  valid_loss: 0.823\n",
      "[epoch: 4, i:  3218]  train_loss: 0.824  |  valid_loss: 0.761\n",
      "[epoch: 4, i:  3305]  train_loss: 0.796  |  valid_loss: 0.937\n",
      "[epoch: 4, i:  3392]  train_loss: 0.797  |  valid_loss: 0.624\n",
      "[epoch: 4, i:  3479]  train_loss: 0.752  |  valid_loss: 0.650\n",
      "[epoch: 4, i:  3566]  train_loss: 0.810  |  valid_loss: 0.687\n",
      "[epoch: 4, i:  3653]  train_loss: 0.804  |  valid_loss: 0.742\n",
      "[epoch: 4, i:  3740]  train_loss: 0.754  |  valid_loss: 0.492\n",
      "[epoch: 4, i:  3827]  train_loss: 0.752  |  valid_loss: 0.776\n",
      "[epoch: 4, i:  3914]  train_loss: 0.829  |  valid_loss: 0.643\n",
      "[epoch: 4, i:  4001]  train_loss: 0.779  |  valid_loss: 0.680\n",
      "[epoch: 4, i:  4088]  train_loss: 0.809  |  valid_loss: 0.752\n",
      "[epoch: 4, i:  4175]  train_loss: 0.760  |  valid_loss: 0.817\n",
      "[epoch: 4, i:  4262]  train_loss: 0.783  |  valid_loss: 0.639\n",
      "[epoch: 4, i:  4349]  train_loss: 0.789  |  valid_loss: 0.757\n",
      "[epoch: 4, i:  4436]  train_loss: 0.798  |  valid_loss: 0.744\n",
      "[epoch: 4, i:  4523]  train_loss: 0.735  |  valid_loss: 0.756\n",
      "[epoch: 4, i:  4610]  train_loss: 0.729  |  valid_loss: 0.584\n",
      "[epoch: 4, i:  4697]  train_loss: 0.803  |  valid_loss: 0.711\n",
      "[epoch: 4, i:  4784]  train_loss: 0.784  |  valid_loss: 0.695\n",
      "[epoch: 4, i:  4871]  train_loss: 0.784  |  valid_loss: 0.845\n",
      "[epoch: 4, i:  4958]  train_loss: 0.809  |  valid_loss: 0.862\n",
      "[epoch: 4, i:  5045]  train_loss: 0.797  |  valid_loss: 0.539\n",
      "[epoch: 4, i:  5132]  train_loss: 0.842  |  valid_loss: 0.732\n",
      "[epoch: 4, i:  5219]  train_loss: 0.815  |  valid_loss: 0.889\n",
      "[epoch: 4, i:  5306]  train_loss: 0.802  |  valid_loss: 0.760\n",
      "[epoch: 4, i:  5393]  train_loss: 0.858  |  valid_loss: 0.716\n",
      "[epoch: 4, i:  5480]  train_loss: 0.795  |  valid_loss: 0.731\n",
      "[epoch: 4, i:  5567]  train_loss: 0.796  |  valid_loss: 0.535\n",
      "[epoch: 4, i:  5654]  train_loss: 0.782  |  valid_loss: 0.774\n",
      "[epoch: 4, i:  5741]  train_loss: 0.755  |  valid_loss: 0.765\n",
      "[epoch: 4, i:  5828]  train_loss: 0.771  |  valid_loss: 0.767\n",
      "[epoch: 4, i:  5915]  train_loss: 0.878  |  valid_loss: 0.861\n",
      "[epoch: 4, i:  6002]  train_loss: 0.852  |  valid_loss: 0.651\n",
      "[epoch: 4, i:  6089]  train_loss: 0.763  |  valid_loss: 0.829\n",
      "[epoch: 4, i:  6176]  train_loss: 0.746  |  valid_loss: 0.897\n",
      "[epoch: 4, i:  6263]  train_loss: 0.796  |  valid_loss: 0.614\n",
      "[epoch: 4, i:  6350]  train_loss: 0.820  |  valid_loss: 0.780\n",
      "[epoch: 4, i:  6437]  train_loss: 0.737  |  valid_loss: 0.549\n",
      "[epoch: 4, i:  6524]  train_loss: 0.808  |  valid_loss: 0.922\n",
      "[epoch: 4, i:  6611]  train_loss: 0.774  |  valid_loss: 0.581\n",
      "[epoch: 4, i:  6698]  train_loss: 0.745  |  valid_loss: 0.856\n",
      "[epoch: 4, i:  6785]  train_loss: 0.782  |  valid_loss: 0.567\n",
      "[epoch: 4, i:  6872]  train_loss: 0.810  |  valid_loss: 0.799\n",
      "[epoch: 4, i:  6959]  train_loss: 0.781  |  valid_loss: 0.905\n",
      "[epoch: 4, i:  7046]  train_loss: 0.697  |  valid_loss: 0.760\n",
      "[epoch: 4, i:  7133]  train_loss: 0.729  |  valid_loss: 0.614\n",
      "[epoch: 4, i:  7220]  train_loss: 0.811  |  valid_loss: 0.664\n",
      "[epoch: 4, i:  7307]  train_loss: 0.752  |  valid_loss: 0.812\n",
      "[epoch: 4, i:  7394]  train_loss: 0.745  |  valid_loss: 0.679\n",
      "[epoch: 4, i:  7481]  train_loss: 0.753  |  valid_loss: 0.781\n",
      "[epoch: 4, i:  7568]  train_loss: 0.771  |  valid_loss: 0.815\n",
      "[epoch: 4, i:  7655]  train_loss: 0.786  |  valid_loss: 0.772\n",
      "[epoch: 4, i:  7742]  train_loss: 0.760  |  valid_loss: 0.866\n",
      "[epoch: 4, i:  7829]  train_loss: 0.842  |  valid_loss: 0.683\n",
      "[epoch: 4, i:  7916]  train_loss: 0.799  |  valid_loss: 0.788\n",
      "[epoch: 4, i:  8003]  train_loss: 0.847  |  valid_loss: 0.650\n",
      "[epoch: 4, i:  8090]  train_loss: 0.769  |  valid_loss: 0.534\n",
      "[epoch: 4, i:  8177]  train_loss: 0.710  |  valid_loss: 0.794\n",
      "[epoch: 4, i:  8264]  train_loss: 0.763  |  valid_loss: 0.840\n",
      "[epoch: 4, i:  8351]  train_loss: 0.787  |  valid_loss: 0.937\n",
      "[epoch: 4, i:  8438]  train_loss: 0.768  |  valid_loss: 0.585\n",
      "[epoch: 4, i:  8525]  train_loss: 0.735  |  valid_loss: 0.724\n",
      "[epoch: 4, i:  8612]  train_loss: 0.790  |  valid_loss: 0.918\n",
      "[epoch: 4, i:  8699]  train_loss: 0.742  |  valid_loss: 0.735\n",
      "--> [End of epoch 4] train_accuracy: 72.58%  |  valid_accuracy: 74.40%\n",
      "--> [Start of epoch 5]  lr: 0.000500\n",
      "[epoch: 5, i:    86]  train_loss: 0.723  |  valid_loss: 0.635\n",
      "[epoch: 5, i:   173]  train_loss: 0.711  |  valid_loss: 0.642\n",
      "[epoch: 5, i:   260]  train_loss: 0.764  |  valid_loss: 0.817\n",
      "[epoch: 5, i:   347]  train_loss: 0.721  |  valid_loss: 0.595\n",
      "[epoch: 5, i:   434]  train_loss: 0.702  |  valid_loss: 0.801\n",
      "[epoch: 5, i:   521]  train_loss: 0.713  |  valid_loss: 0.458\n",
      "[epoch: 5, i:   608]  train_loss: 0.687  |  valid_loss: 0.719\n",
      "[epoch: 5, i:   695]  train_loss: 0.717  |  valid_loss: 0.725\n",
      "[epoch: 5, i:   782]  train_loss: 0.719  |  valid_loss: 0.811\n",
      "[epoch: 5, i:   869]  train_loss: 0.781  |  valid_loss: 0.777\n",
      "[epoch: 5, i:   956]  train_loss: 0.777  |  valid_loss: 0.608\n",
      "[epoch: 5, i:  1043]  train_loss: 0.735  |  valid_loss: 0.637\n",
      "[epoch: 5, i:  1130]  train_loss: 0.766  |  valid_loss: 0.639\n",
      "[epoch: 5, i:  1217]  train_loss: 0.751  |  valid_loss: 0.644\n",
      "[epoch: 5, i:  1304]  train_loss: 0.775  |  valid_loss: 0.569\n",
      "[epoch: 5, i:  1391]  train_loss: 0.786  |  valid_loss: 0.755\n",
      "[epoch: 5, i:  1478]  train_loss: 0.753  |  valid_loss: 0.759\n",
      "[epoch: 5, i:  1565]  train_loss: 0.704  |  valid_loss: 0.790\n",
      "[epoch: 5, i:  1652]  train_loss: 0.765  |  valid_loss: 0.692\n",
      "[epoch: 5, i:  1739]  train_loss: 0.732  |  valid_loss: 0.914\n",
      "[epoch: 5, i:  1826]  train_loss: 0.714  |  valid_loss: 0.899\n",
      "[epoch: 5, i:  1913]  train_loss: 0.658  |  valid_loss: 0.682\n",
      "[epoch: 5, i:  2000]  train_loss: 0.724  |  valid_loss: 0.672\n",
      "[epoch: 5, i:  2087]  train_loss: 0.773  |  valid_loss: 0.788\n",
      "[epoch: 5, i:  2174]  train_loss: 0.711  |  valid_loss: 0.633\n",
      "[epoch: 5, i:  2261]  train_loss: 0.716  |  valid_loss: 0.990\n",
      "[epoch: 5, i:  2348]  train_loss: 0.755  |  valid_loss: 0.698\n",
      "[epoch: 5, i:  2435]  train_loss: 0.794  |  valid_loss: 0.857\n",
      "[epoch: 5, i:  2522]  train_loss: 0.804  |  valid_loss: 0.691\n",
      "[epoch: 5, i:  2609]  train_loss: 0.760  |  valid_loss: 0.622\n",
      "[epoch: 5, i:  2696]  train_loss: 0.779  |  valid_loss: 0.786\n",
      "[epoch: 5, i:  2783]  train_loss: 0.831  |  valid_loss: 0.543\n",
      "[epoch: 5, i:  2870]  train_loss: 0.782  |  valid_loss: 0.824\n",
      "[epoch: 5, i:  2957]  train_loss: 0.766  |  valid_loss: 0.838\n",
      "[epoch: 5, i:  3044]  train_loss: 0.732  |  valid_loss: 0.862\n",
      "[epoch: 5, i:  3131]  train_loss: 0.785  |  valid_loss: 0.804\n",
      "[epoch: 5, i:  3218]  train_loss: 0.830  |  valid_loss: 0.791\n",
      "[epoch: 5, i:  3305]  train_loss: 0.759  |  valid_loss: 0.905\n",
      "[epoch: 5, i:  3392]  train_loss: 0.769  |  valid_loss: 0.749\n",
      "[epoch: 5, i:  3479]  train_loss: 0.779  |  valid_loss: 0.622\n",
      "[epoch: 5, i:  3566]  train_loss: 0.809  |  valid_loss: 0.615\n",
      "[epoch: 5, i:  3653]  train_loss: 0.736  |  valid_loss: 0.725\n",
      "[epoch: 5, i:  3740]  train_loss: 0.791  |  valid_loss: 0.491\n",
      "[epoch: 5, i:  3827]  train_loss: 0.769  |  valid_loss: 0.752\n",
      "[epoch: 5, i:  3914]  train_loss: 0.790  |  valid_loss: 0.649\n",
      "[epoch: 5, i:  4001]  train_loss: 0.757  |  valid_loss: 0.740\n",
      "[epoch: 5, i:  4088]  train_loss: 0.752  |  valid_loss: 0.693\n",
      "[epoch: 5, i:  4175]  train_loss: 0.734  |  valid_loss: 0.692\n",
      "[epoch: 5, i:  4262]  train_loss: 0.777  |  valid_loss: 0.660\n",
      "[epoch: 5, i:  4349]  train_loss: 0.754  |  valid_loss: 0.761\n",
      "[epoch: 5, i:  4436]  train_loss: 0.811  |  valid_loss: 0.702\n",
      "[epoch: 5, i:  4523]  train_loss: 0.795  |  valid_loss: 0.755\n",
      "[epoch: 5, i:  4610]  train_loss: 0.824  |  valid_loss: 0.628\n",
      "[epoch: 5, i:  4697]  train_loss: 0.771  |  valid_loss: 0.703\n",
      "[epoch: 5, i:  4784]  train_loss: 0.850  |  valid_loss: 0.729\n",
      "[epoch: 5, i:  4871]  train_loss: 0.822  |  valid_loss: 0.869\n",
      "[epoch: 5, i:  4958]  train_loss: 0.696  |  valid_loss: 0.823\n",
      "[epoch: 5, i:  5045]  train_loss: 0.763  |  valid_loss: 0.532\n",
      "[epoch: 5, i:  5132]  train_loss: 0.756  |  valid_loss: 0.742\n",
      "[epoch: 5, i:  5219]  train_loss: 0.766  |  valid_loss: 0.918\n",
      "[epoch: 5, i:  5306]  train_loss: 0.798  |  valid_loss: 0.827\n",
      "[epoch: 5, i:  5393]  train_loss: 0.776  |  valid_loss: 0.721\n",
      "[epoch: 5, i:  5480]  train_loss: 0.781  |  valid_loss: 0.764\n",
      "[epoch: 5, i:  5567]  train_loss: 0.697  |  valid_loss: 0.512\n",
      "[epoch: 5, i:  5654]  train_loss: 0.749  |  valid_loss: 0.694\n",
      "[epoch: 5, i:  5741]  train_loss: 0.696  |  valid_loss: 0.743\n",
      "[epoch: 5, i:  5828]  train_loss: 0.712  |  valid_loss: 0.838\n",
      "[epoch: 5, i:  5915]  train_loss: 0.781  |  valid_loss: 0.831\n",
      "[epoch: 5, i:  6002]  train_loss: 0.723  |  valid_loss: 0.653\n",
      "[epoch: 5, i:  6089]  train_loss: 0.778  |  valid_loss: 0.881\n",
      "[epoch: 5, i:  6176]  train_loss: 0.752  |  valid_loss: 0.871\n",
      "[epoch: 5, i:  6263]  train_loss: 0.792  |  valid_loss: 0.664\n",
      "[epoch: 5, i:  6350]  train_loss: 0.757  |  valid_loss: 0.768\n",
      "[epoch: 5, i:  6437]  train_loss: 0.696  |  valid_loss: 0.538\n",
      "[epoch: 5, i:  6524]  train_loss: 0.729  |  valid_loss: 0.965\n",
      "[epoch: 5, i:  6611]  train_loss: 0.791  |  valid_loss: 0.490\n",
      "[epoch: 5, i:  6698]  train_loss: 0.844  |  valid_loss: 0.775\n",
      "[epoch: 5, i:  6785]  train_loss: 0.737  |  valid_loss: 0.577\n",
      "[epoch: 5, i:  6872]  train_loss: 0.806  |  valid_loss: 0.963\n",
      "[epoch: 5, i:  6959]  train_loss: 0.795  |  valid_loss: 0.849\n",
      "[epoch: 5, i:  7046]  train_loss: 0.753  |  valid_loss: 0.719\n",
      "[epoch: 5, i:  7133]  train_loss: 0.806  |  valid_loss: 0.605\n",
      "[epoch: 5, i:  7220]  train_loss: 0.776  |  valid_loss: 0.772\n",
      "[epoch: 5, i:  7307]  train_loss: 0.786  |  valid_loss: 0.815\n",
      "[epoch: 5, i:  7394]  train_loss: 0.724  |  valid_loss: 0.672\n",
      "[epoch: 5, i:  7481]  train_loss: 0.724  |  valid_loss: 0.729\n",
      "[epoch: 5, i:  7568]  train_loss: 0.786  |  valid_loss: 0.813\n",
      "[epoch: 5, i:  7655]  train_loss: 0.716  |  valid_loss: 0.722\n",
      "[epoch: 5, i:  7742]  train_loss: 0.768  |  valid_loss: 0.784\n",
      "[epoch: 5, i:  7829]  train_loss: 0.727  |  valid_loss: 0.529\n",
      "[epoch: 5, i:  7916]  train_loss: 0.666  |  valid_loss: 0.834\n",
      "[epoch: 5, i:  8003]  train_loss: 0.762  |  valid_loss: 0.690\n",
      "[epoch: 5, i:  8090]  train_loss: 0.772  |  valid_loss: 0.515\n",
      "[epoch: 5, i:  8177]  train_loss: 0.751  |  valid_loss: 0.719\n",
      "[epoch: 5, i:  8264]  train_loss: 0.692  |  valid_loss: 0.846\n",
      "[epoch: 5, i:  8351]  train_loss: 0.788  |  valid_loss: 0.817\n",
      "[epoch: 5, i:  8438]  train_loss: 0.676  |  valid_loss: 0.575\n",
      "[epoch: 5, i:  8525]  train_loss: 0.775  |  valid_loss: 0.668\n",
      "[epoch: 5, i:  8612]  train_loss: 0.706  |  valid_loss: 0.954\n",
      "[epoch: 5, i:  8699]  train_loss: 0.837  |  valid_loss: 0.800\n",
      "--> [End of epoch 5] train_accuracy: 73.75%  |  valid_accuracy: 75.00%\n",
      "--> [Start of epoch 6]  lr: 0.000500\n",
      "[epoch: 6, i:    86]  train_loss: 0.768  |  valid_loss: 0.676\n",
      "[epoch: 6, i:   173]  train_loss: 0.712  |  valid_loss: 0.639\n",
      "[epoch: 6, i:   260]  train_loss: 0.712  |  valid_loss: 0.834\n",
      "[epoch: 6, i:   347]  train_loss: 0.770  |  valid_loss: 0.654\n",
      "[epoch: 6, i:   434]  train_loss: 0.798  |  valid_loss: 0.752\n",
      "[epoch: 6, i:   521]  train_loss: 0.660  |  valid_loss: 0.470\n",
      "[epoch: 6, i:   608]  train_loss: 0.724  |  valid_loss: 0.753\n",
      "[epoch: 6, i:   695]  train_loss: 0.759  |  valid_loss: 0.747\n",
      "[epoch: 6, i:   782]  train_loss: 0.688  |  valid_loss: 0.789\n",
      "[epoch: 6, i:   869]  train_loss: 0.664  |  valid_loss: 0.800\n",
      "[epoch: 6, i:   956]  train_loss: 0.735  |  valid_loss: 0.643\n",
      "[epoch: 6, i:  1043]  train_loss: 0.755  |  valid_loss: 0.683\n",
      "[epoch: 6, i:  1130]  train_loss: 0.692  |  valid_loss: 0.653\n",
      "[epoch: 6, i:  1217]  train_loss: 0.785  |  valid_loss: 0.668\n",
      "[epoch: 6, i:  1304]  train_loss: 0.680  |  valid_loss: 0.514\n",
      "[epoch: 6, i:  1391]  train_loss: 0.730  |  valid_loss: 0.737\n",
      "[epoch: 6, i:  1478]  train_loss: 0.731  |  valid_loss: 0.653\n",
      "[epoch: 6, i:  1565]  train_loss: 0.755  |  valid_loss: 0.723\n",
      "[epoch: 6, i:  1652]  train_loss: 0.716  |  valid_loss: 0.734\n",
      "[epoch: 6, i:  1739]  train_loss: 0.708  |  valid_loss: 0.874\n",
      "[epoch: 6, i:  1826]  train_loss: 0.729  |  valid_loss: 0.875\n",
      "[epoch: 6, i:  1913]  train_loss: 0.730  |  valid_loss: 0.720\n",
      "[epoch: 6, i:  2000]  train_loss: 0.736  |  valid_loss: 0.704\n",
      "[epoch: 6, i:  2087]  train_loss: 0.694  |  valid_loss: 0.806\n",
      "[epoch: 6, i:  2174]  train_loss: 0.759  |  valid_loss: 0.638\n",
      "[epoch: 6, i:  2261]  train_loss: 0.802  |  valid_loss: 0.970\n",
      "[epoch: 6, i:  2348]  train_loss: 0.726  |  valid_loss: 0.614\n",
      "[epoch: 6, i:  2435]  train_loss: 0.721  |  valid_loss: 0.815\n",
      "[epoch: 6, i:  2522]  train_loss: 0.707  |  valid_loss: 0.594\n",
      "[epoch: 6, i:  2609]  train_loss: 0.735  |  valid_loss: 0.703\n",
      "[epoch: 6, i:  2696]  train_loss: 0.804  |  valid_loss: 0.824\n",
      "[epoch: 6, i:  2783]  train_loss: 0.734  |  valid_loss: 0.580\n",
      "[epoch: 6, i:  2870]  train_loss: 0.742  |  valid_loss: 0.795\n",
      "[epoch: 6, i:  2957]  train_loss: 0.764  |  valid_loss: 0.778\n",
      "[epoch: 6, i:  3044]  train_loss: 0.722  |  valid_loss: 0.792\n",
      "[epoch: 6, i:  3131]  train_loss: 0.735  |  valid_loss: 0.809\n",
      "[epoch: 6, i:  3218]  train_loss: 0.726  |  valid_loss: 0.740\n",
      "[epoch: 6, i:  3305]  train_loss: 0.734  |  valid_loss: 0.857\n",
      "[epoch: 6, i:  3392]  train_loss: 0.715  |  valid_loss: 0.667\n",
      "[epoch: 6, i:  3479]  train_loss: 0.757  |  valid_loss: 0.604\n",
      "[epoch: 6, i:  3566]  train_loss: 0.670  |  valid_loss: 0.611\n",
      "[epoch: 6, i:  3653]  train_loss: 0.713  |  valid_loss: 0.770\n",
      "[epoch: 6, i:  3740]  train_loss: 0.748  |  valid_loss: 0.497\n",
      "[epoch: 6, i:  3827]  train_loss: 0.747  |  valid_loss: 0.810\n",
      "[epoch: 6, i:  3914]  train_loss: 0.769  |  valid_loss: 0.602\n",
      "[epoch: 6, i:  4001]  train_loss: 0.692  |  valid_loss: 0.703\n",
      "[epoch: 6, i:  4088]  train_loss: 0.705  |  valid_loss: 0.748\n",
      "[epoch: 6, i:  4175]  train_loss: 0.738  |  valid_loss: 0.698\n",
      "[epoch: 6, i:  4262]  train_loss: 0.704  |  valid_loss: 0.645\n",
      "[epoch: 6, i:  4349]  train_loss: 0.761  |  valid_loss: 0.730\n",
      "[epoch: 6, i:  4436]  train_loss: 0.766  |  valid_loss: 0.630\n",
      "[epoch: 6, i:  4523]  train_loss: 0.656  |  valid_loss: 0.799\n",
      "[epoch: 6, i:  4610]  train_loss: 0.699  |  valid_loss: 0.601\n",
      "[epoch: 6, i:  4697]  train_loss: 0.685  |  valid_loss: 0.725\n",
      "[epoch: 6, i:  4784]  train_loss: 0.718  |  valid_loss: 0.741\n",
      "[epoch: 6, i:  4871]  train_loss: 0.763  |  valid_loss: 0.813\n",
      "[epoch: 6, i:  4958]  train_loss: 0.724  |  valid_loss: 0.886\n",
      "[epoch: 6, i:  5045]  train_loss: 0.714  |  valid_loss: 0.525\n",
      "[epoch: 6, i:  5132]  train_loss: 0.697  |  valid_loss: 0.641\n",
      "[epoch: 6, i:  5219]  train_loss: 0.718  |  valid_loss: 0.937\n",
      "[epoch: 6, i:  5306]  train_loss: 0.750  |  valid_loss: 0.790\n",
      "[epoch: 6, i:  5393]  train_loss: 0.705  |  valid_loss: 0.637\n",
      "[epoch: 6, i:  5480]  train_loss: 0.706  |  valid_loss: 0.774\n",
      "[epoch: 6, i:  5567]  train_loss: 0.760  |  valid_loss: 0.402\n",
      "[epoch: 6, i:  5654]  train_loss: 0.765  |  valid_loss: 0.747\n",
      "[epoch: 6, i:  5741]  train_loss: 0.701  |  valid_loss: 0.851\n",
      "[epoch: 6, i:  5828]  train_loss: 0.802  |  valid_loss: 0.652\n",
      "[epoch: 6, i:  5915]  train_loss: 0.697  |  valid_loss: 0.809\n",
      "[epoch: 6, i:  6002]  train_loss: 0.738  |  valid_loss: 0.646\n",
      "[epoch: 6, i:  6089]  train_loss: 0.762  |  valid_loss: 0.957\n",
      "[epoch: 6, i:  6176]  train_loss: 0.783  |  valid_loss: 0.863\n",
      "[epoch: 6, i:  6263]  train_loss: 0.731  |  valid_loss: 0.651\n",
      "[epoch: 6, i:  6350]  train_loss: 0.694  |  valid_loss: 0.714\n",
      "[epoch: 6, i:  6437]  train_loss: 0.766  |  valid_loss: 0.557\n",
      "[epoch: 6, i:  6524]  train_loss: 0.730  |  valid_loss: 0.918\n",
      "[epoch: 6, i:  6611]  train_loss: 0.688  |  valid_loss: 0.561\n",
      "[epoch: 6, i:  6698]  train_loss: 0.714  |  valid_loss: 0.755\n",
      "[epoch: 6, i:  6785]  train_loss: 0.747  |  valid_loss: 0.559\n",
      "[epoch: 6, i:  6872]  train_loss: 0.765  |  valid_loss: 0.833\n",
      "[epoch: 6, i:  6959]  train_loss: 0.732  |  valid_loss: 0.765\n",
      "[epoch: 6, i:  7046]  train_loss: 0.677  |  valid_loss: 0.647\n",
      "[epoch: 6, i:  7133]  train_loss: 0.758  |  valid_loss: 0.649\n",
      "[epoch: 6, i:  7220]  train_loss: 0.751  |  valid_loss: 0.683\n",
      "[epoch: 6, i:  7307]  train_loss: 0.752  |  valid_loss: 0.860\n",
      "[epoch: 6, i:  7394]  train_loss: 0.679  |  valid_loss: 0.681\n",
      "[epoch: 6, i:  7481]  train_loss: 0.725  |  valid_loss: 0.711\n",
      "[epoch: 6, i:  7568]  train_loss: 0.756  |  valid_loss: 0.817\n",
      "[epoch: 6, i:  7655]  train_loss: 0.830  |  valid_loss: 0.716\n",
      "[epoch: 6, i:  7742]  train_loss: 0.690  |  valid_loss: 0.703\n",
      "[epoch: 6, i:  7829]  train_loss: 0.759  |  valid_loss: 0.531\n",
      "[epoch: 6, i:  7916]  train_loss: 0.763  |  valid_loss: 0.792\n",
      "[epoch: 6, i:  8003]  train_loss: 0.872  |  valid_loss: 0.617\n",
      "[epoch: 6, i:  8090]  train_loss: 0.755  |  valid_loss: 0.541\n",
      "[epoch: 6, i:  8177]  train_loss: 0.762  |  valid_loss: 0.699\n",
      "[epoch: 6, i:  8264]  train_loss: 0.688  |  valid_loss: 0.766\n",
      "[epoch: 6, i:  8351]  train_loss: 0.773  |  valid_loss: 0.751\n",
      "[epoch: 6, i:  8438]  train_loss: 0.700  |  valid_loss: 0.568\n",
      "[epoch: 6, i:  8525]  train_loss: 0.786  |  valid_loss: 0.699\n",
      "[epoch: 6, i:  8612]  train_loss: 0.726  |  valid_loss: 0.930\n",
      "[epoch: 6, i:  8699]  train_loss: 0.789  |  valid_loss: 0.739\n",
      "--> [End of epoch 6] train_accuracy: 74.45%  |  valid_accuracy: 75.44%\n",
      "--> [Start of epoch 7]  lr: 0.000500\n",
      "[epoch: 7, i:    86]  train_loss: 0.636  |  valid_loss: 0.579\n",
      "[epoch: 7, i:   173]  train_loss: 0.696  |  valid_loss: 0.584\n",
      "[epoch: 7, i:   260]  train_loss: 0.790  |  valid_loss: 0.680\n",
      "[epoch: 7, i:   347]  train_loss: 0.712  |  valid_loss: 0.570\n",
      "[epoch: 7, i:   434]  train_loss: 0.720  |  valid_loss: 0.765\n",
      "[epoch: 7, i:   521]  train_loss: 0.682  |  valid_loss: 0.499\n",
      "[epoch: 7, i:   608]  train_loss: 0.736  |  valid_loss: 0.729\n",
      "[epoch: 7, i:   695]  train_loss: 0.661  |  valid_loss: 0.745\n",
      "[epoch: 7, i:   782]  train_loss: 0.654  |  valid_loss: 0.738\n",
      "[epoch: 7, i:   869]  train_loss: 0.697  |  valid_loss: 0.738\n",
      "[epoch: 7, i:   956]  train_loss: 0.666  |  valid_loss: 0.572\n",
      "[epoch: 7, i:  1043]  train_loss: 0.680  |  valid_loss: 0.666\n",
      "[epoch: 7, i:  1130]  train_loss: 0.685  |  valid_loss: 0.658\n",
      "[epoch: 7, i:  1217]  train_loss: 0.744  |  valid_loss: 0.634\n",
      "[epoch: 7, i:  1304]  train_loss: 0.698  |  valid_loss: 0.509\n",
      "[epoch: 7, i:  1391]  train_loss: 0.685  |  valid_loss: 0.767\n",
      "[epoch: 7, i:  1478]  train_loss: 0.735  |  valid_loss: 0.720\n",
      "[epoch: 7, i:  1565]  train_loss: 0.716  |  valid_loss: 0.778\n",
      "[epoch: 7, i:  1652]  train_loss: 0.676  |  valid_loss: 0.630\n",
      "[epoch: 7, i:  1739]  train_loss: 0.696  |  valid_loss: 0.915\n",
      "[epoch: 7, i:  1826]  train_loss: 0.779  |  valid_loss: 0.898\n",
      "[epoch: 7, i:  1913]  train_loss: 0.736  |  valid_loss: 0.747\n",
      "[epoch: 7, i:  2000]  train_loss: 0.762  |  valid_loss: 0.613\n",
      "[epoch: 7, i:  2087]  train_loss: 0.669  |  valid_loss: 0.815\n",
      "[epoch: 7, i:  2174]  train_loss: 0.752  |  valid_loss: 0.533\n",
      "[epoch: 7, i:  2261]  train_loss: 0.722  |  valid_loss: 0.988\n",
      "[epoch: 7, i:  2348]  train_loss: 0.687  |  valid_loss: 0.658\n",
      "[epoch: 7, i:  2435]  train_loss: 0.746  |  valid_loss: 0.763\n",
      "[epoch: 7, i:  2522]  train_loss: 0.702  |  valid_loss: 0.599\n",
      "[epoch: 7, i:  2609]  train_loss: 0.780  |  valid_loss: 0.626\n",
      "[epoch: 7, i:  2696]  train_loss: 0.699  |  valid_loss: 0.731\n",
      "[epoch: 7, i:  2783]  train_loss: 0.724  |  valid_loss: 0.529\n",
      "[epoch: 7, i:  2870]  train_loss: 0.740  |  valid_loss: 0.797\n",
      "[epoch: 7, i:  2957]  train_loss: 0.716  |  valid_loss: 0.718\n",
      "[epoch: 7, i:  3044]  train_loss: 0.696  |  valid_loss: 0.810\n",
      "[epoch: 7, i:  3131]  train_loss: 0.751  |  valid_loss: 0.702\n",
      "[epoch: 7, i:  3218]  train_loss: 0.690  |  valid_loss: 0.713\n",
      "[epoch: 7, i:  3305]  train_loss: 0.696  |  valid_loss: 0.841\n",
      "[epoch: 7, i:  3392]  train_loss: 0.725  |  valid_loss: 0.657\n",
      "[epoch: 7, i:  3479]  train_loss: 0.751  |  valid_loss: 0.579\n",
      "[epoch: 7, i:  3566]  train_loss: 0.697  |  valid_loss: 0.662\n",
      "[epoch: 7, i:  3653]  train_loss: 0.706  |  valid_loss: 0.777\n",
      "[epoch: 7, i:  3740]  train_loss: 0.725  |  valid_loss: 0.428\n",
      "[epoch: 7, i:  3827]  train_loss: 0.782  |  valid_loss: 0.836\n",
      "[epoch: 7, i:  3914]  train_loss: 0.674  |  valid_loss: 0.569\n",
      "[epoch: 7, i:  4001]  train_loss: 0.664  |  valid_loss: 0.674\n",
      "[epoch: 7, i:  4088]  train_loss: 0.709  |  valid_loss: 0.684\n",
      "[epoch: 7, i:  4175]  train_loss: 0.717  |  valid_loss: 0.679\n",
      "[epoch: 7, i:  4262]  train_loss: 0.752  |  valid_loss: 0.637\n",
      "[epoch: 7, i:  4349]  train_loss: 0.744  |  valid_loss: 0.755\n",
      "[epoch: 7, i:  4436]  train_loss: 0.678  |  valid_loss: 0.566\n",
      "[epoch: 7, i:  4523]  train_loss: 0.762  |  valid_loss: 0.783\n",
      "[epoch: 7, i:  4610]  train_loss: 0.697  |  valid_loss: 0.550\n",
      "[epoch: 7, i:  4697]  train_loss: 0.693  |  valid_loss: 0.769\n",
      "[epoch: 7, i:  4784]  train_loss: 0.731  |  valid_loss: 0.651\n",
      "[epoch: 7, i:  4871]  train_loss: 0.707  |  valid_loss: 0.790\n",
      "[epoch: 7, i:  4958]  train_loss: 0.712  |  valid_loss: 0.899\n",
      "[epoch: 7, i:  5045]  train_loss: 0.756  |  valid_loss: 0.485\n",
      "[epoch: 7, i:  5132]  train_loss: 0.776  |  valid_loss: 0.693\n",
      "[epoch: 7, i:  5219]  train_loss: 0.742  |  valid_loss: 0.848\n",
      "[epoch: 7, i:  5306]  train_loss: 0.713  |  valid_loss: 0.799\n",
      "[epoch: 7, i:  5393]  train_loss: 0.724  |  valid_loss: 0.630\n",
      "[epoch: 7, i:  5480]  train_loss: 0.717  |  valid_loss: 0.796\n",
      "[epoch: 7, i:  5567]  train_loss: 0.670  |  valid_loss: 0.440\n",
      "[epoch: 7, i:  5654]  train_loss: 0.759  |  valid_loss: 0.683\n",
      "[epoch: 7, i:  5741]  train_loss: 0.731  |  valid_loss: 0.764\n",
      "[epoch: 7, i:  5828]  train_loss: 0.690  |  valid_loss: 0.671\n",
      "[epoch: 7, i:  5915]  train_loss: 0.747  |  valid_loss: 0.726\n",
      "[epoch: 7, i:  6002]  train_loss: 0.740  |  valid_loss: 0.666\n",
      "[epoch: 7, i:  6089]  train_loss: 0.751  |  valid_loss: 0.777\n",
      "[epoch: 7, i:  6176]  train_loss: 0.780  |  valid_loss: 0.893\n",
      "[epoch: 7, i:  6263]  train_loss: 0.671  |  valid_loss: 0.597\n",
      "[epoch: 7, i:  6350]  train_loss: 0.683  |  valid_loss: 0.711\n",
      "[epoch: 7, i:  6437]  train_loss: 0.701  |  valid_loss: 0.484\n",
      "[epoch: 7, i:  6524]  train_loss: 0.813  |  valid_loss: 0.829\n",
      "[epoch: 7, i:  6611]  train_loss: 0.648  |  valid_loss: 0.540\n",
      "[epoch: 7, i:  6698]  train_loss: 0.683  |  valid_loss: 0.799\n",
      "[epoch: 7, i:  6785]  train_loss: 0.656  |  valid_loss: 0.538\n",
      "[epoch: 7, i:  6872]  train_loss: 0.716  |  valid_loss: 0.859\n",
      "[epoch: 7, i:  6959]  train_loss: 0.676  |  valid_loss: 0.754\n",
      "[epoch: 7, i:  7046]  train_loss: 0.743  |  valid_loss: 0.638\n",
      "[epoch: 7, i:  7133]  train_loss: 0.695  |  valid_loss: 0.581\n",
      "[epoch: 7, i:  7220]  train_loss: 0.690  |  valid_loss: 0.749\n",
      "[epoch: 7, i:  7307]  train_loss: 0.671  |  valid_loss: 0.722\n",
      "[epoch: 7, i:  7394]  train_loss: 0.725  |  valid_loss: 0.631\n",
      "[epoch: 7, i:  7481]  train_loss: 0.704  |  valid_loss: 0.717\n",
      "[epoch: 7, i:  7568]  train_loss: 0.682  |  valid_loss: 0.792\n",
      "[epoch: 7, i:  7655]  train_loss: 0.674  |  valid_loss: 0.648\n",
      "[epoch: 7, i:  7742]  train_loss: 0.682  |  valid_loss: 0.678\n",
      "[epoch: 7, i:  7829]  train_loss: 0.682  |  valid_loss: 0.605\n",
      "[epoch: 7, i:  7916]  train_loss: 0.710  |  valid_loss: 0.732\n",
      "[epoch: 7, i:  8003]  train_loss: 0.700  |  valid_loss: 0.674\n",
      "[epoch: 7, i:  8090]  train_loss: 0.701  |  valid_loss: 0.490\n",
      "[epoch: 7, i:  8177]  train_loss: 0.731  |  valid_loss: 0.686\n",
      "[epoch: 7, i:  8264]  train_loss: 0.716  |  valid_loss: 0.753\n",
      "[epoch: 7, i:  8351]  train_loss: 0.781  |  valid_loss: 0.822\n",
      "[epoch: 7, i:  8438]  train_loss: 0.688  |  valid_loss: 0.635\n",
      "[epoch: 7, i:  8525]  train_loss: 0.702  |  valid_loss: 0.665\n",
      "[epoch: 7, i:  8612]  train_loss: 0.764  |  valid_loss: 0.909\n",
      "[epoch: 7, i:  8699]  train_loss: 0.758  |  valid_loss: 0.733\n",
      "--> [End of epoch 7] train_accuracy: 75.14%  |  valid_accuracy: 76.23%\n",
      "--> [Start of epoch 8]  lr: 0.000500\n",
      "[epoch: 8, i:    86]  train_loss: 0.658  |  valid_loss: 0.586\n",
      "[epoch: 8, i:   173]  train_loss: 0.660  |  valid_loss: 0.618\n",
      "[epoch: 8, i:   260]  train_loss: 0.707  |  valid_loss: 0.681\n",
      "[epoch: 8, i:   347]  train_loss: 0.639  |  valid_loss: 0.618\n",
      "[epoch: 8, i:   434]  train_loss: 0.698  |  valid_loss: 0.709\n",
      "[epoch: 8, i:   521]  train_loss: 0.795  |  valid_loss: 0.523\n",
      "[epoch: 8, i:   608]  train_loss: 0.660  |  valid_loss: 0.710\n",
      "[epoch: 8, i:   695]  train_loss: 0.739  |  valid_loss: 0.673\n",
      "[epoch: 8, i:   782]  train_loss: 0.717  |  valid_loss: 0.756\n",
      "[epoch: 8, i:   869]  train_loss: 0.677  |  valid_loss: 0.697\n",
      "[epoch: 8, i:   956]  train_loss: 0.738  |  valid_loss: 0.565\n",
      "[epoch: 8, i:  1043]  train_loss: 0.725  |  valid_loss: 0.691\n",
      "[epoch: 8, i:  1130]  train_loss: 0.733  |  valid_loss: 0.585\n",
      "[epoch: 8, i:  1217]  train_loss: 0.695  |  valid_loss: 0.660\n",
      "[epoch: 8, i:  1304]  train_loss: 0.631  |  valid_loss: 0.511\n",
      "[epoch: 8, i:  1391]  train_loss: 0.738  |  valid_loss: 0.764\n",
      "[epoch: 8, i:  1478]  train_loss: 0.742  |  valid_loss: 0.685\n",
      "[epoch: 8, i:  1565]  train_loss: 0.673  |  valid_loss: 0.710\n",
      "[epoch: 8, i:  1652]  train_loss: 0.637  |  valid_loss: 0.654\n",
      "[epoch: 8, i:  1739]  train_loss: 0.667  |  valid_loss: 0.866\n",
      "[epoch: 8, i:  1826]  train_loss: 0.716  |  valid_loss: 0.921\n",
      "[epoch: 8, i:  1913]  train_loss: 0.709  |  valid_loss: 0.643\n",
      "[epoch: 8, i:  2000]  train_loss: 0.674  |  valid_loss: 0.671\n",
      "[epoch: 8, i:  2087]  train_loss: 0.654  |  valid_loss: 0.844\n",
      "[epoch: 8, i:  2174]  train_loss: 0.796  |  valid_loss: 0.589\n",
      "[epoch: 8, i:  2261]  train_loss: 0.707  |  valid_loss: 1.000\n",
      "[epoch: 8, i:  2348]  train_loss: 0.706  |  valid_loss: 0.602\n",
      "[epoch: 8, i:  2435]  train_loss: 0.660  |  valid_loss: 0.800\n",
      "[epoch: 8, i:  2522]  train_loss: 0.689  |  valid_loss: 0.596\n",
      "[epoch: 8, i:  2609]  train_loss: 0.729  |  valid_loss: 0.643\n",
      "[epoch: 8, i:  2696]  train_loss: 0.714  |  valid_loss: 0.727\n",
      "[epoch: 8, i:  2783]  train_loss: 0.709  |  valid_loss: 0.523\n",
      "[epoch: 8, i:  2870]  train_loss: 0.703  |  valid_loss: 0.720\n",
      "[epoch: 8, i:  2957]  train_loss: 0.771  |  valid_loss: 0.730\n",
      "[epoch: 8, i:  3044]  train_loss: 0.785  |  valid_loss: 0.809\n",
      "[epoch: 8, i:  3131]  train_loss: 0.669  |  valid_loss: 0.689\n",
      "[epoch: 8, i:  3218]  train_loss: 0.720  |  valid_loss: 0.759\n",
      "[epoch: 8, i:  3305]  train_loss: 0.684  |  valid_loss: 0.734\n",
      "[epoch: 8, i:  3392]  train_loss: 0.713  |  valid_loss: 0.662\n",
      "[epoch: 8, i:  3479]  train_loss: 0.683  |  valid_loss: 0.492\n",
      "[epoch: 8, i:  3566]  train_loss: 0.734  |  valid_loss: 0.619\n",
      "[epoch: 8, i:  3653]  train_loss: 0.787  |  valid_loss: 0.812\n",
      "[epoch: 8, i:  3740]  train_loss: 0.655  |  valid_loss: 0.441\n",
      "[epoch: 8, i:  3827]  train_loss: 0.653  |  valid_loss: 0.763\n",
      "[epoch: 8, i:  3914]  train_loss: 0.671  |  valid_loss: 0.554\n",
      "[epoch: 8, i:  4001]  train_loss: 0.714  |  valid_loss: 0.646\n",
      "[epoch: 8, i:  4088]  train_loss: 0.727  |  valid_loss: 0.718\n",
      "[epoch: 8, i:  4175]  train_loss: 0.736  |  valid_loss: 0.643\n",
      "[epoch: 8, i:  4262]  train_loss: 0.712  |  valid_loss: 0.564\n",
      "[epoch: 8, i:  4349]  train_loss: 0.697  |  valid_loss: 0.679\n",
      "[epoch: 8, i:  4436]  train_loss: 0.655  |  valid_loss: 0.568\n",
      "[epoch: 8, i:  4523]  train_loss: 0.686  |  valid_loss: 0.791\n",
      "[epoch: 8, i:  4610]  train_loss: 0.726  |  valid_loss: 0.586\n",
      "[epoch: 8, i:  4697]  train_loss: 0.645  |  valid_loss: 0.651\n",
      "[epoch: 8, i:  4784]  train_loss: 0.706  |  valid_loss: 0.685\n",
      "[epoch: 8, i:  4871]  train_loss: 0.744  |  valid_loss: 0.783\n",
      "[epoch: 8, i:  4958]  train_loss: 0.720  |  valid_loss: 0.908\n",
      "[epoch: 8, i:  5045]  train_loss: 0.732  |  valid_loss: 0.571\n",
      "[epoch: 8, i:  5132]  train_loss: 0.718  |  valid_loss: 0.699\n",
      "[epoch: 8, i:  5219]  train_loss: 0.706  |  valid_loss: 0.798\n",
      "[epoch: 8, i:  5306]  train_loss: 0.686  |  valid_loss: 0.788\n",
      "[epoch: 8, i:  5393]  train_loss: 0.686  |  valid_loss: 0.705\n",
      "[epoch: 8, i:  5480]  train_loss: 0.744  |  valid_loss: 0.782\n",
      "[epoch: 8, i:  5567]  train_loss: 0.687  |  valid_loss: 0.433\n",
      "[epoch: 8, i:  5654]  train_loss: 0.727  |  valid_loss: 0.682\n",
      "[epoch: 8, i:  5741]  train_loss: 0.666  |  valid_loss: 0.790\n",
      "[epoch: 8, i:  5828]  train_loss: 0.695  |  valid_loss: 0.776\n",
      "[epoch: 8, i:  5915]  train_loss: 0.687  |  valid_loss: 0.849\n",
      "[epoch: 8, i:  6002]  train_loss: 0.638  |  valid_loss: 0.655\n",
      "[epoch: 8, i:  6089]  train_loss: 0.647  |  valid_loss: 0.821\n",
      "[epoch: 8, i:  6176]  train_loss: 0.688  |  valid_loss: 0.797\n",
      "[epoch: 8, i:  6263]  train_loss: 0.686  |  valid_loss: 0.666\n",
      "[epoch: 8, i:  6350]  train_loss: 0.725  |  valid_loss: 0.691\n",
      "[epoch: 8, i:  6437]  train_loss: 0.727  |  valid_loss: 0.516\n",
      "[epoch: 8, i:  6524]  train_loss: 0.783  |  valid_loss: 0.739\n",
      "[epoch: 8, i:  6611]  train_loss: 0.678  |  valid_loss: 0.544\n",
      "[epoch: 8, i:  6698]  train_loss: 0.758  |  valid_loss: 0.856\n",
      "[epoch: 8, i:  6785]  train_loss: 0.660  |  valid_loss: 0.611\n",
      "[epoch: 8, i:  6872]  train_loss: 0.626  |  valid_loss: 0.819\n",
      "[epoch: 8, i:  6959]  train_loss: 0.655  |  valid_loss: 0.783\n",
      "[epoch: 8, i:  7046]  train_loss: 0.705  |  valid_loss: 0.634\n",
      "[epoch: 8, i:  7133]  train_loss: 0.665  |  valid_loss: 0.577\n",
      "[epoch: 8, i:  7220]  train_loss: 0.736  |  valid_loss: 0.680\n",
      "[epoch: 8, i:  7307]  train_loss: 0.754  |  valid_loss: 0.731\n",
      "[epoch: 8, i:  7394]  train_loss: 0.725  |  valid_loss: 0.731\n",
      "[epoch: 8, i:  7481]  train_loss: 0.633  |  valid_loss: 0.647\n",
      "[epoch: 8, i:  7568]  train_loss: 0.577  |  valid_loss: 0.741\n",
      "[epoch: 8, i:  7655]  train_loss: 0.750  |  valid_loss: 0.762\n",
      "[epoch: 8, i:  7742]  train_loss: 0.758  |  valid_loss: 0.705\n",
      "[epoch: 8, i:  7829]  train_loss: 0.725  |  valid_loss: 0.527\n",
      "[epoch: 8, i:  7916]  train_loss: 0.627  |  valid_loss: 0.779\n",
      "[epoch: 8, i:  8003]  train_loss: 0.663  |  valid_loss: 0.673\n",
      "[epoch: 8, i:  8090]  train_loss: 0.689  |  valid_loss: 0.548\n",
      "[epoch: 8, i:  8177]  train_loss: 0.694  |  valid_loss: 0.709\n",
      "[epoch: 8, i:  8264]  train_loss: 0.695  |  valid_loss: 0.766\n",
      "[epoch: 8, i:  8351]  train_loss: 0.684  |  valid_loss: 0.772\n",
      "[epoch: 8, i:  8438]  train_loss: 0.735  |  valid_loss: 0.575\n",
      "[epoch: 8, i:  8525]  train_loss: 0.626  |  valid_loss: 0.764\n",
      "[epoch: 8, i:  8612]  train_loss: 0.708  |  valid_loss: 0.875\n",
      "[epoch: 8, i:  8699]  train_loss: 0.681  |  valid_loss: 0.708\n",
      "--> [End of epoch 8] train_accuracy: 75.80%  |  valid_accuracy: 76.32%\n",
      "--> [Start of epoch 9]  lr: 0.000500\n",
      "[epoch: 9, i:    86]  train_loss: 0.663  |  valid_loss: 0.605\n",
      "[epoch: 9, i:   173]  train_loss: 0.640  |  valid_loss: 0.540\n",
      "[epoch: 9, i:   260]  train_loss: 0.588  |  valid_loss: 0.667\n",
      "[epoch: 9, i:   347]  train_loss: 0.673  |  valid_loss: 0.540\n",
      "[epoch: 9, i:   434]  train_loss: 0.638  |  valid_loss: 0.672\n",
      "[epoch: 9, i:   521]  train_loss: 0.656  |  valid_loss: 0.517\n",
      "[epoch: 9, i:   608]  train_loss: 0.680  |  valid_loss: 0.645\n",
      "[epoch: 9, i:   695]  train_loss: 0.653  |  valid_loss: 0.711\n",
      "[epoch: 9, i:   782]  train_loss: 0.692  |  valid_loss: 0.723\n",
      "[epoch: 9, i:   869]  train_loss: 0.667  |  valid_loss: 0.737\n",
      "[epoch: 9, i:   956]  train_loss: 0.716  |  valid_loss: 0.591\n",
      "[epoch: 9, i:  1043]  train_loss: 0.736  |  valid_loss: 0.620\n",
      "[epoch: 9, i:  1130]  train_loss: 0.681  |  valid_loss: 0.629\n",
      "[epoch: 9, i:  1217]  train_loss: 0.673  |  valid_loss: 0.589\n",
      "[epoch: 9, i:  1304]  train_loss: 0.731  |  valid_loss: 0.494\n",
      "[epoch: 9, i:  1391]  train_loss: 0.674  |  valid_loss: 0.697\n",
      "[epoch: 9, i:  1478]  train_loss: 0.649  |  valid_loss: 0.624\n",
      "[epoch: 9, i:  1565]  train_loss: 0.627  |  valid_loss: 0.720\n",
      "[epoch: 9, i:  1652]  train_loss: 0.636  |  valid_loss: 0.631\n",
      "[epoch: 9, i:  1739]  train_loss: 0.615  |  valid_loss: 0.892\n",
      "[epoch: 9, i:  1826]  train_loss: 0.653  |  valid_loss: 0.744\n",
      "[epoch: 9, i:  1913]  train_loss: 0.699  |  valid_loss: 0.715\n",
      "[epoch: 9, i:  2000]  train_loss: 0.650  |  valid_loss: 0.696\n",
      "[epoch: 9, i:  2087]  train_loss: 0.682  |  valid_loss: 0.780\n",
      "[epoch: 9, i:  2174]  train_loss: 0.640  |  valid_loss: 0.596\n",
      "[epoch: 9, i:  2261]  train_loss: 0.738  |  valid_loss: 0.988\n",
      "[epoch: 9, i:  2348]  train_loss: 0.717  |  valid_loss: 0.703\n",
      "[epoch: 9, i:  2435]  train_loss: 0.745  |  valid_loss: 0.698\n",
      "[epoch: 9, i:  2522]  train_loss: 0.683  |  valid_loss: 0.573\n",
      "[epoch: 9, i:  2609]  train_loss: 0.698  |  valid_loss: 0.601\n",
      "[epoch: 9, i:  2696]  train_loss: 0.716  |  valid_loss: 0.767\n",
      "[epoch: 9, i:  2783]  train_loss: 0.658  |  valid_loss: 0.521\n",
      "[epoch: 9, i:  2870]  train_loss: 0.752  |  valid_loss: 0.743\n",
      "[epoch: 9, i:  2957]  train_loss: 0.661  |  valid_loss: 0.681\n",
      "[epoch: 9, i:  3044]  train_loss: 0.657  |  valid_loss: 0.797\n",
      "[epoch: 9, i:  3131]  train_loss: 0.673  |  valid_loss: 0.726\n",
      "[epoch: 9, i:  3218]  train_loss: 0.728  |  valid_loss: 0.777\n",
      "[epoch: 9, i:  3305]  train_loss: 0.650  |  valid_loss: 0.745\n",
      "[epoch: 9, i:  3392]  train_loss: 0.721  |  valid_loss: 0.688\n",
      "[epoch: 9, i:  3479]  train_loss: 0.669  |  valid_loss: 0.500\n",
      "[epoch: 9, i:  3566]  train_loss: 0.797  |  valid_loss: 0.648\n",
      "[epoch: 9, i:  3653]  train_loss: 0.708  |  valid_loss: 0.769\n",
      "[epoch: 9, i:  3740]  train_loss: 0.726  |  valid_loss: 0.418\n",
      "[epoch: 9, i:  3827]  train_loss: 0.671  |  valid_loss: 0.731\n",
      "[epoch: 9, i:  3914]  train_loss: 0.666  |  valid_loss: 0.463\n",
      "[epoch: 9, i:  4001]  train_loss: 0.627  |  valid_loss: 0.703\n",
      "[epoch: 9, i:  4088]  train_loss: 0.602  |  valid_loss: 0.707\n",
      "[epoch: 9, i:  4175]  train_loss: 0.744  |  valid_loss: 0.628\n",
      "[epoch: 9, i:  4262]  train_loss: 0.655  |  valid_loss: 0.583\n",
      "[epoch: 9, i:  4349]  train_loss: 0.676  |  valid_loss: 0.689\n",
      "[epoch: 9, i:  4436]  train_loss: 0.652  |  valid_loss: 0.538\n",
      "[epoch: 9, i:  4523]  train_loss: 0.659  |  valid_loss: 0.745\n",
      "[epoch: 9, i:  4610]  train_loss: 0.671  |  valid_loss: 0.588\n",
      "[epoch: 9, i:  4697]  train_loss: 0.719  |  valid_loss: 0.663\n",
      "[epoch: 9, i:  4784]  train_loss: 0.710  |  valid_loss: 0.701\n",
      "[epoch: 9, i:  4871]  train_loss: 0.659  |  valid_loss: 0.740\n",
      "[epoch: 9, i:  4958]  train_loss: 0.690  |  valid_loss: 0.779\n",
      "[epoch: 9, i:  5045]  train_loss: 0.724  |  valid_loss: 0.526\n",
      "[epoch: 9, i:  5132]  train_loss: 0.666  |  valid_loss: 0.657\n",
      "[epoch: 9, i:  5219]  train_loss: 0.708  |  valid_loss: 0.781\n",
      "[epoch: 9, i:  5306]  train_loss: 0.639  |  valid_loss: 0.814\n",
      "[epoch: 9, i:  5393]  train_loss: 0.730  |  valid_loss: 0.601\n",
      "[epoch: 9, i:  5480]  train_loss: 0.670  |  valid_loss: 0.676\n",
      "[epoch: 9, i:  5567]  train_loss: 0.715  |  valid_loss: 0.455\n",
      "[epoch: 9, i:  5654]  train_loss: 0.668  |  valid_loss: 0.708\n",
      "[epoch: 9, i:  5741]  train_loss: 0.617  |  valid_loss: 0.681\n",
      "[epoch: 9, i:  5828]  train_loss: 0.595  |  valid_loss: 0.654\n",
      "[epoch: 9, i:  5915]  train_loss: 0.692  |  valid_loss: 0.856\n",
      "[epoch: 9, i:  6002]  train_loss: 0.635  |  valid_loss: 0.581\n",
      "[epoch: 9, i:  6089]  train_loss: 0.653  |  valid_loss: 0.825\n",
      "[epoch: 9, i:  6176]  train_loss: 0.743  |  valid_loss: 0.825\n",
      "[epoch: 9, i:  6263]  train_loss: 0.624  |  valid_loss: 0.589\n",
      "[epoch: 9, i:  6350]  train_loss: 0.740  |  valid_loss: 0.650\n",
      "[epoch: 9, i:  6437]  train_loss: 0.728  |  valid_loss: 0.505\n",
      "[epoch: 9, i:  6524]  train_loss: 0.687  |  valid_loss: 0.924\n",
      "[epoch: 9, i:  6611]  train_loss: 0.699  |  valid_loss: 0.587\n",
      "[epoch: 9, i:  6698]  train_loss: 0.672  |  valid_loss: 0.756\n",
      "[epoch: 9, i:  6785]  train_loss: 0.749  |  valid_loss: 0.600\n",
      "[epoch: 9, i:  6872]  train_loss: 0.649  |  valid_loss: 0.804\n",
      "[epoch: 9, i:  6959]  train_loss: 0.630  |  valid_loss: 0.744\n",
      "[epoch: 9, i:  7046]  train_loss: 0.701  |  valid_loss: 0.625\n",
      "[epoch: 9, i:  7133]  train_loss: 0.695  |  valid_loss: 0.543\n",
      "[epoch: 9, i:  7220]  train_loss: 0.618  |  valid_loss: 0.723\n",
      "[epoch: 9, i:  7307]  train_loss: 0.687  |  valid_loss: 0.794\n",
      "[epoch: 9, i:  7394]  train_loss: 0.674  |  valid_loss: 0.639\n",
      "[epoch: 9, i:  7481]  train_loss: 0.631  |  valid_loss: 0.720\n",
      "[epoch: 9, i:  7568]  train_loss: 0.715  |  valid_loss: 0.831\n",
      "[epoch: 9, i:  7655]  train_loss: 0.804  |  valid_loss: 0.762\n",
      "[epoch: 9, i:  7742]  train_loss: 0.576  |  valid_loss: 0.678\n",
      "[epoch: 9, i:  7829]  train_loss: 0.605  |  valid_loss: 0.544\n",
      "[epoch: 9, i:  7916]  train_loss: 0.704  |  valid_loss: 0.833\n",
      "[epoch: 9, i:  8003]  train_loss: 0.694  |  valid_loss: 0.656\n",
      "[epoch: 9, i:  8090]  train_loss: 0.743  |  valid_loss: 0.485\n",
      "[epoch: 9, i:  8177]  train_loss: 0.698  |  valid_loss: 0.601\n",
      "[epoch: 9, i:  8264]  train_loss: 0.688  |  valid_loss: 0.737\n",
      "[epoch: 9, i:  8351]  train_loss: 0.682  |  valid_loss: 0.814\n",
      "[epoch: 9, i:  8438]  train_loss: 0.669  |  valid_loss: 0.506\n",
      "[epoch: 9, i:  8525]  train_loss: 0.696  |  valid_loss: 0.673\n",
      "[epoch: 9, i:  8612]  train_loss: 0.648  |  valid_loss: 0.855\n",
      "[epoch: 9, i:  8699]  train_loss: 0.711  |  valid_loss: 0.681\n",
      "--> [End of epoch 9] train_accuracy: 76.42%  |  valid_accuracy: 76.22%\n",
      "--> [Start of epoch 10]  lr: 0.000500\n",
      "[epoch: 10, i:    86]  train_loss: 0.604  |  valid_loss: 0.552\n",
      "[epoch: 10, i:   173]  train_loss: 0.628  |  valid_loss: 0.525\n",
      "[epoch: 10, i:   260]  train_loss: 0.644  |  valid_loss: 0.745\n",
      "[epoch: 10, i:   347]  train_loss: 0.703  |  valid_loss: 0.558\n",
      "[epoch: 10, i:   434]  train_loss: 0.664  |  valid_loss: 0.749\n",
      "[epoch: 10, i:   521]  train_loss: 0.685  |  valid_loss: 0.466\n",
      "[epoch: 10, i:   608]  train_loss: 0.664  |  valid_loss: 0.783\n",
      "[epoch: 10, i:   695]  train_loss: 0.693  |  valid_loss: 0.693\n",
      "[epoch: 10, i:   782]  train_loss: 0.581  |  valid_loss: 0.758\n",
      "[epoch: 10, i:   869]  train_loss: 0.678  |  valid_loss: 0.709\n",
      "[epoch: 10, i:   956]  train_loss: 0.608  |  valid_loss: 0.529\n",
      "[epoch: 10, i:  1043]  train_loss: 0.636  |  valid_loss: 0.637\n",
      "[epoch: 10, i:  1130]  train_loss: 0.619  |  valid_loss: 0.587\n",
      "[epoch: 10, i:  1217]  train_loss: 0.710  |  valid_loss: 0.626\n",
      "[epoch: 10, i:  1304]  train_loss: 0.609  |  valid_loss: 0.500\n",
      "[epoch: 10, i:  1391]  train_loss: 0.638  |  valid_loss: 0.711\n",
      "[epoch: 10, i:  1478]  train_loss: 0.628  |  valid_loss: 0.679\n",
      "[epoch: 10, i:  1565]  train_loss: 0.637  |  valid_loss: 0.709\n",
      "[epoch: 10, i:  1652]  train_loss: 0.698  |  valid_loss: 0.632\n",
      "[epoch: 10, i:  1739]  train_loss: 0.742  |  valid_loss: 0.801\n",
      "[epoch: 10, i:  1826]  train_loss: 0.735  |  valid_loss: 0.777\n",
      "[epoch: 10, i:  1913]  train_loss: 0.617  |  valid_loss: 0.656\n",
      "[epoch: 10, i:  2000]  train_loss: 0.676  |  valid_loss: 0.645\n",
      "[epoch: 10, i:  2087]  train_loss: 0.694  |  valid_loss: 0.741\n",
      "[epoch: 10, i:  2174]  train_loss: 0.622  |  valid_loss: 0.552\n",
      "[epoch: 10, i:  2261]  train_loss: 0.688  |  valid_loss: 0.966\n",
      "[epoch: 10, i:  2348]  train_loss: 0.719  |  valid_loss: 0.651\n",
      "[epoch: 10, i:  2435]  train_loss: 0.668  |  valid_loss: 0.804\n",
      "[epoch: 10, i:  2522]  train_loss: 0.612  |  valid_loss: 0.666\n",
      "[epoch: 10, i:  2609]  train_loss: 0.649  |  valid_loss: 0.616\n",
      "[epoch: 10, i:  2696]  train_loss: 0.666  |  valid_loss: 0.709\n",
      "[epoch: 10, i:  2783]  train_loss: 0.680  |  valid_loss: 0.552\n",
      "[epoch: 10, i:  2870]  train_loss: 0.651  |  valid_loss: 0.769\n",
      "[epoch: 10, i:  2957]  train_loss: 0.649  |  valid_loss: 0.691\n",
      "[epoch: 10, i:  3044]  train_loss: 0.605  |  valid_loss: 0.725\n",
      "[epoch: 10, i:  3131]  train_loss: 0.640  |  valid_loss: 0.666\n",
      "[epoch: 10, i:  3218]  train_loss: 0.696  |  valid_loss: 0.746\n",
      "[epoch: 10, i:  3305]  train_loss: 0.735  |  valid_loss: 0.684\n",
      "[epoch: 10, i:  3392]  train_loss: 0.759  |  valid_loss: 0.690\n",
      "[epoch: 10, i:  3479]  train_loss: 0.676  |  valid_loss: 0.533\n",
      "[epoch: 10, i:  3566]  train_loss: 0.714  |  valid_loss: 0.615\n",
      "[epoch: 10, i:  3653]  train_loss: 0.717  |  valid_loss: 0.767\n",
      "[epoch: 10, i:  3740]  train_loss: 0.681  |  valid_loss: 0.422\n",
      "[epoch: 10, i:  3827]  train_loss: 0.699  |  valid_loss: 0.790\n",
      "[epoch: 10, i:  3914]  train_loss: 0.651  |  valid_loss: 0.549\n",
      "[epoch: 10, i:  4001]  train_loss: 0.694  |  valid_loss: 0.628\n",
      "[epoch: 10, i:  4088]  train_loss: 0.668  |  valid_loss: 0.650\n",
      "[epoch: 10, i:  4175]  train_loss: 0.658  |  valid_loss: 0.642\n",
      "[epoch: 10, i:  4262]  train_loss: 0.723  |  valid_loss: 0.529\n",
      "[epoch: 10, i:  4349]  train_loss: 0.633  |  valid_loss: 0.671\n",
      "[epoch: 10, i:  4436]  train_loss: 0.646  |  valid_loss: 0.635\n",
      "[epoch: 10, i:  4523]  train_loss: 0.637  |  valid_loss: 0.766\n",
      "[epoch: 10, i:  4610]  train_loss: 0.638  |  valid_loss: 0.538\n",
      "[epoch: 10, i:  4697]  train_loss: 0.680  |  valid_loss: 0.640\n",
      "[epoch: 10, i:  4784]  train_loss: 0.708  |  valid_loss: 0.700\n",
      "[epoch: 10, i:  4871]  train_loss: 0.664  |  valid_loss: 0.804\n",
      "[epoch: 10, i:  4958]  train_loss: 0.746  |  valid_loss: 0.833\n",
      "[epoch: 10, i:  5045]  train_loss: 0.638  |  valid_loss: 0.519\n",
      "[epoch: 10, i:  5132]  train_loss: 0.697  |  valid_loss: 0.596\n",
      "[epoch: 10, i:  5219]  train_loss: 0.642  |  valid_loss: 0.883\n",
      "[epoch: 10, i:  5306]  train_loss: 0.689  |  valid_loss: 0.740\n",
      "[epoch: 10, i:  5393]  train_loss: 0.605  |  valid_loss: 0.590\n",
      "[epoch: 10, i:  5480]  train_loss: 0.681  |  valid_loss: 0.749\n",
      "[epoch: 10, i:  5567]  train_loss: 0.659  |  valid_loss: 0.374\n",
      "[epoch: 10, i:  5654]  train_loss: 0.664  |  valid_loss: 0.652\n",
      "[epoch: 10, i:  5741]  train_loss: 0.734  |  valid_loss: 0.785\n",
      "[epoch: 10, i:  5828]  train_loss: 0.636  |  valid_loss: 0.632\n",
      "[epoch: 10, i:  5915]  train_loss: 0.718  |  valid_loss: 0.785\n",
      "[epoch: 10, i:  6002]  train_loss: 0.703  |  valid_loss: 0.656\n",
      "[epoch: 10, i:  6089]  train_loss: 0.711  |  valid_loss: 0.829\n",
      "[epoch: 10, i:  6176]  train_loss: 0.609  |  valid_loss: 0.755\n",
      "[epoch: 10, i:  6263]  train_loss: 0.622  |  valid_loss: 0.545\n",
      "[epoch: 10, i:  6350]  train_loss: 0.649  |  valid_loss: 0.606\n",
      "[epoch: 10, i:  6437]  train_loss: 0.685  |  valid_loss: 0.547\n",
      "[epoch: 10, i:  6524]  train_loss: 0.711  |  valid_loss: 0.855\n",
      "[epoch: 10, i:  6611]  train_loss: 0.700  |  valid_loss: 0.513\n",
      "[epoch: 10, i:  6698]  train_loss: 0.679  |  valid_loss: 0.829\n",
      "[epoch: 10, i:  6785]  train_loss: 0.639  |  valid_loss: 0.548\n",
      "[epoch: 10, i:  6872]  train_loss: 0.682  |  valid_loss: 0.807\n",
      "[epoch: 10, i:  6959]  train_loss: 0.695  |  valid_loss: 0.746\n",
      "[epoch: 10, i:  7046]  train_loss: 0.691  |  valid_loss: 0.649\n",
      "[epoch: 10, i:  7133]  train_loss: 0.609  |  valid_loss: 0.641\n",
      "[epoch: 10, i:  7220]  train_loss: 0.674  |  valid_loss: 0.706\n",
      "[epoch: 10, i:  7307]  train_loss: 0.674  |  valid_loss: 0.755\n",
      "[epoch: 10, i:  7394]  train_loss: 0.652  |  valid_loss: 0.584\n",
      "[epoch: 10, i:  7481]  train_loss: 0.703  |  valid_loss: 0.711\n",
      "[epoch: 10, i:  7568]  train_loss: 0.660  |  valid_loss: 0.774\n",
      "[epoch: 10, i:  7655]  train_loss: 0.671  |  valid_loss: 0.673\n",
      "[epoch: 10, i:  7742]  train_loss: 0.675  |  valid_loss: 0.652\n",
      "[epoch: 10, i:  7829]  train_loss: 0.644  |  valid_loss: 0.458\n",
      "[epoch: 10, i:  7916]  train_loss: 0.673  |  valid_loss: 0.723\n",
      "[epoch: 10, i:  8003]  train_loss: 0.683  |  valid_loss: 0.625\n",
      "[epoch: 10, i:  8090]  train_loss: 0.587  |  valid_loss: 0.557\n",
      "[epoch: 10, i:  8177]  train_loss: 0.658  |  valid_loss: 0.649\n",
      "[epoch: 10, i:  8264]  train_loss: 0.706  |  valid_loss: 0.717\n",
      "[epoch: 10, i:  8351]  train_loss: 0.659  |  valid_loss: 0.742\n",
      "[epoch: 10, i:  8438]  train_loss: 0.622  |  valid_loss: 0.525\n",
      "[epoch: 10, i:  8525]  train_loss: 0.699  |  valid_loss: 0.630\n",
      "[epoch: 10, i:  8612]  train_loss: 0.708  |  valid_loss: 0.953\n",
      "[epoch: 10, i:  8699]  train_loss: 0.678  |  valid_loss: 0.670\n",
      "--> [End of epoch 10] train_accuracy: 76.70%  |  valid_accuracy: 77.12%\n",
      "--> [Start of epoch 11]  lr: 0.000500\n",
      "[epoch: 11, i:    86]  train_loss: 0.601  |  valid_loss: 0.666\n",
      "[epoch: 11, i:   173]  train_loss: 0.724  |  valid_loss: 0.564\n",
      "[epoch: 11, i:   260]  train_loss: 0.628  |  valid_loss: 0.750\n",
      "[epoch: 11, i:   347]  train_loss: 0.632  |  valid_loss: 0.606\n",
      "[epoch: 11, i:   434]  train_loss: 0.621  |  valid_loss: 0.750\n",
      "[epoch: 11, i:   521]  train_loss: 0.652  |  valid_loss: 0.471\n",
      "[epoch: 11, i:   608]  train_loss: 0.585  |  valid_loss: 0.703\n",
      "[epoch: 11, i:   695]  train_loss: 0.635  |  valid_loss: 0.763\n",
      "[epoch: 11, i:   782]  train_loss: 0.689  |  valid_loss: 0.732\n",
      "[epoch: 11, i:   869]  train_loss: 0.704  |  valid_loss: 0.699\n",
      "[epoch: 11, i:   956]  train_loss: 0.593  |  valid_loss: 0.478\n",
      "[epoch: 11, i:  1043]  train_loss: 0.707  |  valid_loss: 0.617\n",
      "[epoch: 11, i:  1130]  train_loss: 0.649  |  valid_loss: 0.599\n",
      "[epoch: 11, i:  1217]  train_loss: 0.658  |  valid_loss: 0.677\n",
      "[epoch: 11, i:  1304]  train_loss: 0.690  |  valid_loss: 0.457\n",
      "[epoch: 11, i:  1391]  train_loss: 0.689  |  valid_loss: 0.742\n",
      "[epoch: 11, i:  1478]  train_loss: 0.612  |  valid_loss: 0.750\n",
      "[epoch: 11, i:  1565]  train_loss: 0.694  |  valid_loss: 0.757\n",
      "[epoch: 11, i:  1652]  train_loss: 0.684  |  valid_loss: 0.682\n",
      "[epoch: 11, i:  1739]  train_loss: 0.655  |  valid_loss: 0.890\n",
      "[epoch: 11, i:  1826]  train_loss: 0.683  |  valid_loss: 0.859\n",
      "[epoch: 11, i:  1913]  train_loss: 0.634  |  valid_loss: 0.679\n",
      "[epoch: 11, i:  2000]  train_loss: 0.630  |  valid_loss: 0.603\n",
      "[epoch: 11, i:  2087]  train_loss: 0.633  |  valid_loss: 0.746\n",
      "[epoch: 11, i:  2174]  train_loss: 0.692  |  valid_loss: 0.577\n",
      "[epoch: 11, i:  2261]  train_loss: 0.721  |  valid_loss: 0.928\n",
      "[epoch: 11, i:  2348]  train_loss: 0.630  |  valid_loss: 0.634\n",
      "[epoch: 11, i:  2435]  train_loss: 0.606  |  valid_loss: 0.745\n",
      "[epoch: 11, i:  2522]  train_loss: 0.707  |  valid_loss: 0.568\n",
      "[epoch: 11, i:  2609]  train_loss: 0.700  |  valid_loss: 0.600\n",
      "[epoch: 11, i:  2696]  train_loss: 0.649  |  valid_loss: 0.732\n",
      "[epoch: 11, i:  2783]  train_loss: 0.648  |  valid_loss: 0.519\n",
      "[epoch: 11, i:  2870]  train_loss: 0.616  |  valid_loss: 0.752\n",
      "[epoch: 11, i:  2957]  train_loss: 0.627  |  valid_loss: 0.743\n",
      "[epoch: 11, i:  3044]  train_loss: 0.691  |  valid_loss: 0.827\n",
      "[epoch: 11, i:  3131]  train_loss: 0.721  |  valid_loss: 0.701\n",
      "[epoch: 11, i:  3218]  train_loss: 0.695  |  valid_loss: 0.780\n",
      "[epoch: 11, i:  3305]  train_loss: 0.647  |  valid_loss: 0.666\n",
      "[epoch: 11, i:  3392]  train_loss: 0.624  |  valid_loss: 0.679\n",
      "[epoch: 11, i:  3479]  train_loss: 0.675  |  valid_loss: 0.526\n",
      "[epoch: 11, i:  3566]  train_loss: 0.715  |  valid_loss: 0.619\n",
      "[epoch: 11, i:  3653]  train_loss: 0.690  |  valid_loss: 0.759\n",
      "[epoch: 11, i:  3740]  train_loss: 0.650  |  valid_loss: 0.470\n",
      "[epoch: 11, i:  3827]  train_loss: 0.654  |  valid_loss: 0.837\n",
      "[epoch: 11, i:  3914]  train_loss: 0.661  |  valid_loss: 0.563\n",
      "[epoch: 11, i:  4001]  train_loss: 0.643  |  valid_loss: 0.636\n",
      "[epoch: 11, i:  4088]  train_loss: 0.624  |  valid_loss: 0.639\n",
      "[epoch: 11, i:  4175]  train_loss: 0.703  |  valid_loss: 0.576\n",
      "[epoch: 11, i:  4262]  train_loss: 0.614  |  valid_loss: 0.615\n",
      "[epoch: 11, i:  4349]  train_loss: 0.619  |  valid_loss: 0.637\n",
      "[epoch: 11, i:  4436]  train_loss: 0.692  |  valid_loss: 0.541\n",
      "[epoch: 11, i:  4523]  train_loss: 0.620  |  valid_loss: 0.648\n",
      "[epoch: 11, i:  4610]  train_loss: 0.668  |  valid_loss: 0.605\n",
      "[epoch: 11, i:  4697]  train_loss: 0.662  |  valid_loss: 0.547\n",
      "[epoch: 11, i:  4784]  train_loss: 0.708  |  valid_loss: 0.669\n",
      "[epoch: 11, i:  4871]  train_loss: 0.631  |  valid_loss: 0.787\n",
      "[epoch: 11, i:  4958]  train_loss: 0.607  |  valid_loss: 0.812\n",
      "[epoch: 11, i:  5045]  train_loss: 0.590  |  valid_loss: 0.524\n",
      "[epoch: 11, i:  5132]  train_loss: 0.621  |  valid_loss: 0.596\n",
      "[epoch: 11, i:  5219]  train_loss: 0.635  |  valid_loss: 0.845\n",
      "[epoch: 11, i:  5306]  train_loss: 0.605  |  valid_loss: 0.745\n",
      "[epoch: 11, i:  5393]  train_loss: 0.721  |  valid_loss: 0.669\n",
      "[epoch: 11, i:  5480]  train_loss: 0.663  |  valid_loss: 0.704\n",
      "[epoch: 11, i:  5567]  train_loss: 0.617  |  valid_loss: 0.402\n",
      "[epoch: 11, i:  5654]  train_loss: 0.719  |  valid_loss: 0.749\n",
      "[epoch: 11, i:  5741]  train_loss: 0.613  |  valid_loss: 0.685\n",
      "[epoch: 11, i:  5828]  train_loss: 0.680  |  valid_loss: 0.565\n",
      "[epoch: 11, i:  5915]  train_loss: 0.625  |  valid_loss: 0.835\n",
      "[epoch: 11, i:  6002]  train_loss: 0.651  |  valid_loss: 0.592\n",
      "[epoch: 11, i:  6089]  train_loss: 0.673  |  valid_loss: 0.801\n",
      "[epoch: 11, i:  6176]  train_loss: 0.610  |  valid_loss: 0.782\n",
      "[epoch: 11, i:  6263]  train_loss: 0.644  |  valid_loss: 0.659\n",
      "[epoch: 11, i:  6350]  train_loss: 0.651  |  valid_loss: 0.632\n",
      "[epoch: 11, i:  6437]  train_loss: 0.639  |  valid_loss: 0.560\n",
      "[epoch: 11, i:  6524]  train_loss: 0.608  |  valid_loss: 0.737\n",
      "[epoch: 11, i:  6611]  train_loss: 0.774  |  valid_loss: 0.500\n",
      "[epoch: 11, i:  6698]  train_loss: 0.650  |  valid_loss: 0.692\n",
      "[epoch: 11, i:  6785]  train_loss: 0.601  |  valid_loss: 0.580\n",
      "[epoch: 11, i:  6872]  train_loss: 0.660  |  valid_loss: 0.747\n",
      "[epoch: 11, i:  6959]  train_loss: 0.740  |  valid_loss: 0.755\n",
      "[epoch: 11, i:  7046]  train_loss: 0.691  |  valid_loss: 0.640\n",
      "[epoch: 11, i:  7133]  train_loss: 0.730  |  valid_loss: 0.590\n",
      "[epoch: 11, i:  7220]  train_loss: 0.651  |  valid_loss: 0.723\n",
      "[epoch: 11, i:  7307]  train_loss: 0.724  |  valid_loss: 0.852\n",
      "[epoch: 11, i:  7394]  train_loss: 0.696  |  valid_loss: 0.646\n",
      "[epoch: 11, i:  7481]  train_loss: 0.707  |  valid_loss: 0.693\n",
      "[epoch: 11, i:  7568]  train_loss: 0.652  |  valid_loss: 0.855\n",
      "[epoch: 11, i:  7655]  train_loss: 0.710  |  valid_loss: 0.696\n",
      "[epoch: 11, i:  7742]  train_loss: 0.621  |  valid_loss: 0.613\n",
      "[epoch: 11, i:  7829]  train_loss: 0.638  |  valid_loss: 0.521\n",
      "[epoch: 11, i:  7916]  train_loss: 0.689  |  valid_loss: 0.747\n",
      "[epoch: 11, i:  8003]  train_loss: 0.649  |  valid_loss: 0.573\n",
      "[epoch: 11, i:  8090]  train_loss: 0.643  |  valid_loss: 0.512\n",
      "[epoch: 11, i:  8177]  train_loss: 0.760  |  valid_loss: 0.625\n",
      "[epoch: 11, i:  8264]  train_loss: 0.644  |  valid_loss: 0.669\n",
      "[epoch: 11, i:  8351]  train_loss: 0.671  |  valid_loss: 0.668\n",
      "[epoch: 11, i:  8438]  train_loss: 0.621  |  valid_loss: 0.476\n",
      "[epoch: 11, i:  8525]  train_loss: 0.680  |  valid_loss: 0.644\n",
      "[epoch: 11, i:  8612]  train_loss: 0.642  |  valid_loss: 0.867\n",
      "[epoch: 11, i:  8699]  train_loss: 0.676  |  valid_loss: 0.675\n",
      "--> [End of epoch 11] train_accuracy: 77.07%  |  valid_accuracy: 77.41%\n",
      "--> [Start of epoch 12]  lr: 0.000500\n",
      "[epoch: 12, i:    86]  train_loss: 0.680  |  valid_loss: 0.626\n",
      "[epoch: 12, i:   173]  train_loss: 0.593  |  valid_loss: 0.588\n",
      "[epoch: 12, i:   260]  train_loss: 0.547  |  valid_loss: 0.642\n",
      "[epoch: 12, i:   347]  train_loss: 0.650  |  valid_loss: 0.543\n",
      "[epoch: 12, i:   434]  train_loss: 0.656  |  valid_loss: 0.736\n",
      "[epoch: 12, i:   521]  train_loss: 0.676  |  valid_loss: 0.461\n",
      "[epoch: 12, i:   608]  train_loss: 0.607  |  valid_loss: 0.669\n",
      "[epoch: 12, i:   695]  train_loss: 0.634  |  valid_loss: 0.779\n",
      "[epoch: 12, i:   782]  train_loss: 0.586  |  valid_loss: 0.806\n",
      "[epoch: 12, i:   869]  train_loss: 0.670  |  valid_loss: 0.647\n",
      "[epoch: 12, i:   956]  train_loss: 0.586  |  valid_loss: 0.490\n",
      "[epoch: 12, i:  1043]  train_loss: 0.680  |  valid_loss: 0.598\n",
      "[epoch: 12, i:  1130]  train_loss: 0.557  |  valid_loss: 0.556\n",
      "[epoch: 12, i:  1217]  train_loss: 0.558  |  valid_loss: 0.636\n",
      "[epoch: 12, i:  1304]  train_loss: 0.602  |  valid_loss: 0.450\n",
      "[epoch: 12, i:  1391]  train_loss: 0.682  |  valid_loss: 0.694\n",
      "[epoch: 12, i:  1478]  train_loss: 0.701  |  valid_loss: 0.638\n",
      "[epoch: 12, i:  1565]  train_loss: 0.679  |  valid_loss: 0.761\n",
      "[epoch: 12, i:  1652]  train_loss: 0.633  |  valid_loss: 0.651\n",
      "[epoch: 12, i:  1739]  train_loss: 0.718  |  valid_loss: 0.878\n",
      "[epoch: 12, i:  1826]  train_loss: 0.709  |  valid_loss: 0.797\n",
      "[epoch: 12, i:  1913]  train_loss: 0.640  |  valid_loss: 0.658\n",
      "[epoch: 12, i:  2000]  train_loss: 0.669  |  valid_loss: 0.568\n",
      "[epoch: 12, i:  2087]  train_loss: 0.642  |  valid_loss: 0.725\n",
      "[epoch: 12, i:  2174]  train_loss: 0.662  |  valid_loss: 0.584\n",
      "[epoch: 12, i:  2261]  train_loss: 0.654  |  valid_loss: 0.986\n",
      "[epoch: 12, i:  2348]  train_loss: 0.646  |  valid_loss: 0.572\n",
      "[epoch: 12, i:  2435]  train_loss: 0.596  |  valid_loss: 0.693\n",
      "[epoch: 12, i:  2522]  train_loss: 0.645  |  valid_loss: 0.518\n",
      "[epoch: 12, i:  2609]  train_loss: 0.646  |  valid_loss: 0.569\n",
      "[epoch: 12, i:  2696]  train_loss: 0.665  |  valid_loss: 0.762\n",
      "[epoch: 12, i:  2783]  train_loss: 0.576  |  valid_loss: 0.463\n",
      "[epoch: 12, i:  2870]  train_loss: 0.667  |  valid_loss: 0.735\n",
      "[epoch: 12, i:  2957]  train_loss: 0.714  |  valid_loss: 0.786\n",
      "[epoch: 12, i:  3044]  train_loss: 0.639  |  valid_loss: 0.781\n",
      "[epoch: 12, i:  3131]  train_loss: 0.663  |  valid_loss: 0.748\n",
      "[epoch: 12, i:  3218]  train_loss: 0.674  |  valid_loss: 0.718\n",
      "[epoch: 12, i:  3305]  train_loss: 0.643  |  valid_loss: 0.682\n",
      "[epoch: 12, i:  3392]  train_loss: 0.610  |  valid_loss: 0.687\n",
      "[epoch: 12, i:  3479]  train_loss: 0.664  |  valid_loss: 0.462\n",
      "[epoch: 12, i:  3566]  train_loss: 0.613  |  valid_loss: 0.659\n",
      "[epoch: 12, i:  3653]  train_loss: 0.649  |  valid_loss: 0.743\n",
      "[epoch: 12, i:  3740]  train_loss: 0.625  |  valid_loss: 0.431\n",
      "[epoch: 12, i:  3827]  train_loss: 0.652  |  valid_loss: 0.788\n",
      "[epoch: 12, i:  3914]  train_loss: 0.610  |  valid_loss: 0.506\n",
      "[epoch: 12, i:  4001]  train_loss: 0.628  |  valid_loss: 0.667\n",
      "[epoch: 12, i:  4088]  train_loss: 0.661  |  valid_loss: 0.619\n",
      "[epoch: 12, i:  4175]  train_loss: 0.705  |  valid_loss: 0.622\n",
      "[epoch: 12, i:  4262]  train_loss: 0.657  |  valid_loss: 0.553\n",
      "[epoch: 12, i:  4349]  train_loss: 0.624  |  valid_loss: 0.648\n",
      "[epoch: 12, i:  4436]  train_loss: 0.656  |  valid_loss: 0.608\n",
      "[epoch: 12, i:  4523]  train_loss: 0.734  |  valid_loss: 0.800\n",
      "[epoch: 12, i:  4610]  train_loss: 0.685  |  valid_loss: 0.535\n",
      "[epoch: 12, i:  4697]  train_loss: 0.625  |  valid_loss: 0.578\n",
      "[epoch: 12, i:  4784]  train_loss: 0.661  |  valid_loss: 0.686\n",
      "[epoch: 12, i:  4871]  train_loss: 0.594  |  valid_loss: 0.756\n",
      "[epoch: 12, i:  4958]  train_loss: 0.661  |  valid_loss: 0.784\n",
      "[epoch: 12, i:  5045]  train_loss: 0.665  |  valid_loss: 0.506\n",
      "[epoch: 12, i:  5132]  train_loss: 0.692  |  valid_loss: 0.644\n",
      "[epoch: 12, i:  5219]  train_loss: 0.676  |  valid_loss: 0.852\n",
      "[epoch: 12, i:  5306]  train_loss: 0.648  |  valid_loss: 0.729\n",
      "[epoch: 12, i:  5393]  train_loss: 0.644  |  valid_loss: 0.586\n",
      "[epoch: 12, i:  5480]  train_loss: 0.605  |  valid_loss: 0.714\n",
      "[epoch: 12, i:  5567]  train_loss: 0.625  |  valid_loss: 0.409\n",
      "[epoch: 12, i:  5654]  train_loss: 0.669  |  valid_loss: 0.678\n",
      "[epoch: 12, i:  5741]  train_loss: 0.630  |  valid_loss: 0.730\n",
      "[epoch: 12, i:  5828]  train_loss: 0.618  |  valid_loss: 0.645\n",
      "[epoch: 12, i:  5915]  train_loss: 0.656  |  valid_loss: 0.759\n",
      "[epoch: 12, i:  6002]  train_loss: 0.618  |  valid_loss: 0.639\n",
      "[epoch: 12, i:  6089]  train_loss: 0.679  |  valid_loss: 0.832\n",
      "[epoch: 12, i:  6176]  train_loss: 0.686  |  valid_loss: 0.806\n",
      "[epoch: 12, i:  6263]  train_loss: 0.652  |  valid_loss: 0.569\n",
      "[epoch: 12, i:  6350]  train_loss: 0.663  |  valid_loss: 0.644\n",
      "[epoch: 12, i:  6437]  train_loss: 0.640  |  valid_loss: 0.492\n",
      "[epoch: 12, i:  6524]  train_loss: 0.677  |  valid_loss: 0.718\n",
      "[epoch: 12, i:  6611]  train_loss: 0.642  |  valid_loss: 0.472\n",
      "[epoch: 12, i:  6698]  train_loss: 0.682  |  valid_loss: 0.798\n",
      "[epoch: 12, i:  6785]  train_loss: 0.708  |  valid_loss: 0.546\n",
      "[epoch: 12, i:  6872]  train_loss: 0.617  |  valid_loss: 0.774\n",
      "[epoch: 12, i:  6959]  train_loss: 0.690  |  valid_loss: 0.754\n",
      "[epoch: 12, i:  7046]  train_loss: 0.634  |  valid_loss: 0.627\n",
      "[epoch: 12, i:  7133]  train_loss: 0.646  |  valid_loss: 0.568\n",
      "[epoch: 12, i:  7220]  train_loss: 0.701  |  valid_loss: 0.800\n",
      "[epoch: 12, i:  7307]  train_loss: 0.600  |  valid_loss: 0.741\n",
      "[epoch: 12, i:  7394]  train_loss: 0.673  |  valid_loss: 0.597\n",
      "[epoch: 12, i:  7481]  train_loss: 0.646  |  valid_loss: 0.707\n",
      "[epoch: 12, i:  7568]  train_loss: 0.634  |  valid_loss: 0.829\n",
      "[epoch: 12, i:  7655]  train_loss: 0.651  |  valid_loss: 0.644\n",
      "[epoch: 12, i:  7742]  train_loss: 0.609  |  valid_loss: 0.679\n",
      "[epoch: 12, i:  7829]  train_loss: 0.646  |  valid_loss: 0.468\n",
      "[epoch: 12, i:  7916]  train_loss: 0.612  |  valid_loss: 0.776\n",
      "[epoch: 12, i:  8003]  train_loss: 0.572  |  valid_loss: 0.646\n",
      "[epoch: 12, i:  8090]  train_loss: 0.640  |  valid_loss: 0.526\n",
      "[epoch: 12, i:  8177]  train_loss: 0.717  |  valid_loss: 0.585\n",
      "[epoch: 12, i:  8264]  train_loss: 0.673  |  valid_loss: 0.737\n",
      "[epoch: 12, i:  8351]  train_loss: 0.633  |  valid_loss: 0.704\n",
      "[epoch: 12, i:  8438]  train_loss: 0.599  |  valid_loss: 0.488\n",
      "[epoch: 12, i:  8525]  train_loss: 0.659  |  valid_loss: 0.753\n",
      "[epoch: 12, i:  8612]  train_loss: 0.671  |  valid_loss: 0.889\n",
      "[epoch: 12, i:  8699]  train_loss: 0.691  |  valid_loss: 0.571\n",
      "--> [End of epoch 12] train_accuracy: 77.60%  |  valid_accuracy: 77.64%\n",
      "--> [Start of epoch 13]  lr: 0.000500\n",
      "[epoch: 13, i:    86]  train_loss: 0.604  |  valid_loss: 0.664\n",
      "[epoch: 13, i:   173]  train_loss: 0.660  |  valid_loss: 0.588\n",
      "[epoch: 13, i:   260]  train_loss: 0.596  |  valid_loss: 0.726\n",
      "[epoch: 13, i:   347]  train_loss: 0.648  |  valid_loss: 0.583\n",
      "[epoch: 13, i:   434]  train_loss: 0.643  |  valid_loss: 0.763\n",
      "[epoch: 13, i:   521]  train_loss: 0.612  |  valid_loss: 0.485\n",
      "[epoch: 13, i:   608]  train_loss: 0.592  |  valid_loss: 0.616\n",
      "[epoch: 13, i:   695]  train_loss: 0.602  |  valid_loss: 0.686\n",
      "[epoch: 13, i:   782]  train_loss: 0.668  |  valid_loss: 0.731\n",
      "[epoch: 13, i:   869]  train_loss: 0.632  |  valid_loss: 0.699\n",
      "[epoch: 13, i:   956]  train_loss: 0.620  |  valid_loss: 0.515\n",
      "[epoch: 13, i:  1043]  train_loss: 0.607  |  valid_loss: 0.529\n",
      "[epoch: 13, i:  1130]  train_loss: 0.650  |  valid_loss: 0.565\n",
      "[epoch: 13, i:  1217]  train_loss: 0.648  |  valid_loss: 0.620\n",
      "[epoch: 13, i:  1304]  train_loss: 0.666  |  valid_loss: 0.467\n",
      "[epoch: 13, i:  1391]  train_loss: 0.602  |  valid_loss: 0.729\n",
      "[epoch: 13, i:  1478]  train_loss: 0.603  |  valid_loss: 0.588\n",
      "[epoch: 13, i:  1565]  train_loss: 0.661  |  valid_loss: 0.792\n",
      "[epoch: 13, i:  1652]  train_loss: 0.591  |  valid_loss: 0.628\n",
      "[epoch: 13, i:  1739]  train_loss: 0.593  |  valid_loss: 0.747\n",
      "[epoch: 13, i:  1826]  train_loss: 0.672  |  valid_loss: 0.838\n",
      "[epoch: 13, i:  1913]  train_loss: 0.597  |  valid_loss: 0.657\n",
      "[epoch: 13, i:  2000]  train_loss: 0.712  |  valid_loss: 0.601\n",
      "[epoch: 13, i:  2087]  train_loss: 0.643  |  valid_loss: 0.763\n",
      "[epoch: 13, i:  2174]  train_loss: 0.647  |  valid_loss: 0.508\n",
      "[epoch: 13, i:  2261]  train_loss: 0.707  |  valid_loss: 0.960\n",
      "[epoch: 13, i:  2348]  train_loss: 0.581  |  valid_loss: 0.636\n",
      "[epoch: 13, i:  2435]  train_loss: 0.577  |  valid_loss: 0.766\n",
      "[epoch: 13, i:  2522]  train_loss: 0.608  |  valid_loss: 0.554\n",
      "[epoch: 13, i:  2609]  train_loss: 0.591  |  valid_loss: 0.560\n",
      "[epoch: 13, i:  2696]  train_loss: 0.650  |  valid_loss: 0.786\n",
      "[epoch: 13, i:  2783]  train_loss: 0.685  |  valid_loss: 0.511\n",
      "[epoch: 13, i:  2870]  train_loss: 0.672  |  valid_loss: 0.724\n",
      "[epoch: 13, i:  2957]  train_loss: 0.625  |  valid_loss: 0.726\n",
      "[epoch: 13, i:  3044]  train_loss: 0.694  |  valid_loss: 0.735\n",
      "[epoch: 13, i:  3131]  train_loss: 0.649  |  valid_loss: 0.699\n",
      "[epoch: 13, i:  3218]  train_loss: 0.632  |  valid_loss: 0.733\n",
      "[epoch: 13, i:  3305]  train_loss: 0.606  |  valid_loss: 0.681\n",
      "[epoch: 13, i:  3392]  train_loss: 0.615  |  valid_loss: 0.626\n",
      "[epoch: 13, i:  3479]  train_loss: 0.586  |  valid_loss: 0.503\n",
      "[epoch: 13, i:  3566]  train_loss: 0.689  |  valid_loss: 0.593\n",
      "[epoch: 13, i:  3653]  train_loss: 0.627  |  valid_loss: 0.720\n",
      "[epoch: 13, i:  3740]  train_loss: 0.705  |  valid_loss: 0.439\n",
      "[epoch: 13, i:  3827]  train_loss: 0.639  |  valid_loss: 0.739\n",
      "[epoch: 13, i:  3914]  train_loss: 0.643  |  valid_loss: 0.542\n",
      "[epoch: 13, i:  4001]  train_loss: 0.630  |  valid_loss: 0.627\n",
      "[epoch: 13, i:  4088]  train_loss: 0.723  |  valid_loss: 0.644\n",
      "[epoch: 13, i:  4175]  train_loss: 0.589  |  valid_loss: 0.568\n",
      "[epoch: 13, i:  4262]  train_loss: 0.635  |  valid_loss: 0.543\n",
      "[epoch: 13, i:  4349]  train_loss: 0.674  |  valid_loss: 0.595\n",
      "[epoch: 13, i:  4436]  train_loss: 0.641  |  valid_loss: 0.487\n",
      "[epoch: 13, i:  4523]  train_loss: 0.585  |  valid_loss: 0.785\n",
      "[epoch: 13, i:  4610]  train_loss: 0.667  |  valid_loss: 0.572\n",
      "[epoch: 13, i:  4697]  train_loss: 0.663  |  valid_loss: 0.595\n",
      "[epoch: 13, i:  4784]  train_loss: 0.606  |  valid_loss: 0.650\n",
      "[epoch: 13, i:  4871]  train_loss: 0.617  |  valid_loss: 0.783\n",
      "[epoch: 13, i:  4958]  train_loss: 0.652  |  valid_loss: 0.827\n",
      "[epoch: 13, i:  5045]  train_loss: 0.698  |  valid_loss: 0.523\n",
      "[epoch: 13, i:  5132]  train_loss: 0.601  |  valid_loss: 0.681\n",
      "[epoch: 13, i:  5219]  train_loss: 0.602  |  valid_loss: 0.814\n",
      "[epoch: 13, i:  5306]  train_loss: 0.716  |  valid_loss: 0.734\n",
      "[epoch: 13, i:  5393]  train_loss: 0.655  |  valid_loss: 0.620\n",
      "[epoch: 13, i:  5480]  train_loss: 0.640  |  valid_loss: 0.868\n",
      "[epoch: 13, i:  5567]  train_loss: 0.644  |  valid_loss: 0.477\n",
      "[epoch: 13, i:  5654]  train_loss: 0.685  |  valid_loss: 0.581\n",
      "[epoch: 13, i:  5741]  train_loss: 0.599  |  valid_loss: 0.671\n",
      "[epoch: 13, i:  5828]  train_loss: 0.649  |  valid_loss: 0.625\n",
      "[epoch: 13, i:  5915]  train_loss: 0.718  |  valid_loss: 0.697\n",
      "[epoch: 13, i:  6002]  train_loss: 0.705  |  valid_loss: 0.639\n",
      "[epoch: 13, i:  6089]  train_loss: 0.730  |  valid_loss: 0.834\n",
      "[epoch: 13, i:  6176]  train_loss: 0.638  |  valid_loss: 0.709\n",
      "[epoch: 13, i:  6263]  train_loss: 0.627  |  valid_loss: 0.603\n",
      "[epoch: 13, i:  6350]  train_loss: 0.690  |  valid_loss: 0.642\n",
      "[epoch: 13, i:  6437]  train_loss: 0.574  |  valid_loss: 0.414\n",
      "[epoch: 13, i:  6524]  train_loss: 0.690  |  valid_loss: 0.751\n",
      "[epoch: 13, i:  6611]  train_loss: 0.649  |  valid_loss: 0.488\n",
      "[epoch: 13, i:  6698]  train_loss: 0.617  |  valid_loss: 0.739\n",
      "[epoch: 13, i:  6785]  train_loss: 0.656  |  valid_loss: 0.595\n",
      "[epoch: 13, i:  6872]  train_loss: 0.566  |  valid_loss: 0.717\n",
      "[epoch: 13, i:  6959]  train_loss: 0.639  |  valid_loss: 0.817\n",
      "[epoch: 13, i:  7046]  train_loss: 0.612  |  valid_loss: 0.671\n",
      "[epoch: 13, i:  7133]  train_loss: 0.672  |  valid_loss: 0.612\n",
      "[epoch: 13, i:  7220]  train_loss: 0.643  |  valid_loss: 0.689\n",
      "[epoch: 13, i:  7307]  train_loss: 0.606  |  valid_loss: 0.752\n",
      "[epoch: 13, i:  7394]  train_loss: 0.606  |  valid_loss: 0.594\n",
      "[epoch: 13, i:  7481]  train_loss: 0.602  |  valid_loss: 0.625\n",
      "[epoch: 13, i:  7568]  train_loss: 0.595  |  valid_loss: 0.845\n",
      "[epoch: 13, i:  7655]  train_loss: 0.688  |  valid_loss: 0.696\n",
      "[epoch: 13, i:  7742]  train_loss: 0.615  |  valid_loss: 0.758\n",
      "[epoch: 13, i:  7829]  train_loss: 0.592  |  valid_loss: 0.474\n",
      "[epoch: 13, i:  7916]  train_loss: 0.653  |  valid_loss: 0.726\n",
      "[epoch: 13, i:  8003]  train_loss: 0.734  |  valid_loss: 0.578\n",
      "[epoch: 13, i:  8090]  train_loss: 0.565  |  valid_loss: 0.523\n",
      "[epoch: 13, i:  8177]  train_loss: 0.639  |  valid_loss: 0.570\n",
      "[epoch: 13, i:  8264]  train_loss: 0.681  |  valid_loss: 0.740\n",
      "[epoch: 13, i:  8351]  train_loss: 0.621  |  valid_loss: 0.832\n",
      "[epoch: 13, i:  8438]  train_loss: 0.659  |  valid_loss: 0.611\n",
      "[epoch: 13, i:  8525]  train_loss: 0.701  |  valid_loss: 0.671\n",
      "[epoch: 13, i:  8612]  train_loss: 0.661  |  valid_loss: 0.900\n",
      "[epoch: 13, i:  8699]  train_loss: 0.713  |  valid_loss: 0.550\n",
      "--> [End of epoch 13] train_accuracy: 77.71%  |  valid_accuracy: 77.84%\n",
      "--> [Start of epoch 14]  lr: 0.000500\n",
      "[epoch: 14, i:    86]  train_loss: 0.626  |  valid_loss: 0.590\n",
      "[epoch: 14, i:   173]  train_loss: 0.603  |  valid_loss: 0.569\n",
      "[epoch: 14, i:   260]  train_loss: 0.613  |  valid_loss: 0.739\n",
      "[epoch: 14, i:   347]  train_loss: 0.580  |  valid_loss: 0.673\n",
      "[epoch: 14, i:   434]  train_loss: 0.551  |  valid_loss: 0.667\n",
      "[epoch: 14, i:   521]  train_loss: 0.669  |  valid_loss: 0.467\n",
      "[epoch: 14, i:   608]  train_loss: 0.631  |  valid_loss: 0.755\n",
      "[epoch: 14, i:   695]  train_loss: 0.647  |  valid_loss: 0.666\n",
      "[epoch: 14, i:   782]  train_loss: 0.601  |  valid_loss: 0.762\n",
      "[epoch: 14, i:   869]  train_loss: 0.636  |  valid_loss: 0.707\n",
      "[epoch: 14, i:   956]  train_loss: 0.594  |  valid_loss: 0.575\n",
      "[epoch: 14, i:  1043]  train_loss: 0.581  |  valid_loss: 0.632\n",
      "[epoch: 14, i:  1130]  train_loss: 0.660  |  valid_loss: 0.607\n",
      "[epoch: 14, i:  1217]  train_loss: 0.607  |  valid_loss: 0.709\n",
      "[epoch: 14, i:  1304]  train_loss: 0.540  |  valid_loss: 0.472\n",
      "[epoch: 14, i:  1391]  train_loss: 0.568  |  valid_loss: 0.723\n",
      "[epoch: 14, i:  1478]  train_loss: 0.590  |  valid_loss: 0.602\n",
      "[epoch: 14, i:  1565]  train_loss: 0.706  |  valid_loss: 0.712\n",
      "[epoch: 14, i:  1652]  train_loss: 0.550  |  valid_loss: 0.561\n",
      "[epoch: 14, i:  1739]  train_loss: 0.635  |  valid_loss: 0.817\n",
      "[epoch: 14, i:  1826]  train_loss: 0.625  |  valid_loss: 0.798\n",
      "[epoch: 14, i:  1913]  train_loss: 0.676  |  valid_loss: 0.659\n",
      "[epoch: 14, i:  2000]  train_loss: 0.636  |  valid_loss: 0.625\n",
      "[epoch: 14, i:  2087]  train_loss: 0.606  |  valid_loss: 0.773\n",
      "[epoch: 14, i:  2174]  train_loss: 0.590  |  valid_loss: 0.588\n",
      "[epoch: 14, i:  2261]  train_loss: 0.609  |  valid_loss: 0.972\n",
      "[epoch: 14, i:  2348]  train_loss: 0.591  |  valid_loss: 0.640\n",
      "[epoch: 14, i:  2435]  train_loss: 0.645  |  valid_loss: 0.667\n",
      "[epoch: 14, i:  2522]  train_loss: 0.666  |  valid_loss: 0.587\n",
      "[epoch: 14, i:  2609]  train_loss: 0.632  |  valid_loss: 0.578\n",
      "[epoch: 14, i:  2696]  train_loss: 0.562  |  valid_loss: 0.682\n",
      "[epoch: 14, i:  2783]  train_loss: 0.707  |  valid_loss: 0.583\n",
      "[epoch: 14, i:  2870]  train_loss: 0.645  |  valid_loss: 0.642\n",
      "[epoch: 14, i:  2957]  train_loss: 0.631  |  valid_loss: 0.781\n",
      "[epoch: 14, i:  3044]  train_loss: 0.590  |  valid_loss: 0.752\n",
      "[epoch: 14, i:  3131]  train_loss: 0.639  |  valid_loss: 0.722\n",
      "[epoch: 14, i:  3218]  train_loss: 0.608  |  valid_loss: 0.758\n",
      "[epoch: 14, i:  3305]  train_loss: 0.643  |  valid_loss: 0.603\n",
      "[epoch: 14, i:  3392]  train_loss: 0.580  |  valid_loss: 0.693\n",
      "[epoch: 14, i:  3479]  train_loss: 0.647  |  valid_loss: 0.519\n",
      "[epoch: 14, i:  3566]  train_loss: 0.625  |  valid_loss: 0.596\n",
      "[epoch: 14, i:  3653]  train_loss: 0.551  |  valid_loss: 0.697\n",
      "[epoch: 14, i:  3740]  train_loss: 0.626  |  valid_loss: 0.424\n",
      "[epoch: 14, i:  3827]  train_loss: 0.632  |  valid_loss: 0.820\n",
      "[epoch: 14, i:  3914]  train_loss: 0.579  |  valid_loss: 0.553\n",
      "[epoch: 14, i:  4001]  train_loss: 0.559  |  valid_loss: 0.690\n",
      "[epoch: 14, i:  4088]  train_loss: 0.588  |  valid_loss: 0.674\n",
      "[epoch: 14, i:  4175]  train_loss: 0.616  |  valid_loss: 0.630\n",
      "[epoch: 14, i:  4262]  train_loss: 0.684  |  valid_loss: 0.533\n",
      "[epoch: 14, i:  4349]  train_loss: 0.630  |  valid_loss: 0.576\n",
      "[epoch: 14, i:  4436]  train_loss: 0.615  |  valid_loss: 0.555\n",
      "[epoch: 14, i:  4523]  train_loss: 0.652  |  valid_loss: 0.690\n",
      "[epoch: 14, i:  4610]  train_loss: 0.659  |  valid_loss: 0.533\n",
      "[epoch: 14, i:  4697]  train_loss: 0.623  |  valid_loss: 0.554\n",
      "[epoch: 14, i:  4784]  train_loss: 0.704  |  valid_loss: 0.679\n",
      "[epoch: 14, i:  4871]  train_loss: 0.598  |  valid_loss: 0.745\n",
      "[epoch: 14, i:  4958]  train_loss: 0.666  |  valid_loss: 0.839\n",
      "[epoch: 14, i:  5045]  train_loss: 0.686  |  valid_loss: 0.432\n",
      "[epoch: 14, i:  5132]  train_loss: 0.642  |  valid_loss: 0.611\n",
      "[epoch: 14, i:  5219]  train_loss: 0.584  |  valid_loss: 0.875\n",
      "[epoch: 14, i:  5306]  train_loss: 0.699  |  valid_loss: 0.730\n",
      "[epoch: 14, i:  5393]  train_loss: 0.605  |  valid_loss: 0.593\n",
      "[epoch: 14, i:  5480]  train_loss: 0.633  |  valid_loss: 0.824\n",
      "[epoch: 14, i:  5567]  train_loss: 0.634  |  valid_loss: 0.319\n",
      "[epoch: 14, i:  5654]  train_loss: 0.593  |  valid_loss: 0.590\n",
      "[epoch: 14, i:  5741]  train_loss: 0.705  |  valid_loss: 0.663\n",
      "[epoch: 14, i:  5828]  train_loss: 0.632  |  valid_loss: 0.561\n",
      "[epoch: 14, i:  5915]  train_loss: 0.663  |  valid_loss: 0.744\n",
      "[epoch: 14, i:  6002]  train_loss: 0.587  |  valid_loss: 0.621\n",
      "[epoch: 14, i:  6089]  train_loss: 0.592  |  valid_loss: 0.827\n",
      "[epoch: 14, i:  6176]  train_loss: 0.612  |  valid_loss: 0.707\n",
      "[epoch: 14, i:  6263]  train_loss: 0.640  |  valid_loss: 0.513\n",
      "[epoch: 14, i:  6350]  train_loss: 0.625  |  valid_loss: 0.583\n",
      "[epoch: 14, i:  6437]  train_loss: 0.668  |  valid_loss: 0.472\n",
      "[epoch: 14, i:  6524]  train_loss: 0.633  |  valid_loss: 0.862\n",
      "[epoch: 14, i:  6611]  train_loss: 0.649  |  valid_loss: 0.444\n",
      "[epoch: 14, i:  6698]  train_loss: 0.631  |  valid_loss: 0.633\n",
      "[epoch: 14, i:  6785]  train_loss: 0.573  |  valid_loss: 0.551\n",
      "[epoch: 14, i:  6872]  train_loss: 0.643  |  valid_loss: 0.817\n",
      "[epoch: 14, i:  6959]  train_loss: 0.649  |  valid_loss: 0.863\n",
      "[epoch: 14, i:  7046]  train_loss: 0.660  |  valid_loss: 0.629\n",
      "[epoch: 14, i:  7133]  train_loss: 0.672  |  valid_loss: 0.578\n",
      "[epoch: 14, i:  7220]  train_loss: 0.624  |  valid_loss: 0.749\n",
      "[epoch: 14, i:  7307]  train_loss: 0.672  |  valid_loss: 0.804\n",
      "[epoch: 14, i:  7394]  train_loss: 0.602  |  valid_loss: 0.634\n",
      "[epoch: 14, i:  7481]  train_loss: 0.667  |  valid_loss: 0.651\n",
      "[epoch: 14, i:  7568]  train_loss: 0.644  |  valid_loss: 0.813\n",
      "[epoch: 14, i:  7655]  train_loss: 0.630  |  valid_loss: 0.697\n",
      "[epoch: 14, i:  7742]  train_loss: 0.672  |  valid_loss: 0.620\n",
      "[epoch: 14, i:  7829]  train_loss: 0.678  |  valid_loss: 0.468\n",
      "[epoch: 14, i:  7916]  train_loss: 0.549  |  valid_loss: 0.779\n",
      "[epoch: 14, i:  8003]  train_loss: 0.590  |  valid_loss: 0.610\n",
      "[epoch: 14, i:  8090]  train_loss: 0.658  |  valid_loss: 0.483\n",
      "[epoch: 14, i:  8177]  train_loss: 0.692  |  valid_loss: 0.605\n",
      "[epoch: 14, i:  8264]  train_loss: 0.589  |  valid_loss: 0.734\n",
      "[epoch: 14, i:  8351]  train_loss: 0.677  |  valid_loss: 0.719\n",
      "[epoch: 14, i:  8438]  train_loss: 0.650  |  valid_loss: 0.516\n",
      "[epoch: 14, i:  8525]  train_loss: 0.609  |  valid_loss: 0.702\n",
      "[epoch: 14, i:  8612]  train_loss: 0.620  |  valid_loss: 0.893\n",
      "[epoch: 14, i:  8699]  train_loss: 0.683  |  valid_loss: 0.637\n",
      "--> [End of epoch 14] train_accuracy: 78.05%  |  valid_accuracy: 77.99%\n",
      "--> [Start of epoch 15]  lr: 0.000500\n",
      "[epoch: 15, i:    86]  train_loss: 0.652  |  valid_loss: 0.601\n",
      "[epoch: 15, i:   173]  train_loss: 0.606  |  valid_loss: 0.610\n",
      "[epoch: 15, i:   260]  train_loss: 0.680  |  valid_loss: 0.686\n",
      "[epoch: 15, i:   347]  train_loss: 0.543  |  valid_loss: 0.599\n",
      "[epoch: 15, i:   434]  train_loss: 0.574  |  valid_loss: 0.637\n",
      "[epoch: 15, i:   521]  train_loss: 0.556  |  valid_loss: 0.468\n",
      "[epoch: 15, i:   608]  train_loss: 0.649  |  valid_loss: 0.686\n",
      "[epoch: 15, i:   695]  train_loss: 0.612  |  valid_loss: 0.652\n",
      "[epoch: 15, i:   782]  train_loss: 0.554  |  valid_loss: 0.642\n",
      "[epoch: 15, i:   869]  train_loss: 0.557  |  valid_loss: 0.628\n",
      "[epoch: 15, i:   956]  train_loss: 0.585  |  valid_loss: 0.520\n",
      "[epoch: 15, i:  1043]  train_loss: 0.669  |  valid_loss: 0.632\n",
      "[epoch: 15, i:  1130]  train_loss: 0.648  |  valid_loss: 0.561\n",
      "[epoch: 15, i:  1217]  train_loss: 0.582  |  valid_loss: 0.647\n",
      "[epoch: 15, i:  1304]  train_loss: 0.592  |  valid_loss: 0.508\n",
      "[epoch: 15, i:  1391]  train_loss: 0.645  |  valid_loss: 0.648\n",
      "[epoch: 15, i:  1478]  train_loss: 0.563  |  valid_loss: 0.638\n",
      "[epoch: 15, i:  1565]  train_loss: 0.511  |  valid_loss: 0.809\n",
      "[epoch: 15, i:  1652]  train_loss: 0.581  |  valid_loss: 0.623\n",
      "[epoch: 15, i:  1739]  train_loss: 0.692  |  valid_loss: 0.882\n",
      "[epoch: 15, i:  1826]  train_loss: 0.615  |  valid_loss: 0.806\n",
      "[epoch: 15, i:  1913]  train_loss: 0.573  |  valid_loss: 0.695\n",
      "[epoch: 15, i:  2000]  train_loss: 0.699  |  valid_loss: 0.629\n",
      "[epoch: 15, i:  2087]  train_loss: 0.608  |  valid_loss: 0.763\n",
      "[epoch: 15, i:  2174]  train_loss: 0.674  |  valid_loss: 0.552\n",
      "[epoch: 15, i:  2261]  train_loss: 0.612  |  valid_loss: 1.044\n",
      "[epoch: 15, i:  2348]  train_loss: 0.603  |  valid_loss: 0.564\n",
      "[epoch: 15, i:  2435]  train_loss: 0.639  |  valid_loss: 0.712\n",
      "[epoch: 15, i:  2522]  train_loss: 0.634  |  valid_loss: 0.592\n",
      "[epoch: 15, i:  2609]  train_loss: 0.618  |  valid_loss: 0.588\n",
      "[epoch: 15, i:  2696]  train_loss: 0.634  |  valid_loss: 0.710\n",
      "[epoch: 15, i:  2783]  train_loss: 0.567  |  valid_loss: 0.538\n",
      "[epoch: 15, i:  2870]  train_loss: 0.615  |  valid_loss: 0.743\n",
      "[epoch: 15, i:  2957]  train_loss: 0.639  |  valid_loss: 0.743\n",
      "[epoch: 15, i:  3044]  train_loss: 0.562  |  valid_loss: 0.712\n",
      "[epoch: 15, i:  3131]  train_loss: 0.557  |  valid_loss: 0.700\n",
      "[epoch: 15, i:  3218]  train_loss: 0.562  |  valid_loss: 0.765\n",
      "[epoch: 15, i:  3305]  train_loss: 0.636  |  valid_loss: 0.621\n",
      "[epoch: 15, i:  3392]  train_loss: 0.706  |  valid_loss: 0.708\n",
      "[epoch: 15, i:  3479]  train_loss: 0.607  |  valid_loss: 0.467\n",
      "[epoch: 15, i:  3566]  train_loss: 0.636  |  valid_loss: 0.651\n",
      "[epoch: 15, i:  3653]  train_loss: 0.642  |  valid_loss: 0.699\n",
      "[epoch: 15, i:  3740]  train_loss: 0.613  |  valid_loss: 0.460\n",
      "[epoch: 15, i:  3827]  train_loss: 0.626  |  valid_loss: 0.733\n",
      "[epoch: 15, i:  3914]  train_loss: 0.707  |  valid_loss: 0.472\n",
      "[epoch: 15, i:  4001]  train_loss: 0.598  |  valid_loss: 0.656\n",
      "[epoch: 15, i:  4088]  train_loss: 0.688  |  valid_loss: 0.589\n",
      "[epoch: 15, i:  4175]  train_loss: 0.650  |  valid_loss: 0.586\n",
      "[epoch: 15, i:  4262]  train_loss: 0.652  |  valid_loss: 0.548\n",
      "[epoch: 15, i:  4349]  train_loss: 0.613  |  valid_loss: 0.627\n",
      "[epoch: 15, i:  4436]  train_loss: 0.612  |  valid_loss: 0.578\n",
      "[epoch: 15, i:  4523]  train_loss: 0.630  |  valid_loss: 0.684\n",
      "[epoch: 15, i:  4610]  train_loss: 0.632  |  valid_loss: 0.542\n",
      "[epoch: 15, i:  4697]  train_loss: 0.670  |  valid_loss: 0.574\n",
      "[epoch: 15, i:  4784]  train_loss: 0.671  |  valid_loss: 0.686\n",
      "[epoch: 15, i:  4871]  train_loss: 0.643  |  valid_loss: 0.728\n",
      "[epoch: 15, i:  4958]  train_loss: 0.613  |  valid_loss: 0.816\n",
      "[epoch: 15, i:  5045]  train_loss: 0.658  |  valid_loss: 0.558\n",
      "[epoch: 15, i:  5132]  train_loss: 0.582  |  valid_loss: 0.658\n",
      "[epoch: 15, i:  5219]  train_loss: 0.681  |  valid_loss: 0.803\n",
      "[epoch: 15, i:  5306]  train_loss: 0.626  |  valid_loss: 0.827\n",
      "[epoch: 15, i:  5393]  train_loss: 0.615  |  valid_loss: 0.611\n",
      "[epoch: 15, i:  5480]  train_loss: 0.584  |  valid_loss: 0.752\n",
      "[epoch: 15, i:  5567]  train_loss: 0.580  |  valid_loss: 0.393\n",
      "[epoch: 15, i:  5654]  train_loss: 0.610  |  valid_loss: 0.650\n",
      "[epoch: 15, i:  5741]  train_loss: 0.678  |  valid_loss: 0.648\n",
      "[epoch: 15, i:  5828]  train_loss: 0.630  |  valid_loss: 0.537\n",
      "[epoch: 15, i:  5915]  train_loss: 0.488  |  valid_loss: 0.691\n",
      "[epoch: 15, i:  6002]  train_loss: 0.565  |  valid_loss: 0.632\n",
      "[epoch: 15, i:  6089]  train_loss: 0.593  |  valid_loss: 0.761\n",
      "[epoch: 15, i:  6176]  train_loss: 0.634  |  valid_loss: 0.706\n",
      "[epoch: 15, i:  6263]  train_loss: 0.647  |  valid_loss: 0.527\n",
      "[epoch: 15, i:  6350]  train_loss: 0.611  |  valid_loss: 0.584\n",
      "[epoch: 15, i:  6437]  train_loss: 0.576  |  valid_loss: 0.467\n",
      "[epoch: 15, i:  6524]  train_loss: 0.690  |  valid_loss: 0.825\n",
      "[epoch: 15, i:  6611]  train_loss: 0.616  |  valid_loss: 0.484\n",
      "[epoch: 15, i:  6698]  train_loss: 0.615  |  valid_loss: 0.751\n",
      "[epoch: 15, i:  6785]  train_loss: 0.588  |  valid_loss: 0.494\n",
      "[epoch: 15, i:  6872]  train_loss: 0.597  |  valid_loss: 0.839\n",
      "[epoch: 15, i:  6959]  train_loss: 0.606  |  valid_loss: 0.854\n",
      "[epoch: 15, i:  7046]  train_loss: 0.675  |  valid_loss: 0.675\n",
      "[epoch: 15, i:  7133]  train_loss: 0.676  |  valid_loss: 0.603\n",
      "[epoch: 15, i:  7220]  train_loss: 0.667  |  valid_loss: 0.687\n",
      "[epoch: 15, i:  7307]  train_loss: 0.630  |  valid_loss: 0.756\n",
      "[epoch: 15, i:  7394]  train_loss: 0.592  |  valid_loss: 0.623\n",
      "[epoch: 15, i:  7481]  train_loss: 0.636  |  valid_loss: 0.729\n",
      "[epoch: 15, i:  7568]  train_loss: 0.604  |  valid_loss: 0.812\n",
      "[epoch: 15, i:  7655]  train_loss: 0.618  |  valid_loss: 0.638\n",
      "[epoch: 15, i:  7742]  train_loss: 0.657  |  valid_loss: 0.737\n",
      "[epoch: 15, i:  7829]  train_loss: 0.645  |  valid_loss: 0.502\n",
      "[epoch: 15, i:  7916]  train_loss: 0.643  |  valid_loss: 0.675\n",
      "[epoch: 15, i:  8003]  train_loss: 0.612  |  valid_loss: 0.589\n",
      "[epoch: 15, i:  8090]  train_loss: 0.625  |  valid_loss: 0.478\n",
      "[epoch: 15, i:  8177]  train_loss: 0.670  |  valid_loss: 0.582\n",
      "[epoch: 15, i:  8264]  train_loss: 0.655  |  valid_loss: 0.733\n",
      "[epoch: 15, i:  8351]  train_loss: 0.550  |  valid_loss: 0.765\n",
      "[epoch: 15, i:  8438]  train_loss: 0.662  |  valid_loss: 0.537\n",
      "[epoch: 15, i:  8525]  train_loss: 0.627  |  valid_loss: 0.722\n",
      "[epoch: 15, i:  8612]  train_loss: 0.613  |  valid_loss: 0.960\n",
      "[epoch: 15, i:  8699]  train_loss: 0.617  |  valid_loss: 0.662\n",
      "--> [End of epoch 15] train_accuracy: 78.45%  |  valid_accuracy: 77.93%\n",
      "--> [Start of epoch 16]  lr: 0.000500\n",
      "[epoch: 16, i:    86]  train_loss: 0.608  |  valid_loss: 0.576\n",
      "[epoch: 16, i:   173]  train_loss: 0.613  |  valid_loss: 0.594\n",
      "[epoch: 16, i:   260]  train_loss: 0.535  |  valid_loss: 0.731\n",
      "[epoch: 16, i:   347]  train_loss: 0.592  |  valid_loss: 0.562\n",
      "[epoch: 16, i:   434]  train_loss: 0.615  |  valid_loss: 0.689\n",
      "[epoch: 16, i:   521]  train_loss: 0.547  |  valid_loss: 0.387\n",
      "[epoch: 16, i:   608]  train_loss: 0.645  |  valid_loss: 0.720\n",
      "[epoch: 16, i:   695]  train_loss: 0.626  |  valid_loss: 0.648\n",
      "[epoch: 16, i:   782]  train_loss: 0.622  |  valid_loss: 0.700\n",
      "[epoch: 16, i:   869]  train_loss: 0.583  |  valid_loss: 0.621\n",
      "[epoch: 16, i:   956]  train_loss: 0.609  |  valid_loss: 0.483\n",
      "[epoch: 16, i:  1043]  train_loss: 0.659  |  valid_loss: 0.659\n",
      "[epoch: 16, i:  1130]  train_loss: 0.634  |  valid_loss: 0.598\n",
      "[epoch: 16, i:  1217]  train_loss: 0.644  |  valid_loss: 0.639\n",
      "[epoch: 16, i:  1304]  train_loss: 0.573  |  valid_loss: 0.403\n",
      "[epoch: 16, i:  1391]  train_loss: 0.623  |  valid_loss: 0.672\n",
      "[epoch: 16, i:  1478]  train_loss: 0.676  |  valid_loss: 0.656\n",
      "[epoch: 16, i:  1565]  train_loss: 0.592  |  valid_loss: 0.719\n",
      "[epoch: 16, i:  1652]  train_loss: 0.554  |  valid_loss: 0.607\n",
      "[epoch: 16, i:  1739]  train_loss: 0.590  |  valid_loss: 0.853\n",
      "[epoch: 16, i:  1826]  train_loss: 0.562  |  valid_loss: 0.729\n",
      "[epoch: 16, i:  1913]  train_loss: 0.624  |  valid_loss: 0.574\n",
      "[epoch: 16, i:  2000]  train_loss: 0.607  |  valid_loss: 0.655\n",
      "[epoch: 16, i:  2087]  train_loss: 0.562  |  valid_loss: 0.844\n",
      "[epoch: 16, i:  2174]  train_loss: 0.518  |  valid_loss: 0.548\n",
      "[epoch: 16, i:  2261]  train_loss: 0.678  |  valid_loss: 0.920\n",
      "[epoch: 16, i:  2348]  train_loss: 0.586  |  valid_loss: 0.561\n",
      "[epoch: 16, i:  2435]  train_loss: 0.613  |  valid_loss: 0.706\n",
      "[epoch: 16, i:  2522]  train_loss: 0.556  |  valid_loss: 0.595\n",
      "[epoch: 16, i:  2609]  train_loss: 0.660  |  valid_loss: 0.591\n",
      "[epoch: 16, i:  2696]  train_loss: 0.583  |  valid_loss: 0.690\n",
      "[epoch: 16, i:  2783]  train_loss: 0.677  |  valid_loss: 0.502\n",
      "[epoch: 16, i:  2870]  train_loss: 0.604  |  valid_loss: 0.673\n",
      "[epoch: 16, i:  2957]  train_loss: 0.544  |  valid_loss: 0.792\n",
      "[epoch: 16, i:  3044]  train_loss: 0.581  |  valid_loss: 0.749\n",
      "[epoch: 16, i:  3131]  train_loss: 0.589  |  valid_loss: 0.610\n",
      "[epoch: 16, i:  3218]  train_loss: 0.645  |  valid_loss: 0.721\n",
      "[epoch: 16, i:  3305]  train_loss: 0.671  |  valid_loss: 0.720\n",
      "[epoch: 16, i:  3392]  train_loss: 0.571  |  valid_loss: 0.626\n",
      "[epoch: 16, i:  3479]  train_loss: 0.526  |  valid_loss: 0.483\n",
      "[epoch: 16, i:  3566]  train_loss: 0.593  |  valid_loss: 0.620\n",
      "[epoch: 16, i:  3653]  train_loss: 0.625  |  valid_loss: 0.700\n",
      "[epoch: 16, i:  3740]  train_loss: 0.589  |  valid_loss: 0.444\n",
      "[epoch: 16, i:  3827]  train_loss: 0.653  |  valid_loss: 0.776\n",
      "[epoch: 16, i:  3914]  train_loss: 0.619  |  valid_loss: 0.476\n",
      "[epoch: 16, i:  4001]  train_loss: 0.621  |  valid_loss: 0.640\n",
      "[epoch: 16, i:  4088]  train_loss: 0.609  |  valid_loss: 0.678\n",
      "[epoch: 16, i:  4175]  train_loss: 0.669  |  valid_loss: 0.617\n",
      "[epoch: 16, i:  4262]  train_loss: 0.630  |  valid_loss: 0.528\n",
      "[epoch: 16, i:  4349]  train_loss: 0.604  |  valid_loss: 0.584\n",
      "[epoch: 16, i:  4436]  train_loss: 0.596  |  valid_loss: 0.575\n",
      "[epoch: 16, i:  4523]  train_loss: 0.627  |  valid_loss: 0.790\n",
      "[epoch: 16, i:  4610]  train_loss: 0.620  |  valid_loss: 0.498\n",
      "[epoch: 16, i:  4697]  train_loss: 0.569  |  valid_loss: 0.597\n",
      "[epoch: 16, i:  4784]  train_loss: 0.543  |  valid_loss: 0.720\n",
      "[epoch: 16, i:  4871]  train_loss: 0.644  |  valid_loss: 0.764\n",
      "[epoch: 16, i:  4958]  train_loss: 0.626  |  valid_loss: 0.794\n",
      "[epoch: 16, i:  5045]  train_loss: 0.638  |  valid_loss: 0.526\n",
      "[epoch: 16, i:  5132]  train_loss: 0.582  |  valid_loss: 0.615\n",
      "[epoch: 16, i:  5219]  train_loss: 0.608  |  valid_loss: 0.748\n",
      "[epoch: 16, i:  5306]  train_loss: 0.609  |  valid_loss: 0.841\n",
      "[epoch: 16, i:  5393]  train_loss: 0.612  |  valid_loss: 0.566\n",
      "[epoch: 16, i:  5480]  train_loss: 0.688  |  valid_loss: 0.778\n",
      "[epoch: 16, i:  5567]  train_loss: 0.633  |  valid_loss: 0.418\n",
      "[epoch: 16, i:  5654]  train_loss: 0.610  |  valid_loss: 0.628\n",
      "[epoch: 16, i:  5741]  train_loss: 0.670  |  valid_loss: 0.724\n",
      "[epoch: 16, i:  5828]  train_loss: 0.606  |  valid_loss: 0.604\n",
      "[epoch: 16, i:  5915]  train_loss: 0.600  |  valid_loss: 0.749\n",
      "[epoch: 16, i:  6002]  train_loss: 0.541  |  valid_loss: 0.619\n",
      "[epoch: 16, i:  6089]  train_loss: 0.565  |  valid_loss: 0.802\n",
      "[epoch: 16, i:  6176]  train_loss: 0.611  |  valid_loss: 0.770\n",
      "[epoch: 16, i:  6263]  train_loss: 0.578  |  valid_loss: 0.618\n",
      "[epoch: 16, i:  6350]  train_loss: 0.679  |  valid_loss: 0.618\n",
      "[epoch: 16, i:  6437]  train_loss: 0.654  |  valid_loss: 0.427\n",
      "[epoch: 16, i:  6524]  train_loss: 0.637  |  valid_loss: 0.786\n",
      "[epoch: 16, i:  6611]  train_loss: 0.623  |  valid_loss: 0.449\n",
      "[epoch: 16, i:  6698]  train_loss: 0.621  |  valid_loss: 0.775\n",
      "[epoch: 16, i:  6785]  train_loss: 0.617  |  valid_loss: 0.555\n",
      "[epoch: 16, i:  6872]  train_loss: 0.568  |  valid_loss: 0.797\n",
      "[epoch: 16, i:  6959]  train_loss: 0.730  |  valid_loss: 0.829\n",
      "[epoch: 16, i:  7046]  train_loss: 0.655  |  valid_loss: 0.591\n",
      "[epoch: 16, i:  7133]  train_loss: 0.703  |  valid_loss: 0.539\n",
      "[epoch: 16, i:  7220]  train_loss: 0.600  |  valid_loss: 0.742\n",
      "[epoch: 16, i:  7307]  train_loss: 0.588  |  valid_loss: 0.710\n",
      "[epoch: 16, i:  7394]  train_loss: 0.699  |  valid_loss: 0.587\n",
      "[epoch: 16, i:  7481]  train_loss: 0.581  |  valid_loss: 0.714\n",
      "[epoch: 16, i:  7568]  train_loss: 0.693  |  valid_loss: 0.730\n",
      "[epoch: 16, i:  7655]  train_loss: 0.648  |  valid_loss: 0.609\n",
      "[epoch: 16, i:  7742]  train_loss: 0.624  |  valid_loss: 0.667\n",
      "[epoch: 16, i:  7829]  train_loss: 0.643  |  valid_loss: 0.491\n",
      "[epoch: 16, i:  7916]  train_loss: 0.636  |  valid_loss: 0.751\n",
      "[epoch: 16, i:  8003]  train_loss: 0.625  |  valid_loss: 0.593\n",
      "[epoch: 16, i:  8090]  train_loss: 0.635  |  valid_loss: 0.522\n",
      "[epoch: 16, i:  8177]  train_loss: 0.606  |  valid_loss: 0.604\n",
      "[epoch: 16, i:  8264]  train_loss: 0.611  |  valid_loss: 0.754\n",
      "[epoch: 16, i:  8351]  train_loss: 0.622  |  valid_loss: 0.718\n",
      "[epoch: 16, i:  8438]  train_loss: 0.669  |  valid_loss: 0.516\n",
      "[epoch: 16, i:  8525]  train_loss: 0.703  |  valid_loss: 0.703\n",
      "[epoch: 16, i:  8612]  train_loss: 0.597  |  valid_loss: 0.903\n",
      "[epoch: 16, i:  8699]  train_loss: 0.668  |  valid_loss: 0.601\n",
      "--> [End of epoch 16] train_accuracy: 78.51%  |  valid_accuracy: 78.10%\n",
      "--> [Start of epoch 17]  lr: 0.000500\n",
      "[epoch: 17, i:    86]  train_loss: 0.535  |  valid_loss: 0.589\n",
      "[epoch: 17, i:   173]  train_loss: 0.622  |  valid_loss: 0.636\n",
      "[epoch: 17, i:   260]  train_loss: 0.580  |  valid_loss: 0.701\n",
      "[epoch: 17, i:   347]  train_loss: 0.638  |  valid_loss: 0.547\n",
      "[epoch: 17, i:   434]  train_loss: 0.536  |  valid_loss: 0.670\n",
      "[epoch: 17, i:   521]  train_loss: 0.603  |  valid_loss: 0.423\n",
      "[epoch: 17, i:   608]  train_loss: 0.595  |  valid_loss: 0.675\n",
      "[epoch: 17, i:   695]  train_loss: 0.616  |  valid_loss: 0.633\n",
      "[epoch: 17, i:   782]  train_loss: 0.637  |  valid_loss: 0.686\n",
      "[epoch: 17, i:   869]  train_loss: 0.564  |  valid_loss: 0.680\n",
      "[epoch: 17, i:   956]  train_loss: 0.586  |  valid_loss: 0.514\n",
      "[epoch: 17, i:  1043]  train_loss: 0.569  |  valid_loss: 0.615\n",
      "[epoch: 17, i:  1130]  train_loss: 0.627  |  valid_loss: 0.517\n",
      "[epoch: 17, i:  1217]  train_loss: 0.652  |  valid_loss: 0.603\n",
      "[epoch: 17, i:  1304]  train_loss: 0.703  |  valid_loss: 0.460\n",
      "[epoch: 17, i:  1391]  train_loss: 0.641  |  valid_loss: 0.742\n",
      "[epoch: 17, i:  1478]  train_loss: 0.639  |  valid_loss: 0.574\n",
      "[epoch: 17, i:  1565]  train_loss: 0.557  |  valid_loss: 0.694\n",
      "[epoch: 17, i:  1652]  train_loss: 0.601  |  valid_loss: 0.621\n",
      "[epoch: 17, i:  1739]  train_loss: 0.611  |  valid_loss: 0.862\n",
      "[epoch: 17, i:  1826]  train_loss: 0.581  |  valid_loss: 0.794\n",
      "[epoch: 17, i:  1913]  train_loss: 0.575  |  valid_loss: 0.696\n",
      "[epoch: 17, i:  2000]  train_loss: 0.601  |  valid_loss: 0.729\n",
      "[epoch: 17, i:  2087]  train_loss: 0.593  |  valid_loss: 0.818\n",
      "[epoch: 17, i:  2174]  train_loss: 0.617  |  valid_loss: 0.516\n",
      "[epoch: 17, i:  2261]  train_loss: 0.571  |  valid_loss: 0.895\n",
      "[epoch: 17, i:  2348]  train_loss: 0.584  |  valid_loss: 0.591\n",
      "[epoch: 17, i:  2435]  train_loss: 0.596  |  valid_loss: 0.670\n",
      "[epoch: 17, i:  2522]  train_loss: 0.548  |  valid_loss: 0.611\n",
      "[epoch: 17, i:  2609]  train_loss: 0.693  |  valid_loss: 0.603\n",
      "[epoch: 17, i:  2696]  train_loss: 0.649  |  valid_loss: 0.717\n",
      "[epoch: 17, i:  2783]  train_loss: 0.642  |  valid_loss: 0.495\n",
      "[epoch: 17, i:  2870]  train_loss: 0.620  |  valid_loss: 0.693\n",
      "[epoch: 17, i:  2957]  train_loss: 0.612  |  valid_loss: 0.699\n",
      "[epoch: 17, i:  3044]  train_loss: 0.570  |  valid_loss: 0.726\n",
      "[epoch: 17, i:  3131]  train_loss: 0.603  |  valid_loss: 0.637\n",
      "[epoch: 17, i:  3218]  train_loss: 0.598  |  valid_loss: 0.688\n",
      "[epoch: 17, i:  3305]  train_loss: 0.584  |  valid_loss: 0.584\n",
      "[epoch: 17, i:  3392]  train_loss: 0.616  |  valid_loss: 0.645\n",
      "[epoch: 17, i:  3479]  train_loss: 0.628  |  valid_loss: 0.507\n",
      "[epoch: 17, i:  3566]  train_loss: 0.636  |  valid_loss: 0.596\n",
      "[epoch: 17, i:  3653]  train_loss: 0.562  |  valid_loss: 0.754\n",
      "[epoch: 17, i:  3740]  train_loss: 0.517  |  valid_loss: 0.396\n",
      "[epoch: 17, i:  3827]  train_loss: 0.561  |  valid_loss: 0.799\n",
      "[epoch: 17, i:  3914]  train_loss: 0.528  |  valid_loss: 0.536\n",
      "[epoch: 17, i:  4001]  train_loss: 0.627  |  valid_loss: 0.600\n",
      "[epoch: 17, i:  4088]  train_loss: 0.587  |  valid_loss: 0.597\n",
      "[epoch: 17, i:  4175]  train_loss: 0.580  |  valid_loss: 0.575\n",
      "[epoch: 17, i:  4262]  train_loss: 0.665  |  valid_loss: 0.521\n",
      "[epoch: 17, i:  4349]  train_loss: 0.603  |  valid_loss: 0.641\n",
      "[epoch: 17, i:  4436]  train_loss: 0.575  |  valid_loss: 0.520\n",
      "[epoch: 17, i:  4523]  train_loss: 0.596  |  valid_loss: 0.760\n",
      "[epoch: 17, i:  4610]  train_loss: 0.645  |  valid_loss: 0.537\n",
      "[epoch: 17, i:  4697]  train_loss: 0.587  |  valid_loss: 0.605\n",
      "[epoch: 17, i:  4784]  train_loss: 0.565  |  valid_loss: 0.671\n",
      "[epoch: 17, i:  4871]  train_loss: 0.598  |  valid_loss: 0.723\n",
      "[epoch: 17, i:  4958]  train_loss: 0.673  |  valid_loss: 0.918\n",
      "[epoch: 17, i:  5045]  train_loss: 0.590  |  valid_loss: 0.469\n",
      "[epoch: 17, i:  5132]  train_loss: 0.607  |  valid_loss: 0.719\n",
      "[epoch: 17, i:  5219]  train_loss: 0.586  |  valid_loss: 0.845\n",
      "[epoch: 17, i:  5306]  train_loss: 0.669  |  valid_loss: 0.741\n",
      "[epoch: 17, i:  5393]  train_loss: 0.649  |  valid_loss: 0.575\n",
      "[epoch: 17, i:  5480]  train_loss: 0.560  |  valid_loss: 0.803\n",
      "[epoch: 17, i:  5567]  train_loss: 0.668  |  valid_loss: 0.346\n",
      "[epoch: 17, i:  5654]  train_loss: 0.577  |  valid_loss: 0.559\n",
      "[epoch: 17, i:  5741]  train_loss: 0.600  |  valid_loss: 0.632\n",
      "[epoch: 17, i:  5828]  train_loss: 0.582  |  valid_loss: 0.575\n",
      "[epoch: 17, i:  5915]  train_loss: 0.654  |  valid_loss: 0.695\n",
      "[epoch: 17, i:  6002]  train_loss: 0.603  |  valid_loss: 0.599\n",
      "[epoch: 17, i:  6089]  train_loss: 0.593  |  valid_loss: 0.809\n",
      "[epoch: 17, i:  6176]  train_loss: 0.677  |  valid_loss: 0.672\n",
      "[epoch: 17, i:  6263]  train_loss: 0.631  |  valid_loss: 0.656\n",
      "[epoch: 17, i:  6350]  train_loss: 0.688  |  valid_loss: 0.594\n",
      "[epoch: 17, i:  6437]  train_loss: 0.646  |  valid_loss: 0.415\n",
      "[epoch: 17, i:  6524]  train_loss: 0.658  |  valid_loss: 0.807\n",
      "[epoch: 17, i:  6611]  train_loss: 0.638  |  valid_loss: 0.426\n",
      "[epoch: 17, i:  6698]  train_loss: 0.567  |  valid_loss: 0.662\n",
      "[epoch: 17, i:  6785]  train_loss: 0.691  |  valid_loss: 0.458\n",
      "[epoch: 17, i:  6872]  train_loss: 0.624  |  valid_loss: 0.789\n",
      "[epoch: 17, i:  6959]  train_loss: 0.513  |  valid_loss: 0.664\n",
      "[epoch: 17, i:  7046]  train_loss: 0.569  |  valid_loss: 0.672\n",
      "[epoch: 17, i:  7133]  train_loss: 0.613  |  valid_loss: 0.514\n",
      "[epoch: 17, i:  7220]  train_loss: 0.603  |  valid_loss: 0.667\n",
      "[epoch: 17, i:  7307]  train_loss: 0.613  |  valid_loss: 0.718\n",
      "[epoch: 17, i:  7394]  train_loss: 0.636  |  valid_loss: 0.562\n",
      "[epoch: 17, i:  7481]  train_loss: 0.585  |  valid_loss: 0.654\n",
      "[epoch: 17, i:  7568]  train_loss: 0.595  |  valid_loss: 0.740\n",
      "[epoch: 17, i:  7655]  train_loss: 0.673  |  valid_loss: 0.626\n",
      "[epoch: 17, i:  7742]  train_loss: 0.627  |  valid_loss: 0.669\n",
      "[epoch: 17, i:  7829]  train_loss: 0.554  |  valid_loss: 0.455\n",
      "[epoch: 17, i:  7916]  train_loss: 0.647  |  valid_loss: 0.708\n",
      "[epoch: 17, i:  8003]  train_loss: 0.683  |  valid_loss: 0.645\n",
      "[epoch: 17, i:  8090]  train_loss: 0.645  |  valid_loss: 0.481\n",
      "[epoch: 17, i:  8177]  train_loss: 0.622  |  valid_loss: 0.598\n",
      "[epoch: 17, i:  8264]  train_loss: 0.541  |  valid_loss: 0.701\n",
      "[epoch: 17, i:  8351]  train_loss: 0.645  |  valid_loss: 0.730\n",
      "[epoch: 17, i:  8438]  train_loss: 0.645  |  valid_loss: 0.508\n",
      "[epoch: 17, i:  8525]  train_loss: 0.647  |  valid_loss: 0.759\n",
      "[epoch: 17, i:  8612]  train_loss: 0.625  |  valid_loss: 0.776\n",
      "[epoch: 17, i:  8699]  train_loss: 0.653  |  valid_loss: 0.610\n",
      "--> [End of epoch 17] train_accuracy: 78.66%  |  valid_accuracy: 78.37%\n",
      "--> [Start of epoch 18]  lr: 0.000500\n",
      "[epoch: 18, i:    86]  train_loss: 0.591  |  valid_loss: 0.610\n",
      "[epoch: 18, i:   173]  train_loss: 0.529  |  valid_loss: 0.599\n",
      "[epoch: 18, i:   260]  train_loss: 0.625  |  valid_loss: 0.596\n",
      "[epoch: 18, i:   347]  train_loss: 0.524  |  valid_loss: 0.647\n",
      "[epoch: 18, i:   434]  train_loss: 0.513  |  valid_loss: 0.653\n",
      "[epoch: 18, i:   521]  train_loss: 0.628  |  valid_loss: 0.470\n",
      "[epoch: 18, i:   608]  train_loss: 0.614  |  valid_loss: 0.745\n",
      "[epoch: 18, i:   695]  train_loss: 0.556  |  valid_loss: 0.682\n",
      "[epoch: 18, i:   782]  train_loss: 0.541  |  valid_loss: 0.701\n",
      "[epoch: 18, i:   869]  train_loss: 0.613  |  valid_loss: 0.703\n",
      "[epoch: 18, i:   956]  train_loss: 0.613  |  valid_loss: 0.553\n",
      "[epoch: 18, i:  1043]  train_loss: 0.573  |  valid_loss: 0.671\n",
      "[epoch: 18, i:  1130]  train_loss: 0.598  |  valid_loss: 0.558\n",
      "[epoch: 18, i:  1217]  train_loss: 0.538  |  valid_loss: 0.740\n",
      "[epoch: 18, i:  1304]  train_loss: 0.564  |  valid_loss: 0.479\n",
      "[epoch: 18, i:  1391]  train_loss: 0.595  |  valid_loss: 0.710\n",
      "[epoch: 18, i:  1478]  train_loss: 0.608  |  valid_loss: 0.610\n",
      "[epoch: 18, i:  1565]  train_loss: 0.629  |  valid_loss: 0.675\n",
      "[epoch: 18, i:  1652]  train_loss: 0.618  |  valid_loss: 0.588\n",
      "[epoch: 18, i:  1739]  train_loss: 0.674  |  valid_loss: 0.865\n",
      "[epoch: 18, i:  1826]  train_loss: 0.555  |  valid_loss: 0.752\n",
      "[epoch: 18, i:  1913]  train_loss: 0.600  |  valid_loss: 0.737\n",
      "[epoch: 18, i:  2000]  train_loss: 0.616  |  valid_loss: 0.721\n",
      "[epoch: 18, i:  2087]  train_loss: 0.659  |  valid_loss: 0.800\n",
      "[epoch: 18, i:  2174]  train_loss: 0.599  |  valid_loss: 0.567\n",
      "[epoch: 18, i:  2261]  train_loss: 0.612  |  valid_loss: 0.907\n",
      "[epoch: 18, i:  2348]  train_loss: 0.585  |  valid_loss: 0.583\n",
      "[epoch: 18, i:  2435]  train_loss: 0.631  |  valid_loss: 0.754\n",
      "[epoch: 18, i:  2522]  train_loss: 0.570  |  valid_loss: 0.554\n",
      "[epoch: 18, i:  2609]  train_loss: 0.715  |  valid_loss: 0.631\n",
      "[epoch: 18, i:  2696]  train_loss: 0.617  |  valid_loss: 0.733\n",
      "[epoch: 18, i:  2783]  train_loss: 0.568  |  valid_loss: 0.498\n",
      "[epoch: 18, i:  2870]  train_loss: 0.588  |  valid_loss: 0.638\n",
      "[epoch: 18, i:  2957]  train_loss: 0.618  |  valid_loss: 0.789\n",
      "[epoch: 18, i:  3044]  train_loss: 0.628  |  valid_loss: 0.735\n",
      "[epoch: 18, i:  3131]  train_loss: 0.651  |  valid_loss: 0.649\n",
      "[epoch: 18, i:  3218]  train_loss: 0.612  |  valid_loss: 0.760\n",
      "[epoch: 18, i:  3305]  train_loss: 0.599  |  valid_loss: 0.647\n",
      "[epoch: 18, i:  3392]  train_loss: 0.590  |  valid_loss: 0.636\n",
      "[epoch: 18, i:  3479]  train_loss: 0.585  |  valid_loss: 0.450\n",
      "[epoch: 18, i:  3566]  train_loss: 0.628  |  valid_loss: 0.663\n",
      "[epoch: 18, i:  3653]  train_loss: 0.589  |  valid_loss: 0.659\n",
      "[epoch: 18, i:  3740]  train_loss: 0.621  |  valid_loss: 0.456\n",
      "[epoch: 18, i:  3827]  train_loss: 0.642  |  valid_loss: 0.804\n",
      "[epoch: 18, i:  3914]  train_loss: 0.578  |  valid_loss: 0.504\n",
      "[epoch: 18, i:  4001]  train_loss: 0.631  |  valid_loss: 0.617\n",
      "[epoch: 18, i:  4088]  train_loss: 0.685  |  valid_loss: 0.584\n",
      "[epoch: 18, i:  4175]  train_loss: 0.649  |  valid_loss: 0.548\n",
      "[epoch: 18, i:  4262]  train_loss: 0.625  |  valid_loss: 0.520\n",
      "[epoch: 18, i:  4349]  train_loss: 0.568  |  valid_loss: 0.661\n",
      "[epoch: 18, i:  4436]  train_loss: 0.597  |  valid_loss: 0.573\n",
      "[epoch: 18, i:  4523]  train_loss: 0.593  |  valid_loss: 0.732\n",
      "[epoch: 18, i:  4610]  train_loss: 0.606  |  valid_loss: 0.597\n",
      "[epoch: 18, i:  4697]  train_loss: 0.605  |  valid_loss: 0.652\n",
      "[epoch: 18, i:  4784]  train_loss: 0.664  |  valid_loss: 0.614\n",
      "[epoch: 18, i:  4871]  train_loss: 0.612  |  valid_loss: 0.807\n",
      "[epoch: 18, i:  4958]  train_loss: 0.670  |  valid_loss: 0.817\n",
      "[epoch: 18, i:  5045]  train_loss: 0.612  |  valid_loss: 0.464\n",
      "[epoch: 18, i:  5132]  train_loss: 0.577  |  valid_loss: 0.609\n",
      "[epoch: 18, i:  5219]  train_loss: 0.613  |  valid_loss: 0.770\n",
      "[epoch: 18, i:  5306]  train_loss: 0.626  |  valid_loss: 0.696\n",
      "[epoch: 18, i:  5393]  train_loss: 0.559  |  valid_loss: 0.650\n",
      "[epoch: 18, i:  5480]  train_loss: 0.591  |  valid_loss: 0.790\n",
      "[epoch: 18, i:  5567]  train_loss: 0.592  |  valid_loss: 0.374\n",
      "[epoch: 18, i:  5654]  train_loss: 0.548  |  valid_loss: 0.647\n",
      "[epoch: 18, i:  5741]  train_loss: 0.576  |  valid_loss: 0.694\n",
      "[epoch: 18, i:  5828]  train_loss: 0.586  |  valid_loss: 0.524\n",
      "[epoch: 18, i:  5915]  train_loss: 0.648  |  valid_loss: 0.738\n",
      "[epoch: 18, i:  6002]  train_loss: 0.596  |  valid_loss: 0.636\n",
      "[epoch: 18, i:  6089]  train_loss: 0.670  |  valid_loss: 0.749\n",
      "[epoch: 18, i:  6176]  train_loss: 0.597  |  valid_loss: 0.735\n",
      "[epoch: 18, i:  6263]  train_loss: 0.542  |  valid_loss: 0.527\n",
      "[epoch: 18, i:  6350]  train_loss: 0.620  |  valid_loss: 0.557\n",
      "[epoch: 18, i:  6437]  train_loss: 0.607  |  valid_loss: 0.489\n",
      "[epoch: 18, i:  6524]  train_loss: 0.624  |  valid_loss: 0.762\n",
      "[epoch: 18, i:  6611]  train_loss: 0.650  |  valid_loss: 0.454\n",
      "[epoch: 18, i:  6698]  train_loss: 0.589  |  valid_loss: 0.679\n",
      "[epoch: 18, i:  6785]  train_loss: 0.589  |  valid_loss: 0.504\n",
      "[epoch: 18, i:  6872]  train_loss: 0.583  |  valid_loss: 0.735\n",
      "[epoch: 18, i:  6959]  train_loss: 0.566  |  valid_loss: 0.706\n",
      "[epoch: 18, i:  7046]  train_loss: 0.561  |  valid_loss: 0.671\n",
      "[epoch: 18, i:  7133]  train_loss: 0.586  |  valid_loss: 0.515\n",
      "[epoch: 18, i:  7220]  train_loss: 0.594  |  valid_loss: 0.717\n",
      "[epoch: 18, i:  7307]  train_loss: 0.557  |  valid_loss: 0.735\n",
      "[epoch: 18, i:  7394]  train_loss: 0.573  |  valid_loss: 0.607\n",
      "[epoch: 18, i:  7481]  train_loss: 0.592  |  valid_loss: 0.656\n",
      "[epoch: 18, i:  7568]  train_loss: 0.621  |  valid_loss: 0.773\n",
      "[epoch: 18, i:  7655]  train_loss: 0.553  |  valid_loss: 0.699\n",
      "[epoch: 18, i:  7742]  train_loss: 0.601  |  valid_loss: 0.727\n",
      "[epoch: 18, i:  7829]  train_loss: 0.564  |  valid_loss: 0.429\n",
      "[epoch: 18, i:  7916]  train_loss: 0.553  |  valid_loss: 0.715\n",
      "[epoch: 18, i:  8003]  train_loss: 0.611  |  valid_loss: 0.614\n",
      "[epoch: 18, i:  8090]  train_loss: 0.635  |  valid_loss: 0.473\n",
      "[epoch: 18, i:  8177]  train_loss: 0.658  |  valid_loss: 0.615\n",
      "[epoch: 18, i:  8264]  train_loss: 0.615  |  valid_loss: 0.673\n",
      "[epoch: 18, i:  8351]  train_loss: 0.622  |  valid_loss: 0.737\n",
      "[epoch: 18, i:  8438]  train_loss: 0.534  |  valid_loss: 0.449\n",
      "[epoch: 18, i:  8525]  train_loss: 0.651  |  valid_loss: 0.700\n",
      "[epoch: 18, i:  8612]  train_loss: 0.584  |  valid_loss: 0.805\n",
      "[epoch: 18, i:  8699]  train_loss: 0.606  |  valid_loss: 0.628\n",
      "--> [End of epoch 18] train_accuracy: 79.14%  |  valid_accuracy: 78.32%\n",
      "--> [Start of epoch 19]  lr: 0.000500\n",
      "[epoch: 19, i:    86]  train_loss: 0.568  |  valid_loss: 0.620\n",
      "[epoch: 19, i:   173]  train_loss: 0.594  |  valid_loss: 0.607\n",
      "[epoch: 19, i:   260]  train_loss: 0.627  |  valid_loss: 0.693\n",
      "[epoch: 19, i:   347]  train_loss: 0.572  |  valid_loss: 0.520\n",
      "[epoch: 19, i:   434]  train_loss: 0.624  |  valid_loss: 0.649\n",
      "[epoch: 19, i:   521]  train_loss: 0.545  |  valid_loss: 0.441\n",
      "[epoch: 19, i:   608]  train_loss: 0.605  |  valid_loss: 0.743\n",
      "[epoch: 19, i:   695]  train_loss: 0.575  |  valid_loss: 0.661\n",
      "[epoch: 19, i:   782]  train_loss: 0.601  |  valid_loss: 0.697\n",
      "[epoch: 19, i:   869]  train_loss: 0.542  |  valid_loss: 0.678\n",
      "[epoch: 19, i:   956]  train_loss: 0.581  |  valid_loss: 0.638\n",
      "[epoch: 19, i:  1043]  train_loss: 0.575  |  valid_loss: 0.604\n",
      "[epoch: 19, i:  1130]  train_loss: 0.593  |  valid_loss: 0.572\n",
      "[epoch: 19, i:  1217]  train_loss: 0.588  |  valid_loss: 0.662\n",
      "[epoch: 19, i:  1304]  train_loss: 0.605  |  valid_loss: 0.498\n",
      "[epoch: 19, i:  1391]  train_loss: 0.601  |  valid_loss: 0.659\n",
      "[epoch: 19, i:  1478]  train_loss: 0.570  |  valid_loss: 0.595\n",
      "[epoch: 19, i:  1565]  train_loss: 0.590  |  valid_loss: 0.774\n",
      "[epoch: 19, i:  1652]  train_loss: 0.594  |  valid_loss: 0.627\n",
      "[epoch: 19, i:  1739]  train_loss: 0.558  |  valid_loss: 0.802\n",
      "[epoch: 19, i:  1826]  train_loss: 0.613  |  valid_loss: 0.756\n",
      "[epoch: 19, i:  1913]  train_loss: 0.506  |  valid_loss: 0.661\n",
      "[epoch: 19, i:  2000]  train_loss: 0.602  |  valid_loss: 0.633\n",
      "[epoch: 19, i:  2087]  train_loss: 0.582  |  valid_loss: 0.816\n",
      "[epoch: 19, i:  2174]  train_loss: 0.626  |  valid_loss: 0.607\n",
      "[epoch: 19, i:  2261]  train_loss: 0.484  |  valid_loss: 0.982\n",
      "[epoch: 19, i:  2348]  train_loss: 0.588  |  valid_loss: 0.611\n",
      "[epoch: 19, i:  2435]  train_loss: 0.586  |  valid_loss: 0.641\n",
      "[epoch: 19, i:  2522]  train_loss: 0.667  |  valid_loss: 0.587\n",
      "[epoch: 19, i:  2609]  train_loss: 0.646  |  valid_loss: 0.625\n",
      "[epoch: 19, i:  2696]  train_loss: 0.650  |  valid_loss: 0.731\n",
      "[epoch: 19, i:  2783]  train_loss: 0.595  |  valid_loss: 0.509\n",
      "[epoch: 19, i:  2870]  train_loss: 0.536  |  valid_loss: 0.754\n",
      "[epoch: 19, i:  2957]  train_loss: 0.592  |  valid_loss: 0.779\n",
      "[epoch: 19, i:  3044]  train_loss: 0.604  |  valid_loss: 0.676\n",
      "[epoch: 19, i:  3131]  train_loss: 0.623  |  valid_loss: 0.652\n",
      "[epoch: 19, i:  3218]  train_loss: 0.609  |  valid_loss: 0.757\n",
      "[epoch: 19, i:  3305]  train_loss: 0.565  |  valid_loss: 0.706\n",
      "[epoch: 19, i:  3392]  train_loss: 0.602  |  valid_loss: 0.610\n",
      "[epoch: 19, i:  3479]  train_loss: 0.625  |  valid_loss: 0.588\n",
      "[epoch: 19, i:  3566]  train_loss: 0.592  |  valid_loss: 0.590\n",
      "[epoch: 19, i:  3653]  train_loss: 0.585  |  valid_loss: 0.701\n",
      "[epoch: 19, i:  3740]  train_loss: 0.622  |  valid_loss: 0.376\n",
      "[epoch: 19, i:  3827]  train_loss: 0.562  |  valid_loss: 0.866\n",
      "[epoch: 19, i:  3914]  train_loss: 0.584  |  valid_loss: 0.512\n",
      "[epoch: 19, i:  4001]  train_loss: 0.627  |  valid_loss: 0.551\n",
      "[epoch: 19, i:  4088]  train_loss: 0.597  |  valid_loss: 0.620\n",
      "[epoch: 19, i:  4175]  train_loss: 0.599  |  valid_loss: 0.641\n",
      "[epoch: 19, i:  4262]  train_loss: 0.559  |  valid_loss: 0.483\n",
      "[epoch: 19, i:  4349]  train_loss: 0.608  |  valid_loss: 0.610\n",
      "[epoch: 19, i:  4436]  train_loss: 0.565  |  valid_loss: 0.579\n",
      "[epoch: 19, i:  4523]  train_loss: 0.610  |  valid_loss: 0.727\n",
      "[epoch: 19, i:  4610]  train_loss: 0.562  |  valid_loss: 0.550\n",
      "[epoch: 19, i:  4697]  train_loss: 0.656  |  valid_loss: 0.653\n",
      "[epoch: 19, i:  4784]  train_loss: 0.550  |  valid_loss: 0.681\n",
      "[epoch: 19, i:  4871]  train_loss: 0.637  |  valid_loss: 0.729\n",
      "[epoch: 19, i:  4958]  train_loss: 0.605  |  valid_loss: 0.765\n",
      "[epoch: 19, i:  5045]  train_loss: 0.597  |  valid_loss: 0.438\n",
      "[epoch: 19, i:  5132]  train_loss: 0.529  |  valid_loss: 0.742\n",
      "[epoch: 19, i:  5219]  train_loss: 0.711  |  valid_loss: 0.804\n",
      "[epoch: 19, i:  5306]  train_loss: 0.614  |  valid_loss: 0.680\n",
      "[epoch: 19, i:  5393]  train_loss: 0.575  |  valid_loss: 0.525\n",
      "[epoch: 19, i:  5480]  train_loss: 0.583  |  valid_loss: 0.755\n",
      "[epoch: 19, i:  5567]  train_loss: 0.591  |  valid_loss: 0.390\n",
      "[epoch: 19, i:  5654]  train_loss: 0.622  |  valid_loss: 0.604\n",
      "[epoch: 19, i:  5741]  train_loss: 0.569  |  valid_loss: 0.657\n",
      "[epoch: 19, i:  5828]  train_loss: 0.647  |  valid_loss: 0.546\n",
      "[epoch: 19, i:  5915]  train_loss: 0.591  |  valid_loss: 0.713\n",
      "[epoch: 19, i:  6002]  train_loss: 0.642  |  valid_loss: 0.571\n",
      "[epoch: 19, i:  6089]  train_loss: 0.560  |  valid_loss: 0.862\n",
      "[epoch: 19, i:  6176]  train_loss: 0.676  |  valid_loss: 0.697\n",
      "[epoch: 19, i:  6263]  train_loss: 0.578  |  valid_loss: 0.588\n",
      "[epoch: 19, i:  6350]  train_loss: 0.543  |  valid_loss: 0.564\n",
      "[epoch: 19, i:  6437]  train_loss: 0.602  |  valid_loss: 0.465\n",
      "[epoch: 19, i:  6524]  train_loss: 0.610  |  valid_loss: 0.815\n",
      "[epoch: 19, i:  6611]  train_loss: 0.644  |  valid_loss: 0.465\n",
      "[epoch: 19, i:  6698]  train_loss: 0.642  |  valid_loss: 0.711\n",
      "[epoch: 19, i:  6785]  train_loss: 0.596  |  valid_loss: 0.497\n",
      "[epoch: 19, i:  6872]  train_loss: 0.592  |  valid_loss: 0.690\n",
      "[epoch: 19, i:  6959]  train_loss: 0.604  |  valid_loss: 0.746\n",
      "[epoch: 19, i:  7046]  train_loss: 0.632  |  valid_loss: 0.632\n",
      "[epoch: 19, i:  7133]  train_loss: 0.635  |  valid_loss: 0.494\n",
      "[epoch: 19, i:  7220]  train_loss: 0.559  |  valid_loss: 0.686\n",
      "[epoch: 19, i:  7307]  train_loss: 0.610  |  valid_loss: 0.762\n",
      "[epoch: 19, i:  7394]  train_loss: 0.516  |  valid_loss: 0.652\n",
      "[epoch: 19, i:  7481]  train_loss: 0.580  |  valid_loss: 0.592\n",
      "[epoch: 19, i:  7568]  train_loss: 0.554  |  valid_loss: 0.810\n",
      "[epoch: 19, i:  7655]  train_loss: 0.595  |  valid_loss: 0.660\n",
      "[epoch: 19, i:  7742]  train_loss: 0.608  |  valid_loss: 0.707\n",
      "[epoch: 19, i:  7829]  train_loss: 0.579  |  valid_loss: 0.531\n",
      "[epoch: 19, i:  7916]  train_loss: 0.594  |  valid_loss: 0.668\n",
      "[epoch: 19, i:  8003]  train_loss: 0.619  |  valid_loss: 0.508\n",
      "[epoch: 19, i:  8090]  train_loss: 0.600  |  valid_loss: 0.493\n",
      "[epoch: 19, i:  8177]  train_loss: 0.558  |  valid_loss: 0.604\n",
      "[epoch: 19, i:  8264]  train_loss: 0.602  |  valid_loss: 0.707\n",
      "[epoch: 19, i:  8351]  train_loss: 0.661  |  valid_loss: 0.696\n",
      "[epoch: 19, i:  8438]  train_loss: 0.573  |  valid_loss: 0.481\n",
      "[epoch: 19, i:  8525]  train_loss: 0.633  |  valid_loss: 0.704\n",
      "[epoch: 19, i:  8612]  train_loss: 0.608  |  valid_loss: 0.970\n",
      "[epoch: 19, i:  8699]  train_loss: 0.524  |  valid_loss: 0.665\n",
      "--> [End of epoch 19] train_accuracy: 79.24%  |  valid_accuracy: 77.97%\n",
      "--> [Start of epoch 20]  lr: 0.000500\n",
      "[epoch: 20, i:    86]  train_loss: 0.527  |  valid_loss: 0.534\n",
      "[epoch: 20, i:   173]  train_loss: 0.602  |  valid_loss: 0.599\n",
      "[epoch: 20, i:   260]  train_loss: 0.587  |  valid_loss: 0.709\n",
      "[epoch: 20, i:   347]  train_loss: 0.608  |  valid_loss: 0.535\n",
      "[epoch: 20, i:   434]  train_loss: 0.598  |  valid_loss: 0.681\n",
      "[epoch: 20, i:   521]  train_loss: 0.607  |  valid_loss: 0.448\n",
      "[epoch: 20, i:   608]  train_loss: 0.584  |  valid_loss: 0.597\n",
      "[epoch: 20, i:   695]  train_loss: 0.467  |  valid_loss: 0.629\n",
      "[epoch: 20, i:   782]  train_loss: 0.600  |  valid_loss: 0.682\n",
      "[epoch: 20, i:   869]  train_loss: 0.568  |  valid_loss: 0.699\n",
      "[epoch: 20, i:   956]  train_loss: 0.576  |  valid_loss: 0.583\n",
      "[epoch: 20, i:  1043]  train_loss: 0.602  |  valid_loss: 0.564\n",
      "[epoch: 20, i:  1130]  train_loss: 0.535  |  valid_loss: 0.615\n",
      "[epoch: 20, i:  1217]  train_loss: 0.501  |  valid_loss: 0.637\n",
      "[epoch: 20, i:  1304]  train_loss: 0.612  |  valid_loss: 0.423\n",
      "[epoch: 20, i:  1391]  train_loss: 0.651  |  valid_loss: 0.716\n",
      "[epoch: 20, i:  1478]  train_loss: 0.640  |  valid_loss: 0.655\n",
      "[epoch: 20, i:  1565]  train_loss: 0.657  |  valid_loss: 0.765\n",
      "[epoch: 20, i:  1652]  train_loss: 0.632  |  valid_loss: 0.675\n",
      "[epoch: 20, i:  1739]  train_loss: 0.564  |  valid_loss: 0.819\n",
      "[epoch: 20, i:  1826]  train_loss: 0.600  |  valid_loss: 0.749\n",
      "[epoch: 20, i:  1913]  train_loss: 0.538  |  valid_loss: 0.642\n",
      "[epoch: 20, i:  2000]  train_loss: 0.584  |  valid_loss: 0.663\n",
      "[epoch: 20, i:  2087]  train_loss: 0.614  |  valid_loss: 0.851\n",
      "[epoch: 20, i:  2174]  train_loss: 0.611  |  valid_loss: 0.548\n",
      "[epoch: 20, i:  2261]  train_loss: 0.633  |  valid_loss: 0.921\n",
      "[epoch: 20, i:  2348]  train_loss: 0.567  |  valid_loss: 0.523\n",
      "[epoch: 20, i:  2435]  train_loss: 0.592  |  valid_loss: 0.657\n",
      "[epoch: 20, i:  2522]  train_loss: 0.526  |  valid_loss: 0.562\n",
      "[epoch: 20, i:  2609]  train_loss: 0.601  |  valid_loss: 0.540\n",
      "[epoch: 20, i:  2696]  train_loss: 0.634  |  valid_loss: 0.667\n",
      "[epoch: 20, i:  2783]  train_loss: 0.603  |  valid_loss: 0.522\n",
      "[epoch: 20, i:  2870]  train_loss: 0.580  |  valid_loss: 0.695\n",
      "[epoch: 20, i:  2957]  train_loss: 0.563  |  valid_loss: 0.700\n",
      "[epoch: 20, i:  3044]  train_loss: 0.563  |  valid_loss: 0.679\n",
      "[epoch: 20, i:  3131]  train_loss: 0.563  |  valid_loss: 0.679\n",
      "[epoch: 20, i:  3218]  train_loss: 0.582  |  valid_loss: 0.717\n",
      "[epoch: 20, i:  3305]  train_loss: 0.592  |  valid_loss: 0.702\n",
      "[epoch: 20, i:  3392]  train_loss: 0.573  |  valid_loss: 0.623\n",
      "[epoch: 20, i:  3479]  train_loss: 0.579  |  valid_loss: 0.466\n",
      "[epoch: 20, i:  3566]  train_loss: 0.598  |  valid_loss: 0.583\n",
      "[epoch: 20, i:  3653]  train_loss: 0.532  |  valid_loss: 0.764\n",
      "[epoch: 20, i:  3740]  train_loss: 0.653  |  valid_loss: 0.445\n",
      "[epoch: 20, i:  3827]  train_loss: 0.600  |  valid_loss: 0.756\n",
      "[epoch: 20, i:  3914]  train_loss: 0.677  |  valid_loss: 0.496\n",
      "[epoch: 20, i:  4001]  train_loss: 0.542  |  valid_loss: 0.614\n",
      "[epoch: 20, i:  4088]  train_loss: 0.588  |  valid_loss: 0.640\n",
      "[epoch: 20, i:  4175]  train_loss: 0.609  |  valid_loss: 0.619\n",
      "[epoch: 20, i:  4262]  train_loss: 0.549  |  valid_loss: 0.528\n",
      "[epoch: 20, i:  4349]  train_loss: 0.575  |  valid_loss: 0.577\n",
      "[epoch: 20, i:  4436]  train_loss: 0.616  |  valid_loss: 0.531\n",
      "[epoch: 20, i:  4523]  train_loss: 0.564  |  valid_loss: 0.751\n",
      "[epoch: 20, i:  4610]  train_loss: 0.654  |  valid_loss: 0.521\n",
      "[epoch: 20, i:  4697]  train_loss: 0.622  |  valid_loss: 0.657\n",
      "[epoch: 20, i:  4784]  train_loss: 0.540  |  valid_loss: 0.631\n",
      "[epoch: 20, i:  4871]  train_loss: 0.586  |  valid_loss: 0.739\n",
      "[epoch: 20, i:  4958]  train_loss: 0.620  |  valid_loss: 0.797\n",
      "[epoch: 20, i:  5045]  train_loss: 0.562  |  valid_loss: 0.464\n",
      "[epoch: 20, i:  5132]  train_loss: 0.628  |  valid_loss: 0.675\n",
      "[epoch: 20, i:  5219]  train_loss: 0.616  |  valid_loss: 0.813\n",
      "[epoch: 20, i:  5306]  train_loss: 0.573  |  valid_loss: 0.773\n",
      "[epoch: 20, i:  5393]  train_loss: 0.583  |  valid_loss: 0.583\n",
      "[epoch: 20, i:  5480]  train_loss: 0.616  |  valid_loss: 0.805\n",
      "[epoch: 20, i:  5567]  train_loss: 0.597  |  valid_loss: 0.367\n",
      "[epoch: 20, i:  5654]  train_loss: 0.579  |  valid_loss: 0.597\n",
      "[epoch: 20, i:  5741]  train_loss: 0.576  |  valid_loss: 0.671\n",
      "[epoch: 20, i:  5828]  train_loss: 0.668  |  valid_loss: 0.668\n",
      "[epoch: 20, i:  5915]  train_loss: 0.596  |  valid_loss: 0.749\n",
      "[epoch: 20, i:  6002]  train_loss: 0.602  |  valid_loss: 0.590\n",
      "[epoch: 20, i:  6089]  train_loss: 0.617  |  valid_loss: 0.698\n",
      "[epoch: 20, i:  6176]  train_loss: 0.563  |  valid_loss: 0.713\n",
      "[epoch: 20, i:  6263]  train_loss: 0.560  |  valid_loss: 0.516\n",
      "[epoch: 20, i:  6350]  train_loss: 0.590  |  valid_loss: 0.561\n",
      "[epoch: 20, i:  6437]  train_loss: 0.556  |  valid_loss: 0.426\n",
      "[epoch: 20, i:  6524]  train_loss: 0.582  |  valid_loss: 0.765\n",
      "[epoch: 20, i:  6611]  train_loss: 0.631  |  valid_loss: 0.469\n",
      "[epoch: 20, i:  6698]  train_loss: 0.571  |  valid_loss: 0.748\n",
      "[epoch: 20, i:  6785]  train_loss: 0.608  |  valid_loss: 0.535\n",
      "[epoch: 20, i:  6872]  train_loss: 0.605  |  valid_loss: 0.812\n",
      "[epoch: 20, i:  6959]  train_loss: 0.612  |  valid_loss: 0.742\n",
      "[epoch: 20, i:  7046]  train_loss: 0.630  |  valid_loss: 0.647\n",
      "[epoch: 20, i:  7133]  train_loss: 0.583  |  valid_loss: 0.543\n",
      "[epoch: 20, i:  7220]  train_loss: 0.604  |  valid_loss: 0.709\n",
      "[epoch: 20, i:  7307]  train_loss: 0.599  |  valid_loss: 0.816\n",
      "[epoch: 20, i:  7394]  train_loss: 0.583  |  valid_loss: 0.580\n",
      "[epoch: 20, i:  7481]  train_loss: 0.661  |  valid_loss: 0.604\n",
      "[epoch: 20, i:  7568]  train_loss: 0.571  |  valid_loss: 0.805\n",
      "[epoch: 20, i:  7655]  train_loss: 0.574  |  valid_loss: 0.632\n",
      "[epoch: 20, i:  7742]  train_loss: 0.607  |  valid_loss: 0.637\n",
      "[epoch: 20, i:  7829]  train_loss: 0.583  |  valid_loss: 0.526\n",
      "[epoch: 20, i:  7916]  train_loss: 0.655  |  valid_loss: 0.676\n",
      "[epoch: 20, i:  8003]  train_loss: 0.599  |  valid_loss: 0.557\n",
      "[epoch: 20, i:  8090]  train_loss: 0.598  |  valid_loss: 0.472\n",
      "[epoch: 20, i:  8177]  train_loss: 0.599  |  valid_loss: 0.625\n",
      "[epoch: 20, i:  8264]  train_loss: 0.593  |  valid_loss: 0.703\n",
      "[epoch: 20, i:  8351]  train_loss: 0.680  |  valid_loss: 0.754\n",
      "[epoch: 20, i:  8438]  train_loss: 0.577  |  valid_loss: 0.471\n",
      "[epoch: 20, i:  8525]  train_loss: 0.593  |  valid_loss: 0.730\n",
      "[epoch: 20, i:  8612]  train_loss: 0.573  |  valid_loss: 0.865\n",
      "[epoch: 20, i:  8699]  train_loss: 0.598  |  valid_loss: 0.612\n",
      "--> [End of epoch 20] train_accuracy: 79.21%  |  valid_accuracy: 78.08%\n",
      "--> [Start of epoch 21]  lr: 0.000500\n",
      "[epoch: 21, i:    86]  train_loss: 0.624  |  valid_loss: 0.557\n",
      "[epoch: 21, i:   173]  train_loss: 0.588  |  valid_loss: 0.517\n",
      "[epoch: 21, i:   260]  train_loss: 0.567  |  valid_loss: 0.693\n",
      "[epoch: 21, i:   347]  train_loss: 0.555  |  valid_loss: 0.545\n",
      "[epoch: 21, i:   434]  train_loss: 0.570  |  valid_loss: 0.654\n",
      "[epoch: 21, i:   521]  train_loss: 0.590  |  valid_loss: 0.458\n",
      "[epoch: 21, i:   608]  train_loss: 0.569  |  valid_loss: 0.727\n",
      "[epoch: 21, i:   695]  train_loss: 0.564  |  valid_loss: 0.702\n",
      "[epoch: 21, i:   782]  train_loss: 0.570  |  valid_loss: 0.753\n",
      "[epoch: 21, i:   869]  train_loss: 0.607  |  valid_loss: 0.669\n",
      "[epoch: 21, i:   956]  train_loss: 0.548  |  valid_loss: 0.563\n",
      "[epoch: 21, i:  1043]  train_loss: 0.615  |  valid_loss: 0.664\n",
      "[epoch: 21, i:  1130]  train_loss: 0.550  |  valid_loss: 0.585\n",
      "[epoch: 21, i:  1217]  train_loss: 0.668  |  valid_loss: 0.712\n",
      "[epoch: 21, i:  1304]  train_loss: 0.656  |  valid_loss: 0.430\n",
      "[epoch: 21, i:  1391]  train_loss: 0.627  |  valid_loss: 0.653\n",
      "[epoch: 21, i:  1478]  train_loss: 0.598  |  valid_loss: 0.575\n",
      "[epoch: 21, i:  1565]  train_loss: 0.619  |  valid_loss: 0.799\n",
      "[epoch: 21, i:  1652]  train_loss: 0.612  |  valid_loss: 0.528\n",
      "[epoch: 21, i:  1739]  train_loss: 0.616  |  valid_loss: 0.802\n",
      "[epoch: 21, i:  1826]  train_loss: 0.588  |  valid_loss: 0.796\n",
      "[epoch: 21, i:  1913]  train_loss: 0.621  |  valid_loss: 0.674\n",
      "[epoch: 21, i:  2000]  train_loss: 0.574  |  valid_loss: 0.638\n",
      "[epoch: 21, i:  2087]  train_loss: 0.574  |  valid_loss: 0.766\n",
      "[epoch: 21, i:  2174]  train_loss: 0.593  |  valid_loss: 0.622\n",
      "[epoch: 21, i:  2261]  train_loss: 0.588  |  valid_loss: 0.974\n",
      "[epoch: 21, i:  2348]  train_loss: 0.576  |  valid_loss: 0.573\n",
      "[epoch: 21, i:  2435]  train_loss: 0.576  |  valid_loss: 0.780\n",
      "[epoch: 21, i:  2522]  train_loss: 0.607  |  valid_loss: 0.565\n",
      "[epoch: 21, i:  2609]  train_loss: 0.542  |  valid_loss: 0.582\n",
      "[epoch: 21, i:  2696]  train_loss: 0.518  |  valid_loss: 0.732\n",
      "[epoch: 21, i:  2783]  train_loss: 0.516  |  valid_loss: 0.551\n",
      "[epoch: 21, i:  2870]  train_loss: 0.542  |  valid_loss: 0.685\n",
      "[epoch: 21, i:  2957]  train_loss: 0.628  |  valid_loss: 0.750\n",
      "[epoch: 21, i:  3044]  train_loss: 0.618  |  valid_loss: 0.655\n",
      "[epoch: 21, i:  3131]  train_loss: 0.630  |  valid_loss: 0.633\n",
      "[epoch: 21, i:  3218]  train_loss: 0.610  |  valid_loss: 0.697\n",
      "[epoch: 21, i:  3305]  train_loss: 0.557  |  valid_loss: 0.608\n",
      "[epoch: 21, i:  3392]  train_loss: 0.613  |  valid_loss: 0.599\n",
      "[epoch: 21, i:  3479]  train_loss: 0.576  |  valid_loss: 0.479\n",
      "[epoch: 21, i:  3566]  train_loss: 0.571  |  valid_loss: 0.649\n",
      "[epoch: 21, i:  3653]  train_loss: 0.505  |  valid_loss: 0.715\n",
      "[epoch: 21, i:  3740]  train_loss: 0.588  |  valid_loss: 0.468\n",
      "[epoch: 21, i:  3827]  train_loss: 0.539  |  valid_loss: 0.732\n",
      "[epoch: 21, i:  3914]  train_loss: 0.645  |  valid_loss: 0.486\n",
      "[epoch: 21, i:  4001]  train_loss: 0.594  |  valid_loss: 0.670\n",
      "[epoch: 21, i:  4088]  train_loss: 0.628  |  valid_loss: 0.573\n",
      "[epoch: 21, i:  4175]  train_loss: 0.606  |  valid_loss: 0.579\n",
      "[epoch: 21, i:  4262]  train_loss: 0.525  |  valid_loss: 0.593\n",
      "[epoch: 21, i:  4349]  train_loss: 0.574  |  valid_loss: 0.597\n",
      "[epoch: 21, i:  4436]  train_loss: 0.559  |  valid_loss: 0.584\n",
      "[epoch: 21, i:  4523]  train_loss: 0.652  |  valid_loss: 0.728\n",
      "[epoch: 21, i:  4610]  train_loss: 0.609  |  valid_loss: 0.502\n",
      "[epoch: 21, i:  4697]  train_loss: 0.559  |  valid_loss: 0.524\n",
      "[epoch: 21, i:  4784]  train_loss: 0.568  |  valid_loss: 0.634\n",
      "[epoch: 21, i:  4871]  train_loss: 0.622  |  valid_loss: 0.694\n",
      "[epoch: 21, i:  4958]  train_loss: 0.654  |  valid_loss: 0.750\n",
      "[epoch: 21, i:  5045]  train_loss: 0.546  |  valid_loss: 0.388\n",
      "[epoch: 21, i:  5132]  train_loss: 0.661  |  valid_loss: 0.639\n",
      "[epoch: 21, i:  5219]  train_loss: 0.584  |  valid_loss: 0.824\n",
      "[epoch: 21, i:  5306]  train_loss: 0.608  |  valid_loss: 0.683\n",
      "[epoch: 21, i:  5393]  train_loss: 0.555  |  valid_loss: 0.579\n",
      "[epoch: 21, i:  5480]  train_loss: 0.609  |  valid_loss: 0.772\n",
      "[epoch: 21, i:  5567]  train_loss: 0.603  |  valid_loss: 0.374\n",
      "[epoch: 21, i:  5654]  train_loss: 0.654  |  valid_loss: 0.644\n",
      "[epoch: 21, i:  5741]  train_loss: 0.589  |  valid_loss: 0.650\n",
      "[epoch: 21, i:  5828]  train_loss: 0.593  |  valid_loss: 0.564\n",
      "[epoch: 21, i:  5915]  train_loss: 0.545  |  valid_loss: 0.712\n",
      "[epoch: 21, i:  6002]  train_loss: 0.633  |  valid_loss: 0.546\n",
      "[epoch: 21, i:  6089]  train_loss: 0.625  |  valid_loss: 0.747\n",
      "[epoch: 21, i:  6176]  train_loss: 0.581  |  valid_loss: 0.705\n",
      "[epoch: 21, i:  6263]  train_loss: 0.575  |  valid_loss: 0.593\n",
      "[epoch: 21, i:  6350]  train_loss: 0.642  |  valid_loss: 0.609\n",
      "[epoch: 21, i:  6437]  train_loss: 0.590  |  valid_loss: 0.494\n",
      "[epoch: 21, i:  6524]  train_loss: 0.570  |  valid_loss: 0.705\n",
      "[epoch: 21, i:  6611]  train_loss: 0.606  |  valid_loss: 0.443\n",
      "[epoch: 21, i:  6698]  train_loss: 0.593  |  valid_loss: 0.712\n",
      "[epoch: 21, i:  6785]  train_loss: 0.594  |  valid_loss: 0.560\n",
      "[epoch: 21, i:  6872]  train_loss: 0.574  |  valid_loss: 0.714\n",
      "[epoch: 21, i:  6959]  train_loss: 0.628  |  valid_loss: 0.736\n",
      "[epoch: 21, i:  7046]  train_loss: 0.693  |  valid_loss: 0.613\n",
      "[epoch: 21, i:  7133]  train_loss: 0.576  |  valid_loss: 0.581\n",
      "[epoch: 21, i:  7220]  train_loss: 0.586  |  valid_loss: 0.684\n",
      "[epoch: 21, i:  7307]  train_loss: 0.579  |  valid_loss: 0.811\n",
      "[epoch: 21, i:  7394]  train_loss: 0.660  |  valid_loss: 0.659\n",
      "[epoch: 21, i:  7481]  train_loss: 0.548  |  valid_loss: 0.625\n",
      "[epoch: 21, i:  7568]  train_loss: 0.665  |  valid_loss: 0.805\n",
      "[epoch: 21, i:  7655]  train_loss: 0.633  |  valid_loss: 0.653\n",
      "[epoch: 21, i:  7742]  train_loss: 0.611  |  valid_loss: 0.639\n",
      "[epoch: 21, i:  7829]  train_loss: 0.617  |  valid_loss: 0.557\n",
      "[epoch: 21, i:  7916]  train_loss: 0.578  |  valid_loss: 0.688\n",
      "[epoch: 21, i:  8003]  train_loss: 0.634  |  valid_loss: 0.534\n",
      "[epoch: 21, i:  8090]  train_loss: 0.622  |  valid_loss: 0.494\n",
      "[epoch: 21, i:  8177]  train_loss: 0.621  |  valid_loss: 0.552\n",
      "[epoch: 21, i:  8264]  train_loss: 0.638  |  valid_loss: 0.725\n",
      "[epoch: 21, i:  8351]  train_loss: 0.643  |  valid_loss: 0.860\n",
      "[epoch: 21, i:  8438]  train_loss: 0.621  |  valid_loss: 0.490\n",
      "[epoch: 21, i:  8525]  train_loss: 0.607  |  valid_loss: 0.711\n",
      "[epoch: 21, i:  8612]  train_loss: 0.555  |  valid_loss: 0.882\n",
      "[epoch: 21, i:  8699]  train_loss: 0.553  |  valid_loss: 0.574\n",
      "--> [End of epoch 21] train_accuracy: 79.52%  |  valid_accuracy: 78.46%\n",
      "--> [Start of epoch 22]  lr: 0.000500\n",
      "[epoch: 22, i:    86]  train_loss: 0.594  |  valid_loss: 0.517\n",
      "[epoch: 22, i:   173]  train_loss: 0.582  |  valid_loss: 0.591\n",
      "[epoch: 22, i:   260]  train_loss: 0.574  |  valid_loss: 0.770\n",
      "[epoch: 22, i:   347]  train_loss: 0.554  |  valid_loss: 0.558\n",
      "[epoch: 22, i:   434]  train_loss: 0.517  |  valid_loss: 0.716\n",
      "[epoch: 22, i:   521]  train_loss: 0.556  |  valid_loss: 0.423\n",
      "[epoch: 22, i:   608]  train_loss: 0.570  |  valid_loss: 0.677\n",
      "[epoch: 22, i:   695]  train_loss: 0.554  |  valid_loss: 0.687\n",
      "[epoch: 22, i:   782]  train_loss: 0.565  |  valid_loss: 0.770\n",
      "[epoch: 22, i:   869]  train_loss: 0.573  |  valid_loss: 0.603\n",
      "[epoch: 22, i:   956]  train_loss: 0.498  |  valid_loss: 0.500\n",
      "[epoch: 22, i:  1043]  train_loss: 0.566  |  valid_loss: 0.638\n",
      "[epoch: 22, i:  1130]  train_loss: 0.618  |  valid_loss: 0.610\n",
      "[epoch: 22, i:  1217]  train_loss: 0.519  |  valid_loss: 0.698\n",
      "[epoch: 22, i:  1304]  train_loss: 0.546  |  valid_loss: 0.460\n",
      "[epoch: 22, i:  1391]  train_loss: 0.551  |  valid_loss: 0.737\n",
      "[epoch: 22, i:  1478]  train_loss: 0.547  |  valid_loss: 0.601\n",
      "[epoch: 22, i:  1565]  train_loss: 0.613  |  valid_loss: 0.692\n",
      "[epoch: 22, i:  1652]  train_loss: 0.599  |  valid_loss: 0.570\n",
      "[epoch: 22, i:  1739]  train_loss: 0.633  |  valid_loss: 0.764\n",
      "[epoch: 22, i:  1826]  train_loss: 0.556  |  valid_loss: 0.844\n",
      "[epoch: 22, i:  1913]  train_loss: 0.586  |  valid_loss: 0.678\n",
      "[epoch: 22, i:  2000]  train_loss: 0.572  |  valid_loss: 0.705\n",
      "[epoch: 22, i:  2087]  train_loss: 0.612  |  valid_loss: 0.766\n",
      "[epoch: 22, i:  2174]  train_loss: 0.532  |  valid_loss: 0.566\n",
      "[epoch: 22, i:  2261]  train_loss: 0.518  |  valid_loss: 0.826\n",
      "[epoch: 22, i:  2348]  train_loss: 0.587  |  valid_loss: 0.629\n",
      "[epoch: 22, i:  2435]  train_loss: 0.497  |  valid_loss: 0.683\n",
      "[epoch: 22, i:  2522]  train_loss: 0.563  |  valid_loss: 0.497\n",
      "[epoch: 22, i:  2609]  train_loss: 0.581  |  valid_loss: 0.601\n",
      "[epoch: 22, i:  2696]  train_loss: 0.561  |  valid_loss: 0.731\n",
      "[epoch: 22, i:  2783]  train_loss: 0.484  |  valid_loss: 0.476\n",
      "[epoch: 22, i:  2870]  train_loss: 0.616  |  valid_loss: 0.640\n",
      "[epoch: 22, i:  2957]  train_loss: 0.600  |  valid_loss: 0.694\n",
      "[epoch: 22, i:  3044]  train_loss: 0.583  |  valid_loss: 0.750\n",
      "[epoch: 22, i:  3131]  train_loss: 0.551  |  valid_loss: 0.616\n",
      "[epoch: 22, i:  3218]  train_loss: 0.600  |  valid_loss: 0.836\n",
      "[epoch: 22, i:  3305]  train_loss: 0.601  |  valid_loss: 0.670\n",
      "[epoch: 22, i:  3392]  train_loss: 0.526  |  valid_loss: 0.655\n",
      "[epoch: 22, i:  3479]  train_loss: 0.579  |  valid_loss: 0.435\n",
      "[epoch: 22, i:  3566]  train_loss: 0.542  |  valid_loss: 0.568\n",
      "[epoch: 22, i:  3653]  train_loss: 0.574  |  valid_loss: 0.745\n",
      "[epoch: 22, i:  3740]  train_loss: 0.655  |  valid_loss: 0.391\n",
      "[epoch: 22, i:  3827]  train_loss: 0.617  |  valid_loss: 0.734\n",
      "[epoch: 22, i:  3914]  train_loss: 0.570  |  valid_loss: 0.549\n",
      "[epoch: 22, i:  4001]  train_loss: 0.575  |  valid_loss: 0.635\n",
      "[epoch: 22, i:  4088]  train_loss: 0.534  |  valid_loss: 0.624\n",
      "[epoch: 22, i:  4175]  train_loss: 0.538  |  valid_loss: 0.632\n",
      "[epoch: 22, i:  4262]  train_loss: 0.596  |  valid_loss: 0.534\n",
      "[epoch: 22, i:  4349]  train_loss: 0.607  |  valid_loss: 0.633\n",
      "[epoch: 22, i:  4436]  train_loss: 0.578  |  valid_loss: 0.579\n",
      "[epoch: 22, i:  4523]  train_loss: 0.531  |  valid_loss: 0.799\n",
      "[epoch: 22, i:  4610]  train_loss: 0.635  |  valid_loss: 0.546\n",
      "[epoch: 22, i:  4697]  train_loss: 0.597  |  valid_loss: 0.574\n",
      "[epoch: 22, i:  4784]  train_loss: 0.661  |  valid_loss: 0.642\n",
      "[epoch: 22, i:  4871]  train_loss: 0.610  |  valid_loss: 0.663\n",
      "[epoch: 22, i:  4958]  train_loss: 0.580  |  valid_loss: 0.752\n",
      "[epoch: 22, i:  5045]  train_loss: 0.601  |  valid_loss: 0.443\n",
      "[epoch: 22, i:  5132]  train_loss: 0.589  |  valid_loss: 0.617\n",
      "[epoch: 22, i:  5219]  train_loss: 0.640  |  valid_loss: 0.818\n",
      "[epoch: 22, i:  5306]  train_loss: 0.537  |  valid_loss: 0.690\n",
      "[epoch: 22, i:  5393]  train_loss: 0.601  |  valid_loss: 0.595\n",
      "[epoch: 22, i:  5480]  train_loss: 0.634  |  valid_loss: 0.807\n",
      "[epoch: 22, i:  5567]  train_loss: 0.608  |  valid_loss: 0.415\n",
      "[epoch: 22, i:  5654]  train_loss: 0.702  |  valid_loss: 0.732\n",
      "[epoch: 22, i:  5741]  train_loss: 0.584  |  valid_loss: 0.665\n",
      "[epoch: 22, i:  5828]  train_loss: 0.623  |  valid_loss: 0.560\n",
      "[epoch: 22, i:  5915]  train_loss: 0.655  |  valid_loss: 0.788\n",
      "[epoch: 22, i:  6002]  train_loss: 0.573  |  valid_loss: 0.616\n",
      "[epoch: 22, i:  6089]  train_loss: 0.597  |  valid_loss: 0.787\n",
      "[epoch: 22, i:  6176]  train_loss: 0.606  |  valid_loss: 0.731\n",
      "[epoch: 22, i:  6263]  train_loss: 0.588  |  valid_loss: 0.577\n",
      "[epoch: 22, i:  6350]  train_loss: 0.594  |  valid_loss: 0.593\n",
      "[epoch: 22, i:  6437]  train_loss: 0.607  |  valid_loss: 0.336\n",
      "[epoch: 22, i:  6524]  train_loss: 0.553  |  valid_loss: 0.797\n",
      "[epoch: 22, i:  6611]  train_loss: 0.576  |  valid_loss: 0.438\n",
      "[epoch: 22, i:  6698]  train_loss: 0.570  |  valid_loss: 0.745\n",
      "[epoch: 22, i:  6785]  train_loss: 0.558  |  valid_loss: 0.554\n",
      "[epoch: 22, i:  6872]  train_loss: 0.552  |  valid_loss: 0.713\n",
      "[epoch: 22, i:  6959]  train_loss: 0.620  |  valid_loss: 0.650\n",
      "[epoch: 22, i:  7046]  train_loss: 0.582  |  valid_loss: 0.640\n",
      "[epoch: 22, i:  7133]  train_loss: 0.592  |  valid_loss: 0.554\n",
      "[epoch: 22, i:  7220]  train_loss: 0.641  |  valid_loss: 0.679\n",
      "[epoch: 22, i:  7307]  train_loss: 0.650  |  valid_loss: 0.673\n",
      "[epoch: 22, i:  7394]  train_loss: 0.582  |  valid_loss: 0.552\n",
      "[epoch: 22, i:  7481]  train_loss: 0.567  |  valid_loss: 0.649\n",
      "[epoch: 22, i:  7568]  train_loss: 0.625  |  valid_loss: 0.801\n",
      "[epoch: 22, i:  7655]  train_loss: 0.528  |  valid_loss: 0.633\n",
      "[epoch: 22, i:  7742]  train_loss: 0.572  |  valid_loss: 0.599\n",
      "[epoch: 22, i:  7829]  train_loss: 0.591  |  valid_loss: 0.518\n",
      "[epoch: 22, i:  7916]  train_loss: 0.589  |  valid_loss: 0.699\n",
      "[epoch: 22, i:  8003]  train_loss: 0.649  |  valid_loss: 0.513\n",
      "[epoch: 22, i:  8090]  train_loss: 0.550  |  valid_loss: 0.522\n",
      "[epoch: 22, i:  8177]  train_loss: 0.568  |  valid_loss: 0.603\n",
      "[epoch: 22, i:  8264]  train_loss: 0.542  |  valid_loss: 0.725\n",
      "[epoch: 22, i:  8351]  train_loss: 0.562  |  valid_loss: 0.714\n",
      "[epoch: 22, i:  8438]  train_loss: 0.578  |  valid_loss: 0.427\n",
      "[epoch: 22, i:  8525]  train_loss: 0.622  |  valid_loss: 0.626\n",
      "[epoch: 22, i:  8612]  train_loss: 0.594  |  valid_loss: 0.805\n",
      "[epoch: 22, i:  8699]  train_loss: 0.572  |  valid_loss: 0.660\n",
      "--> [End of epoch 22] train_accuracy: 79.63%  |  valid_accuracy: 78.55%\n",
      "--> [Start of epoch 23]  lr: 0.000500\n",
      "[epoch: 23, i:    86]  train_loss: 0.567  |  valid_loss: 0.573\n",
      "[epoch: 23, i:   173]  train_loss: 0.587  |  valid_loss: 0.606\n",
      "[epoch: 23, i:   260]  train_loss: 0.521  |  valid_loss: 0.676\n",
      "[epoch: 23, i:   347]  train_loss: 0.600  |  valid_loss: 0.670\n",
      "[epoch: 23, i:   434]  train_loss: 0.574  |  valid_loss: 0.664\n",
      "[epoch: 23, i:   521]  train_loss: 0.551  |  valid_loss: 0.472\n",
      "[epoch: 23, i:   608]  train_loss: 0.593  |  valid_loss: 0.684\n",
      "[epoch: 23, i:   695]  train_loss: 0.598  |  valid_loss: 0.702\n",
      "[epoch: 23, i:   782]  train_loss: 0.523  |  valid_loss: 0.705\n",
      "[epoch: 23, i:   869]  train_loss: 0.561  |  valid_loss: 0.654\n",
      "[epoch: 23, i:   956]  train_loss: 0.500  |  valid_loss: 0.510\n",
      "[epoch: 23, i:  1043]  train_loss: 0.547  |  valid_loss: 0.550\n",
      "[epoch: 23, i:  1130]  train_loss: 0.566  |  valid_loss: 0.549\n",
      "[epoch: 23, i:  1217]  train_loss: 0.551  |  valid_loss: 0.709\n",
      "[epoch: 23, i:  1304]  train_loss: 0.581  |  valid_loss: 0.409\n",
      "[epoch: 23, i:  1391]  train_loss: 0.559  |  valid_loss: 0.661\n",
      "[epoch: 23, i:  1478]  train_loss: 0.581  |  valid_loss: 0.671\n",
      "[epoch: 23, i:  1565]  train_loss: 0.588  |  valid_loss: 0.696\n",
      "[epoch: 23, i:  1652]  train_loss: 0.571  |  valid_loss: 0.652\n",
      "[epoch: 23, i:  1739]  train_loss: 0.543  |  valid_loss: 0.842\n",
      "[epoch: 23, i:  1826]  train_loss: 0.635  |  valid_loss: 0.726\n",
      "[epoch: 23, i:  1913]  train_loss: 0.555  |  valid_loss: 0.666\n",
      "[epoch: 23, i:  2000]  train_loss: 0.552  |  valid_loss: 0.668\n",
      "[epoch: 23, i:  2087]  train_loss: 0.569  |  valid_loss: 0.798\n",
      "[epoch: 23, i:  2174]  train_loss: 0.533  |  valid_loss: 0.515\n",
      "[epoch: 23, i:  2261]  train_loss: 0.556  |  valid_loss: 0.906\n",
      "[epoch: 23, i:  2348]  train_loss: 0.554  |  valid_loss: 0.502\n",
      "[epoch: 23, i:  2435]  train_loss: 0.597  |  valid_loss: 0.693\n",
      "[epoch: 23, i:  2522]  train_loss: 0.574  |  valid_loss: 0.520\n",
      "[epoch: 23, i:  2609]  train_loss: 0.592  |  valid_loss: 0.537\n",
      "[epoch: 23, i:  2696]  train_loss: 0.541  |  valid_loss: 0.673\n",
      "[epoch: 23, i:  2783]  train_loss: 0.534  |  valid_loss: 0.457\n",
      "[epoch: 23, i:  2870]  train_loss: 0.550  |  valid_loss: 0.685\n",
      "[epoch: 23, i:  2957]  train_loss: 0.570  |  valid_loss: 0.731\n",
      "[epoch: 23, i:  3044]  train_loss: 0.513  |  valid_loss: 0.769\n",
      "[epoch: 23, i:  3131]  train_loss: 0.594  |  valid_loss: 0.556\n",
      "[epoch: 23, i:  3218]  train_loss: 0.563  |  valid_loss: 0.754\n",
      "[epoch: 23, i:  3305]  train_loss: 0.557  |  valid_loss: 0.610\n",
      "[epoch: 23, i:  3392]  train_loss: 0.663  |  valid_loss: 0.562\n",
      "[epoch: 23, i:  3479]  train_loss: 0.617  |  valid_loss: 0.489\n",
      "[epoch: 23, i:  3566]  train_loss: 0.510  |  valid_loss: 0.635\n",
      "[epoch: 23, i:  3653]  train_loss: 0.536  |  valid_loss: 0.617\n",
      "[epoch: 23, i:  3740]  train_loss: 0.523  |  valid_loss: 0.409\n",
      "[epoch: 23, i:  3827]  train_loss: 0.577  |  valid_loss: 0.788\n",
      "[epoch: 23, i:  3914]  train_loss: 0.622  |  valid_loss: 0.482\n",
      "[epoch: 23, i:  4001]  train_loss: 0.577  |  valid_loss: 0.642\n",
      "[epoch: 23, i:  4088]  train_loss: 0.634  |  valid_loss: 0.620\n",
      "[epoch: 23, i:  4175]  train_loss: 0.568  |  valid_loss: 0.513\n",
      "[epoch: 23, i:  4262]  train_loss: 0.581  |  valid_loss: 0.544\n",
      "[epoch: 23, i:  4349]  train_loss: 0.572  |  valid_loss: 0.618\n",
      "[epoch: 23, i:  4436]  train_loss: 0.557  |  valid_loss: 0.596\n",
      "[epoch: 23, i:  4523]  train_loss: 0.626  |  valid_loss: 0.647\n",
      "[epoch: 23, i:  4610]  train_loss: 0.601  |  valid_loss: 0.609\n",
      "[epoch: 23, i:  4697]  train_loss: 0.592  |  valid_loss: 0.589\n",
      "[epoch: 23, i:  4784]  train_loss: 0.573  |  valid_loss: 0.687\n",
      "[epoch: 23, i:  4871]  train_loss: 0.622  |  valid_loss: 0.738\n",
      "[epoch: 23, i:  4958]  train_loss: 0.587  |  valid_loss: 0.839\n",
      "[epoch: 23, i:  5045]  train_loss: 0.598  |  valid_loss: 0.482\n",
      "[epoch: 23, i:  5132]  train_loss: 0.610  |  valid_loss: 0.682\n",
      "[epoch: 23, i:  5219]  train_loss: 0.548  |  valid_loss: 0.727\n",
      "[epoch: 23, i:  5306]  train_loss: 0.576  |  valid_loss: 0.719\n",
      "[epoch: 23, i:  5393]  train_loss: 0.626  |  valid_loss: 0.594\n",
      "[epoch: 23, i:  5480]  train_loss: 0.539  |  valid_loss: 0.831\n",
      "[epoch: 23, i:  5567]  train_loss: 0.670  |  valid_loss: 0.417\n",
      "[epoch: 23, i:  5654]  train_loss: 0.631  |  valid_loss: 0.606\n",
      "[epoch: 23, i:  5741]  train_loss: 0.581  |  valid_loss: 0.674\n",
      "[epoch: 23, i:  5828]  train_loss: 0.575  |  valid_loss: 0.593\n",
      "[epoch: 23, i:  5915]  train_loss: 0.599  |  valid_loss: 0.730\n",
      "[epoch: 23, i:  6002]  train_loss: 0.589  |  valid_loss: 0.584\n",
      "[epoch: 23, i:  6089]  train_loss: 0.568  |  valid_loss: 0.788\n",
      "[epoch: 23, i:  6176]  train_loss: 0.625  |  valid_loss: 0.624\n",
      "[epoch: 23, i:  6263]  train_loss: 0.568  |  valid_loss: 0.532\n",
      "[epoch: 23, i:  6350]  train_loss: 0.568  |  valid_loss: 0.559\n",
      "[epoch: 23, i:  6437]  train_loss: 0.587  |  valid_loss: 0.404\n",
      "[epoch: 23, i:  6524]  train_loss: 0.632  |  valid_loss: 0.783\n",
      "[epoch: 23, i:  6611]  train_loss: 0.636  |  valid_loss: 0.457\n",
      "[epoch: 23, i:  6698]  train_loss: 0.628  |  valid_loss: 0.672\n",
      "[epoch: 23, i:  6785]  train_loss: 0.641  |  valid_loss: 0.583\n",
      "[epoch: 23, i:  6872]  train_loss: 0.547  |  valid_loss: 0.771\n",
      "[epoch: 23, i:  6959]  train_loss: 0.669  |  valid_loss: 0.679\n",
      "[epoch: 23, i:  7046]  train_loss: 0.500  |  valid_loss: 0.618\n",
      "[epoch: 23, i:  7133]  train_loss: 0.547  |  valid_loss: 0.470\n",
      "[epoch: 23, i:  7220]  train_loss: 0.554  |  valid_loss: 0.796\n",
      "[epoch: 23, i:  7307]  train_loss: 0.549  |  valid_loss: 0.808\n",
      "[epoch: 23, i:  7394]  train_loss: 0.688  |  valid_loss: 0.537\n",
      "[epoch: 23, i:  7481]  train_loss: 0.646  |  valid_loss: 0.576\n",
      "[epoch: 23, i:  7568]  train_loss: 0.604  |  valid_loss: 0.716\n",
      "[epoch: 23, i:  7655]  train_loss: 0.530  |  valid_loss: 0.647\n",
      "[epoch: 23, i:  7742]  train_loss: 0.611  |  valid_loss: 0.576\n",
      "[epoch: 23, i:  7829]  train_loss: 0.584  |  valid_loss: 0.404\n",
      "[epoch: 23, i:  7916]  train_loss: 0.669  |  valid_loss: 0.690\n",
      "[epoch: 23, i:  8003]  train_loss: 0.539  |  valid_loss: 0.485\n",
      "[epoch: 23, i:  8090]  train_loss: 0.565  |  valid_loss: 0.498\n",
      "[epoch: 23, i:  8177]  train_loss: 0.556  |  valid_loss: 0.529\n",
      "[epoch: 23, i:  8264]  train_loss: 0.616  |  valid_loss: 0.637\n",
      "[epoch: 23, i:  8351]  train_loss: 0.575  |  valid_loss: 0.701\n",
      "[epoch: 23, i:  8438]  train_loss: 0.670  |  valid_loss: 0.478\n",
      "[epoch: 23, i:  8525]  train_loss: 0.542  |  valid_loss: 0.657\n",
      "[epoch: 23, i:  8612]  train_loss: 0.588  |  valid_loss: 0.928\n",
      "[epoch: 23, i:  8699]  train_loss: 0.600  |  valid_loss: 0.607\n",
      "--> [End of epoch 23] train_accuracy: 79.80%  |  valid_accuracy: 78.60%\n",
      "--> [Start of epoch 24]  lr: 0.000500\n",
      "[epoch: 24, i:    86]  train_loss: 0.582  |  valid_loss: 0.545\n",
      "[epoch: 24, i:   173]  train_loss: 0.610  |  valid_loss: 0.545\n",
      "[epoch: 24, i:   260]  train_loss: 0.583  |  valid_loss: 0.647\n",
      "[epoch: 24, i:   347]  train_loss: 0.551  |  valid_loss: 0.550\n",
      "[epoch: 24, i:   434]  train_loss: 0.644  |  valid_loss: 0.734\n",
      "[epoch: 24, i:   521]  train_loss: 0.580  |  valid_loss: 0.412\n",
      "[epoch: 24, i:   608]  train_loss: 0.538  |  valid_loss: 0.643\n",
      "[epoch: 24, i:   695]  train_loss: 0.604  |  valid_loss: 0.572\n",
      "[epoch: 24, i:   782]  train_loss: 0.568  |  valid_loss: 0.668\n",
      "[epoch: 24, i:   869]  train_loss: 0.645  |  valid_loss: 0.729\n",
      "[epoch: 24, i:   956]  train_loss: 0.592  |  valid_loss: 0.532\n",
      "[epoch: 24, i:  1043]  train_loss: 0.509  |  valid_loss: 0.603\n",
      "[epoch: 24, i:  1130]  train_loss: 0.539  |  valid_loss: 0.561\n",
      "[epoch: 24, i:  1217]  train_loss: 0.460  |  valid_loss: 0.654\n",
      "[epoch: 24, i:  1304]  train_loss: 0.592  |  valid_loss: 0.414\n",
      "[epoch: 24, i:  1391]  train_loss: 0.580  |  valid_loss: 0.681\n",
      "[epoch: 24, i:  1478]  train_loss: 0.565  |  valid_loss: 0.585\n",
      "[epoch: 24, i:  1565]  train_loss: 0.605  |  valid_loss: 0.745\n",
      "[epoch: 24, i:  1652]  train_loss: 0.530  |  valid_loss: 0.583\n",
      "[epoch: 24, i:  1739]  train_loss: 0.495  |  valid_loss: 0.921\n",
      "[epoch: 24, i:  1826]  train_loss: 0.545  |  valid_loss: 0.774\n",
      "[epoch: 24, i:  1913]  train_loss: 0.525  |  valid_loss: 0.586\n",
      "[epoch: 24, i:  2000]  train_loss: 0.544  |  valid_loss: 0.683\n",
      "[epoch: 24, i:  2087]  train_loss: 0.555  |  valid_loss: 0.783\n",
      "[epoch: 24, i:  2174]  train_loss: 0.584  |  valid_loss: 0.525\n",
      "[epoch: 24, i:  2261]  train_loss: 0.537  |  valid_loss: 0.877\n",
      "[epoch: 24, i:  2348]  train_loss: 0.630  |  valid_loss: 0.525\n",
      "[epoch: 24, i:  2435]  train_loss: 0.529  |  valid_loss: 0.700\n",
      "[epoch: 24, i:  2522]  train_loss: 0.583  |  valid_loss: 0.523\n",
      "[epoch: 24, i:  2609]  train_loss: 0.578  |  valid_loss: 0.574\n",
      "[epoch: 24, i:  2696]  train_loss: 0.666  |  valid_loss: 0.747\n",
      "[epoch: 24, i:  2783]  train_loss: 0.534  |  valid_loss: 0.479\n",
      "[epoch: 24, i:  2870]  train_loss: 0.566  |  valid_loss: 0.669\n",
      "[epoch: 24, i:  2957]  train_loss: 0.565  |  valid_loss: 0.800\n",
      "[epoch: 24, i:  3044]  train_loss: 0.552  |  valid_loss: 0.706\n",
      "[epoch: 24, i:  3131]  train_loss: 0.549  |  valid_loss: 0.610\n",
      "[epoch: 24, i:  3218]  train_loss: 0.544  |  valid_loss: 0.716\n",
      "[epoch: 24, i:  3305]  train_loss: 0.566  |  valid_loss: 0.576\n",
      "[epoch: 24, i:  3392]  train_loss: 0.552  |  valid_loss: 0.571\n",
      "[epoch: 24, i:  3479]  train_loss: 0.566  |  valid_loss: 0.497\n",
      "[epoch: 24, i:  3566]  train_loss: 0.637  |  valid_loss: 0.593\n",
      "[epoch: 24, i:  3653]  train_loss: 0.538  |  valid_loss: 0.731\n",
      "[epoch: 24, i:  3740]  train_loss: 0.625  |  valid_loss: 0.441\n",
      "[epoch: 24, i:  3827]  train_loss: 0.562  |  valid_loss: 0.801\n",
      "[epoch: 24, i:  3914]  train_loss: 0.629  |  valid_loss: 0.496\n",
      "[epoch: 24, i:  4001]  train_loss: 0.539  |  valid_loss: 0.658\n",
      "[epoch: 24, i:  4088]  train_loss: 0.603  |  valid_loss: 0.601\n",
      "[epoch: 24, i:  4175]  train_loss: 0.602  |  valid_loss: 0.553\n",
      "[epoch: 24, i:  4262]  train_loss: 0.603  |  valid_loss: 0.450\n",
      "[epoch: 24, i:  4349]  train_loss: 0.541  |  valid_loss: 0.586\n",
      "[epoch: 24, i:  4436]  train_loss: 0.559  |  valid_loss: 0.614\n",
      "[epoch: 24, i:  4523]  train_loss: 0.664  |  valid_loss: 0.734\n",
      "[epoch: 24, i:  4610]  train_loss: 0.578  |  valid_loss: 0.577\n",
      "[epoch: 24, i:  4697]  train_loss: 0.539  |  valid_loss: 0.660\n",
      "[epoch: 24, i:  4784]  train_loss: 0.570  |  valid_loss: 0.662\n",
      "[epoch: 24, i:  4871]  train_loss: 0.642  |  valid_loss: 0.709\n",
      "[epoch: 24, i:  4958]  train_loss: 0.617  |  valid_loss: 0.777\n",
      "[epoch: 24, i:  5045]  train_loss: 0.610  |  valid_loss: 0.453\n",
      "[epoch: 24, i:  5132]  train_loss: 0.583  |  valid_loss: 0.668\n",
      "[epoch: 24, i:  5219]  train_loss: 0.557  |  valid_loss: 0.766\n",
      "[epoch: 24, i:  5306]  train_loss: 0.588  |  valid_loss: 0.686\n",
      "[epoch: 24, i:  5393]  train_loss: 0.613  |  valid_loss: 0.509\n",
      "[epoch: 24, i:  5480]  train_loss: 0.564  |  valid_loss: 0.732\n",
      "[epoch: 24, i:  5567]  train_loss: 0.590  |  valid_loss: 0.363\n",
      "[epoch: 24, i:  5654]  train_loss: 0.660  |  valid_loss: 0.622\n",
      "[epoch: 24, i:  5741]  train_loss: 0.514  |  valid_loss: 0.695\n",
      "[epoch: 24, i:  5828]  train_loss: 0.606  |  valid_loss: 0.624\n",
      "[epoch: 24, i:  5915]  train_loss: 0.609  |  valid_loss: 0.652\n",
      "[epoch: 24, i:  6002]  train_loss: 0.575  |  valid_loss: 0.589\n",
      "[epoch: 24, i:  6089]  train_loss: 0.660  |  valid_loss: 0.797\n",
      "[epoch: 24, i:  6176]  train_loss: 0.596  |  valid_loss: 0.669\n",
      "[epoch: 24, i:  6263]  train_loss: 0.578  |  valid_loss: 0.553\n",
      "[epoch: 24, i:  6350]  train_loss: 0.591  |  valid_loss: 0.581\n",
      "[epoch: 24, i:  6437]  train_loss: 0.517  |  valid_loss: 0.331\n",
      "[epoch: 24, i:  6524]  train_loss: 0.557  |  valid_loss: 0.787\n",
      "[epoch: 24, i:  6611]  train_loss: 0.585  |  valid_loss: 0.438\n",
      "[epoch: 24, i:  6698]  train_loss: 0.581  |  valid_loss: 0.752\n",
      "[epoch: 24, i:  6785]  train_loss: 0.541  |  valid_loss: 0.529\n",
      "[epoch: 24, i:  6872]  train_loss: 0.591  |  valid_loss: 0.681\n",
      "[epoch: 24, i:  6959]  train_loss: 0.517  |  valid_loss: 0.712\n",
      "[epoch: 24, i:  7046]  train_loss: 0.623  |  valid_loss: 0.610\n",
      "[epoch: 24, i:  7133]  train_loss: 0.578  |  valid_loss: 0.501\n",
      "[epoch: 24, i:  7220]  train_loss: 0.675  |  valid_loss: 0.729\n",
      "[epoch: 24, i:  7307]  train_loss: 0.532  |  valid_loss: 0.777\n",
      "[epoch: 24, i:  7394]  train_loss: 0.654  |  valid_loss: 0.567\n",
      "[epoch: 24, i:  7481]  train_loss: 0.590  |  valid_loss: 0.617\n",
      "[epoch: 24, i:  7568]  train_loss: 0.642  |  valid_loss: 0.841\n",
      "[epoch: 24, i:  7655]  train_loss: 0.624  |  valid_loss: 0.665\n",
      "[epoch: 24, i:  7742]  train_loss: 0.638  |  valid_loss: 0.634\n",
      "[epoch: 24, i:  7829]  train_loss: 0.532  |  valid_loss: 0.462\n",
      "[epoch: 24, i:  7916]  train_loss: 0.581  |  valid_loss: 0.702\n",
      "[epoch: 24, i:  8003]  train_loss: 0.542  |  valid_loss: 0.512\n",
      "[epoch: 24, i:  8090]  train_loss: 0.538  |  valid_loss: 0.507\n",
      "[epoch: 24, i:  8177]  train_loss: 0.579  |  valid_loss: 0.583\n",
      "[epoch: 24, i:  8264]  train_loss: 0.590  |  valid_loss: 0.632\n",
      "[epoch: 24, i:  8351]  train_loss: 0.601  |  valid_loss: 0.723\n",
      "[epoch: 24, i:  8438]  train_loss: 0.568  |  valid_loss: 0.466\n",
      "[epoch: 24, i:  8525]  train_loss: 0.550  |  valid_loss: 0.602\n",
      "[epoch: 24, i:  8612]  train_loss: 0.628  |  valid_loss: 0.834\n",
      "[epoch: 24, i:  8699]  train_loss: 0.593  |  valid_loss: 0.622\n",
      "--> [End of epoch 24] train_accuracy: 79.89%  |  valid_accuracy: 78.71%\n",
      "--> [Start of epoch 25]  lr: 0.000500\n",
      "[epoch: 25, i:    86]  train_loss: 0.585  |  valid_loss: 0.540\n",
      "[epoch: 25, i:   173]  train_loss: 0.531  |  valid_loss: 0.589\n",
      "[epoch: 25, i:   260]  train_loss: 0.560  |  valid_loss: 0.692\n",
      "[epoch: 25, i:   347]  train_loss: 0.605  |  valid_loss: 0.510\n",
      "[epoch: 25, i:   434]  train_loss: 0.590  |  valid_loss: 0.729\n",
      "[epoch: 25, i:   521]  train_loss: 0.582  |  valid_loss: 0.437\n",
      "[epoch: 25, i:   608]  train_loss: 0.497  |  valid_loss: 0.696\n",
      "[epoch: 25, i:   695]  train_loss: 0.488  |  valid_loss: 0.636\n",
      "[epoch: 25, i:   782]  train_loss: 0.626  |  valid_loss: 0.708\n",
      "[epoch: 25, i:   869]  train_loss: 0.560  |  valid_loss: 0.755\n",
      "[epoch: 25, i:   956]  train_loss: 0.549  |  valid_loss: 0.530\n",
      "[epoch: 25, i:  1043]  train_loss: 0.560  |  valid_loss: 0.568\n",
      "[epoch: 25, i:  1130]  train_loss: 0.611  |  valid_loss: 0.587\n",
      "[epoch: 25, i:  1217]  train_loss: 0.563  |  valid_loss: 0.716\n",
      "[epoch: 25, i:  1304]  train_loss: 0.569  |  valid_loss: 0.401\n",
      "[epoch: 25, i:  1391]  train_loss: 0.548  |  valid_loss: 0.644\n",
      "[epoch: 25, i:  1478]  train_loss: 0.546  |  valid_loss: 0.625\n",
      "[epoch: 25, i:  1565]  train_loss: 0.609  |  valid_loss: 0.793\n",
      "[epoch: 25, i:  1652]  train_loss: 0.543  |  valid_loss: 0.613\n",
      "[epoch: 25, i:  1739]  train_loss: 0.551  |  valid_loss: 0.911\n",
      "[epoch: 25, i:  1826]  train_loss: 0.536  |  valid_loss: 0.792\n",
      "[epoch: 25, i:  1913]  train_loss: 0.551  |  valid_loss: 0.646\n",
      "[epoch: 25, i:  2000]  train_loss: 0.548  |  valid_loss: 0.638\n",
      "[epoch: 25, i:  2087]  train_loss: 0.581  |  valid_loss: 0.789\n",
      "[epoch: 25, i:  2174]  train_loss: 0.575  |  valid_loss: 0.499\n",
      "[epoch: 25, i:  2261]  train_loss: 0.621  |  valid_loss: 0.832\n",
      "[epoch: 25, i:  2348]  train_loss: 0.523  |  valid_loss: 0.534\n",
      "[epoch: 25, i:  2435]  train_loss: 0.581  |  valid_loss: 0.709\n",
      "[epoch: 25, i:  2522]  train_loss: 0.561  |  valid_loss: 0.477\n",
      "[epoch: 25, i:  2609]  train_loss: 0.585  |  valid_loss: 0.523\n",
      "[epoch: 25, i:  2696]  train_loss: 0.518  |  valid_loss: 0.725\n",
      "[epoch: 25, i:  2783]  train_loss: 0.559  |  valid_loss: 0.510\n",
      "[epoch: 25, i:  2870]  train_loss: 0.612  |  valid_loss: 0.708\n",
      "[epoch: 25, i:  2957]  train_loss: 0.579  |  valid_loss: 0.710\n",
      "[epoch: 25, i:  3044]  train_loss: 0.571  |  valid_loss: 0.798\n",
      "[epoch: 25, i:  3131]  train_loss: 0.542  |  valid_loss: 0.675\n",
      "[epoch: 25, i:  3218]  train_loss: 0.634  |  valid_loss: 0.677\n",
      "[epoch: 25, i:  3305]  train_loss: 0.599  |  valid_loss: 0.718\n",
      "[epoch: 25, i:  3392]  train_loss: 0.557  |  valid_loss: 0.602\n",
      "[epoch: 25, i:  3479]  train_loss: 0.509  |  valid_loss: 0.520\n",
      "[epoch: 25, i:  3566]  train_loss: 0.523  |  valid_loss: 0.569\n",
      "[epoch: 25, i:  3653]  train_loss: 0.556  |  valid_loss: 0.648\n",
      "[epoch: 25, i:  3740]  train_loss: 0.570  |  valid_loss: 0.408\n",
      "[epoch: 25, i:  3827]  train_loss: 0.544  |  valid_loss: 0.879\n",
      "[epoch: 25, i:  3914]  train_loss: 0.577  |  valid_loss: 0.510\n",
      "[epoch: 25, i:  4001]  train_loss: 0.532  |  valid_loss: 0.634\n",
      "[epoch: 25, i:  4088]  train_loss: 0.694  |  valid_loss: 0.687\n",
      "[epoch: 25, i:  4175]  train_loss: 0.579  |  valid_loss: 0.567\n",
      "[epoch: 25, i:  4262]  train_loss: 0.568  |  valid_loss: 0.527\n",
      "[epoch: 25, i:  4349]  train_loss: 0.596  |  valid_loss: 0.563\n",
      "[epoch: 25, i:  4436]  train_loss: 0.605  |  valid_loss: 0.585\n",
      "[epoch: 25, i:  4523]  train_loss: 0.518  |  valid_loss: 0.728\n",
      "[epoch: 25, i:  4610]  train_loss: 0.588  |  valid_loss: 0.550\n",
      "[epoch: 25, i:  4697]  train_loss: 0.601  |  valid_loss: 0.586\n",
      "[epoch: 25, i:  4784]  train_loss: 0.601  |  valid_loss: 0.628\n",
      "[epoch: 25, i:  4871]  train_loss: 0.539  |  valid_loss: 0.715\n",
      "[epoch: 25, i:  4958]  train_loss: 0.488  |  valid_loss: 0.809\n",
      "[epoch: 25, i:  5045]  train_loss: 0.585  |  valid_loss: 0.455\n",
      "[epoch: 25, i:  5132]  train_loss: 0.542  |  valid_loss: 0.601\n",
      "[epoch: 25, i:  5219]  train_loss: 0.560  |  valid_loss: 0.769\n",
      "[epoch: 25, i:  5306]  train_loss: 0.555  |  valid_loss: 0.710\n",
      "[epoch: 25, i:  5393]  train_loss: 0.545  |  valid_loss: 0.539\n",
      "[epoch: 25, i:  5480]  train_loss: 0.612  |  valid_loss: 0.758\n",
      "[epoch: 25, i:  5567]  train_loss: 0.574  |  valid_loss: 0.346\n",
      "[epoch: 25, i:  5654]  train_loss: 0.583  |  valid_loss: 0.608\n",
      "[epoch: 25, i:  5741]  train_loss: 0.563  |  valid_loss: 0.694\n",
      "[epoch: 25, i:  5828]  train_loss: 0.546  |  valid_loss: 0.614\n",
      "[epoch: 25, i:  5915]  train_loss: 0.622  |  valid_loss: 0.726\n",
      "[epoch: 25, i:  6002]  train_loss: 0.645  |  valid_loss: 0.531\n",
      "[epoch: 25, i:  6089]  train_loss: 0.640  |  valid_loss: 0.757\n",
      "[epoch: 25, i:  6176]  train_loss: 0.625  |  valid_loss: 0.673\n",
      "[epoch: 25, i:  6263]  train_loss: 0.582  |  valid_loss: 0.627\n",
      "[epoch: 25, i:  6350]  train_loss: 0.539  |  valid_loss: 0.483\n",
      "[epoch: 25, i:  6437]  train_loss: 0.583  |  valid_loss: 0.387\n",
      "[epoch: 25, i:  6524]  train_loss: 0.573  |  valid_loss: 0.754\n",
      "[epoch: 25, i:  6611]  train_loss: 0.553  |  valid_loss: 0.519\n",
      "[epoch: 25, i:  6698]  train_loss: 0.687  |  valid_loss: 0.636\n",
      "[epoch: 25, i:  6785]  train_loss: 0.531  |  valid_loss: 0.506\n",
      "[epoch: 25, i:  6872]  train_loss: 0.541  |  valid_loss: 0.753\n",
      "[epoch: 25, i:  6959]  train_loss: 0.546  |  valid_loss: 0.712\n",
      "[epoch: 25, i:  7046]  train_loss: 0.590  |  valid_loss: 0.674\n",
      "[epoch: 25, i:  7133]  train_loss: 0.643  |  valid_loss: 0.586\n",
      "[epoch: 25, i:  7220]  train_loss: 0.560  |  valid_loss: 0.756\n",
      "[epoch: 25, i:  7307]  train_loss: 0.614  |  valid_loss: 0.749\n",
      "[epoch: 25, i:  7394]  train_loss: 0.618  |  valid_loss: 0.593\n",
      "[epoch: 25, i:  7481]  train_loss: 0.547  |  valid_loss: 0.642\n",
      "[epoch: 25, i:  7568]  train_loss: 0.630  |  valid_loss: 0.796\n",
      "[epoch: 25, i:  7655]  train_loss: 0.563  |  valid_loss: 0.591\n",
      "[epoch: 25, i:  7742]  train_loss: 0.546  |  valid_loss: 0.573\n",
      "[epoch: 25, i:  7829]  train_loss: 0.527  |  valid_loss: 0.417\n",
      "[epoch: 25, i:  7916]  train_loss: 0.621  |  valid_loss: 0.665\n",
      "[epoch: 25, i:  8003]  train_loss: 0.549  |  valid_loss: 0.528\n",
      "[epoch: 25, i:  8090]  train_loss: 0.573  |  valid_loss: 0.489\n",
      "[epoch: 25, i:  8177]  train_loss: 0.580  |  valid_loss: 0.649\n",
      "[epoch: 25, i:  8264]  train_loss: 0.653  |  valid_loss: 0.682\n",
      "[epoch: 25, i:  8351]  train_loss: 0.555  |  valid_loss: 0.662\n",
      "[epoch: 25, i:  8438]  train_loss: 0.556  |  valid_loss: 0.436\n",
      "[epoch: 25, i:  8525]  train_loss: 0.571  |  valid_loss: 0.628\n",
      "[epoch: 25, i:  8612]  train_loss: 0.507  |  valid_loss: 0.821\n",
      "[epoch: 25, i:  8699]  train_loss: 0.567  |  valid_loss: 0.638\n",
      "--> [End of epoch 25] train_accuracy: 80.25%  |  valid_accuracy: 78.94%\n",
      "--> [Start of epoch 26]  lr: 0.000500\n",
      "[epoch: 26, i:    86]  train_loss: 0.531  |  valid_loss: 0.544\n",
      "[epoch: 26, i:   173]  train_loss: 0.575  |  valid_loss: 0.591\n",
      "[epoch: 26, i:   260]  train_loss: 0.497  |  valid_loss: 0.658\n",
      "[epoch: 26, i:   347]  train_loss: 0.499  |  valid_loss: 0.506\n",
      "[epoch: 26, i:   434]  train_loss: 0.507  |  valid_loss: 0.713\n",
      "[epoch: 26, i:   521]  train_loss: 0.531  |  valid_loss: 0.378\n",
      "[epoch: 26, i:   608]  train_loss: 0.519  |  valid_loss: 0.692\n",
      "[epoch: 26, i:   695]  train_loss: 0.579  |  valid_loss: 0.712\n",
      "[epoch: 26, i:   782]  train_loss: 0.531  |  valid_loss: 0.757\n",
      "[epoch: 26, i:   869]  train_loss: 0.566  |  valid_loss: 0.678\n",
      "[epoch: 26, i:   956]  train_loss: 0.557  |  valid_loss: 0.530\n",
      "[epoch: 26, i:  1043]  train_loss: 0.557  |  valid_loss: 0.605\n",
      "[epoch: 26, i:  1130]  train_loss: 0.531  |  valid_loss: 0.510\n",
      "[epoch: 26, i:  1217]  train_loss: 0.554  |  valid_loss: 0.609\n",
      "[epoch: 26, i:  1304]  train_loss: 0.584  |  valid_loss: 0.442\n",
      "[epoch: 26, i:  1391]  train_loss: 0.574  |  valid_loss: 0.707\n",
      "[epoch: 26, i:  1478]  train_loss: 0.523  |  valid_loss: 0.647\n",
      "[epoch: 26, i:  1565]  train_loss: 0.590  |  valid_loss: 0.767\n",
      "[epoch: 26, i:  1652]  train_loss: 0.549  |  valid_loss: 0.595\n",
      "[epoch: 26, i:  1739]  train_loss: 0.564  |  valid_loss: 0.865\n",
      "[epoch: 26, i:  1826]  train_loss: 0.575  |  valid_loss: 0.753\n",
      "[epoch: 26, i:  1913]  train_loss: 0.554  |  valid_loss: 0.643\n",
      "[epoch: 26, i:  2000]  train_loss: 0.611  |  valid_loss: 0.681\n",
      "[epoch: 26, i:  2087]  train_loss: 0.573  |  valid_loss: 0.804\n",
      "[epoch: 26, i:  2174]  train_loss: 0.551  |  valid_loss: 0.552\n",
      "[epoch: 26, i:  2261]  train_loss: 0.525  |  valid_loss: 0.947\n",
      "[epoch: 26, i:  2348]  train_loss: 0.543  |  valid_loss: 0.497\n",
      "[epoch: 26, i:  2435]  train_loss: 0.519  |  valid_loss: 0.686\n",
      "[epoch: 26, i:  2522]  train_loss: 0.575  |  valid_loss: 0.461\n",
      "[epoch: 26, i:  2609]  train_loss: 0.567  |  valid_loss: 0.626\n",
      "[epoch: 26, i:  2696]  train_loss: 0.572  |  valid_loss: 0.701\n",
      "[epoch: 26, i:  2783]  train_loss: 0.564  |  valid_loss: 0.440\n",
      "[epoch: 26, i:  2870]  train_loss: 0.567  |  valid_loss: 0.722\n",
      "[epoch: 26, i:  2957]  train_loss: 0.538  |  valid_loss: 0.788\n",
      "[epoch: 26, i:  3044]  train_loss: 0.594  |  valid_loss: 0.725\n",
      "[epoch: 26, i:  3131]  train_loss: 0.550  |  valid_loss: 0.648\n",
      "[epoch: 26, i:  3218]  train_loss: 0.544  |  valid_loss: 0.710\n",
      "[epoch: 26, i:  3305]  train_loss: 0.586  |  valid_loss: 0.675\n",
      "[epoch: 26, i:  3392]  train_loss: 0.594  |  valid_loss: 0.596\n",
      "[epoch: 26, i:  3479]  train_loss: 0.581  |  valid_loss: 0.524\n",
      "[epoch: 26, i:  3566]  train_loss: 0.582  |  valid_loss: 0.633\n",
      "[epoch: 26, i:  3653]  train_loss: 0.571  |  valid_loss: 0.748\n",
      "[epoch: 26, i:  3740]  train_loss: 0.473  |  valid_loss: 0.469\n",
      "[epoch: 26, i:  3827]  train_loss: 0.582  |  valid_loss: 0.855\n",
      "[epoch: 26, i:  3914]  train_loss: 0.618  |  valid_loss: 0.488\n",
      "[epoch: 26, i:  4001]  train_loss: 0.597  |  valid_loss: 0.634\n",
      "[epoch: 26, i:  4088]  train_loss: 0.600  |  valid_loss: 0.556\n",
      "[epoch: 26, i:  4175]  train_loss: 0.506  |  valid_loss: 0.551\n",
      "[epoch: 26, i:  4262]  train_loss: 0.532  |  valid_loss: 0.490\n",
      "[epoch: 26, i:  4349]  train_loss: 0.594  |  valid_loss: 0.579\n",
      "[epoch: 26, i:  4436]  train_loss: 0.583  |  valid_loss: 0.585\n",
      "[epoch: 26, i:  4523]  train_loss: 0.517  |  valid_loss: 0.640\n",
      "[epoch: 26, i:  4610]  train_loss: 0.600  |  valid_loss: 0.525\n",
      "[epoch: 26, i:  4697]  train_loss: 0.638  |  valid_loss: 0.550\n",
      "[epoch: 26, i:  4784]  train_loss: 0.554  |  valid_loss: 0.634\n",
      "[epoch: 26, i:  4871]  train_loss: 0.614  |  valid_loss: 0.733\n",
      "[epoch: 26, i:  4958]  train_loss: 0.603  |  valid_loss: 0.737\n",
      "[epoch: 26, i:  5045]  train_loss: 0.578  |  valid_loss: 0.454\n",
      "[epoch: 26, i:  5132]  train_loss: 0.557  |  valid_loss: 0.585\n",
      "[epoch: 26, i:  5219]  train_loss: 0.514  |  valid_loss: 0.762\n",
      "[epoch: 26, i:  5306]  train_loss: 0.534  |  valid_loss: 0.701\n",
      "[epoch: 26, i:  5393]  train_loss: 0.563  |  valid_loss: 0.569\n",
      "[epoch: 26, i:  5480]  train_loss: 0.589  |  valid_loss: 0.823\n",
      "[epoch: 26, i:  5567]  train_loss: 0.534  |  valid_loss: 0.418\n",
      "[epoch: 26, i:  5654]  train_loss: 0.606  |  valid_loss: 0.587\n",
      "[epoch: 26, i:  5741]  train_loss: 0.601  |  valid_loss: 0.737\n",
      "[epoch: 26, i:  5828]  train_loss: 0.565  |  valid_loss: 0.626\n",
      "[epoch: 26, i:  5915]  train_loss: 0.584  |  valid_loss: 0.753\n",
      "[epoch: 26, i:  6002]  train_loss: 0.556  |  valid_loss: 0.526\n",
      "[epoch: 26, i:  6089]  train_loss: 0.549  |  valid_loss: 0.766\n",
      "[epoch: 26, i:  6176]  train_loss: 0.578  |  valid_loss: 0.725\n",
      "[epoch: 26, i:  6263]  train_loss: 0.517  |  valid_loss: 0.558\n",
      "[epoch: 26, i:  6350]  train_loss: 0.562  |  valid_loss: 0.593\n",
      "[epoch: 26, i:  6437]  train_loss: 0.612  |  valid_loss: 0.413\n",
      "[epoch: 26, i:  6524]  train_loss: 0.594  |  valid_loss: 0.766\n",
      "[epoch: 26, i:  6611]  train_loss: 0.545  |  valid_loss: 0.437\n",
      "[epoch: 26, i:  6698]  train_loss: 0.609  |  valid_loss: 0.698\n",
      "[epoch: 26, i:  6785]  train_loss: 0.557  |  valid_loss: 0.445\n",
      "[epoch: 26, i:  6872]  train_loss: 0.594  |  valid_loss: 0.750\n",
      "[epoch: 26, i:  6959]  train_loss: 0.517  |  valid_loss: 0.750\n",
      "[epoch: 26, i:  7046]  train_loss: 0.641  |  valid_loss: 0.619\n",
      "[epoch: 26, i:  7133]  train_loss: 0.564  |  valid_loss: 0.565\n",
      "[epoch: 26, i:  7220]  train_loss: 0.628  |  valid_loss: 0.705\n",
      "[epoch: 26, i:  7307]  train_loss: 0.618  |  valid_loss: 0.775\n",
      "[epoch: 26, i:  7394]  train_loss: 0.557  |  valid_loss: 0.567\n",
      "[epoch: 26, i:  7481]  train_loss: 0.627  |  valid_loss: 0.603\n",
      "[epoch: 26, i:  7568]  train_loss: 0.554  |  valid_loss: 0.740\n",
      "[epoch: 26, i:  7655]  train_loss: 0.575  |  valid_loss: 0.684\n",
      "[epoch: 26, i:  7742]  train_loss: 0.549  |  valid_loss: 0.624\n",
      "[epoch: 26, i:  7829]  train_loss: 0.571  |  valid_loss: 0.457\n",
      "[epoch: 26, i:  7916]  train_loss: 0.576  |  valid_loss: 0.741\n",
      "[epoch: 26, i:  8003]  train_loss: 0.607  |  valid_loss: 0.558\n",
      "[epoch: 26, i:  8090]  train_loss: 0.602  |  valid_loss: 0.498\n",
      "[epoch: 26, i:  8177]  train_loss: 0.597  |  valid_loss: 0.618\n",
      "[epoch: 26, i:  8264]  train_loss: 0.582  |  valid_loss: 0.647\n",
      "[epoch: 26, i:  8351]  train_loss: 0.579  |  valid_loss: 0.653\n",
      "[epoch: 26, i:  8438]  train_loss: 0.588  |  valid_loss: 0.459\n",
      "[epoch: 26, i:  8525]  train_loss: 0.619  |  valid_loss: 0.603\n",
      "[epoch: 26, i:  8612]  train_loss: 0.560  |  valid_loss: 0.852\n",
      "[epoch: 26, i:  8699]  train_loss: 0.622  |  valid_loss: 0.661\n",
      "--> [End of epoch 26] train_accuracy: 80.21%  |  valid_accuracy: 78.68%\n",
      "--> [Start of epoch 27]  lr: 0.000500\n",
      "[epoch: 27, i:    86]  train_loss: 0.492  |  valid_loss: 0.504\n",
      "[epoch: 27, i:   173]  train_loss: 0.578  |  valid_loss: 0.563\n",
      "[epoch: 27, i:   260]  train_loss: 0.563  |  valid_loss: 0.719\n",
      "[epoch: 27, i:   347]  train_loss: 0.534  |  valid_loss: 0.527\n",
      "[epoch: 27, i:   434]  train_loss: 0.455  |  valid_loss: 0.687\n",
      "[epoch: 27, i:   521]  train_loss: 0.603  |  valid_loss: 0.412\n",
      "[epoch: 27, i:   608]  train_loss: 0.482  |  valid_loss: 0.666\n",
      "[epoch: 27, i:   695]  train_loss: 0.577  |  valid_loss: 0.598\n",
      "[epoch: 27, i:   782]  train_loss: 0.526  |  valid_loss: 0.791\n",
      "[epoch: 27, i:   869]  train_loss: 0.526  |  valid_loss: 0.688\n",
      "[epoch: 27, i:   956]  train_loss: 0.508  |  valid_loss: 0.589\n",
      "[epoch: 27, i:  1043]  train_loss: 0.559  |  valid_loss: 0.573\n",
      "[epoch: 27, i:  1130]  train_loss: 0.531  |  valid_loss: 0.607\n",
      "[epoch: 27, i:  1217]  train_loss: 0.621  |  valid_loss: 0.608\n",
      "[epoch: 27, i:  1304]  train_loss: 0.616  |  valid_loss: 0.432\n",
      "[epoch: 27, i:  1391]  train_loss: 0.573  |  valid_loss: 0.692\n",
      "[epoch: 27, i:  1478]  train_loss: 0.516  |  valid_loss: 0.608\n",
      "[epoch: 27, i:  1565]  train_loss: 0.570  |  valid_loss: 0.781\n",
      "[epoch: 27, i:  1652]  train_loss: 0.555  |  valid_loss: 0.568\n",
      "[epoch: 27, i:  1739]  train_loss: 0.553  |  valid_loss: 0.824\n",
      "[epoch: 27, i:  1826]  train_loss: 0.539  |  valid_loss: 0.790\n",
      "[epoch: 27, i:  1913]  train_loss: 0.621  |  valid_loss: 0.668\n",
      "[epoch: 27, i:  2000]  train_loss: 0.502  |  valid_loss: 0.595\n",
      "[epoch: 27, i:  2087]  train_loss: 0.570  |  valid_loss: 0.723\n",
      "[epoch: 27, i:  2174]  train_loss: 0.591  |  valid_loss: 0.540\n",
      "[epoch: 27, i:  2261]  train_loss: 0.605  |  valid_loss: 0.898\n",
      "[epoch: 27, i:  2348]  train_loss: 0.591  |  valid_loss: 0.556\n",
      "[epoch: 27, i:  2435]  train_loss: 0.542  |  valid_loss: 0.640\n",
      "[epoch: 27, i:  2522]  train_loss: 0.601  |  valid_loss: 0.479\n",
      "[epoch: 27, i:  2609]  train_loss: 0.551  |  valid_loss: 0.553\n",
      "[epoch: 27, i:  2696]  train_loss: 0.623  |  valid_loss: 0.675\n",
      "[epoch: 27, i:  2783]  train_loss: 0.617  |  valid_loss: 0.477\n",
      "[epoch: 27, i:  2870]  train_loss: 0.579  |  valid_loss: 0.765\n",
      "[epoch: 27, i:  2957]  train_loss: 0.554  |  valid_loss: 0.645\n",
      "[epoch: 27, i:  3044]  train_loss: 0.538  |  valid_loss: 0.765\n",
      "[epoch: 27, i:  3131]  train_loss: 0.518  |  valid_loss: 0.658\n",
      "[epoch: 27, i:  3218]  train_loss: 0.603  |  valid_loss: 0.715\n",
      "[epoch: 27, i:  3305]  train_loss: 0.545  |  valid_loss: 0.604\n",
      "[epoch: 27, i:  3392]  train_loss: 0.555  |  valid_loss: 0.627\n",
      "[epoch: 27, i:  3479]  train_loss: 0.589  |  valid_loss: 0.480\n",
      "[epoch: 27, i:  3566]  train_loss: 0.584  |  valid_loss: 0.653\n",
      "[epoch: 27, i:  3653]  train_loss: 0.576  |  valid_loss: 0.683\n",
      "[epoch: 27, i:  3740]  train_loss: 0.543  |  valid_loss: 0.366\n",
      "[epoch: 27, i:  3827]  train_loss: 0.556  |  valid_loss: 0.812\n",
      "[epoch: 27, i:  3914]  train_loss: 0.572  |  valid_loss: 0.460\n",
      "[epoch: 27, i:  4001]  train_loss: 0.573  |  valid_loss: 0.640\n",
      "[epoch: 27, i:  4088]  train_loss: 0.568  |  valid_loss: 0.570\n",
      "[epoch: 27, i:  4175]  train_loss: 0.596  |  valid_loss: 0.574\n",
      "[epoch: 27, i:  4262]  train_loss: 0.591  |  valid_loss: 0.590\n",
      "[epoch: 27, i:  4349]  train_loss: 0.575  |  valid_loss: 0.675\n",
      "[epoch: 27, i:  4436]  train_loss: 0.575  |  valid_loss: 0.541\n",
      "[epoch: 27, i:  4523]  train_loss: 0.582  |  valid_loss: 0.725\n",
      "[epoch: 27, i:  4610]  train_loss: 0.586  |  valid_loss: 0.586\n",
      "[epoch: 27, i:  4697]  train_loss: 0.554  |  valid_loss: 0.600\n",
      "[epoch: 27, i:  4784]  train_loss: 0.569  |  valid_loss: 0.686\n",
      "[epoch: 27, i:  4871]  train_loss: 0.555  |  valid_loss: 0.755\n",
      "[epoch: 27, i:  4958]  train_loss: 0.515  |  valid_loss: 0.776\n",
      "[epoch: 27, i:  5045]  train_loss: 0.610  |  valid_loss: 0.442\n",
      "[epoch: 27, i:  5132]  train_loss: 0.589  |  valid_loss: 0.675\n",
      "[epoch: 27, i:  5219]  train_loss: 0.588  |  valid_loss: 0.678\n",
      "[epoch: 27, i:  5306]  train_loss: 0.615  |  valid_loss: 0.682\n",
      "[epoch: 27, i:  5393]  train_loss: 0.620  |  valid_loss: 0.578\n",
      "[epoch: 27, i:  5480]  train_loss: 0.584  |  valid_loss: 0.866\n",
      "[epoch: 27, i:  5567]  train_loss: 0.624  |  valid_loss: 0.381\n",
      "[epoch: 27, i:  5654]  train_loss: 0.494  |  valid_loss: 0.614\n",
      "[epoch: 27, i:  5741]  train_loss: 0.593  |  valid_loss: 0.682\n",
      "[epoch: 27, i:  5828]  train_loss: 0.601  |  valid_loss: 0.687\n",
      "[epoch: 27, i:  5915]  train_loss: 0.552  |  valid_loss: 0.724\n",
      "[epoch: 27, i:  6002]  train_loss: 0.577  |  valid_loss: 0.480\n",
      "[epoch: 27, i:  6089]  train_loss: 0.570  |  valid_loss: 0.837\n",
      "[epoch: 27, i:  6176]  train_loss: 0.603  |  valid_loss: 0.660\n",
      "[epoch: 27, i:  6263]  train_loss: 0.643  |  valid_loss: 0.561\n",
      "[epoch: 27, i:  6350]  train_loss: 0.577  |  valid_loss: 0.558\n",
      "[epoch: 27, i:  6437]  train_loss: 0.568  |  valid_loss: 0.373\n",
      "[epoch: 27, i:  6524]  train_loss: 0.534  |  valid_loss: 0.798\n",
      "[epoch: 27, i:  6611]  train_loss: 0.585  |  valid_loss: 0.421\n",
      "[epoch: 27, i:  6698]  train_loss: 0.564  |  valid_loss: 0.772\n",
      "[epoch: 27, i:  6785]  train_loss: 0.634  |  valid_loss: 0.518\n",
      "[epoch: 27, i:  6872]  train_loss: 0.528  |  valid_loss: 0.695\n",
      "[epoch: 27, i:  6959]  train_loss: 0.581  |  valid_loss: 0.690\n",
      "[epoch: 27, i:  7046]  train_loss: 0.562  |  valid_loss: 0.651\n",
      "[epoch: 27, i:  7133]  train_loss: 0.526  |  valid_loss: 0.571\n",
      "[epoch: 27, i:  7220]  train_loss: 0.584  |  valid_loss: 0.755\n",
      "[epoch: 27, i:  7307]  train_loss: 0.573  |  valid_loss: 0.741\n",
      "[epoch: 27, i:  7394]  train_loss: 0.517  |  valid_loss: 0.609\n",
      "[epoch: 27, i:  7481]  train_loss: 0.539  |  valid_loss: 0.538\n",
      "[epoch: 27, i:  7568]  train_loss: 0.576  |  valid_loss: 0.749\n",
      "[epoch: 27, i:  7655]  train_loss: 0.567  |  valid_loss: 0.619\n",
      "[epoch: 27, i:  7742]  train_loss: 0.538  |  valid_loss: 0.731\n",
      "[epoch: 27, i:  7829]  train_loss: 0.637  |  valid_loss: 0.454\n",
      "[epoch: 27, i:  7916]  train_loss: 0.581  |  valid_loss: 0.683\n",
      "[epoch: 27, i:  8003]  train_loss: 0.537  |  valid_loss: 0.518\n",
      "[epoch: 27, i:  8090]  train_loss: 0.579  |  valid_loss: 0.517\n",
      "[epoch: 27, i:  8177]  train_loss: 0.555  |  valid_loss: 0.571\n",
      "[epoch: 27, i:  8264]  train_loss: 0.559  |  valid_loss: 0.614\n",
      "[epoch: 27, i:  8351]  train_loss: 0.575  |  valid_loss: 0.749\n",
      "[epoch: 27, i:  8438]  train_loss: 0.596  |  valid_loss: 0.484\n",
      "[epoch: 27, i:  8525]  train_loss: 0.536  |  valid_loss: 0.611\n",
      "[epoch: 27, i:  8612]  train_loss: 0.600  |  valid_loss: 0.892\n",
      "[epoch: 27, i:  8699]  train_loss: 0.570  |  valid_loss: 0.636\n",
      "--> [End of epoch 27] train_accuracy: 80.35%  |  valid_accuracy: 78.82%\n",
      "--> [Start of epoch 28]  lr: 0.000500\n",
      "[epoch: 28, i:    86]  train_loss: 0.536  |  valid_loss: 0.486\n",
      "[epoch: 28, i:   173]  train_loss: 0.489  |  valid_loss: 0.518\n",
      "[epoch: 28, i:   260]  train_loss: 0.561  |  valid_loss: 0.649\n",
      "[epoch: 28, i:   347]  train_loss: 0.584  |  valid_loss: 0.547\n",
      "[epoch: 28, i:   434]  train_loss: 0.551  |  valid_loss: 0.689\n",
      "[epoch: 28, i:   521]  train_loss: 0.585  |  valid_loss: 0.426\n",
      "[epoch: 28, i:   608]  train_loss: 0.528  |  valid_loss: 0.651\n",
      "[epoch: 28, i:   695]  train_loss: 0.563  |  valid_loss: 0.569\n",
      "[epoch: 28, i:   782]  train_loss: 0.556  |  valid_loss: 0.760\n",
      "[epoch: 28, i:   869]  train_loss: 0.572  |  valid_loss: 0.673\n",
      "[epoch: 28, i:   956]  train_loss: 0.530  |  valid_loss: 0.561\n",
      "[epoch: 28, i:  1043]  train_loss: 0.520  |  valid_loss: 0.561\n",
      "[epoch: 28, i:  1130]  train_loss: 0.528  |  valid_loss: 0.522\n",
      "[epoch: 28, i:  1217]  train_loss: 0.492  |  valid_loss: 0.590\n",
      "[epoch: 28, i:  1304]  train_loss: 0.566  |  valid_loss: 0.535\n",
      "[epoch: 28, i:  1391]  train_loss: 0.556  |  valid_loss: 0.645\n",
      "[epoch: 28, i:  1478]  train_loss: 0.548  |  valid_loss: 0.691\n",
      "[epoch: 28, i:  1565]  train_loss: 0.490  |  valid_loss: 0.725\n",
      "[epoch: 28, i:  1652]  train_loss: 0.550  |  valid_loss: 0.588\n",
      "[epoch: 28, i:  1739]  train_loss: 0.545  |  valid_loss: 0.850\n",
      "[epoch: 28, i:  1826]  train_loss: 0.616  |  valid_loss: 0.810\n",
      "[epoch: 28, i:  1913]  train_loss: 0.560  |  valid_loss: 0.641\n",
      "[epoch: 28, i:  2000]  train_loss: 0.505  |  valid_loss: 0.654\n",
      "[epoch: 28, i:  2087]  train_loss: 0.512  |  valid_loss: 0.781\n",
      "[epoch: 28, i:  2174]  train_loss: 0.468  |  valid_loss: 0.582\n",
      "[epoch: 28, i:  2261]  train_loss: 0.538  |  valid_loss: 0.837\n",
      "[epoch: 28, i:  2348]  train_loss: 0.569  |  valid_loss: 0.490\n",
      "[epoch: 28, i:  2435]  train_loss: 0.522  |  valid_loss: 0.610\n",
      "[epoch: 28, i:  2522]  train_loss: 0.572  |  valid_loss: 0.538\n",
      "[epoch: 28, i:  2609]  train_loss: 0.558  |  valid_loss: 0.553\n",
      "[epoch: 28, i:  2696]  train_loss: 0.576  |  valid_loss: 0.698\n",
      "[epoch: 28, i:  2783]  train_loss: 0.494  |  valid_loss: 0.445\n",
      "[epoch: 28, i:  2870]  train_loss: 0.561  |  valid_loss: 0.720\n",
      "[epoch: 28, i:  2957]  train_loss: 0.554  |  valid_loss: 0.631\n",
      "[epoch: 28, i:  3044]  train_loss: 0.547  |  valid_loss: 0.740\n",
      "[epoch: 28, i:  3131]  train_loss: 0.591  |  valid_loss: 0.602\n",
      "[epoch: 28, i:  3218]  train_loss: 0.483  |  valid_loss: 0.669\n",
      "[epoch: 28, i:  3305]  train_loss: 0.607  |  valid_loss: 0.543\n",
      "[epoch: 28, i:  3392]  train_loss: 0.623  |  valid_loss: 0.503\n",
      "[epoch: 28, i:  3479]  train_loss: 0.518  |  valid_loss: 0.500\n",
      "[epoch: 28, i:  3566]  train_loss: 0.534  |  valid_loss: 0.554\n",
      "[epoch: 28, i:  3653]  train_loss: 0.516  |  valid_loss: 0.704\n",
      "[epoch: 28, i:  3740]  train_loss: 0.614  |  valid_loss: 0.463\n",
      "[epoch: 28, i:  3827]  train_loss: 0.561  |  valid_loss: 0.792\n",
      "[epoch: 28, i:  3914]  train_loss: 0.554  |  valid_loss: 0.565\n",
      "[epoch: 28, i:  4001]  train_loss: 0.613  |  valid_loss: 0.580\n",
      "[epoch: 28, i:  4088]  train_loss: 0.572  |  valid_loss: 0.616\n",
      "[epoch: 28, i:  4175]  train_loss: 0.538  |  valid_loss: 0.602\n",
      "[epoch: 28, i:  4262]  train_loss: 0.573  |  valid_loss: 0.490\n",
      "[epoch: 28, i:  4349]  train_loss: 0.525  |  valid_loss: 0.504\n",
      "[epoch: 28, i:  4436]  train_loss: 0.597  |  valid_loss: 0.594\n",
      "[epoch: 28, i:  4523]  train_loss: 0.600  |  valid_loss: 0.715\n",
      "[epoch: 28, i:  4610]  train_loss: 0.586  |  valid_loss: 0.553\n",
      "[epoch: 28, i:  4697]  train_loss: 0.608  |  valid_loss: 0.561\n",
      "[epoch: 28, i:  4784]  train_loss: 0.554  |  valid_loss: 0.614\n",
      "[epoch: 28, i:  4871]  train_loss: 0.629  |  valid_loss: 0.709\n",
      "[epoch: 28, i:  4958]  train_loss: 0.597  |  valid_loss: 0.831\n",
      "[epoch: 28, i:  5045]  train_loss: 0.569  |  valid_loss: 0.457\n",
      "[epoch: 28, i:  5132]  train_loss: 0.596  |  valid_loss: 0.672\n",
      "[epoch: 28, i:  5219]  train_loss: 0.660  |  valid_loss: 0.769\n",
      "[epoch: 28, i:  5306]  train_loss: 0.561  |  valid_loss: 0.647\n",
      "[epoch: 28, i:  5393]  train_loss: 0.577  |  valid_loss: 0.578\n",
      "[epoch: 28, i:  5480]  train_loss: 0.543  |  valid_loss: 0.777\n",
      "[epoch: 28, i:  5567]  train_loss: 0.577  |  valid_loss: 0.350\n",
      "[epoch: 28, i:  5654]  train_loss: 0.595  |  valid_loss: 0.659\n",
      "[epoch: 28, i:  5741]  train_loss: 0.623  |  valid_loss: 0.725\n",
      "[epoch: 28, i:  5828]  train_loss: 0.534  |  valid_loss: 0.631\n",
      "[epoch: 28, i:  5915]  train_loss: 0.567  |  valid_loss: 0.763\n",
      "[epoch: 28, i:  6002]  train_loss: 0.576  |  valid_loss: 0.501\n",
      "[epoch: 28, i:  6089]  train_loss: 0.550  |  valid_loss: 0.713\n",
      "[epoch: 28, i:  6176]  train_loss: 0.575  |  valid_loss: 0.631\n",
      "[epoch: 28, i:  6263]  train_loss: 0.574  |  valid_loss: 0.525\n",
      "[epoch: 28, i:  6350]  train_loss: 0.634  |  valid_loss: 0.569\n",
      "[epoch: 28, i:  6437]  train_loss: 0.611  |  valid_loss: 0.400\n",
      "[epoch: 28, i:  6524]  train_loss: 0.595  |  valid_loss: 0.807\n",
      "[epoch: 28, i:  6611]  train_loss: 0.556  |  valid_loss: 0.409\n",
      "[epoch: 28, i:  6698]  train_loss: 0.575  |  valid_loss: 0.725\n",
      "[epoch: 28, i:  6785]  train_loss: 0.545  |  valid_loss: 0.492\n",
      "[epoch: 28, i:  6872]  train_loss: 0.559  |  valid_loss: 0.748\n",
      "[epoch: 28, i:  6959]  train_loss: 0.511  |  valid_loss: 0.676\n",
      "[epoch: 28, i:  7046]  train_loss: 0.582  |  valid_loss: 0.589\n",
      "[epoch: 28, i:  7133]  train_loss: 0.571  |  valid_loss: 0.523\n",
      "[epoch: 28, i:  7220]  train_loss: 0.576  |  valid_loss: 0.689\n",
      "[epoch: 28, i:  7307]  train_loss: 0.594  |  valid_loss: 0.719\n",
      "[epoch: 28, i:  7394]  train_loss: 0.526  |  valid_loss: 0.555\n",
      "[epoch: 28, i:  7481]  train_loss: 0.564  |  valid_loss: 0.545\n",
      "[epoch: 28, i:  7568]  train_loss: 0.609  |  valid_loss: 0.717\n",
      "[epoch: 28, i:  7655]  train_loss: 0.525  |  valid_loss: 0.602\n",
      "[epoch: 28, i:  7742]  train_loss: 0.556  |  valid_loss: 0.603\n",
      "[epoch: 28, i:  7829]  train_loss: 0.539  |  valid_loss: 0.461\n",
      "[epoch: 28, i:  7916]  train_loss: 0.521  |  valid_loss: 0.700\n",
      "[epoch: 28, i:  8003]  train_loss: 0.677  |  valid_loss: 0.469\n",
      "[epoch: 28, i:  8090]  train_loss: 0.533  |  valid_loss: 0.552\n",
      "[epoch: 28, i:  8177]  train_loss: 0.651  |  valid_loss: 0.564\n",
      "[epoch: 28, i:  8264]  train_loss: 0.588  |  valid_loss: 0.571\n",
      "[epoch: 28, i:  8351]  train_loss: 0.557  |  valid_loss: 0.670\n",
      "[epoch: 28, i:  8438]  train_loss: 0.564  |  valid_loss: 0.506\n",
      "[epoch: 28, i:  8525]  train_loss: 0.525  |  valid_loss: 0.594\n",
      "[epoch: 28, i:  8612]  train_loss: 0.579  |  valid_loss: 0.823\n",
      "[epoch: 28, i:  8699]  train_loss: 0.617  |  valid_loss: 0.656\n",
      "--> [End of epoch 28] train_accuracy: 80.39%  |  valid_accuracy: 78.82%\n",
      "--> [Start of epoch 29]  lr: 0.000500\n",
      "[epoch: 29, i:    86]  train_loss: 0.542  |  valid_loss: 0.581\n",
      "[epoch: 29, i:   173]  train_loss: 0.585  |  valid_loss: 0.595\n",
      "[epoch: 29, i:   260]  train_loss: 0.536  |  valid_loss: 0.688\n",
      "[epoch: 29, i:   347]  train_loss: 0.505  |  valid_loss: 0.558\n",
      "[epoch: 29, i:   434]  train_loss: 0.482  |  valid_loss: 0.704\n",
      "[epoch: 29, i:   521]  train_loss: 0.547  |  valid_loss: 0.425\n",
      "[epoch: 29, i:   608]  train_loss: 0.629  |  valid_loss: 0.632\n",
      "[epoch: 29, i:   695]  train_loss: 0.593  |  valid_loss: 0.678\n",
      "[epoch: 29, i:   782]  train_loss: 0.512  |  valid_loss: 0.686\n",
      "[epoch: 29, i:   869]  train_loss: 0.603  |  valid_loss: 0.692\n",
      "[epoch: 29, i:   956]  train_loss: 0.524  |  valid_loss: 0.536\n",
      "[epoch: 29, i:  1043]  train_loss: 0.513  |  valid_loss: 0.666\n",
      "[epoch: 29, i:  1130]  train_loss: 0.524  |  valid_loss: 0.551\n",
      "[epoch: 29, i:  1217]  train_loss: 0.514  |  valid_loss: 0.585\n",
      "[epoch: 29, i:  1304]  train_loss: 0.524  |  valid_loss: 0.465\n",
      "[epoch: 29, i:  1391]  train_loss: 0.501  |  valid_loss: 0.643\n",
      "[epoch: 29, i:  1478]  train_loss: 0.545  |  valid_loss: 0.639\n",
      "[epoch: 29, i:  1565]  train_loss: 0.593  |  valid_loss: 0.737\n",
      "[epoch: 29, i:  1652]  train_loss: 0.562  |  valid_loss: 0.625\n",
      "[epoch: 29, i:  1739]  train_loss: 0.609  |  valid_loss: 0.855\n",
      "[epoch: 29, i:  1826]  train_loss: 0.511  |  valid_loss: 0.752\n",
      "[epoch: 29, i:  1913]  train_loss: 0.505  |  valid_loss: 0.724\n",
      "[epoch: 29, i:  2000]  train_loss: 0.547  |  valid_loss: 0.707\n",
      "[epoch: 29, i:  2087]  train_loss: 0.555  |  valid_loss: 0.744\n",
      "[epoch: 29, i:  2174]  train_loss: 0.546  |  valid_loss: 0.581\n",
      "[epoch: 29, i:  2261]  train_loss: 0.525  |  valid_loss: 0.913\n",
      "[epoch: 29, i:  2348]  train_loss: 0.537  |  valid_loss: 0.515\n",
      "[epoch: 29, i:  2435]  train_loss: 0.558  |  valid_loss: 0.662\n",
      "[epoch: 29, i:  2522]  train_loss: 0.562  |  valid_loss: 0.510\n",
      "[epoch: 29, i:  2609]  train_loss: 0.554  |  valid_loss: 0.547\n",
      "[epoch: 29, i:  2696]  train_loss: 0.508  |  valid_loss: 0.664\n",
      "[epoch: 29, i:  2783]  train_loss: 0.575  |  valid_loss: 0.477\n",
      "[epoch: 29, i:  2870]  train_loss: 0.641  |  valid_loss: 0.693\n",
      "[epoch: 29, i:  2957]  train_loss: 0.513  |  valid_loss: 0.705\n",
      "[epoch: 29, i:  3044]  train_loss: 0.532  |  valid_loss: 0.699\n",
      "[epoch: 29, i:  3131]  train_loss: 0.549  |  valid_loss: 0.649\n",
      "[epoch: 29, i:  3218]  train_loss: 0.524  |  valid_loss: 0.673\n",
      "[epoch: 29, i:  3305]  train_loss: 0.503  |  valid_loss: 0.695\n",
      "[epoch: 29, i:  3392]  train_loss: 0.535  |  valid_loss: 0.571\n",
      "[epoch: 29, i:  3479]  train_loss: 0.501  |  valid_loss: 0.518\n",
      "[epoch: 29, i:  3566]  train_loss: 0.604  |  valid_loss: 0.583\n",
      "[epoch: 29, i:  3653]  train_loss: 0.543  |  valid_loss: 0.579\n",
      "[epoch: 29, i:  3740]  train_loss: 0.530  |  valid_loss: 0.438\n",
      "[epoch: 29, i:  3827]  train_loss: 0.560  |  valid_loss: 0.755\n",
      "[epoch: 29, i:  3914]  train_loss: 0.588  |  valid_loss: 0.459\n",
      "[epoch: 29, i:  4001]  train_loss: 0.562  |  valid_loss: 0.656\n",
      "[epoch: 29, i:  4088]  train_loss: 0.577  |  valid_loss: 0.658\n",
      "[epoch: 29, i:  4175]  train_loss: 0.601  |  valid_loss: 0.595\n",
      "[epoch: 29, i:  4262]  train_loss: 0.621  |  valid_loss: 0.480\n",
      "[epoch: 29, i:  4349]  train_loss: 0.574  |  valid_loss: 0.558\n",
      "[epoch: 29, i:  4436]  train_loss: 0.537  |  valid_loss: 0.611\n",
      "[epoch: 29, i:  4523]  train_loss: 0.563  |  valid_loss: 0.735\n",
      "[epoch: 29, i:  4610]  train_loss: 0.558  |  valid_loss: 0.555\n",
      "[epoch: 29, i:  4697]  train_loss: 0.568  |  valid_loss: 0.581\n",
      "[epoch: 29, i:  4784]  train_loss: 0.576  |  valid_loss: 0.676\n",
      "[epoch: 29, i:  4871]  train_loss: 0.528  |  valid_loss: 0.728\n",
      "[epoch: 29, i:  4958]  train_loss: 0.546  |  valid_loss: 0.831\n",
      "[epoch: 29, i:  5045]  train_loss: 0.579  |  valid_loss: 0.408\n",
      "[epoch: 29, i:  5132]  train_loss: 0.611  |  valid_loss: 0.584\n",
      "[epoch: 29, i:  5219]  train_loss: 0.545  |  valid_loss: 0.658\n",
      "[epoch: 29, i:  5306]  train_loss: 0.574  |  valid_loss: 0.631\n",
      "[epoch: 29, i:  5393]  train_loss: 0.595  |  valid_loss: 0.464\n",
      "[epoch: 29, i:  5480]  train_loss: 0.506  |  valid_loss: 0.851\n",
      "[epoch: 29, i:  5567]  train_loss: 0.505  |  valid_loss: 0.395\n",
      "[epoch: 29, i:  5654]  train_loss: 0.571  |  valid_loss: 0.594\n",
      "[epoch: 29, i:  5741]  train_loss: 0.514  |  valid_loss: 0.720\n",
      "[epoch: 29, i:  5828]  train_loss: 0.527  |  valid_loss: 0.557\n",
      "[epoch: 29, i:  5915]  train_loss: 0.553  |  valid_loss: 0.698\n",
      "[epoch: 29, i:  6002]  train_loss: 0.622  |  valid_loss: 0.537\n",
      "[epoch: 29, i:  6089]  train_loss: 0.563  |  valid_loss: 0.768\n",
      "[epoch: 29, i:  6176]  train_loss: 0.535  |  valid_loss: 0.727\n",
      "[epoch: 29, i:  6263]  train_loss: 0.516  |  valid_loss: 0.595\n",
      "[epoch: 29, i:  6350]  train_loss: 0.601  |  valid_loss: 0.582\n",
      "[epoch: 29, i:  6437]  train_loss: 0.507  |  valid_loss: 0.362\n",
      "[epoch: 29, i:  6524]  train_loss: 0.648  |  valid_loss: 0.798\n",
      "[epoch: 29, i:  6611]  train_loss: 0.670  |  valid_loss: 0.412\n",
      "[epoch: 29, i:  6698]  train_loss: 0.615  |  valid_loss: 0.736\n",
      "[epoch: 29, i:  6785]  train_loss: 0.602  |  valid_loss: 0.501\n",
      "[epoch: 29, i:  6872]  train_loss: 0.602  |  valid_loss: 0.698\n",
      "[epoch: 29, i:  6959]  train_loss: 0.522  |  valid_loss: 0.694\n",
      "[epoch: 29, i:  7046]  train_loss: 0.608  |  valid_loss: 0.640\n",
      "[epoch: 29, i:  7133]  train_loss: 0.587  |  valid_loss: 0.549\n",
      "[epoch: 29, i:  7220]  train_loss: 0.580  |  valid_loss: 0.729\n",
      "[epoch: 29, i:  7307]  train_loss: 0.551  |  valid_loss: 0.735\n",
      "[epoch: 29, i:  7394]  train_loss: 0.556  |  valid_loss: 0.499\n",
      "[epoch: 29, i:  7481]  train_loss: 0.599  |  valid_loss: 0.652\n",
      "[epoch: 29, i:  7568]  train_loss: 0.556  |  valid_loss: 0.812\n",
      "[epoch: 29, i:  7655]  train_loss: 0.589  |  valid_loss: 0.605\n",
      "[epoch: 29, i:  7742]  train_loss: 0.571  |  valid_loss: 0.634\n",
      "[epoch: 29, i:  7829]  train_loss: 0.571  |  valid_loss: 0.423\n",
      "[epoch: 29, i:  7916]  train_loss: 0.519  |  valid_loss: 0.672\n",
      "[epoch: 29, i:  8003]  train_loss: 0.614  |  valid_loss: 0.460\n",
      "[epoch: 29, i:  8090]  train_loss: 0.569  |  valid_loss: 0.500\n",
      "[epoch: 29, i:  8177]  train_loss: 0.608  |  valid_loss: 0.546\n",
      "[epoch: 29, i:  8264]  train_loss: 0.559  |  valid_loss: 0.683\n",
      "[epoch: 29, i:  8351]  train_loss: 0.523  |  valid_loss: 0.698\n",
      "[epoch: 29, i:  8438]  train_loss: 0.533  |  valid_loss: 0.521\n",
      "[epoch: 29, i:  8525]  train_loss: 0.570  |  valid_loss: 0.645\n",
      "[epoch: 29, i:  8612]  train_loss: 0.579  |  valid_loss: 0.952\n",
      "[epoch: 29, i:  8699]  train_loss: 0.502  |  valid_loss: 0.646\n",
      "--> [End of epoch 29] train_accuracy: 80.64%  |  valid_accuracy: 79.15%\n",
      "--> [Start of epoch 30]  lr: 0.000500\n",
      "[epoch: 30, i:    86]  train_loss: 0.585  |  valid_loss: 0.604\n",
      "[epoch: 30, i:   173]  train_loss: 0.536  |  valid_loss: 0.606\n",
      "[epoch: 30, i:   260]  train_loss: 0.524  |  valid_loss: 0.632\n",
      "[epoch: 30, i:   347]  train_loss: 0.490  |  valid_loss: 0.564\n",
      "[epoch: 30, i:   434]  train_loss: 0.532  |  valid_loss: 0.688\n",
      "[epoch: 30, i:   521]  train_loss: 0.526  |  valid_loss: 0.460\n",
      "[epoch: 30, i:   608]  train_loss: 0.559  |  valid_loss: 0.664\n",
      "[epoch: 30, i:   695]  train_loss: 0.548  |  valid_loss: 0.678\n",
      "[epoch: 30, i:   782]  train_loss: 0.529  |  valid_loss: 0.720\n",
      "[epoch: 30, i:   869]  train_loss: 0.542  |  valid_loss: 0.677\n",
      "[epoch: 30, i:   956]  train_loss: 0.504  |  valid_loss: 0.543\n",
      "[epoch: 30, i:  1043]  train_loss: 0.519  |  valid_loss: 0.634\n",
      "[epoch: 30, i:  1130]  train_loss: 0.564  |  valid_loss: 0.524\n",
      "[epoch: 30, i:  1217]  train_loss: 0.544  |  valid_loss: 0.614\n",
      "[epoch: 30, i:  1304]  train_loss: 0.582  |  valid_loss: 0.428\n",
      "[epoch: 30, i:  1391]  train_loss: 0.577  |  valid_loss: 0.692\n",
      "[epoch: 30, i:  1478]  train_loss: 0.501  |  valid_loss: 0.565\n",
      "[epoch: 30, i:  1565]  train_loss: 0.505  |  valid_loss: 0.764\n",
      "[epoch: 30, i:  1652]  train_loss: 0.620  |  valid_loss: 0.614\n",
      "[epoch: 30, i:  1739]  train_loss: 0.527  |  valid_loss: 0.802\n",
      "[epoch: 30, i:  1826]  train_loss: 0.516  |  valid_loss: 0.851\n",
      "[epoch: 30, i:  1913]  train_loss: 0.607  |  valid_loss: 0.633\n",
      "[epoch: 30, i:  2000]  train_loss: 0.524  |  valid_loss: 0.616\n",
      "[epoch: 30, i:  2087]  train_loss: 0.515  |  valid_loss: 0.794\n",
      "[epoch: 30, i:  2174]  train_loss: 0.600  |  valid_loss: 0.598\n",
      "[epoch: 30, i:  2261]  train_loss: 0.561  |  valid_loss: 0.969\n",
      "[epoch: 30, i:  2348]  train_loss: 0.607  |  valid_loss: 0.462\n",
      "[epoch: 30, i:  2435]  train_loss: 0.572  |  valid_loss: 0.646\n",
      "[epoch: 30, i:  2522]  train_loss: 0.477  |  valid_loss: 0.461\n",
      "[epoch: 30, i:  2609]  train_loss: 0.612  |  valid_loss: 0.593\n",
      "[epoch: 30, i:  2696]  train_loss: 0.594  |  valid_loss: 0.699\n",
      "[epoch: 30, i:  2783]  train_loss: 0.497  |  valid_loss: 0.437\n",
      "[epoch: 30, i:  2870]  train_loss: 0.570  |  valid_loss: 0.705\n",
      "[epoch: 30, i:  2957]  train_loss: 0.542  |  valid_loss: 0.740\n",
      "[epoch: 30, i:  3044]  train_loss: 0.538  |  valid_loss: 0.711\n",
      "[epoch: 30, i:  3131]  train_loss: 0.578  |  valid_loss: 0.634\n",
      "[epoch: 30, i:  3218]  train_loss: 0.484  |  valid_loss: 0.704\n",
      "[epoch: 30, i:  3305]  train_loss: 0.577  |  valid_loss: 0.625\n",
      "[epoch: 30, i:  3392]  train_loss: 0.546  |  valid_loss: 0.543\n",
      "[epoch: 30, i:  3479]  train_loss: 0.577  |  valid_loss: 0.545\n",
      "[epoch: 30, i:  3566]  train_loss: 0.595  |  valid_loss: 0.639\n",
      "[epoch: 30, i:  3653]  train_loss: 0.567  |  valid_loss: 0.651\n",
      "[epoch: 30, i:  3740]  train_loss: 0.519  |  valid_loss: 0.410\n",
      "[epoch: 30, i:  3827]  train_loss: 0.569  |  valid_loss: 0.794\n",
      "[epoch: 30, i:  3914]  train_loss: 0.601  |  valid_loss: 0.488\n",
      "[epoch: 30, i:  4001]  train_loss: 0.585  |  valid_loss: 0.698\n",
      "[epoch: 30, i:  4088]  train_loss: 0.602  |  valid_loss: 0.552\n",
      "[epoch: 30, i:  4175]  train_loss: 0.570  |  valid_loss: 0.582\n",
      "[epoch: 30, i:  4262]  train_loss: 0.709  |  valid_loss: 0.513\n",
      "[epoch: 30, i:  4349]  train_loss: 0.587  |  valid_loss: 0.616\n",
      "[epoch: 30, i:  4436]  train_loss: 0.572  |  valid_loss: 0.591\n",
      "[epoch: 30, i:  4523]  train_loss: 0.597  |  valid_loss: 0.726\n",
      "[epoch: 30, i:  4610]  train_loss: 0.566  |  valid_loss: 0.613\n",
      "[epoch: 30, i:  4697]  train_loss: 0.579  |  valid_loss: 0.550\n",
      "[epoch: 30, i:  4784]  train_loss: 0.573  |  valid_loss: 0.690\n",
      "[epoch: 30, i:  4871]  train_loss: 0.589  |  valid_loss: 0.677\n",
      "[epoch: 30, i:  4958]  train_loss: 0.519  |  valid_loss: 0.841\n",
      "[epoch: 30, i:  5045]  train_loss: 0.532  |  valid_loss: 0.455\n",
      "[epoch: 30, i:  5132]  train_loss: 0.589  |  valid_loss: 0.609\n",
      "[epoch: 30, i:  5219]  train_loss: 0.567  |  valid_loss: 0.736\n",
      "[epoch: 30, i:  5306]  train_loss: 0.509  |  valid_loss: 0.651\n",
      "[epoch: 30, i:  5393]  train_loss: 0.552  |  valid_loss: 0.620\n",
      "[epoch: 30, i:  5480]  train_loss: 0.545  |  valid_loss: 0.830\n",
      "[epoch: 30, i:  5567]  train_loss: 0.579  |  valid_loss: 0.361\n",
      "[epoch: 30, i:  5654]  train_loss: 0.577  |  valid_loss: 0.633\n",
      "[epoch: 30, i:  5741]  train_loss: 0.501  |  valid_loss: 0.713\n",
      "[epoch: 30, i:  5828]  train_loss: 0.570  |  valid_loss: 0.517\n",
      "[epoch: 30, i:  5915]  train_loss: 0.585  |  valid_loss: 0.722\n",
      "[epoch: 30, i:  6002]  train_loss: 0.605  |  valid_loss: 0.522\n",
      "[epoch: 30, i:  6089]  train_loss: 0.563  |  valid_loss: 0.788\n",
      "[epoch: 30, i:  6176]  train_loss: 0.539  |  valid_loss: 0.694\n",
      "[epoch: 30, i:  6263]  train_loss: 0.519  |  valid_loss: 0.569\n",
      "[epoch: 30, i:  6350]  train_loss: 0.504  |  valid_loss: 0.553\n",
      "[epoch: 30, i:  6437]  train_loss: 0.578  |  valid_loss: 0.462\n",
      "[epoch: 30, i:  6524]  train_loss: 0.511  |  valid_loss: 0.785\n",
      "[epoch: 30, i:  6611]  train_loss: 0.556  |  valid_loss: 0.416\n",
      "[epoch: 30, i:  6698]  train_loss: 0.527  |  valid_loss: 0.740\n",
      "[epoch: 30, i:  6785]  train_loss: 0.598  |  valid_loss: 0.447\n",
      "[epoch: 30, i:  6872]  train_loss: 0.531  |  valid_loss: 0.715\n",
      "[epoch: 30, i:  6959]  train_loss: 0.626  |  valid_loss: 0.700\n",
      "[epoch: 30, i:  7046]  train_loss: 0.554  |  valid_loss: 0.644\n",
      "[epoch: 30, i:  7133]  train_loss: 0.555  |  valid_loss: 0.530\n",
      "[epoch: 30, i:  7220]  train_loss: 0.598  |  valid_loss: 0.740\n",
      "[epoch: 30, i:  7307]  train_loss: 0.607  |  valid_loss: 0.719\n",
      "[epoch: 30, i:  7394]  train_loss: 0.546  |  valid_loss: 0.589\n",
      "[epoch: 30, i:  7481]  train_loss: 0.597  |  valid_loss: 0.611\n",
      "[epoch: 30, i:  7568]  train_loss: 0.540  |  valid_loss: 0.823\n",
      "[epoch: 30, i:  7655]  train_loss: 0.588  |  valid_loss: 0.642\n",
      "[epoch: 30, i:  7742]  train_loss: 0.582  |  valid_loss: 0.596\n",
      "[epoch: 30, i:  7829]  train_loss: 0.589  |  valid_loss: 0.496\n",
      "[epoch: 30, i:  7916]  train_loss: 0.565  |  valid_loss: 0.742\n",
      "[epoch: 30, i:  8003]  train_loss: 0.544  |  valid_loss: 0.463\n",
      "[epoch: 30, i:  8090]  train_loss: 0.539  |  valid_loss: 0.525\n",
      "[epoch: 30, i:  8177]  train_loss: 0.539  |  valid_loss: 0.566\n",
      "[epoch: 30, i:  8264]  train_loss: 0.541  |  valid_loss: 0.670\n",
      "[epoch: 30, i:  8351]  train_loss: 0.585  |  valid_loss: 0.730\n",
      "[epoch: 30, i:  8438]  train_loss: 0.630  |  valid_loss: 0.445\n",
      "[epoch: 30, i:  8525]  train_loss: 0.606  |  valid_loss: 0.595\n",
      "[epoch: 30, i:  8612]  train_loss: 0.598  |  valid_loss: 0.881\n",
      "[epoch: 30, i:  8699]  train_loss: 0.558  |  valid_loss: 0.603\n",
      "--> [End of epoch 30] train_accuracy: 80.59%  |  valid_accuracy: 78.86%\n",
      "--> [Start of epoch 31]  lr: 0.000500\n",
      "[epoch: 31, i:    86]  train_loss: 0.532  |  valid_loss: 0.588\n",
      "[epoch: 31, i:   173]  train_loss: 0.495  |  valid_loss: 0.581\n",
      "[epoch: 31, i:   260]  train_loss: 0.559  |  valid_loss: 0.691\n",
      "[epoch: 31, i:   347]  train_loss: 0.515  |  valid_loss: 0.505\n",
      "[epoch: 31, i:   434]  train_loss: 0.502  |  valid_loss: 0.568\n",
      "[epoch: 31, i:   521]  train_loss: 0.585  |  valid_loss: 0.429\n",
      "[epoch: 31, i:   608]  train_loss: 0.538  |  valid_loss: 0.649\n",
      "[epoch: 31, i:   695]  train_loss: 0.552  |  valid_loss: 0.640\n",
      "[epoch: 31, i:   782]  train_loss: 0.554  |  valid_loss: 0.746\n",
      "[epoch: 31, i:   869]  train_loss: 0.583  |  valid_loss: 0.614\n",
      "[epoch: 31, i:   956]  train_loss: 0.520  |  valid_loss: 0.569\n",
      "[epoch: 31, i:  1043]  train_loss: 0.500  |  valid_loss: 0.623\n",
      "[epoch: 31, i:  1130]  train_loss: 0.553  |  valid_loss: 0.473\n",
      "[epoch: 31, i:  1217]  train_loss: 0.555  |  valid_loss: 0.637\n",
      "[epoch: 31, i:  1304]  train_loss: 0.551  |  valid_loss: 0.426\n",
      "[epoch: 31, i:  1391]  train_loss: 0.586  |  valid_loss: 0.674\n",
      "[epoch: 31, i:  1478]  train_loss: 0.499  |  valid_loss: 0.636\n",
      "[epoch: 31, i:  1565]  train_loss: 0.543  |  valid_loss: 0.781\n",
      "[epoch: 31, i:  1652]  train_loss: 0.537  |  valid_loss: 0.592\n",
      "[epoch: 31, i:  1739]  train_loss: 0.483  |  valid_loss: 0.868\n",
      "[epoch: 31, i:  1826]  train_loss: 0.562  |  valid_loss: 0.781\n",
      "[epoch: 31, i:  1913]  train_loss: 0.582  |  valid_loss: 0.608\n",
      "[epoch: 31, i:  2000]  train_loss: 0.521  |  valid_loss: 0.605\n",
      "[epoch: 31, i:  2087]  train_loss: 0.514  |  valid_loss: 0.860\n",
      "[epoch: 31, i:  2174]  train_loss: 0.607  |  valid_loss: 0.562\n",
      "[epoch: 31, i:  2261]  train_loss: 0.517  |  valid_loss: 0.937\n",
      "[epoch: 31, i:  2348]  train_loss: 0.520  |  valid_loss: 0.527\n",
      "[epoch: 31, i:  2435]  train_loss: 0.494  |  valid_loss: 0.653\n",
      "[epoch: 31, i:  2522]  train_loss: 0.609  |  valid_loss: 0.493\n",
      "[epoch: 31, i:  2609]  train_loss: 0.597  |  valid_loss: 0.565\n",
      "[epoch: 31, i:  2696]  train_loss: 0.553  |  valid_loss: 0.697\n",
      "[epoch: 31, i:  2783]  train_loss: 0.553  |  valid_loss: 0.422\n",
      "[epoch: 31, i:  2870]  train_loss: 0.611  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  2957]  train_loss: 0.633  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  3044]  train_loss: 0.595  |  valid_loss: 0.789\n",
      "[epoch: 31, i:  3131]  train_loss: 0.574  |  valid_loss: 0.632\n",
      "[epoch: 31, i:  3218]  train_loss: 0.529  |  valid_loss: 0.674\n",
      "[epoch: 31, i:  3305]  train_loss: 0.571  |  valid_loss: 0.627\n",
      "[epoch: 31, i:  3392]  train_loss: 0.562  |  valid_loss: 0.575\n",
      "[epoch: 31, i:  3479]  train_loss: 0.568  |  valid_loss: 0.499\n",
      "[epoch: 31, i:  3566]  train_loss: 0.624  |  valid_loss: 0.657\n",
      "[epoch: 31, i:  3653]  train_loss: 0.505  |  valid_loss: 0.681\n",
      "[epoch: 31, i:  3740]  train_loss: 0.527  |  valid_loss: 0.446\n",
      "[epoch: 31, i:  3827]  train_loss: 0.507  |  valid_loss: 0.856\n",
      "[epoch: 31, i:  3914]  train_loss: 0.520  |  valid_loss: 0.480\n",
      "[epoch: 31, i:  4001]  train_loss: 0.604  |  valid_loss: 0.625\n",
      "[epoch: 31, i:  4088]  train_loss: 0.601  |  valid_loss: 0.621\n",
      "[epoch: 31, i:  4175]  train_loss: 0.563  |  valid_loss: 0.632\n",
      "[epoch: 31, i:  4262]  train_loss: 0.560  |  valid_loss: 0.450\n",
      "[epoch: 31, i:  4349]  train_loss: 0.622  |  valid_loss: 0.612\n",
      "[epoch: 31, i:  4436]  train_loss: 0.523  |  valid_loss: 0.642\n",
      "[epoch: 31, i:  4523]  train_loss: 0.535  |  valid_loss: 0.692\n",
      "[epoch: 31, i:  4610]  train_loss: 0.542  |  valid_loss: 0.500\n",
      "[epoch: 31, i:  4697]  train_loss: 0.512  |  valid_loss: 0.637\n",
      "[epoch: 31, i:  4784]  train_loss: 0.618  |  valid_loss: 0.680\n",
      "[epoch: 31, i:  4871]  train_loss: 0.550  |  valid_loss: 0.748\n",
      "[epoch: 31, i:  4958]  train_loss: 0.563  |  valid_loss: 0.755\n",
      "[epoch: 31, i:  5045]  train_loss: 0.535  |  valid_loss: 0.445\n",
      "[epoch: 31, i:  5132]  train_loss: 0.599  |  valid_loss: 0.600\n",
      "[epoch: 31, i:  5219]  train_loss: 0.551  |  valid_loss: 0.715\n",
      "[epoch: 31, i:  5306]  train_loss: 0.507  |  valid_loss: 0.796\n",
      "[epoch: 31, i:  5393]  train_loss: 0.616  |  valid_loss: 0.549\n",
      "[epoch: 31, i:  5480]  train_loss: 0.607  |  valid_loss: 0.861\n",
      "[epoch: 31, i:  5567]  train_loss: 0.547  |  valid_loss: 0.425\n",
      "[epoch: 31, i:  5654]  train_loss: 0.654  |  valid_loss: 0.561\n",
      "[epoch: 31, i:  5741]  train_loss: 0.567  |  valid_loss: 0.631\n",
      "[epoch: 31, i:  5828]  train_loss: 0.526  |  valid_loss: 0.577\n",
      "[epoch: 31, i:  5915]  train_loss: 0.554  |  valid_loss: 0.696\n",
      "[epoch: 31, i:  6002]  train_loss: 0.586  |  valid_loss: 0.493\n",
      "[epoch: 31, i:  6089]  train_loss: 0.530  |  valid_loss: 0.763\n",
      "[epoch: 31, i:  6176]  train_loss: 0.592  |  valid_loss: 0.607\n",
      "[epoch: 31, i:  6263]  train_loss: 0.514  |  valid_loss: 0.554\n",
      "[epoch: 31, i:  6350]  train_loss: 0.549  |  valid_loss: 0.605\n",
      "[epoch: 31, i:  6437]  train_loss: 0.547  |  valid_loss: 0.459\n",
      "[epoch: 31, i:  6524]  train_loss: 0.599  |  valid_loss: 0.772\n",
      "[epoch: 31, i:  6611]  train_loss: 0.552  |  valid_loss: 0.404\n",
      "[epoch: 31, i:  6698]  train_loss: 0.539  |  valid_loss: 0.757\n",
      "[epoch: 31, i:  6785]  train_loss: 0.547  |  valid_loss: 0.416\n",
      "[epoch: 31, i:  6872]  train_loss: 0.582  |  valid_loss: 0.643\n",
      "[epoch: 31, i:  6959]  train_loss: 0.616  |  valid_loss: 0.648\n",
      "[epoch: 31, i:  7046]  train_loss: 0.581  |  valid_loss: 0.661\n",
      "[epoch: 31, i:  7133]  train_loss: 0.503  |  valid_loss: 0.509\n",
      "[epoch: 31, i:  7220]  train_loss: 0.538  |  valid_loss: 0.683\n",
      "[epoch: 31, i:  7307]  train_loss: 0.631  |  valid_loss: 0.777\n",
      "[epoch: 31, i:  7394]  train_loss: 0.588  |  valid_loss: 0.537\n",
      "[epoch: 31, i:  7481]  train_loss: 0.545  |  valid_loss: 0.574\n",
      "[epoch: 31, i:  7568]  train_loss: 0.542  |  valid_loss: 0.863\n",
      "[epoch: 31, i:  7655]  train_loss: 0.506  |  valid_loss: 0.643\n",
      "[epoch: 31, i:  7742]  train_loss: 0.591  |  valid_loss: 0.675\n",
      "[epoch: 31, i:  7829]  train_loss: 0.510  |  valid_loss: 0.420\n",
      "[epoch: 31, i:  7916]  train_loss: 0.582  |  valid_loss: 0.624\n",
      "[epoch: 31, i:  8003]  train_loss: 0.631  |  valid_loss: 0.564\n",
      "[epoch: 31, i:  8090]  train_loss: 0.506  |  valid_loss: 0.540\n",
      "[epoch: 31, i:  8177]  train_loss: 0.521  |  valid_loss: 0.620\n",
      "[epoch: 31, i:  8264]  train_loss: 0.505  |  valid_loss: 0.650\n",
      "[epoch: 31, i:  8351]  train_loss: 0.552  |  valid_loss: 0.710\n",
      "[epoch: 31, i:  8438]  train_loss: 0.528  |  valid_loss: 0.462\n",
      "[epoch: 31, i:  8525]  train_loss: 0.548  |  valid_loss: 0.632\n",
      "[epoch: 31, i:  8612]  train_loss: 0.589  |  valid_loss: 0.850\n",
      "[epoch: 31, i:  8699]  train_loss: 0.552  |  valid_loss: 0.601\n",
      "--> [End of epoch 31] train_accuracy: 80.80%  |  valid_accuracy: 79.00%\n",
      "--> [Start of epoch 32]  lr: 0.000500\n",
      "[epoch: 32, i:    86]  train_loss: 0.643  |  valid_loss: 0.587\n",
      "[epoch: 32, i:   173]  train_loss: 0.512  |  valid_loss: 0.611\n",
      "[epoch: 32, i:   260]  train_loss: 0.542  |  valid_loss: 0.727\n",
      "[epoch: 32, i:   347]  train_loss: 0.535  |  valid_loss: 0.479\n",
      "[epoch: 32, i:   434]  train_loss: 0.527  |  valid_loss: 0.654\n",
      "[epoch: 32, i:   521]  train_loss: 0.507  |  valid_loss: 0.411\n",
      "[epoch: 32, i:   608]  train_loss: 0.534  |  valid_loss: 0.683\n",
      "[epoch: 32, i:   695]  train_loss: 0.541  |  valid_loss: 0.552\n",
      "[epoch: 32, i:   782]  train_loss: 0.530  |  valid_loss: 0.746\n",
      "[epoch: 32, i:   869]  train_loss: 0.528  |  valid_loss: 0.662\n",
      "[epoch: 32, i:   956]  train_loss: 0.560  |  valid_loss: 0.575\n",
      "[epoch: 32, i:  1043]  train_loss: 0.534  |  valid_loss: 0.587\n",
      "[epoch: 32, i:  1130]  train_loss: 0.511  |  valid_loss: 0.531\n",
      "[epoch: 32, i:  1217]  train_loss: 0.570  |  valid_loss: 0.654\n",
      "[epoch: 32, i:  1304]  train_loss: 0.567  |  valid_loss: 0.356\n",
      "[epoch: 32, i:  1391]  train_loss: 0.553  |  valid_loss: 0.659\n",
      "[epoch: 32, i:  1478]  train_loss: 0.560  |  valid_loss: 0.571\n",
      "[epoch: 32, i:  1565]  train_loss: 0.533  |  valid_loss: 0.773\n",
      "[epoch: 32, i:  1652]  train_loss: 0.569  |  valid_loss: 0.571\n",
      "[epoch: 32, i:  1739]  train_loss: 0.535  |  valid_loss: 0.841\n",
      "[epoch: 32, i:  1826]  train_loss: 0.514  |  valid_loss: 0.741\n",
      "[epoch: 32, i:  1913]  train_loss: 0.505  |  valid_loss: 0.550\n",
      "[epoch: 32, i:  2000]  train_loss: 0.550  |  valid_loss: 0.583\n",
      "[epoch: 32, i:  2087]  train_loss: 0.584  |  valid_loss: 0.786\n",
      "[epoch: 32, i:  2174]  train_loss: 0.534  |  valid_loss: 0.622\n",
      "[epoch: 32, i:  2261]  train_loss: 0.517  |  valid_loss: 0.915\n",
      "[epoch: 32, i:  2348]  train_loss: 0.485  |  valid_loss: 0.475\n",
      "[epoch: 32, i:  2435]  train_loss: 0.554  |  valid_loss: 0.681\n",
      "[epoch: 32, i:  2522]  train_loss: 0.490  |  valid_loss: 0.475\n",
      "[epoch: 32, i:  2609]  train_loss: 0.549  |  valid_loss: 0.547\n",
      "[epoch: 32, i:  2696]  train_loss: 0.531  |  valid_loss: 0.707\n",
      "[epoch: 32, i:  2783]  train_loss: 0.575  |  valid_loss: 0.440\n",
      "[epoch: 32, i:  2870]  train_loss: 0.515  |  valid_loss: 0.687\n",
      "[epoch: 32, i:  2957]  train_loss: 0.593  |  valid_loss: 0.739\n",
      "[epoch: 32, i:  3044]  train_loss: 0.624  |  valid_loss: 0.714\n",
      "[epoch: 32, i:  3131]  train_loss: 0.513  |  valid_loss: 0.623\n",
      "[epoch: 32, i:  3218]  train_loss: 0.579  |  valid_loss: 0.774\n",
      "[epoch: 32, i:  3305]  train_loss: 0.563  |  valid_loss: 0.611\n",
      "[epoch: 32, i:  3392]  train_loss: 0.556  |  valid_loss: 0.664\n",
      "[epoch: 32, i:  3479]  train_loss: 0.569  |  valid_loss: 0.631\n",
      "[epoch: 32, i:  3566]  train_loss: 0.582  |  valid_loss: 0.560\n",
      "[epoch: 32, i:  3653]  train_loss: 0.576  |  valid_loss: 0.670\n",
      "[epoch: 32, i:  3740]  train_loss: 0.546  |  valid_loss: 0.389\n",
      "[epoch: 32, i:  3827]  train_loss: 0.550  |  valid_loss: 0.755\n",
      "[epoch: 32, i:  3914]  train_loss: 0.559  |  valid_loss: 0.497\n",
      "[epoch: 32, i:  4001]  train_loss: 0.504  |  valid_loss: 0.639\n",
      "[epoch: 32, i:  4088]  train_loss: 0.587  |  valid_loss: 0.622\n",
      "[epoch: 32, i:  4175]  train_loss: 0.504  |  valid_loss: 0.652\n",
      "[epoch: 32, i:  4262]  train_loss: 0.566  |  valid_loss: 0.555\n",
      "[epoch: 32, i:  4349]  train_loss: 0.509  |  valid_loss: 0.546\n",
      "[epoch: 32, i:  4436]  train_loss: 0.499  |  valid_loss: 0.661\n",
      "[epoch: 32, i:  4523]  train_loss: 0.565  |  valid_loss: 0.764\n",
      "[epoch: 32, i:  4610]  train_loss: 0.565  |  valid_loss: 0.485\n",
      "[epoch: 32, i:  4697]  train_loss: 0.498  |  valid_loss: 0.692\n",
      "[epoch: 32, i:  4784]  train_loss: 0.600  |  valid_loss: 0.673\n",
      "[epoch: 32, i:  4871]  train_loss: 0.580  |  valid_loss: 0.735\n",
      "[epoch: 32, i:  4958]  train_loss: 0.530  |  valid_loss: 0.760\n",
      "[epoch: 32, i:  5045]  train_loss: 0.537  |  valid_loss: 0.430\n",
      "[epoch: 32, i:  5132]  train_loss: 0.496  |  valid_loss: 0.642\n",
      "[epoch: 32, i:  5219]  train_loss: 0.583  |  valid_loss: 0.705\n",
      "[epoch: 32, i:  5306]  train_loss: 0.546  |  valid_loss: 0.638\n",
      "[epoch: 32, i:  5393]  train_loss: 0.568  |  valid_loss: 0.522\n",
      "[epoch: 32, i:  5480]  train_loss: 0.580  |  valid_loss: 0.750\n",
      "[epoch: 32, i:  5567]  train_loss: 0.547  |  valid_loss: 0.359\n",
      "[epoch: 32, i:  5654]  train_loss: 0.557  |  valid_loss: 0.625\n",
      "[epoch: 32, i:  5741]  train_loss: 0.603  |  valid_loss: 0.741\n",
      "[epoch: 32, i:  5828]  train_loss: 0.557  |  valid_loss: 0.582\n",
      "[epoch: 32, i:  5915]  train_loss: 0.524  |  valid_loss: 0.717\n",
      "[epoch: 32, i:  6002]  train_loss: 0.628  |  valid_loss: 0.490\n",
      "[epoch: 32, i:  6089]  train_loss: 0.600  |  valid_loss: 0.854\n",
      "[epoch: 32, i:  6176]  train_loss: 0.584  |  valid_loss: 0.625\n",
      "[epoch: 32, i:  6263]  train_loss: 0.538  |  valid_loss: 0.601\n",
      "[epoch: 32, i:  6350]  train_loss: 0.630  |  valid_loss: 0.550\n",
      "[epoch: 32, i:  6437]  train_loss: 0.580  |  valid_loss: 0.389\n",
      "[epoch: 32, i:  6524]  train_loss: 0.578  |  valid_loss: 0.748\n",
      "[epoch: 32, i:  6611]  train_loss: 0.585  |  valid_loss: 0.436\n",
      "[epoch: 32, i:  6698]  train_loss: 0.524  |  valid_loss: 0.763\n",
      "[epoch: 32, i:  6785]  train_loss: 0.544  |  valid_loss: 0.467\n",
      "[epoch: 32, i:  6872]  train_loss: 0.572  |  valid_loss: 0.721\n",
      "[epoch: 32, i:  6959]  train_loss: 0.548  |  valid_loss: 0.652\n",
      "[epoch: 32, i:  7046]  train_loss: 0.536  |  valid_loss: 0.592\n",
      "[epoch: 32, i:  7133]  train_loss: 0.546  |  valid_loss: 0.519\n",
      "[epoch: 32, i:  7220]  train_loss: 0.616  |  valid_loss: 0.659\n",
      "[epoch: 32, i:  7307]  train_loss: 0.580  |  valid_loss: 0.732\n",
      "[epoch: 32, i:  7394]  train_loss: 0.603  |  valid_loss: 0.590\n",
      "[epoch: 32, i:  7481]  train_loss: 0.579  |  valid_loss: 0.597\n",
      "[epoch: 32, i:  7568]  train_loss: 0.572  |  valid_loss: 0.785\n",
      "[epoch: 32, i:  7655]  train_loss: 0.538  |  valid_loss: 0.695\n",
      "[epoch: 32, i:  7742]  train_loss: 0.635  |  valid_loss: 0.693\n",
      "[epoch: 32, i:  7829]  train_loss: 0.589  |  valid_loss: 0.496\n",
      "[epoch: 32, i:  7916]  train_loss: 0.533  |  valid_loss: 0.712\n",
      "[epoch: 32, i:  8003]  train_loss: 0.588  |  valid_loss: 0.475\n",
      "[epoch: 32, i:  8090]  train_loss: 0.508  |  valid_loss: 0.556\n",
      "[epoch: 32, i:  8177]  train_loss: 0.599  |  valid_loss: 0.602\n",
      "[epoch: 32, i:  8264]  train_loss: 0.543  |  valid_loss: 0.580\n",
      "[epoch: 32, i:  8351]  train_loss: 0.604  |  valid_loss: 0.722\n",
      "[epoch: 32, i:  8438]  train_loss: 0.550  |  valid_loss: 0.431\n",
      "[epoch: 32, i:  8525]  train_loss: 0.531  |  valid_loss: 0.603\n",
      "[epoch: 32, i:  8612]  train_loss: 0.610  |  valid_loss: 0.857\n",
      "[epoch: 32, i:  8699]  train_loss: 0.536  |  valid_loss: 0.615\n",
      "--> [End of epoch 32] train_accuracy: 80.65%  |  valid_accuracy: 78.72%\n",
      "--> [Start of epoch 33]  lr: 0.000500\n",
      "[epoch: 33, i:    86]  train_loss: 0.546  |  valid_loss: 0.554\n",
      "[epoch: 33, i:   173]  train_loss: 0.530  |  valid_loss: 0.607\n",
      "[epoch: 33, i:   260]  train_loss: 0.458  |  valid_loss: 0.743\n",
      "[epoch: 33, i:   347]  train_loss: 0.482  |  valid_loss: 0.510\n",
      "[epoch: 33, i:   434]  train_loss: 0.512  |  valid_loss: 0.600\n",
      "[epoch: 33, i:   521]  train_loss: 0.561  |  valid_loss: 0.401\n",
      "[epoch: 33, i:   608]  train_loss: 0.493  |  valid_loss: 0.621\n",
      "[epoch: 33, i:   695]  train_loss: 0.504  |  valid_loss: 0.648\n",
      "[epoch: 33, i:   782]  train_loss: 0.502  |  valid_loss: 0.730\n",
      "[epoch: 33, i:   869]  train_loss: 0.507  |  valid_loss: 0.670\n",
      "[epoch: 33, i:   956]  train_loss: 0.495  |  valid_loss: 0.548\n",
      "[epoch: 33, i:  1043]  train_loss: 0.517  |  valid_loss: 0.624\n",
      "[epoch: 33, i:  1130]  train_loss: 0.537  |  valid_loss: 0.511\n",
      "[epoch: 33, i:  1217]  train_loss: 0.601  |  valid_loss: 0.684\n",
      "[epoch: 33, i:  1304]  train_loss: 0.571  |  valid_loss: 0.463\n",
      "[epoch: 33, i:  1391]  train_loss: 0.611  |  valid_loss: 0.698\n",
      "[epoch: 33, i:  1478]  train_loss: 0.557  |  valid_loss: 0.534\n",
      "[epoch: 33, i:  1565]  train_loss: 0.508  |  valid_loss: 0.743\n",
      "[epoch: 33, i:  1652]  train_loss: 0.595  |  valid_loss: 0.588\n",
      "[epoch: 33, i:  1739]  train_loss: 0.500  |  valid_loss: 0.879\n",
      "[epoch: 33, i:  1826]  train_loss: 0.503  |  valid_loss: 0.797\n",
      "[epoch: 33, i:  1913]  train_loss: 0.545  |  valid_loss: 0.649\n",
      "[epoch: 33, i:  2000]  train_loss: 0.559  |  valid_loss: 0.616\n",
      "[epoch: 33, i:  2087]  train_loss: 0.524  |  valid_loss: 0.789\n",
      "[epoch: 33, i:  2174]  train_loss: 0.585  |  valid_loss: 0.590\n",
      "[epoch: 33, i:  2261]  train_loss: 0.528  |  valid_loss: 0.878\n",
      "[epoch: 33, i:  2348]  train_loss: 0.593  |  valid_loss: 0.489\n",
      "[epoch: 33, i:  2435]  train_loss: 0.557  |  valid_loss: 0.713\n",
      "[epoch: 33, i:  2522]  train_loss: 0.574  |  valid_loss: 0.463\n",
      "[epoch: 33, i:  2609]  train_loss: 0.575  |  valid_loss: 0.521\n",
      "[epoch: 33, i:  2696]  train_loss: 0.535  |  valid_loss: 0.689\n",
      "[epoch: 33, i:  2783]  train_loss: 0.575  |  valid_loss: 0.421\n",
      "[epoch: 33, i:  2870]  train_loss: 0.520  |  valid_loss: 0.739\n",
      "[epoch: 33, i:  2957]  train_loss: 0.555  |  valid_loss: 0.782\n",
      "[epoch: 33, i:  3044]  train_loss: 0.562  |  valid_loss: 0.713\n",
      "[epoch: 33, i:  3131]  train_loss: 0.553  |  valid_loss: 0.589\n",
      "[epoch: 33, i:  3218]  train_loss: 0.473  |  valid_loss: 0.713\n",
      "[epoch: 33, i:  3305]  train_loss: 0.600  |  valid_loss: 0.638\n",
      "[epoch: 33, i:  3392]  train_loss: 0.580  |  valid_loss: 0.630\n",
      "[epoch: 33, i:  3479]  train_loss: 0.611  |  valid_loss: 0.581\n",
      "[epoch: 33, i:  3566]  train_loss: 0.519  |  valid_loss: 0.675\n",
      "[epoch: 33, i:  3653]  train_loss: 0.578  |  valid_loss: 0.647\n",
      "[epoch: 33, i:  3740]  train_loss: 0.530  |  valid_loss: 0.428\n",
      "[epoch: 33, i:  3827]  train_loss: 0.561  |  valid_loss: 0.783\n",
      "[epoch: 33, i:  3914]  train_loss: 0.586  |  valid_loss: 0.427\n",
      "[epoch: 33, i:  4001]  train_loss: 0.579  |  valid_loss: 0.694\n",
      "[epoch: 33, i:  4088]  train_loss: 0.577  |  valid_loss: 0.563\n",
      "[epoch: 33, i:  4175]  train_loss: 0.536  |  valid_loss: 0.595\n",
      "[epoch: 33, i:  4262]  train_loss: 0.550  |  valid_loss: 0.450\n",
      "[epoch: 33, i:  4349]  train_loss: 0.603  |  valid_loss: 0.584\n",
      "[epoch: 33, i:  4436]  train_loss: 0.653  |  valid_loss: 0.534\n",
      "[epoch: 33, i:  4523]  train_loss: 0.537  |  valid_loss: 0.726\n",
      "[epoch: 33, i:  4610]  train_loss: 0.672  |  valid_loss: 0.503\n",
      "[epoch: 33, i:  4697]  train_loss: 0.514  |  valid_loss: 0.613\n",
      "[epoch: 33, i:  4784]  train_loss: 0.512  |  valid_loss: 0.634\n",
      "[epoch: 33, i:  4871]  train_loss: 0.585  |  valid_loss: 0.781\n",
      "[epoch: 33, i:  4958]  train_loss: 0.524  |  valid_loss: 0.858\n",
      "[epoch: 33, i:  5045]  train_loss: 0.644  |  valid_loss: 0.403\n",
      "[epoch: 33, i:  5132]  train_loss: 0.523  |  valid_loss: 0.655\n",
      "[epoch: 33, i:  5219]  train_loss: 0.568  |  valid_loss: 0.710\n",
      "[epoch: 33, i:  5306]  train_loss: 0.562  |  valid_loss: 0.709\n",
      "[epoch: 33, i:  5393]  train_loss: 0.579  |  valid_loss: 0.542\n",
      "[epoch: 33, i:  5480]  train_loss: 0.525  |  valid_loss: 0.747\n",
      "[epoch: 33, i:  5567]  train_loss: 0.584  |  valid_loss: 0.374\n",
      "[epoch: 33, i:  5654]  train_loss: 0.541  |  valid_loss: 0.570\n",
      "[epoch: 33, i:  5741]  train_loss: 0.553  |  valid_loss: 0.614\n",
      "[epoch: 33, i:  5828]  train_loss: 0.575  |  valid_loss: 0.636\n",
      "[epoch: 33, i:  5915]  train_loss: 0.550  |  valid_loss: 0.710\n",
      "[epoch: 33, i:  6002]  train_loss: 0.542  |  valid_loss: 0.563\n",
      "[epoch: 33, i:  6089]  train_loss: 0.508  |  valid_loss: 0.845\n",
      "[epoch: 33, i:  6176]  train_loss: 0.527  |  valid_loss: 0.608\n",
      "[epoch: 33, i:  6263]  train_loss: 0.497  |  valid_loss: 0.508\n",
      "[epoch: 33, i:  6350]  train_loss: 0.555  |  valid_loss: 0.522\n",
      "[epoch: 33, i:  6437]  train_loss: 0.513  |  valid_loss: 0.528\n",
      "[epoch: 33, i:  6524]  train_loss: 0.577  |  valid_loss: 0.832\n",
      "[epoch: 33, i:  6611]  train_loss: 0.566  |  valid_loss: 0.404\n",
      "[epoch: 33, i:  6698]  train_loss: 0.565  |  valid_loss: 0.649\n",
      "[epoch: 33, i:  6785]  train_loss: 0.550  |  valid_loss: 0.517\n",
      "[epoch: 33, i:  6872]  train_loss: 0.526  |  valid_loss: 0.788\n",
      "[epoch: 33, i:  6959]  train_loss: 0.603  |  valid_loss: 0.681\n",
      "[epoch: 33, i:  7046]  train_loss: 0.584  |  valid_loss: 0.652\n",
      "[epoch: 33, i:  7133]  train_loss: 0.572  |  valid_loss: 0.531\n",
      "[epoch: 33, i:  7220]  train_loss: 0.597  |  valid_loss: 0.653\n",
      "[epoch: 33, i:  7307]  train_loss: 0.526  |  valid_loss: 0.810\n",
      "[epoch: 33, i:  7394]  train_loss: 0.560  |  valid_loss: 0.649\n",
      "[epoch: 33, i:  7481]  train_loss: 0.509  |  valid_loss: 0.610\n",
      "[epoch: 33, i:  7568]  train_loss: 0.547  |  valid_loss: 0.749\n",
      "[epoch: 33, i:  7655]  train_loss: 0.572  |  valid_loss: 0.623\n",
      "[epoch: 33, i:  7742]  train_loss: 0.605  |  valid_loss: 0.705\n",
      "[epoch: 33, i:  7829]  train_loss: 0.562  |  valid_loss: 0.468\n",
      "[epoch: 33, i:  7916]  train_loss: 0.536  |  valid_loss: 0.697\n",
      "[epoch: 33, i:  8003]  train_loss: 0.562  |  valid_loss: 0.533\n",
      "[epoch: 33, i:  8090]  train_loss: 0.628  |  valid_loss: 0.507\n",
      "[epoch: 33, i:  8177]  train_loss: 0.616  |  valid_loss: 0.610\n",
      "[epoch: 33, i:  8264]  train_loss: 0.506  |  valid_loss: 0.609\n",
      "[epoch: 33, i:  8351]  train_loss: 0.562  |  valid_loss: 0.667\n",
      "[epoch: 33, i:  8438]  train_loss: 0.531  |  valid_loss: 0.473\n",
      "[epoch: 33, i:  8525]  train_loss: 0.563  |  valid_loss: 0.595\n",
      "[epoch: 33, i:  8612]  train_loss: 0.583  |  valid_loss: 0.902\n",
      "[epoch: 33, i:  8699]  train_loss: 0.526  |  valid_loss: 0.612\n",
      "--> [End of epoch 33] train_accuracy: 80.88%  |  valid_accuracy: 79.03%\n",
      "--> [Start of epoch 34]  lr: 0.000500\n",
      "[epoch: 34, i:    86]  train_loss: 0.575  |  valid_loss: 0.568\n",
      "[epoch: 34, i:   173]  train_loss: 0.535  |  valid_loss: 0.597\n",
      "[epoch: 34, i:   260]  train_loss: 0.571  |  valid_loss: 0.625\n",
      "[epoch: 34, i:   347]  train_loss: 0.457  |  valid_loss: 0.626\n",
      "[epoch: 34, i:   434]  train_loss: 0.506  |  valid_loss: 0.687\n",
      "[epoch: 34, i:   521]  train_loss: 0.536  |  valid_loss: 0.431\n",
      "[epoch: 34, i:   608]  train_loss: 0.568  |  valid_loss: 0.727\n",
      "[epoch: 34, i:   695]  train_loss: 0.485  |  valid_loss: 0.654\n",
      "[epoch: 34, i:   782]  train_loss: 0.489  |  valid_loss: 0.694\n",
      "[epoch: 34, i:   869]  train_loss: 0.471  |  valid_loss: 0.618\n",
      "[epoch: 34, i:   956]  train_loss: 0.602  |  valid_loss: 0.513\n",
      "[epoch: 34, i:  1043]  train_loss: 0.579  |  valid_loss: 0.579\n",
      "[epoch: 34, i:  1130]  train_loss: 0.465  |  valid_loss: 0.515\n",
      "[epoch: 34, i:  1217]  train_loss: 0.507  |  valid_loss: 0.709\n",
      "[epoch: 34, i:  1304]  train_loss: 0.613  |  valid_loss: 0.486\n",
      "[epoch: 34, i:  1391]  train_loss: 0.561  |  valid_loss: 0.693\n",
      "[epoch: 34, i:  1478]  train_loss: 0.567  |  valid_loss: 0.537\n",
      "[epoch: 34, i:  1565]  train_loss: 0.574  |  valid_loss: 0.736\n",
      "[epoch: 34, i:  1652]  train_loss: 0.576  |  valid_loss: 0.635\n",
      "[epoch: 34, i:  1739]  train_loss: 0.541  |  valid_loss: 0.863\n",
      "[epoch: 34, i:  1826]  train_loss: 0.502  |  valid_loss: 0.762\n",
      "[epoch: 34, i:  1913]  train_loss: 0.576  |  valid_loss: 0.585\n",
      "[epoch: 34, i:  2000]  train_loss: 0.484  |  valid_loss: 0.618\n",
      "[epoch: 34, i:  2087]  train_loss: 0.554  |  valid_loss: 0.760\n",
      "[epoch: 34, i:  2174]  train_loss: 0.547  |  valid_loss: 0.533\n",
      "[epoch: 34, i:  2261]  train_loss: 0.561  |  valid_loss: 0.798\n",
      "[epoch: 34, i:  2348]  train_loss: 0.554  |  valid_loss: 0.569\n",
      "[epoch: 34, i:  2435]  train_loss: 0.533  |  valid_loss: 0.727\n",
      "[epoch: 34, i:  2522]  train_loss: 0.607  |  valid_loss: 0.487\n",
      "[epoch: 34, i:  2609]  train_loss: 0.571  |  valid_loss: 0.578\n",
      "[epoch: 34, i:  2696]  train_loss: 0.577  |  valid_loss: 0.722\n",
      "[epoch: 34, i:  2783]  train_loss: 0.549  |  valid_loss: 0.430\n",
      "[epoch: 34, i:  2870]  train_loss: 0.589  |  valid_loss: 0.784\n",
      "[epoch: 34, i:  2957]  train_loss: 0.498  |  valid_loss: 0.726\n",
      "[epoch: 34, i:  3044]  train_loss: 0.526  |  valid_loss: 0.765\n",
      "[epoch: 34, i:  3131]  train_loss: 0.549  |  valid_loss: 0.611\n",
      "[epoch: 34, i:  3218]  train_loss: 0.505  |  valid_loss: 0.676\n",
      "[epoch: 34, i:  3305]  train_loss: 0.499  |  valid_loss: 0.644\n",
      "[epoch: 34, i:  3392]  train_loss: 0.587  |  valid_loss: 0.548\n",
      "[epoch: 34, i:  3479]  train_loss: 0.542  |  valid_loss: 0.538\n",
      "[epoch: 34, i:  3566]  train_loss: 0.486  |  valid_loss: 0.535\n",
      "[epoch: 34, i:  3653]  train_loss: 0.615  |  valid_loss: 0.524\n",
      "[epoch: 34, i:  3740]  train_loss: 0.550  |  valid_loss: 0.480\n",
      "[epoch: 34, i:  3827]  train_loss: 0.528  |  valid_loss: 0.771\n",
      "[epoch: 34, i:  3914]  train_loss: 0.552  |  valid_loss: 0.481\n",
      "[epoch: 34, i:  4001]  train_loss: 0.545  |  valid_loss: 0.685\n",
      "[epoch: 34, i:  4088]  train_loss: 0.466  |  valid_loss: 0.673\n",
      "[epoch: 34, i:  4175]  train_loss: 0.612  |  valid_loss: 0.560\n",
      "[epoch: 34, i:  4262]  train_loss: 0.534  |  valid_loss: 0.467\n",
      "[epoch: 34, i:  4349]  train_loss: 0.562  |  valid_loss: 0.564\n",
      "[epoch: 34, i:  4436]  train_loss: 0.625  |  valid_loss: 0.620\n",
      "[epoch: 34, i:  4523]  train_loss: 0.560  |  valid_loss: 0.739\n",
      "[epoch: 34, i:  4610]  train_loss: 0.614  |  valid_loss: 0.505\n",
      "[epoch: 34, i:  4697]  train_loss: 0.568  |  valid_loss: 0.559\n",
      "[epoch: 34, i:  4784]  train_loss: 0.600  |  valid_loss: 0.677\n",
      "[epoch: 34, i:  4871]  train_loss: 0.518  |  valid_loss: 0.776\n",
      "[epoch: 34, i:  4958]  train_loss: 0.581  |  valid_loss: 0.860\n",
      "[epoch: 34, i:  5045]  train_loss: 0.628  |  valid_loss: 0.450\n",
      "[epoch: 34, i:  5132]  train_loss: 0.542  |  valid_loss: 0.592\n",
      "[epoch: 34, i:  5219]  train_loss: 0.542  |  valid_loss: 0.826\n",
      "[epoch: 34, i:  5306]  train_loss: 0.491  |  valid_loss: 0.682\n",
      "[epoch: 34, i:  5393]  train_loss: 0.558  |  valid_loss: 0.560\n",
      "[epoch: 34, i:  5480]  train_loss: 0.514  |  valid_loss: 0.774\n",
      "[epoch: 34, i:  5567]  train_loss: 0.531  |  valid_loss: 0.335\n",
      "[epoch: 34, i:  5654]  train_loss: 0.596  |  valid_loss: 0.655\n",
      "[epoch: 34, i:  5741]  train_loss: 0.523  |  valid_loss: 0.666\n",
      "[epoch: 34, i:  5828]  train_loss: 0.562  |  valid_loss: 0.571\n",
      "[epoch: 34, i:  5915]  train_loss: 0.568  |  valid_loss: 0.740\n",
      "[epoch: 34, i:  6002]  train_loss: 0.598  |  valid_loss: 0.573\n",
      "[epoch: 34, i:  6089]  train_loss: 0.586  |  valid_loss: 0.831\n",
      "[epoch: 34, i:  6176]  train_loss: 0.553  |  valid_loss: 0.667\n",
      "[epoch: 34, i:  6263]  train_loss: 0.516  |  valid_loss: 0.560\n",
      "[epoch: 34, i:  6350]  train_loss: 0.619  |  valid_loss: 0.542\n",
      "[epoch: 34, i:  6437]  train_loss: 0.524  |  valid_loss: 0.437\n",
      "[epoch: 34, i:  6524]  train_loss: 0.554  |  valid_loss: 0.790\n",
      "[epoch: 34, i:  6611]  train_loss: 0.572  |  valid_loss: 0.384\n",
      "[epoch: 34, i:  6698]  train_loss: 0.514  |  valid_loss: 0.709\n",
      "[epoch: 34, i:  6785]  train_loss: 0.504  |  valid_loss: 0.440\n",
      "[epoch: 34, i:  6872]  train_loss: 0.506  |  valid_loss: 0.778\n",
      "[epoch: 34, i:  6959]  train_loss: 0.531  |  valid_loss: 0.637\n",
      "[epoch: 34, i:  7046]  train_loss: 0.566  |  valid_loss: 0.629\n",
      "[epoch: 34, i:  7133]  train_loss: 0.538  |  valid_loss: 0.471\n",
      "[epoch: 34, i:  7220]  train_loss: 0.523  |  valid_loss: 0.693\n",
      "[epoch: 34, i:  7307]  train_loss: 0.552  |  valid_loss: 0.740\n",
      "[epoch: 34, i:  7394]  train_loss: 0.556  |  valid_loss: 0.555\n",
      "[epoch: 34, i:  7481]  train_loss: 0.531  |  valid_loss: 0.606\n",
      "[epoch: 34, i:  7568]  train_loss: 0.531  |  valid_loss: 0.757\n",
      "[epoch: 34, i:  7655]  train_loss: 0.591  |  valid_loss: 0.667\n",
      "[epoch: 34, i:  7742]  train_loss: 0.582  |  valid_loss: 0.662\n",
      "[epoch: 34, i:  7829]  train_loss: 0.517  |  valid_loss: 0.449\n",
      "[epoch: 34, i:  7916]  train_loss: 0.603  |  valid_loss: 0.626\n",
      "[epoch: 34, i:  8003]  train_loss: 0.522  |  valid_loss: 0.455\n",
      "[epoch: 34, i:  8090]  train_loss: 0.549  |  valid_loss: 0.522\n",
      "[epoch: 34, i:  8177]  train_loss: 0.571  |  valid_loss: 0.630\n",
      "[epoch: 34, i:  8264]  train_loss: 0.475  |  valid_loss: 0.602\n",
      "[epoch: 34, i:  8351]  train_loss: 0.576  |  valid_loss: 0.701\n",
      "[epoch: 34, i:  8438]  train_loss: 0.557  |  valid_loss: 0.487\n",
      "[epoch: 34, i:  8525]  train_loss: 0.570  |  valid_loss: 0.651\n",
      "[epoch: 34, i:  8612]  train_loss: 0.564  |  valid_loss: 0.824\n",
      "[epoch: 34, i:  8699]  train_loss: 0.571  |  valid_loss: 0.618\n",
      "--> [End of epoch 34] train_accuracy: 80.82%  |  valid_accuracy: 79.00%\n",
      "--> [Start of epoch 35]  lr: 0.000050\n",
      "[epoch: 35, i:    86]  train_loss: 0.556  |  valid_loss: 0.616\n",
      "[epoch: 35, i:   173]  train_loss: 0.485  |  valid_loss: 0.627\n",
      "[epoch: 35, i:   260]  train_loss: 0.467  |  valid_loss: 0.692\n",
      "[epoch: 35, i:   347]  train_loss: 0.526  |  valid_loss: 0.604\n",
      "[epoch: 35, i:   434]  train_loss: 0.553  |  valid_loss: 0.570\n",
      "[epoch: 35, i:   521]  train_loss: 0.501  |  valid_loss: 0.412\n",
      "[epoch: 35, i:   608]  train_loss: 0.488  |  valid_loss: 0.659\n",
      "[epoch: 35, i:   695]  train_loss: 0.499  |  valid_loss: 0.597\n",
      "[epoch: 35, i:   782]  train_loss: 0.518  |  valid_loss: 0.706\n",
      "[epoch: 35, i:   869]  train_loss: 0.509  |  valid_loss: 0.719\n",
      "[epoch: 35, i:   956]  train_loss: 0.551  |  valid_loss: 0.555\n",
      "[epoch: 35, i:  1043]  train_loss: 0.485  |  valid_loss: 0.502\n",
      "[epoch: 35, i:  1130]  train_loss: 0.496  |  valid_loss: 0.493\n",
      "[epoch: 35, i:  1217]  train_loss: 0.549  |  valid_loss: 0.667\n",
      "[epoch: 35, i:  1304]  train_loss: 0.531  |  valid_loss: 0.422\n",
      "[epoch: 35, i:  1391]  train_loss: 0.524  |  valid_loss: 0.630\n",
      "[epoch: 35, i:  1478]  train_loss: 0.520  |  valid_loss: 0.500\n",
      "[epoch: 35, i:  1565]  train_loss: 0.485  |  valid_loss: 0.735\n",
      "[epoch: 35, i:  1652]  train_loss: 0.526  |  valid_loss: 0.560\n",
      "[epoch: 35, i:  1739]  train_loss: 0.556  |  valid_loss: 0.938\n",
      "[epoch: 35, i:  1826]  train_loss: 0.521  |  valid_loss: 0.748\n",
      "[epoch: 35, i:  1913]  train_loss: 0.484  |  valid_loss: 0.660\n",
      "[epoch: 35, i:  2000]  train_loss: 0.520  |  valid_loss: 0.640\n",
      "[epoch: 35, i:  2087]  train_loss: 0.547  |  valid_loss: 0.763\n",
      "[epoch: 35, i:  2174]  train_loss: 0.532  |  valid_loss: 0.550\n",
      "[epoch: 35, i:  2261]  train_loss: 0.512  |  valid_loss: 0.878\n",
      "[epoch: 35, i:  2348]  train_loss: 0.491  |  valid_loss: 0.483\n",
      "[epoch: 35, i:  2435]  train_loss: 0.509  |  valid_loss: 0.697\n",
      "[epoch: 35, i:  2522]  train_loss: 0.525  |  valid_loss: 0.428\n",
      "[epoch: 35, i:  2609]  train_loss: 0.472  |  valid_loss: 0.575\n",
      "[epoch: 35, i:  2696]  train_loss: 0.522  |  valid_loss: 0.627\n",
      "[epoch: 35, i:  2783]  train_loss: 0.462  |  valid_loss: 0.401\n",
      "[epoch: 35, i:  2870]  train_loss: 0.480  |  valid_loss: 0.797\n",
      "[epoch: 35, i:  2957]  train_loss: 0.440  |  valid_loss: 0.733\n",
      "[epoch: 35, i:  3044]  train_loss: 0.495  |  valid_loss: 0.671\n",
      "[epoch: 35, i:  3131]  train_loss: 0.540  |  valid_loss: 0.596\n",
      "[epoch: 35, i:  3218]  train_loss: 0.530  |  valid_loss: 0.689\n",
      "[epoch: 35, i:  3305]  train_loss: 0.495  |  valid_loss: 0.609\n",
      "[epoch: 35, i:  3392]  train_loss: 0.443  |  valid_loss: 0.582\n",
      "[epoch: 35, i:  3479]  train_loss: 0.490  |  valid_loss: 0.587\n",
      "[epoch: 35, i:  3566]  train_loss: 0.482  |  valid_loss: 0.546\n",
      "[epoch: 35, i:  3653]  train_loss: 0.506  |  valid_loss: 0.582\n",
      "[epoch: 35, i:  3740]  train_loss: 0.481  |  valid_loss: 0.386\n",
      "[epoch: 35, i:  3827]  train_loss: 0.526  |  valid_loss: 0.771\n",
      "[epoch: 35, i:  3914]  train_loss: 0.543  |  valid_loss: 0.495\n",
      "[epoch: 35, i:  4001]  train_loss: 0.462  |  valid_loss: 0.595\n",
      "[epoch: 35, i:  4088]  train_loss: 0.503  |  valid_loss: 0.562\n",
      "[epoch: 35, i:  4175]  train_loss: 0.445  |  valid_loss: 0.569\n",
      "[epoch: 35, i:  4262]  train_loss: 0.489  |  valid_loss: 0.495\n",
      "[epoch: 35, i:  4349]  train_loss: 0.597  |  valid_loss: 0.596\n",
      "[epoch: 35, i:  4436]  train_loss: 0.518  |  valid_loss: 0.581\n",
      "[epoch: 35, i:  4523]  train_loss: 0.498  |  valid_loss: 0.682\n",
      "[epoch: 35, i:  4610]  train_loss: 0.453  |  valid_loss: 0.511\n",
      "[epoch: 35, i:  4697]  train_loss: 0.501  |  valid_loss: 0.634\n",
      "[epoch: 35, i:  4784]  train_loss: 0.492  |  valid_loss: 0.640\n",
      "[epoch: 35, i:  4871]  train_loss: 0.498  |  valid_loss: 0.716\n",
      "[epoch: 35, i:  4958]  train_loss: 0.431  |  valid_loss: 0.790\n",
      "[epoch: 35, i:  5045]  train_loss: 0.433  |  valid_loss: 0.414\n",
      "[epoch: 35, i:  5132]  train_loss: 0.491  |  valid_loss: 0.562\n",
      "[epoch: 35, i:  5219]  train_loss: 0.477  |  valid_loss: 0.736\n",
      "[epoch: 35, i:  5306]  train_loss: 0.454  |  valid_loss: 0.626\n",
      "[epoch: 35, i:  5393]  train_loss: 0.480  |  valid_loss: 0.507\n",
      "[epoch: 35, i:  5480]  train_loss: 0.528  |  valid_loss: 0.738\n",
      "[epoch: 35, i:  5567]  train_loss: 0.447  |  valid_loss: 0.312\n",
      "[epoch: 35, i:  5654]  train_loss: 0.510  |  valid_loss: 0.561\n",
      "[epoch: 35, i:  5741]  train_loss: 0.525  |  valid_loss: 0.674\n",
      "[epoch: 35, i:  5828]  train_loss: 0.390  |  valid_loss: 0.572\n",
      "[epoch: 35, i:  5915]  train_loss: 0.474  |  valid_loss: 0.703\n",
      "[epoch: 35, i:  6002]  train_loss: 0.498  |  valid_loss: 0.520\n",
      "[epoch: 35, i:  6089]  train_loss: 0.520  |  valid_loss: 0.741\n",
      "[epoch: 35, i:  6176]  train_loss: 0.423  |  valid_loss: 0.613\n",
      "[epoch: 35, i:  6263]  train_loss: 0.526  |  valid_loss: 0.568\n",
      "[epoch: 35, i:  6350]  train_loss: 0.440  |  valid_loss: 0.511\n",
      "[epoch: 35, i:  6437]  train_loss: 0.484  |  valid_loss: 0.437\n",
      "[epoch: 35, i:  6524]  train_loss: 0.466  |  valid_loss: 0.739\n",
      "[epoch: 35, i:  6611]  train_loss: 0.456  |  valid_loss: 0.386\n",
      "[epoch: 35, i:  6698]  train_loss: 0.495  |  valid_loss: 0.684\n",
      "[epoch: 35, i:  6785]  train_loss: 0.464  |  valid_loss: 0.461\n",
      "[epoch: 35, i:  6872]  train_loss: 0.493  |  valid_loss: 0.717\n",
      "[epoch: 35, i:  6959]  train_loss: 0.501  |  valid_loss: 0.648\n",
      "[epoch: 35, i:  7046]  train_loss: 0.491  |  valid_loss: 0.580\n",
      "[epoch: 35, i:  7133]  train_loss: 0.533  |  valid_loss: 0.441\n",
      "[epoch: 35, i:  7220]  train_loss: 0.475  |  valid_loss: 0.741\n",
      "[epoch: 35, i:  7307]  train_loss: 0.557  |  valid_loss: 0.734\n",
      "[epoch: 35, i:  7394]  train_loss: 0.414  |  valid_loss: 0.531\n",
      "[epoch: 35, i:  7481]  train_loss: 0.535  |  valid_loss: 0.562\n",
      "[epoch: 35, i:  7568]  train_loss: 0.495  |  valid_loss: 0.709\n",
      "[epoch: 35, i:  7655]  train_loss: 0.509  |  valid_loss: 0.640\n",
      "[epoch: 35, i:  7742]  train_loss: 0.485  |  valid_loss: 0.583\n",
      "[epoch: 35, i:  7829]  train_loss: 0.498  |  valid_loss: 0.428\n",
      "[epoch: 35, i:  7916]  train_loss: 0.457  |  valid_loss: 0.585\n",
      "[epoch: 35, i:  8003]  train_loss: 0.475  |  valid_loss: 0.436\n",
      "[epoch: 35, i:  8090]  train_loss: 0.487  |  valid_loss: 0.455\n",
      "[epoch: 35, i:  8177]  train_loss: 0.482  |  valid_loss: 0.562\n",
      "[epoch: 35, i:  8264]  train_loss: 0.453  |  valid_loss: 0.602\n",
      "[epoch: 35, i:  8351]  train_loss: 0.526  |  valid_loss: 0.664\n",
      "[epoch: 35, i:  8438]  train_loss: 0.536  |  valid_loss: 0.446\n",
      "[epoch: 35, i:  8525]  train_loss: 0.519  |  valid_loss: 0.593\n",
      "[epoch: 35, i:  8612]  train_loss: 0.483  |  valid_loss: 0.840\n",
      "[epoch: 35, i:  8699]  train_loss: 0.458  |  valid_loss: 0.589\n",
      "--> [End of epoch 35] train_accuracy: 82.73%  |  valid_accuracy: 79.71%\n",
      "--> [Start of epoch 36]  lr: 0.000050\n",
      "[epoch: 36, i:    86]  train_loss: 0.490  |  valid_loss: 0.560\n",
      "[epoch: 36, i:   173]  train_loss: 0.476  |  valid_loss: 0.593\n",
      "[epoch: 36, i:   260]  train_loss: 0.493  |  valid_loss: 0.691\n",
      "[epoch: 36, i:   347]  train_loss: 0.452  |  valid_loss: 0.515\n",
      "[epoch: 36, i:   434]  train_loss: 0.456  |  valid_loss: 0.563\n",
      "[epoch: 36, i:   521]  train_loss: 0.461  |  valid_loss: 0.363\n",
      "[epoch: 36, i:   608]  train_loss: 0.494  |  valid_loss: 0.663\n",
      "[epoch: 36, i:   695]  train_loss: 0.506  |  valid_loss: 0.654\n",
      "[epoch: 36, i:   782]  train_loss: 0.479  |  valid_loss: 0.694\n",
      "[epoch: 36, i:   869]  train_loss: 0.449  |  valid_loss: 0.672\n",
      "[epoch: 36, i:   956]  train_loss: 0.479  |  valid_loss: 0.549\n",
      "[epoch: 36, i:  1043]  train_loss: 0.520  |  valid_loss: 0.532\n",
      "[epoch: 36, i:  1130]  train_loss: 0.477  |  valid_loss: 0.494\n",
      "[epoch: 36, i:  1217]  train_loss: 0.480  |  valid_loss: 0.596\n",
      "[epoch: 36, i:  1304]  train_loss: 0.511  |  valid_loss: 0.425\n",
      "[epoch: 36, i:  1391]  train_loss: 0.444  |  valid_loss: 0.629\n",
      "[epoch: 36, i:  1478]  train_loss: 0.429  |  valid_loss: 0.518\n",
      "[epoch: 36, i:  1565]  train_loss: 0.498  |  valid_loss: 0.778\n",
      "[epoch: 36, i:  1652]  train_loss: 0.448  |  valid_loss: 0.551\n",
      "[epoch: 36, i:  1739]  train_loss: 0.424  |  valid_loss: 0.926\n",
      "[epoch: 36, i:  1826]  train_loss: 0.488  |  valid_loss: 0.719\n",
      "[epoch: 36, i:  1913]  train_loss: 0.521  |  valid_loss: 0.596\n",
      "[epoch: 36, i:  2000]  train_loss: 0.539  |  valid_loss: 0.618\n",
      "[epoch: 36, i:  2087]  train_loss: 0.496  |  valid_loss: 0.766\n",
      "[epoch: 36, i:  2174]  train_loss: 0.447  |  valid_loss: 0.519\n",
      "[epoch: 36, i:  2261]  train_loss: 0.509  |  valid_loss: 0.832\n",
      "[epoch: 36, i:  2348]  train_loss: 0.469  |  valid_loss: 0.493\n",
      "[epoch: 36, i:  2435]  train_loss: 0.535  |  valid_loss: 0.677\n",
      "[epoch: 36, i:  2522]  train_loss: 0.488  |  valid_loss: 0.432\n",
      "[epoch: 36, i:  2609]  train_loss: 0.490  |  valid_loss: 0.530\n",
      "[epoch: 36, i:  2696]  train_loss: 0.457  |  valid_loss: 0.635\n",
      "[epoch: 36, i:  2783]  train_loss: 0.471  |  valid_loss: 0.385\n",
      "[epoch: 36, i:  2870]  train_loss: 0.467  |  valid_loss: 0.752\n",
      "[epoch: 36, i:  2957]  train_loss: 0.572  |  valid_loss: 0.703\n",
      "[epoch: 36, i:  3044]  train_loss: 0.489  |  valid_loss: 0.641\n",
      "[epoch: 36, i:  3131]  train_loss: 0.460  |  valid_loss: 0.602\n",
      "[epoch: 36, i:  3218]  train_loss: 0.487  |  valid_loss: 0.649\n",
      "[epoch: 36, i:  3305]  train_loss: 0.435  |  valid_loss: 0.563\n",
      "[epoch: 36, i:  3392]  train_loss: 0.458  |  valid_loss: 0.509\n",
      "[epoch: 36, i:  3479]  train_loss: 0.534  |  valid_loss: 0.580\n",
      "[epoch: 36, i:  3566]  train_loss: 0.553  |  valid_loss: 0.521\n",
      "[epoch: 36, i:  3653]  train_loss: 0.517  |  valid_loss: 0.574\n",
      "[epoch: 36, i:  3740]  train_loss: 0.470  |  valid_loss: 0.394\n",
      "[epoch: 36, i:  3827]  train_loss: 0.493  |  valid_loss: 0.746\n",
      "[epoch: 36, i:  3914]  train_loss: 0.510  |  valid_loss: 0.453\n",
      "[epoch: 36, i:  4001]  train_loss: 0.431  |  valid_loss: 0.594\n",
      "[epoch: 36, i:  4088]  train_loss: 0.425  |  valid_loss: 0.568\n",
      "[epoch: 36, i:  4175]  train_loss: 0.511  |  valid_loss: 0.554\n",
      "[epoch: 36, i:  4262]  train_loss: 0.443  |  valid_loss: 0.450\n",
      "[epoch: 36, i:  4349]  train_loss: 0.495  |  valid_loss: 0.583\n",
      "[epoch: 36, i:  4436]  train_loss: 0.412  |  valid_loss: 0.549\n",
      "[epoch: 36, i:  4523]  train_loss: 0.476  |  valid_loss: 0.707\n",
      "[epoch: 36, i:  4610]  train_loss: 0.467  |  valid_loss: 0.520\n",
      "[epoch: 36, i:  4697]  train_loss: 0.514  |  valid_loss: 0.625\n",
      "[epoch: 36, i:  4784]  train_loss: 0.520  |  valid_loss: 0.658\n",
      "[epoch: 36, i:  4871]  train_loss: 0.449  |  valid_loss: 0.676\n",
      "[epoch: 36, i:  4958]  train_loss: 0.456  |  valid_loss: 0.758\n",
      "[epoch: 36, i:  5045]  train_loss: 0.446  |  valid_loss: 0.400\n",
      "[epoch: 36, i:  5132]  train_loss: 0.519  |  valid_loss: 0.599\n",
      "[epoch: 36, i:  5219]  train_loss: 0.480  |  valid_loss: 0.754\n",
      "[epoch: 36, i:  5306]  train_loss: 0.463  |  valid_loss: 0.622\n",
      "[epoch: 36, i:  5393]  train_loss: 0.461  |  valid_loss: 0.546\n",
      "[epoch: 36, i:  5480]  train_loss: 0.492  |  valid_loss: 0.737\n",
      "[epoch: 36, i:  5567]  train_loss: 0.505  |  valid_loss: 0.330\n",
      "[epoch: 36, i:  5654]  train_loss: 0.507  |  valid_loss: 0.556\n",
      "[epoch: 36, i:  5741]  train_loss: 0.471  |  valid_loss: 0.643\n",
      "[epoch: 36, i:  5828]  train_loss: 0.474  |  valid_loss: 0.551\n",
      "[epoch: 36, i:  5915]  train_loss: 0.417  |  valid_loss: 0.730\n",
      "[epoch: 36, i:  6002]  train_loss: 0.492  |  valid_loss: 0.480\n",
      "[epoch: 36, i:  6089]  train_loss: 0.486  |  valid_loss: 0.753\n",
      "[epoch: 36, i:  6176]  train_loss: 0.482  |  valid_loss: 0.609\n",
      "[epoch: 36, i:  6263]  train_loss: 0.547  |  valid_loss: 0.569\n",
      "[epoch: 36, i:  6350]  train_loss: 0.405  |  valid_loss: 0.506\n",
      "[epoch: 36, i:  6437]  train_loss: 0.494  |  valid_loss: 0.437\n",
      "[epoch: 36, i:  6524]  train_loss: 0.507  |  valid_loss: 0.757\n",
      "[epoch: 36, i:  6611]  train_loss: 0.461  |  valid_loss: 0.400\n",
      "[epoch: 36, i:  6698]  train_loss: 0.454  |  valid_loss: 0.679\n",
      "[epoch: 36, i:  6785]  train_loss: 0.466  |  valid_loss: 0.416\n",
      "[epoch: 36, i:  6872]  train_loss: 0.455  |  valid_loss: 0.721\n",
      "[epoch: 36, i:  6959]  train_loss: 0.499  |  valid_loss: 0.639\n",
      "[epoch: 36, i:  7046]  train_loss: 0.473  |  valid_loss: 0.600\n",
      "[epoch: 36, i:  7133]  train_loss: 0.494  |  valid_loss: 0.448\n",
      "[epoch: 36, i:  7220]  train_loss: 0.464  |  valid_loss: 0.693\n",
      "[epoch: 36, i:  7307]  train_loss: 0.465  |  valid_loss: 0.695\n",
      "[epoch: 36, i:  7394]  train_loss: 0.450  |  valid_loss: 0.543\n",
      "[epoch: 36, i:  7481]  train_loss: 0.537  |  valid_loss: 0.598\n",
      "[epoch: 36, i:  7568]  train_loss: 0.477  |  valid_loss: 0.676\n",
      "[epoch: 36, i:  7655]  train_loss: 0.483  |  valid_loss: 0.627\n",
      "[epoch: 36, i:  7742]  train_loss: 0.435  |  valid_loss: 0.596\n",
      "[epoch: 36, i:  7829]  train_loss: 0.448  |  valid_loss: 0.408\n",
      "[epoch: 36, i:  7916]  train_loss: 0.461  |  valid_loss: 0.596\n",
      "[epoch: 36, i:  8003]  train_loss: 0.432  |  valid_loss: 0.438\n",
      "[epoch: 36, i:  8090]  train_loss: 0.463  |  valid_loss: 0.452\n",
      "[epoch: 36, i:  8177]  train_loss: 0.443  |  valid_loss: 0.547\n",
      "[epoch: 36, i:  8264]  train_loss: 0.491  |  valid_loss: 0.574\n",
      "[epoch: 36, i:  8351]  train_loss: 0.451  |  valid_loss: 0.659\n",
      "[epoch: 36, i:  8438]  train_loss: 0.474  |  valid_loss: 0.451\n",
      "[epoch: 36, i:  8525]  train_loss: 0.500  |  valid_loss: 0.556\n",
      "[epoch: 36, i:  8612]  train_loss: 0.413  |  valid_loss: 0.860\n",
      "[epoch: 36, i:  8699]  train_loss: 0.486  |  valid_loss: 0.585\n",
      "--> [End of epoch 36] train_accuracy: 83.45%  |  valid_accuracy: 80.52%\n",
      "--> [Start of epoch 37]  lr: 0.000050\n",
      "[epoch: 37, i:    86]  train_loss: 0.485  |  valid_loss: 0.565\n",
      "[epoch: 37, i:   173]  train_loss: 0.467  |  valid_loss: 0.596\n",
      "[epoch: 37, i:   260]  train_loss: 0.512  |  valid_loss: 0.645\n",
      "[epoch: 37, i:   347]  train_loss: 0.425  |  valid_loss: 0.561\n",
      "[epoch: 37, i:   434]  train_loss: 0.496  |  valid_loss: 0.586\n",
      "[epoch: 37, i:   521]  train_loss: 0.493  |  valid_loss: 0.361\n",
      "[epoch: 37, i:   608]  train_loss: 0.493  |  valid_loss: 0.633\n",
      "[epoch: 37, i:   695]  train_loss: 0.548  |  valid_loss: 0.651\n",
      "[epoch: 37, i:   782]  train_loss: 0.525  |  valid_loss: 0.670\n",
      "[epoch: 37, i:   869]  train_loss: 0.462  |  valid_loss: 0.677\n",
      "[epoch: 37, i:   956]  train_loss: 0.419  |  valid_loss: 0.531\n",
      "[epoch: 37, i:  1043]  train_loss: 0.432  |  valid_loss: 0.534\n",
      "[epoch: 37, i:  1130]  train_loss: 0.485  |  valid_loss: 0.456\n",
      "[epoch: 37, i:  1217]  train_loss: 0.468  |  valid_loss: 0.603\n",
      "[epoch: 37, i:  1304]  train_loss: 0.444  |  valid_loss: 0.403\n",
      "[epoch: 37, i:  1391]  train_loss: 0.393  |  valid_loss: 0.642\n",
      "[epoch: 37, i:  1478]  train_loss: 0.498  |  valid_loss: 0.516\n",
      "[epoch: 37, i:  1565]  train_loss: 0.484  |  valid_loss: 0.759\n",
      "[epoch: 37, i:  1652]  train_loss: 0.505  |  valid_loss: 0.562\n",
      "[epoch: 37, i:  1739]  train_loss: 0.441  |  valid_loss: 0.903\n",
      "[epoch: 37, i:  1826]  train_loss: 0.459  |  valid_loss: 0.690\n",
      "[epoch: 37, i:  1913]  train_loss: 0.498  |  valid_loss: 0.624\n",
      "[epoch: 37, i:  2000]  train_loss: 0.505  |  valid_loss: 0.604\n",
      "[epoch: 37, i:  2087]  train_loss: 0.466  |  valid_loss: 0.728\n",
      "[epoch: 37, i:  2174]  train_loss: 0.518  |  valid_loss: 0.521\n",
      "[epoch: 37, i:  2261]  train_loss: 0.447  |  valid_loss: 0.834\n",
      "[epoch: 37, i:  2348]  train_loss: 0.441  |  valid_loss: 0.449\n",
      "[epoch: 37, i:  2435]  train_loss: 0.509  |  valid_loss: 0.660\n",
      "[epoch: 37, i:  2522]  train_loss: 0.443  |  valid_loss: 0.430\n",
      "[epoch: 37, i:  2609]  train_loss: 0.447  |  valid_loss: 0.545\n",
      "[epoch: 37, i:  2696]  train_loss: 0.477  |  valid_loss: 0.648\n",
      "[epoch: 37, i:  2783]  train_loss: 0.551  |  valid_loss: 0.396\n",
      "[epoch: 37, i:  2870]  train_loss: 0.434  |  valid_loss: 0.753\n",
      "[epoch: 37, i:  2957]  train_loss: 0.511  |  valid_loss: 0.688\n",
      "[epoch: 37, i:  3044]  train_loss: 0.415  |  valid_loss: 0.650\n",
      "[epoch: 37, i:  3131]  train_loss: 0.450  |  valid_loss: 0.554\n",
      "[epoch: 37, i:  3218]  train_loss: 0.534  |  valid_loss: 0.674\n",
      "[epoch: 37, i:  3305]  train_loss: 0.511  |  valid_loss: 0.615\n",
      "[epoch: 37, i:  3392]  train_loss: 0.486  |  valid_loss: 0.521\n",
      "[epoch: 37, i:  3479]  train_loss: 0.461  |  valid_loss: 0.574\n",
      "[epoch: 37, i:  3566]  train_loss: 0.453  |  valid_loss: 0.511\n",
      "[epoch: 37, i:  3653]  train_loss: 0.433  |  valid_loss: 0.573\n",
      "[epoch: 37, i:  3740]  train_loss: 0.437  |  valid_loss: 0.391\n",
      "[epoch: 37, i:  3827]  train_loss: 0.478  |  valid_loss: 0.753\n",
      "[epoch: 37, i:  3914]  train_loss: 0.479  |  valid_loss: 0.445\n",
      "[epoch: 37, i:  4001]  train_loss: 0.479  |  valid_loss: 0.602\n",
      "[epoch: 37, i:  4088]  train_loss: 0.466  |  valid_loss: 0.540\n",
      "[epoch: 37, i:  4175]  train_loss: 0.509  |  valid_loss: 0.539\n",
      "[epoch: 37, i:  4262]  train_loss: 0.508  |  valid_loss: 0.463\n",
      "[epoch: 37, i:  4349]  train_loss: 0.544  |  valid_loss: 0.575\n",
      "[epoch: 37, i:  4436]  train_loss: 0.456  |  valid_loss: 0.555\n",
      "[epoch: 37, i:  4523]  train_loss: 0.482  |  valid_loss: 0.679\n",
      "[epoch: 37, i:  4610]  train_loss: 0.428  |  valid_loss: 0.484\n",
      "[epoch: 37, i:  4697]  train_loss: 0.455  |  valid_loss: 0.581\n",
      "[epoch: 37, i:  4784]  train_loss: 0.508  |  valid_loss: 0.648\n",
      "[epoch: 37, i:  4871]  train_loss: 0.477  |  valid_loss: 0.724\n",
      "[epoch: 37, i:  4958]  train_loss: 0.466  |  valid_loss: 0.757\n",
      "[epoch: 37, i:  5045]  train_loss: 0.433  |  valid_loss: 0.418\n",
      "[epoch: 37, i:  5132]  train_loss: 0.460  |  valid_loss: 0.578\n",
      "[epoch: 37, i:  5219]  train_loss: 0.476  |  valid_loss: 0.774\n",
      "[epoch: 37, i:  5306]  train_loss: 0.486  |  valid_loss: 0.624\n",
      "[epoch: 37, i:  5393]  train_loss: 0.483  |  valid_loss: 0.551\n",
      "[epoch: 37, i:  5480]  train_loss: 0.422  |  valid_loss: 0.739\n",
      "[epoch: 37, i:  5567]  train_loss: 0.452  |  valid_loss: 0.322\n",
      "[epoch: 37, i:  5654]  train_loss: 0.466  |  valid_loss: 0.535\n",
      "[epoch: 37, i:  5741]  train_loss: 0.484  |  valid_loss: 0.648\n",
      "[epoch: 37, i:  5828]  train_loss: 0.535  |  valid_loss: 0.590\n",
      "[epoch: 37, i:  5915]  train_loss: 0.427  |  valid_loss: 0.752\n",
      "[epoch: 37, i:  6002]  train_loss: 0.476  |  valid_loss: 0.505\n",
      "[epoch: 37, i:  6089]  train_loss: 0.449  |  valid_loss: 0.793\n",
      "[epoch: 37, i:  6176]  train_loss: 0.490  |  valid_loss: 0.587\n",
      "[epoch: 37, i:  6263]  train_loss: 0.448  |  valid_loss: 0.567\n",
      "[epoch: 37, i:  6350]  train_loss: 0.473  |  valid_loss: 0.517\n",
      "[epoch: 37, i:  6437]  train_loss: 0.482  |  valid_loss: 0.429\n",
      "[epoch: 37, i:  6524]  train_loss: 0.491  |  valid_loss: 0.763\n",
      "[epoch: 37, i:  6611]  train_loss: 0.463  |  valid_loss: 0.415\n",
      "[epoch: 37, i:  6698]  train_loss: 0.507  |  valid_loss: 0.688\n",
      "[epoch: 37, i:  6785]  train_loss: 0.483  |  valid_loss: 0.433\n",
      "[epoch: 37, i:  6872]  train_loss: 0.447  |  valid_loss: 0.732\n",
      "[epoch: 37, i:  6959]  train_loss: 0.513  |  valid_loss: 0.641\n",
      "[epoch: 37, i:  7046]  train_loss: 0.473  |  valid_loss: 0.586\n",
      "[epoch: 37, i:  7133]  train_loss: 0.479  |  valid_loss: 0.469\n",
      "[epoch: 37, i:  7220]  train_loss: 0.523  |  valid_loss: 0.705\n",
      "[epoch: 37, i:  7307]  train_loss: 0.462  |  valid_loss: 0.732\n",
      "[epoch: 37, i:  7394]  train_loss: 0.503  |  valid_loss: 0.506\n",
      "[epoch: 37, i:  7481]  train_loss: 0.460  |  valid_loss: 0.540\n",
      "[epoch: 37, i:  7568]  train_loss: 0.439  |  valid_loss: 0.723\n",
      "[epoch: 37, i:  7655]  train_loss: 0.490  |  valid_loss: 0.620\n",
      "[epoch: 37, i:  7742]  train_loss: 0.436  |  valid_loss: 0.601\n",
      "[epoch: 37, i:  7829]  train_loss: 0.465  |  valid_loss: 0.399\n",
      "[epoch: 37, i:  7916]  train_loss: 0.403  |  valid_loss: 0.589\n",
      "[epoch: 37, i:  8003]  train_loss: 0.461  |  valid_loss: 0.436\n",
      "[epoch: 37, i:  8090]  train_loss: 0.427  |  valid_loss: 0.484\n",
      "[epoch: 37, i:  8177]  train_loss: 0.441  |  valid_loss: 0.505\n",
      "[epoch: 37, i:  8264]  train_loss: 0.450  |  valid_loss: 0.616\n",
      "[epoch: 37, i:  8351]  train_loss: 0.386  |  valid_loss: 0.653\n",
      "[epoch: 37, i:  8438]  train_loss: 0.493  |  valid_loss: 0.455\n",
      "[epoch: 37, i:  8525]  train_loss: 0.425  |  valid_loss: 0.553\n",
      "[epoch: 37, i:  8612]  train_loss: 0.455  |  valid_loss: 0.844\n",
      "[epoch: 37, i:  8699]  train_loss: 0.467  |  valid_loss: 0.608\n",
      "--> [End of epoch 37] train_accuracy: 83.83%  |  valid_accuracy: 80.56%\n",
      "--> [Start of epoch 38]  lr: 0.000050\n",
      "[epoch: 38, i:    86]  train_loss: 0.397  |  valid_loss: 0.557\n",
      "[epoch: 38, i:   173]  train_loss: 0.436  |  valid_loss: 0.600\n",
      "[epoch: 38, i:   260]  train_loss: 0.447  |  valid_loss: 0.664\n",
      "[epoch: 38, i:   347]  train_loss: 0.436  |  valid_loss: 0.504\n",
      "[epoch: 38, i:   434]  train_loss: 0.482  |  valid_loss: 0.559\n",
      "[epoch: 38, i:   521]  train_loss: 0.451  |  valid_loss: 0.365\n",
      "[epoch: 38, i:   608]  train_loss: 0.544  |  valid_loss: 0.601\n",
      "[epoch: 38, i:   695]  train_loss: 0.460  |  valid_loss: 0.622\n",
      "[epoch: 38, i:   782]  train_loss: 0.450  |  valid_loss: 0.713\n",
      "[epoch: 38, i:   869]  train_loss: 0.438  |  valid_loss: 0.686\n",
      "[epoch: 38, i:   956]  train_loss: 0.400  |  valid_loss: 0.502\n",
      "[epoch: 38, i:  1043]  train_loss: 0.443  |  valid_loss: 0.536\n",
      "[epoch: 38, i:  1130]  train_loss: 0.421  |  valid_loss: 0.495\n",
      "[epoch: 38, i:  1217]  train_loss: 0.478  |  valid_loss: 0.643\n",
      "[epoch: 38, i:  1304]  train_loss: 0.423  |  valid_loss: 0.428\n",
      "[epoch: 38, i:  1391]  train_loss: 0.526  |  valid_loss: 0.689\n",
      "[epoch: 38, i:  1478]  train_loss: 0.435  |  valid_loss: 0.499\n",
      "[epoch: 38, i:  1565]  train_loss: 0.451  |  valid_loss: 0.769\n",
      "[epoch: 38, i:  1652]  train_loss: 0.454  |  valid_loss: 0.616\n",
      "[epoch: 38, i:  1739]  train_loss: 0.441  |  valid_loss: 0.894\n",
      "[epoch: 38, i:  1826]  train_loss: 0.485  |  valid_loss: 0.690\n",
      "[epoch: 38, i:  1913]  train_loss: 0.513  |  valid_loss: 0.603\n",
      "[epoch: 38, i:  2000]  train_loss: 0.473  |  valid_loss: 0.580\n",
      "[epoch: 38, i:  2087]  train_loss: 0.480  |  valid_loss: 0.736\n",
      "[epoch: 38, i:  2174]  train_loss: 0.453  |  valid_loss: 0.562\n",
      "[epoch: 38, i:  2261]  train_loss: 0.447  |  valid_loss: 0.822\n",
      "[epoch: 38, i:  2348]  train_loss: 0.432  |  valid_loss: 0.482\n",
      "[epoch: 38, i:  2435]  train_loss: 0.436  |  valid_loss: 0.663\n",
      "[epoch: 38, i:  2522]  train_loss: 0.499  |  valid_loss: 0.419\n",
      "[epoch: 38, i:  2609]  train_loss: 0.414  |  valid_loss: 0.547\n",
      "[epoch: 38, i:  2696]  train_loss: 0.411  |  valid_loss: 0.645\n",
      "[epoch: 38, i:  2783]  train_loss: 0.470  |  valid_loss: 0.375\n",
      "[epoch: 38, i:  2870]  train_loss: 0.472  |  valid_loss: 0.695\n",
      "[epoch: 38, i:  2957]  train_loss: 0.462  |  valid_loss: 0.697\n",
      "[epoch: 38, i:  3044]  train_loss: 0.460  |  valid_loss: 0.675\n",
      "[epoch: 38, i:  3131]  train_loss: 0.508  |  valid_loss: 0.558\n",
      "[epoch: 38, i:  3218]  train_loss: 0.447  |  valid_loss: 0.632\n",
      "[epoch: 38, i:  3305]  train_loss: 0.443  |  valid_loss: 0.597\n",
      "[epoch: 38, i:  3392]  train_loss: 0.473  |  valid_loss: 0.499\n",
      "[epoch: 38, i:  3479]  train_loss: 0.458  |  valid_loss: 0.554\n",
      "[epoch: 38, i:  3566]  train_loss: 0.465  |  valid_loss: 0.556\n",
      "[epoch: 38, i:  3653]  train_loss: 0.477  |  valid_loss: 0.611\n",
      "[epoch: 38, i:  3740]  train_loss: 0.445  |  valid_loss: 0.393\n",
      "[epoch: 38, i:  3827]  train_loss: 0.436  |  valid_loss: 0.827\n",
      "[epoch: 38, i:  3914]  train_loss: 0.520  |  valid_loss: 0.414\n",
      "[epoch: 38, i:  4001]  train_loss: 0.483  |  valid_loss: 0.618\n",
      "[epoch: 38, i:  4088]  train_loss: 0.487  |  valid_loss: 0.503\n",
      "[epoch: 38, i:  4175]  train_loss: 0.473  |  valid_loss: 0.560\n",
      "[epoch: 38, i:  4262]  train_loss: 0.527  |  valid_loss: 0.455\n",
      "[epoch: 38, i:  4349]  train_loss: 0.502  |  valid_loss: 0.583\n",
      "[epoch: 38, i:  4436]  train_loss: 0.454  |  valid_loss: 0.550\n",
      "[epoch: 38, i:  4523]  train_loss: 0.489  |  valid_loss: 0.704\n",
      "[epoch: 38, i:  4610]  train_loss: 0.490  |  valid_loss: 0.497\n",
      "[epoch: 38, i:  4697]  train_loss: 0.445  |  valid_loss: 0.598\n",
      "[epoch: 38, i:  4784]  train_loss: 0.465  |  valid_loss: 0.635\n",
      "[epoch: 38, i:  4871]  train_loss: 0.463  |  valid_loss: 0.685\n",
      "[epoch: 38, i:  4958]  train_loss: 0.450  |  valid_loss: 0.755\n",
      "[epoch: 38, i:  5045]  train_loss: 0.434  |  valid_loss: 0.414\n",
      "[epoch: 38, i:  5132]  train_loss: 0.498  |  valid_loss: 0.587\n",
      "[epoch: 38, i:  5219]  train_loss: 0.522  |  valid_loss: 0.754\n",
      "[epoch: 38, i:  5306]  train_loss: 0.461  |  valid_loss: 0.592\n",
      "[epoch: 38, i:  5393]  train_loss: 0.471  |  valid_loss: 0.475\n",
      "[epoch: 38, i:  5480]  train_loss: 0.505  |  valid_loss: 0.800\n",
      "[epoch: 38, i:  5567]  train_loss: 0.441  |  valid_loss: 0.315\n",
      "[epoch: 38, i:  5654]  train_loss: 0.465  |  valid_loss: 0.560\n",
      "[epoch: 38, i:  5741]  train_loss: 0.418  |  valid_loss: 0.653\n",
      "[epoch: 38, i:  5828]  train_loss: 0.412  |  valid_loss: 0.561\n",
      "[epoch: 38, i:  5915]  train_loss: 0.508  |  valid_loss: 0.733\n",
      "[epoch: 38, i:  6002]  train_loss: 0.473  |  valid_loss: 0.486\n",
      "[epoch: 38, i:  6089]  train_loss: 0.428  |  valid_loss: 0.746\n",
      "[epoch: 38, i:  6176]  train_loss: 0.496  |  valid_loss: 0.602\n",
      "[epoch: 38, i:  6263]  train_loss: 0.444  |  valid_loss: 0.551\n",
      "[epoch: 38, i:  6350]  train_loss: 0.436  |  valid_loss: 0.507\n",
      "[epoch: 38, i:  6437]  train_loss: 0.488  |  valid_loss: 0.427\n",
      "[epoch: 38, i:  6524]  train_loss: 0.419  |  valid_loss: 0.754\n",
      "[epoch: 38, i:  6611]  train_loss: 0.484  |  valid_loss: 0.383\n",
      "[epoch: 38, i:  6698]  train_loss: 0.448  |  valid_loss: 0.678\n",
      "[epoch: 38, i:  6785]  train_loss: 0.478  |  valid_loss: 0.443\n",
      "[epoch: 38, i:  6872]  train_loss: 0.416  |  valid_loss: 0.744\n",
      "[epoch: 38, i:  6959]  train_loss: 0.456  |  valid_loss: 0.646\n",
      "[epoch: 38, i:  7046]  train_loss: 0.500  |  valid_loss: 0.573\n",
      "[epoch: 38, i:  7133]  train_loss: 0.438  |  valid_loss: 0.430\n",
      "[epoch: 38, i:  7220]  train_loss: 0.493  |  valid_loss: 0.727\n",
      "[epoch: 38, i:  7307]  train_loss: 0.439  |  valid_loss: 0.717\n",
      "[epoch: 38, i:  7394]  train_loss: 0.500  |  valid_loss: 0.539\n",
      "[epoch: 38, i:  7481]  train_loss: 0.433  |  valid_loss: 0.575\n",
      "[epoch: 38, i:  7568]  train_loss: 0.441  |  valid_loss: 0.707\n",
      "[epoch: 38, i:  7655]  train_loss: 0.496  |  valid_loss: 0.617\n",
      "[epoch: 38, i:  7742]  train_loss: 0.446  |  valid_loss: 0.578\n",
      "[epoch: 38, i:  7829]  train_loss: 0.475  |  valid_loss: 0.408\n",
      "[epoch: 38, i:  7916]  train_loss: 0.438  |  valid_loss: 0.610\n",
      "[epoch: 38, i:  8003]  train_loss: 0.425  |  valid_loss: 0.436\n",
      "[epoch: 38, i:  8090]  train_loss: 0.484  |  valid_loss: 0.477\n",
      "[epoch: 38, i:  8177]  train_loss: 0.492  |  valid_loss: 0.542\n",
      "[epoch: 38, i:  8264]  train_loss: 0.526  |  valid_loss: 0.633\n",
      "[epoch: 38, i:  8351]  train_loss: 0.485  |  valid_loss: 0.664\n",
      "[epoch: 38, i:  8438]  train_loss: 0.492  |  valid_loss: 0.453\n",
      "[epoch: 38, i:  8525]  train_loss: 0.442  |  valid_loss: 0.548\n",
      "[epoch: 38, i:  8612]  train_loss: 0.465  |  valid_loss: 0.839\n",
      "[epoch: 38, i:  8699]  train_loss: 0.483  |  valid_loss: 0.598\n",
      "--> [End of epoch 38] train_accuracy: 84.03%  |  valid_accuracy: 80.78%\n",
      "--> [Start of epoch 39]  lr: 0.000050\n",
      "[epoch: 39, i:    86]  train_loss: 0.419  |  valid_loss: 0.570\n",
      "[epoch: 39, i:   173]  train_loss: 0.429  |  valid_loss: 0.613\n",
      "[epoch: 39, i:   260]  train_loss: 0.465  |  valid_loss: 0.727\n",
      "[epoch: 39, i:   347]  train_loss: 0.481  |  valid_loss: 0.518\n",
      "[epoch: 39, i:   434]  train_loss: 0.422  |  valid_loss: 0.587\n",
      "[epoch: 39, i:   521]  train_loss: 0.463  |  valid_loss: 0.362\n",
      "[epoch: 39, i:   608]  train_loss: 0.466  |  valid_loss: 0.635\n",
      "[epoch: 39, i:   695]  train_loss: 0.474  |  valid_loss: 0.616\n",
      "[epoch: 39, i:   782]  train_loss: 0.423  |  valid_loss: 0.675\n",
      "[epoch: 39, i:   869]  train_loss: 0.469  |  valid_loss: 0.650\n",
      "[epoch: 39, i:   956]  train_loss: 0.452  |  valid_loss: 0.540\n",
      "[epoch: 39, i:  1043]  train_loss: 0.420  |  valid_loss: 0.543\n",
      "[epoch: 39, i:  1130]  train_loss: 0.498  |  valid_loss: 0.511\n",
      "[epoch: 39, i:  1217]  train_loss: 0.473  |  valid_loss: 0.593\n",
      "[epoch: 39, i:  1304]  train_loss: 0.471  |  valid_loss: 0.409\n",
      "[epoch: 39, i:  1391]  train_loss: 0.444  |  valid_loss: 0.645\n",
      "[epoch: 39, i:  1478]  train_loss: 0.422  |  valid_loss: 0.505\n",
      "[epoch: 39, i:  1565]  train_loss: 0.436  |  valid_loss: 0.759\n",
      "[epoch: 39, i:  1652]  train_loss: 0.467  |  valid_loss: 0.569\n",
      "[epoch: 39, i:  1739]  train_loss: 0.520  |  valid_loss: 0.861\n",
      "[epoch: 39, i:  1826]  train_loss: 0.442  |  valid_loss: 0.720\n",
      "[epoch: 39, i:  1913]  train_loss: 0.462  |  valid_loss: 0.597\n",
      "[epoch: 39, i:  2000]  train_loss: 0.453  |  valid_loss: 0.631\n",
      "[epoch: 39, i:  2087]  train_loss: 0.508  |  valid_loss: 0.777\n",
      "[epoch: 39, i:  2174]  train_loss: 0.470  |  valid_loss: 0.509\n",
      "[epoch: 39, i:  2261]  train_loss: 0.405  |  valid_loss: 0.843\n",
      "[epoch: 39, i:  2348]  train_loss: 0.485  |  valid_loss: 0.480\n",
      "[epoch: 39, i:  2435]  train_loss: 0.468  |  valid_loss: 0.645\n",
      "[epoch: 39, i:  2522]  train_loss: 0.399  |  valid_loss: 0.427\n",
      "[epoch: 39, i:  2609]  train_loss: 0.462  |  valid_loss: 0.558\n",
      "[epoch: 39, i:  2696]  train_loss: 0.445  |  valid_loss: 0.649\n",
      "[epoch: 39, i:  2783]  train_loss: 0.511  |  valid_loss: 0.395\n",
      "[epoch: 39, i:  2870]  train_loss: 0.482  |  valid_loss: 0.707\n",
      "[epoch: 39, i:  2957]  train_loss: 0.444  |  valid_loss: 0.738\n",
      "[epoch: 39, i:  3044]  train_loss: 0.448  |  valid_loss: 0.656\n",
      "[epoch: 39, i:  3131]  train_loss: 0.459  |  valid_loss: 0.542\n",
      "[epoch: 39, i:  3218]  train_loss: 0.473  |  valid_loss: 0.665\n",
      "[epoch: 39, i:  3305]  train_loss: 0.535  |  valid_loss: 0.604\n",
      "[epoch: 39, i:  3392]  train_loss: 0.411  |  valid_loss: 0.504\n",
      "[epoch: 39, i:  3479]  train_loss: 0.478  |  valid_loss: 0.538\n",
      "[epoch: 39, i:  3566]  train_loss: 0.474  |  valid_loss: 0.506\n",
      "[epoch: 39, i:  3653]  train_loss: 0.535  |  valid_loss: 0.600\n",
      "[epoch: 39, i:  3740]  train_loss: 0.444  |  valid_loss: 0.390\n",
      "[epoch: 39, i:  3827]  train_loss: 0.495  |  valid_loss: 0.792\n",
      "[epoch: 39, i:  3914]  train_loss: 0.460  |  valid_loss: 0.403\n",
      "[epoch: 39, i:  4001]  train_loss: 0.463  |  valid_loss: 0.609\n",
      "[epoch: 39, i:  4088]  train_loss: 0.480  |  valid_loss: 0.542\n",
      "[epoch: 39, i:  4175]  train_loss: 0.493  |  valid_loss: 0.556\n",
      "[epoch: 39, i:  4262]  train_loss: 0.462  |  valid_loss: 0.470\n",
      "[epoch: 39, i:  4349]  train_loss: 0.433  |  valid_loss: 0.577\n",
      "[epoch: 39, i:  4436]  train_loss: 0.422  |  valid_loss: 0.573\n",
      "[epoch: 39, i:  4523]  train_loss: 0.467  |  valid_loss: 0.680\n",
      "[epoch: 39, i:  4610]  train_loss: 0.421  |  valid_loss: 0.490\n",
      "[epoch: 39, i:  4697]  train_loss: 0.443  |  valid_loss: 0.595\n",
      "[epoch: 39, i:  4784]  train_loss: 0.482  |  valid_loss: 0.653\n",
      "[epoch: 39, i:  4871]  train_loss: 0.471  |  valid_loss: 0.688\n",
      "[epoch: 39, i:  4958]  train_loss: 0.475  |  valid_loss: 0.792\n",
      "[epoch: 39, i:  5045]  train_loss: 0.455  |  valid_loss: 0.420\n",
      "[epoch: 39, i:  5132]  train_loss: 0.421  |  valid_loss: 0.599\n",
      "[epoch: 39, i:  5219]  train_loss: 0.376  |  valid_loss: 0.719\n",
      "[epoch: 39, i:  5306]  train_loss: 0.459  |  valid_loss: 0.602\n",
      "[epoch: 39, i:  5393]  train_loss: 0.477  |  valid_loss: 0.502\n",
      "[epoch: 39, i:  5480]  train_loss: 0.462  |  valid_loss: 0.777\n",
      "[epoch: 39, i:  5567]  train_loss: 0.483  |  valid_loss: 0.324\n",
      "[epoch: 39, i:  5654]  train_loss: 0.443  |  valid_loss: 0.539\n",
      "[epoch: 39, i:  5741]  train_loss: 0.468  |  valid_loss: 0.593\n",
      "[epoch: 39, i:  5828]  train_loss: 0.469  |  valid_loss: 0.563\n",
      "[epoch: 39, i:  5915]  train_loss: 0.409  |  valid_loss: 0.713\n",
      "[epoch: 39, i:  6002]  train_loss: 0.487  |  valid_loss: 0.477\n",
      "[epoch: 39, i:  6089]  train_loss: 0.441  |  valid_loss: 0.785\n",
      "[epoch: 39, i:  6176]  train_loss: 0.417  |  valid_loss: 0.610\n",
      "[epoch: 39, i:  6263]  train_loss: 0.436  |  valid_loss: 0.564\n",
      "[epoch: 39, i:  6350]  train_loss: 0.422  |  valid_loss: 0.486\n",
      "[epoch: 39, i:  6437]  train_loss: 0.478  |  valid_loss: 0.418\n",
      "[epoch: 39, i:  6524]  train_loss: 0.444  |  valid_loss: 0.738\n",
      "[epoch: 39, i:  6611]  train_loss: 0.433  |  valid_loss: 0.378\n",
      "[epoch: 39, i:  6698]  train_loss: 0.403  |  valid_loss: 0.671\n",
      "[epoch: 39, i:  6785]  train_loss: 0.479  |  valid_loss: 0.440\n",
      "[epoch: 39, i:  6872]  train_loss: 0.473  |  valid_loss: 0.725\n",
      "[epoch: 39, i:  6959]  train_loss: 0.488  |  valid_loss: 0.631\n",
      "[epoch: 39, i:  7046]  train_loss: 0.442  |  valid_loss: 0.557\n",
      "[epoch: 39, i:  7133]  train_loss: 0.550  |  valid_loss: 0.460\n",
      "[epoch: 39, i:  7220]  train_loss: 0.496  |  valid_loss: 0.732\n",
      "[epoch: 39, i:  7307]  train_loss: 0.446  |  valid_loss: 0.720\n",
      "[epoch: 39, i:  7394]  train_loss: 0.472  |  valid_loss: 0.562\n",
      "[epoch: 39, i:  7481]  train_loss: 0.436  |  valid_loss: 0.582\n",
      "[epoch: 39, i:  7568]  train_loss: 0.496  |  valid_loss: 0.710\n",
      "[epoch: 39, i:  7655]  train_loss: 0.469  |  valid_loss: 0.661\n",
      "[epoch: 39, i:  7742]  train_loss: 0.444  |  valid_loss: 0.599\n",
      "[epoch: 39, i:  7829]  train_loss: 0.473  |  valid_loss: 0.411\n",
      "[epoch: 39, i:  7916]  train_loss: 0.479  |  valid_loss: 0.618\n",
      "[epoch: 39, i:  8003]  train_loss: 0.455  |  valid_loss: 0.416\n",
      "[epoch: 39, i:  8090]  train_loss: 0.418  |  valid_loss: 0.471\n",
      "[epoch: 39, i:  8177]  train_loss: 0.528  |  valid_loss: 0.539\n",
      "[epoch: 39, i:  8264]  train_loss: 0.479  |  valid_loss: 0.619\n",
      "[epoch: 39, i:  8351]  train_loss: 0.512  |  valid_loss: 0.674\n",
      "[epoch: 39, i:  8438]  train_loss: 0.458  |  valid_loss: 0.478\n",
      "[epoch: 39, i:  8525]  train_loss: 0.474  |  valid_loss: 0.570\n",
      "[epoch: 39, i:  8612]  train_loss: 0.458  |  valid_loss: 0.855\n",
      "[epoch: 39, i:  8699]  train_loss: 0.457  |  valid_loss: 0.554\n",
      "--> [End of epoch 39] train_accuracy: 83.99%  |  valid_accuracy: 80.57%\n",
      "--> [Start of epoch 40]  lr: 0.000050\n",
      "[epoch: 40, i:    86]  train_loss: 0.435  |  valid_loss: 0.571\n",
      "[epoch: 40, i:   173]  train_loss: 0.467  |  valid_loss: 0.620\n",
      "[epoch: 40, i:   260]  train_loss: 0.447  |  valid_loss: 0.670\n",
      "[epoch: 40, i:   347]  train_loss: 0.435  |  valid_loss: 0.525\n",
      "[epoch: 40, i:   434]  train_loss: 0.478  |  valid_loss: 0.577\n",
      "[epoch: 40, i:   521]  train_loss: 0.462  |  valid_loss: 0.369\n",
      "[epoch: 40, i:   608]  train_loss: 0.498  |  valid_loss: 0.652\n",
      "[epoch: 40, i:   695]  train_loss: 0.474  |  valid_loss: 0.581\n",
      "[epoch: 40, i:   782]  train_loss: 0.468  |  valid_loss: 0.674\n",
      "[epoch: 40, i:   869]  train_loss: 0.479  |  valid_loss: 0.652\n",
      "[epoch: 40, i:   956]  train_loss: 0.503  |  valid_loss: 0.556\n",
      "[epoch: 40, i:  1043]  train_loss: 0.441  |  valid_loss: 0.546\n",
      "[epoch: 40, i:  1130]  train_loss: 0.534  |  valid_loss: 0.517\n",
      "[epoch: 40, i:  1217]  train_loss: 0.460  |  valid_loss: 0.622\n",
      "[epoch: 40, i:  1304]  train_loss: 0.507  |  valid_loss: 0.406\n",
      "[epoch: 40, i:  1391]  train_loss: 0.441  |  valid_loss: 0.642\n",
      "[epoch: 40, i:  1478]  train_loss: 0.444  |  valid_loss: 0.540\n",
      "[epoch: 40, i:  1565]  train_loss: 0.441  |  valid_loss: 0.754\n",
      "[epoch: 40, i:  1652]  train_loss: 0.528  |  valid_loss: 0.539\n",
      "[epoch: 40, i:  1739]  train_loss: 0.439  |  valid_loss: 0.868\n",
      "[epoch: 40, i:  1826]  train_loss: 0.447  |  valid_loss: 0.725\n",
      "[epoch: 40, i:  1913]  train_loss: 0.503  |  valid_loss: 0.627\n",
      "[epoch: 40, i:  2000]  train_loss: 0.460  |  valid_loss: 0.577\n",
      "[epoch: 40, i:  2087]  train_loss: 0.489  |  valid_loss: 0.767\n",
      "[epoch: 40, i:  2174]  train_loss: 0.492  |  valid_loss: 0.525\n",
      "[epoch: 40, i:  2261]  train_loss: 0.468  |  valid_loss: 0.821\n",
      "[epoch: 40, i:  2348]  train_loss: 0.462  |  valid_loss: 0.434\n",
      "[epoch: 40, i:  2435]  train_loss: 0.435  |  valid_loss: 0.624\n",
      "[epoch: 40, i:  2522]  train_loss: 0.473  |  valid_loss: 0.439\n",
      "[epoch: 40, i:  2609]  train_loss: 0.408  |  valid_loss: 0.529\n",
      "[epoch: 40, i:  2696]  train_loss: 0.398  |  valid_loss: 0.663\n",
      "[epoch: 40, i:  2783]  train_loss: 0.474  |  valid_loss: 0.401\n",
      "[epoch: 40, i:  2870]  train_loss: 0.442  |  valid_loss: 0.719\n",
      "[epoch: 40, i:  2957]  train_loss: 0.494  |  valid_loss: 0.704\n",
      "[epoch: 40, i:  3044]  train_loss: 0.463  |  valid_loss: 0.654\n",
      "[epoch: 40, i:  3131]  train_loss: 0.453  |  valid_loss: 0.549\n",
      "[epoch: 40, i:  3218]  train_loss: 0.404  |  valid_loss: 0.675\n",
      "[epoch: 40, i:  3305]  train_loss: 0.440  |  valid_loss: 0.600\n",
      "[epoch: 40, i:  3392]  train_loss: 0.426  |  valid_loss: 0.520\n",
      "[epoch: 40, i:  3479]  train_loss: 0.410  |  valid_loss: 0.537\n",
      "[epoch: 40, i:  3566]  train_loss: 0.461  |  valid_loss: 0.500\n",
      "[epoch: 40, i:  3653]  train_loss: 0.428  |  valid_loss: 0.594\n",
      "[epoch: 40, i:  3740]  train_loss: 0.477  |  valid_loss: 0.409\n",
      "[epoch: 40, i:  3827]  train_loss: 0.440  |  valid_loss: 0.776\n",
      "[epoch: 40, i:  3914]  train_loss: 0.477  |  valid_loss: 0.419\n",
      "[epoch: 40, i:  4001]  train_loss: 0.515  |  valid_loss: 0.608\n",
      "[epoch: 40, i:  4088]  train_loss: 0.486  |  valid_loss: 0.564\n",
      "[epoch: 40, i:  4175]  train_loss: 0.494  |  valid_loss: 0.577\n",
      "[epoch: 40, i:  4262]  train_loss: 0.433  |  valid_loss: 0.440\n",
      "[epoch: 40, i:  4349]  train_loss: 0.448  |  valid_loss: 0.551\n",
      "[epoch: 40, i:  4436]  train_loss: 0.423  |  valid_loss: 0.549\n",
      "[epoch: 40, i:  4523]  train_loss: 0.424  |  valid_loss: 0.677\n",
      "[epoch: 40, i:  4610]  train_loss: 0.476  |  valid_loss: 0.470\n",
      "[epoch: 40, i:  4697]  train_loss: 0.449  |  valid_loss: 0.585\n",
      "[epoch: 40, i:  4784]  train_loss: 0.451  |  valid_loss: 0.646\n",
      "[epoch: 40, i:  4871]  train_loss: 0.441  |  valid_loss: 0.640\n",
      "[epoch: 40, i:  4958]  train_loss: 0.482  |  valid_loss: 0.743\n",
      "[epoch: 40, i:  5045]  train_loss: 0.437  |  valid_loss: 0.388\n",
      "[epoch: 40, i:  5132]  train_loss: 0.497  |  valid_loss: 0.571\n",
      "[epoch: 40, i:  5219]  train_loss: 0.539  |  valid_loss: 0.715\n",
      "[epoch: 40, i:  5306]  train_loss: 0.469  |  valid_loss: 0.668\n",
      "[epoch: 40, i:  5393]  train_loss: 0.484  |  valid_loss: 0.526\n",
      "[epoch: 40, i:  5480]  train_loss: 0.413  |  valid_loss: 0.756\n",
      "[epoch: 40, i:  5567]  train_loss: 0.476  |  valid_loss: 0.342\n",
      "[epoch: 40, i:  5654]  train_loss: 0.505  |  valid_loss: 0.547\n",
      "[epoch: 40, i:  5741]  train_loss: 0.471  |  valid_loss: 0.615\n",
      "[epoch: 40, i:  5828]  train_loss: 0.456  |  valid_loss: 0.589\n",
      "[epoch: 40, i:  5915]  train_loss: 0.533  |  valid_loss: 0.703\n",
      "[epoch: 40, i:  6002]  train_loss: 0.455  |  valid_loss: 0.457\n",
      "[epoch: 40, i:  6089]  train_loss: 0.449  |  valid_loss: 0.812\n",
      "[epoch: 40, i:  6176]  train_loss: 0.452  |  valid_loss: 0.572\n",
      "[epoch: 40, i:  6263]  train_loss: 0.491  |  valid_loss: 0.513\n",
      "[epoch: 40, i:  6350]  train_loss: 0.406  |  valid_loss: 0.508\n",
      "[epoch: 40, i:  6437]  train_loss: 0.465  |  valid_loss: 0.429\n",
      "[epoch: 40, i:  6524]  train_loss: 0.409  |  valid_loss: 0.741\n",
      "[epoch: 40, i:  6611]  train_loss: 0.469  |  valid_loss: 0.391\n",
      "[epoch: 40, i:  6698]  train_loss: 0.524  |  valid_loss: 0.638\n",
      "[epoch: 40, i:  6785]  train_loss: 0.434  |  valid_loss: 0.448\n",
      "[epoch: 40, i:  6872]  train_loss: 0.438  |  valid_loss: 0.671\n",
      "[epoch: 40, i:  6959]  train_loss: 0.422  |  valid_loss: 0.622\n",
      "[epoch: 40, i:  7046]  train_loss: 0.381  |  valid_loss: 0.583\n",
      "[epoch: 40, i:  7133]  train_loss: 0.483  |  valid_loss: 0.470\n",
      "[epoch: 40, i:  7220]  train_loss: 0.469  |  valid_loss: 0.741\n",
      "[epoch: 40, i:  7307]  train_loss: 0.471  |  valid_loss: 0.669\n",
      "[epoch: 40, i:  7394]  train_loss: 0.488  |  valid_loss: 0.575\n",
      "[epoch: 40, i:  7481]  train_loss: 0.469  |  valid_loss: 0.590\n",
      "[epoch: 40, i:  7568]  train_loss: 0.451  |  valid_loss: 0.728\n",
      "[epoch: 40, i:  7655]  train_loss: 0.490  |  valid_loss: 0.615\n",
      "[epoch: 40, i:  7742]  train_loss: 0.436  |  valid_loss: 0.614\n",
      "[epoch: 40, i:  7829]  train_loss: 0.460  |  valid_loss: 0.398\n",
      "[epoch: 40, i:  7916]  train_loss: 0.468  |  valid_loss: 0.576\n",
      "[epoch: 40, i:  8003]  train_loss: 0.405  |  valid_loss: 0.412\n",
      "[epoch: 40, i:  8090]  train_loss: 0.455  |  valid_loss: 0.452\n",
      "[epoch: 40, i:  8177]  train_loss: 0.475  |  valid_loss: 0.541\n",
      "[epoch: 40, i:  8264]  train_loss: 0.485  |  valid_loss: 0.602\n",
      "[epoch: 40, i:  8351]  train_loss: 0.466  |  valid_loss: 0.668\n",
      "[epoch: 40, i:  8438]  train_loss: 0.414  |  valid_loss: 0.455\n",
      "[epoch: 40, i:  8525]  train_loss: 0.454  |  valid_loss: 0.546\n",
      "[epoch: 40, i:  8612]  train_loss: 0.454  |  valid_loss: 0.848\n",
      "[epoch: 40, i:  8699]  train_loss: 0.467  |  valid_loss: 0.577\n",
      "--> [End of epoch 40] train_accuracy: 84.02%  |  valid_accuracy: 80.56%\n",
      "--> [Start of epoch 41]  lr: 0.000050\n",
      "[epoch: 41, i:    86]  train_loss: 0.438  |  valid_loss: 0.571\n",
      "[epoch: 41, i:   173]  train_loss: 0.452  |  valid_loss: 0.627\n",
      "[epoch: 41, i:   260]  train_loss: 0.431  |  valid_loss: 0.681\n",
      "[epoch: 41, i:   347]  train_loss: 0.400  |  valid_loss: 0.521\n",
      "[epoch: 41, i:   434]  train_loss: 0.452  |  valid_loss: 0.573\n",
      "[epoch: 41, i:   521]  train_loss: 0.413  |  valid_loss: 0.350\n",
      "[epoch: 41, i:   608]  train_loss: 0.413  |  valid_loss: 0.593\n",
      "[epoch: 41, i:   695]  train_loss: 0.450  |  valid_loss: 0.611\n",
      "[epoch: 41, i:   782]  train_loss: 0.420  |  valid_loss: 0.709\n",
      "[epoch: 41, i:   869]  train_loss: 0.439  |  valid_loss: 0.643\n",
      "[epoch: 41, i:   956]  train_loss: 0.462  |  valid_loss: 0.563\n",
      "[epoch: 41, i:  1043]  train_loss: 0.461  |  valid_loss: 0.540\n",
      "[epoch: 41, i:  1130]  train_loss: 0.461  |  valid_loss: 0.502\n",
      "[epoch: 41, i:  1217]  train_loss: 0.426  |  valid_loss: 0.618\n",
      "[epoch: 41, i:  1304]  train_loss: 0.464  |  valid_loss: 0.414\n",
      "[epoch: 41, i:  1391]  train_loss: 0.444  |  valid_loss: 0.651\n",
      "[epoch: 41, i:  1478]  train_loss: 0.445  |  valid_loss: 0.520\n",
      "[epoch: 41, i:  1565]  train_loss: 0.494  |  valid_loss: 0.769\n",
      "[epoch: 41, i:  1652]  train_loss: 0.476  |  valid_loss: 0.581\n",
      "[epoch: 41, i:  1739]  train_loss: 0.449  |  valid_loss: 0.902\n",
      "[epoch: 41, i:  1826]  train_loss: 0.455  |  valid_loss: 0.705\n",
      "[epoch: 41, i:  1913]  train_loss: 0.491  |  valid_loss: 0.582\n",
      "[epoch: 41, i:  2000]  train_loss: 0.425  |  valid_loss: 0.591\n",
      "[epoch: 41, i:  2087]  train_loss: 0.451  |  valid_loss: 0.782\n",
      "[epoch: 41, i:  2174]  train_loss: 0.469  |  valid_loss: 0.502\n",
      "[epoch: 41, i:  2261]  train_loss: 0.499  |  valid_loss: 0.857\n",
      "[epoch: 41, i:  2348]  train_loss: 0.453  |  valid_loss: 0.456\n",
      "[epoch: 41, i:  2435]  train_loss: 0.463  |  valid_loss: 0.656\n",
      "[epoch: 41, i:  2522]  train_loss: 0.491  |  valid_loss: 0.442\n",
      "[epoch: 41, i:  2609]  train_loss: 0.417  |  valid_loss: 0.536\n",
      "[epoch: 41, i:  2696]  train_loss: 0.487  |  valid_loss: 0.679\n",
      "[epoch: 41, i:  2783]  train_loss: 0.485  |  valid_loss: 0.423\n",
      "[epoch: 41, i:  2870]  train_loss: 0.468  |  valid_loss: 0.717\n",
      "[epoch: 41, i:  2957]  train_loss: 0.434  |  valid_loss: 0.656\n",
      "[epoch: 41, i:  3044]  train_loss: 0.473  |  valid_loss: 0.667\n",
      "[epoch: 41, i:  3131]  train_loss: 0.446  |  valid_loss: 0.546\n",
      "[epoch: 41, i:  3218]  train_loss: 0.500  |  valid_loss: 0.681\n",
      "[epoch: 41, i:  3305]  train_loss: 0.480  |  valid_loss: 0.569\n",
      "[epoch: 41, i:  3392]  train_loss: 0.463  |  valid_loss: 0.528\n",
      "[epoch: 41, i:  3479]  train_loss: 0.468  |  valid_loss: 0.538\n",
      "[epoch: 41, i:  3566]  train_loss: 0.469  |  valid_loss: 0.526\n",
      "[epoch: 41, i:  3653]  train_loss: 0.451  |  valid_loss: 0.582\n",
      "[epoch: 41, i:  3740]  train_loss: 0.468  |  valid_loss: 0.373\n",
      "[epoch: 41, i:  3827]  train_loss: 0.412  |  valid_loss: 0.807\n",
      "[epoch: 41, i:  3914]  train_loss: 0.442  |  valid_loss: 0.427\n",
      "[epoch: 41, i:  4001]  train_loss: 0.457  |  valid_loss: 0.622\n",
      "[epoch: 41, i:  4088]  train_loss: 0.463  |  valid_loss: 0.544\n",
      "[epoch: 41, i:  4175]  train_loss: 0.462  |  valid_loss: 0.542\n",
      "[epoch: 41, i:  4262]  train_loss: 0.438  |  valid_loss: 0.404\n",
      "[epoch: 41, i:  4349]  train_loss: 0.423  |  valid_loss: 0.598\n",
      "[epoch: 41, i:  4436]  train_loss: 0.491  |  valid_loss: 0.554\n",
      "[epoch: 41, i:  4523]  train_loss: 0.433  |  valid_loss: 0.695\n",
      "[epoch: 41, i:  4610]  train_loss: 0.470  |  valid_loss: 0.475\n",
      "[epoch: 41, i:  4697]  train_loss: 0.448  |  valid_loss: 0.583\n",
      "[epoch: 41, i:  4784]  train_loss: 0.500  |  valid_loss: 0.627\n",
      "[epoch: 41, i:  4871]  train_loss: 0.452  |  valid_loss: 0.709\n",
      "[epoch: 41, i:  4958]  train_loss: 0.438  |  valid_loss: 0.744\n",
      "[epoch: 41, i:  5045]  train_loss: 0.433  |  valid_loss: 0.408\n",
      "[epoch: 41, i:  5132]  train_loss: 0.490  |  valid_loss: 0.625\n",
      "[epoch: 41, i:  5219]  train_loss: 0.452  |  valid_loss: 0.735\n",
      "[epoch: 41, i:  5306]  train_loss: 0.464  |  valid_loss: 0.613\n",
      "[epoch: 41, i:  5393]  train_loss: 0.474  |  valid_loss: 0.483\n",
      "[epoch: 41, i:  5480]  train_loss: 0.437  |  valid_loss: 0.789\n",
      "[epoch: 41, i:  5567]  train_loss: 0.448  |  valid_loss: 0.328\n",
      "[epoch: 41, i:  5654]  train_loss: 0.494  |  valid_loss: 0.538\n",
      "[epoch: 41, i:  5741]  train_loss: 0.449  |  valid_loss: 0.616\n",
      "[epoch: 41, i:  5828]  train_loss: 0.420  |  valid_loss: 0.581\n",
      "[epoch: 41, i:  5915]  train_loss: 0.468  |  valid_loss: 0.716\n",
      "[epoch: 41, i:  6002]  train_loss: 0.461  |  valid_loss: 0.451\n",
      "[epoch: 41, i:  6089]  train_loss: 0.451  |  valid_loss: 0.773\n",
      "[epoch: 41, i:  6176]  train_loss: 0.461  |  valid_loss: 0.599\n",
      "[epoch: 41, i:  6263]  train_loss: 0.446  |  valid_loss: 0.553\n",
      "[epoch: 41, i:  6350]  train_loss: 0.444  |  valid_loss: 0.486\n",
      "[epoch: 41, i:  6437]  train_loss: 0.476  |  valid_loss: 0.419\n",
      "[epoch: 41, i:  6524]  train_loss: 0.415  |  valid_loss: 0.750\n",
      "[epoch: 41, i:  6611]  train_loss: 0.503  |  valid_loss: 0.387\n",
      "[epoch: 41, i:  6698]  train_loss: 0.467  |  valid_loss: 0.669\n",
      "[epoch: 41, i:  6785]  train_loss: 0.484  |  valid_loss: 0.438\n",
      "[epoch: 41, i:  6872]  train_loss: 0.441  |  valid_loss: 0.736\n",
      "[epoch: 41, i:  6959]  train_loss: 0.482  |  valid_loss: 0.663\n",
      "[epoch: 41, i:  7046]  train_loss: 0.432  |  valid_loss: 0.565\n",
      "[epoch: 41, i:  7133]  train_loss: 0.437  |  valid_loss: 0.453\n",
      "[epoch: 41, i:  7220]  train_loss: 0.428  |  valid_loss: 0.719\n",
      "[epoch: 41, i:  7307]  train_loss: 0.460  |  valid_loss: 0.742\n",
      "[epoch: 41, i:  7394]  train_loss: 0.467  |  valid_loss: 0.580\n",
      "[epoch: 41, i:  7481]  train_loss: 0.471  |  valid_loss: 0.566\n",
      "[epoch: 41, i:  7568]  train_loss: 0.469  |  valid_loss: 0.694\n",
      "[epoch: 41, i:  7655]  train_loss: 0.432  |  valid_loss: 0.645\n",
      "[epoch: 41, i:  7742]  train_loss: 0.424  |  valid_loss: 0.589\n",
      "[epoch: 41, i:  7829]  train_loss: 0.390  |  valid_loss: 0.417\n",
      "[epoch: 41, i:  7916]  train_loss: 0.510  |  valid_loss: 0.590\n",
      "[epoch: 41, i:  8003]  train_loss: 0.434  |  valid_loss: 0.404\n",
      "[epoch: 41, i:  8090]  train_loss: 0.439  |  valid_loss: 0.469\n",
      "[epoch: 41, i:  8177]  train_loss: 0.454  |  valid_loss: 0.534\n",
      "[epoch: 41, i:  8264]  train_loss: 0.458  |  valid_loss: 0.613\n",
      "[epoch: 41, i:  8351]  train_loss: 0.480  |  valid_loss: 0.645\n",
      "[epoch: 41, i:  8438]  train_loss: 0.420  |  valid_loss: 0.449\n",
      "[epoch: 41, i:  8525]  train_loss: 0.430  |  valid_loss: 0.539\n",
      "[epoch: 41, i:  8612]  train_loss: 0.450  |  valid_loss: 0.853\n",
      "[epoch: 41, i:  8699]  train_loss: 0.525  |  valid_loss: 0.569\n",
      "--> [End of epoch 41] train_accuracy: 84.24%  |  valid_accuracy: 80.62%\n",
      "--> [Start of epoch 42]  lr: 0.000050\n",
      "[epoch: 42, i:    86]  train_loss: 0.391  |  valid_loss: 0.597\n",
      "[epoch: 42, i:   173]  train_loss: 0.442  |  valid_loss: 0.649\n",
      "[epoch: 42, i:   260]  train_loss: 0.447  |  valid_loss: 0.672\n",
      "[epoch: 42, i:   347]  train_loss: 0.536  |  valid_loss: 0.530\n",
      "[epoch: 42, i:   434]  train_loss: 0.478  |  valid_loss: 0.580\n",
      "[epoch: 42, i:   521]  train_loss: 0.537  |  valid_loss: 0.356\n",
      "[epoch: 42, i:   608]  train_loss: 0.487  |  valid_loss: 0.624\n",
      "[epoch: 42, i:   695]  train_loss: 0.495  |  valid_loss: 0.624\n",
      "[epoch: 42, i:   782]  train_loss: 0.452  |  valid_loss: 0.664\n",
      "[epoch: 42, i:   869]  train_loss: 0.405  |  valid_loss: 0.682\n",
      "[epoch: 42, i:   956]  train_loss: 0.421  |  valid_loss: 0.525\n",
      "[epoch: 42, i:  1043]  train_loss: 0.482  |  valid_loss: 0.589\n",
      "[epoch: 42, i:  1130]  train_loss: 0.433  |  valid_loss: 0.495\n",
      "[epoch: 42, i:  1217]  train_loss: 0.483  |  valid_loss: 0.682\n",
      "[epoch: 42, i:  1304]  train_loss: 0.463  |  valid_loss: 0.404\n",
      "[epoch: 42, i:  1391]  train_loss: 0.424  |  valid_loss: 0.651\n",
      "[epoch: 42, i:  1478]  train_loss: 0.438  |  valid_loss: 0.527\n",
      "[epoch: 42, i:  1565]  train_loss: 0.432  |  valid_loss: 0.764\n",
      "[epoch: 42, i:  1652]  train_loss: 0.446  |  valid_loss: 0.598\n",
      "[epoch: 42, i:  1739]  train_loss: 0.423  |  valid_loss: 0.879\n",
      "[epoch: 42, i:  1826]  train_loss: 0.428  |  valid_loss: 0.706\n",
      "[epoch: 42, i:  1913]  train_loss: 0.384  |  valid_loss: 0.618\n",
      "[epoch: 42, i:  2000]  train_loss: 0.471  |  valid_loss: 0.593\n",
      "[epoch: 42, i:  2087]  train_loss: 0.413  |  valid_loss: 0.782\n",
      "[epoch: 42, i:  2174]  train_loss: 0.433  |  valid_loss: 0.519\n",
      "[epoch: 42, i:  2261]  train_loss: 0.441  |  valid_loss: 0.808\n",
      "[epoch: 42, i:  2348]  train_loss: 0.509  |  valid_loss: 0.484\n",
      "[epoch: 42, i:  2435]  train_loss: 0.407  |  valid_loss: 0.664\n",
      "[epoch: 42, i:  2522]  train_loss: 0.437  |  valid_loss: 0.452\n",
      "[epoch: 42, i:  2609]  train_loss: 0.527  |  valid_loss: 0.535\n",
      "[epoch: 42, i:  2696]  train_loss: 0.467  |  valid_loss: 0.653\n",
      "[epoch: 42, i:  2783]  train_loss: 0.451  |  valid_loss: 0.385\n",
      "[epoch: 42, i:  2870]  train_loss: 0.481  |  valid_loss: 0.704\n",
      "[epoch: 42, i:  2957]  train_loss: 0.471  |  valid_loss: 0.715\n",
      "[epoch: 42, i:  3044]  train_loss: 0.459  |  valid_loss: 0.662\n",
      "[epoch: 42, i:  3131]  train_loss: 0.453  |  valid_loss: 0.559\n",
      "[epoch: 42, i:  3218]  train_loss: 0.484  |  valid_loss: 0.668\n",
      "[epoch: 42, i:  3305]  train_loss: 0.467  |  valid_loss: 0.550\n",
      "[epoch: 42, i:  3392]  train_loss: 0.521  |  valid_loss: 0.524\n",
      "[epoch: 42, i:  3479]  train_loss: 0.398  |  valid_loss: 0.572\n",
      "[epoch: 42, i:  3566]  train_loss: 0.447  |  valid_loss: 0.523\n",
      "[epoch: 42, i:  3653]  train_loss: 0.446  |  valid_loss: 0.579\n",
      "[epoch: 42, i:  3740]  train_loss: 0.439  |  valid_loss: 0.426\n",
      "[epoch: 42, i:  3827]  train_loss: 0.410  |  valid_loss: 0.789\n",
      "[epoch: 42, i:  3914]  train_loss: 0.493  |  valid_loss: 0.414\n",
      "[epoch: 42, i:  4001]  train_loss: 0.358  |  valid_loss: 0.629\n",
      "[epoch: 42, i:  4088]  train_loss: 0.476  |  valid_loss: 0.515\n",
      "[epoch: 42, i:  4175]  train_loss: 0.439  |  valid_loss: 0.566\n",
      "[epoch: 42, i:  4262]  train_loss: 0.457  |  valid_loss: 0.413\n",
      "[epoch: 42, i:  4349]  train_loss: 0.494  |  valid_loss: 0.568\n",
      "[epoch: 42, i:  4436]  train_loss: 0.448  |  valid_loss: 0.536\n",
      "[epoch: 42, i:  4523]  train_loss: 0.477  |  valid_loss: 0.715\n",
      "[epoch: 42, i:  4610]  train_loss: 0.438  |  valid_loss: 0.482\n",
      "[epoch: 42, i:  4697]  train_loss: 0.438  |  valid_loss: 0.574\n",
      "[epoch: 42, i:  4784]  train_loss: 0.405  |  valid_loss: 0.661\n",
      "[epoch: 42, i:  4871]  train_loss: 0.484  |  valid_loss: 0.704\n",
      "[epoch: 42, i:  4958]  train_loss: 0.435  |  valid_loss: 0.778\n",
      "[epoch: 42, i:  5045]  train_loss: 0.428  |  valid_loss: 0.412\n",
      "[epoch: 42, i:  5132]  train_loss: 0.411  |  valid_loss: 0.584\n",
      "[epoch: 42, i:  5219]  train_loss: 0.428  |  valid_loss: 0.717\n",
      "[epoch: 42, i:  5306]  train_loss: 0.476  |  valid_loss: 0.643\n",
      "[epoch: 42, i:  5393]  train_loss: 0.447  |  valid_loss: 0.536\n",
      "[epoch: 42, i:  5480]  train_loss: 0.438  |  valid_loss: 0.745\n",
      "[epoch: 42, i:  5567]  train_loss: 0.508  |  valid_loss: 0.308\n",
      "[epoch: 42, i:  5654]  train_loss: 0.432  |  valid_loss: 0.544\n",
      "[epoch: 42, i:  5741]  train_loss: 0.482  |  valid_loss: 0.628\n",
      "[epoch: 42, i:  5828]  train_loss: 0.434  |  valid_loss: 0.569\n",
      "[epoch: 42, i:  5915]  train_loss: 0.479  |  valid_loss: 0.664\n",
      "[epoch: 42, i:  6002]  train_loss: 0.526  |  valid_loss: 0.457\n",
      "[epoch: 42, i:  6089]  train_loss: 0.495  |  valid_loss: 0.812\n",
      "[epoch: 42, i:  6176]  train_loss: 0.467  |  valid_loss: 0.539\n",
      "[epoch: 42, i:  6263]  train_loss: 0.480  |  valid_loss: 0.604\n",
      "[epoch: 42, i:  6350]  train_loss: 0.438  |  valid_loss: 0.528\n",
      "[epoch: 42, i:  6437]  train_loss: 0.446  |  valid_loss: 0.421\n",
      "[epoch: 42, i:  6524]  train_loss: 0.414  |  valid_loss: 0.758\n",
      "[epoch: 42, i:  6611]  train_loss: 0.412  |  valid_loss: 0.375\n",
      "[epoch: 42, i:  6698]  train_loss: 0.518  |  valid_loss: 0.672\n",
      "[epoch: 42, i:  6785]  train_loss: 0.434  |  valid_loss: 0.439\n",
      "[epoch: 42, i:  6872]  train_loss: 0.441  |  valid_loss: 0.764\n",
      "[epoch: 42, i:  6959]  train_loss: 0.451  |  valid_loss: 0.644\n",
      "[epoch: 42, i:  7046]  train_loss: 0.434  |  valid_loss: 0.587\n",
      "[epoch: 42, i:  7133]  train_loss: 0.429  |  valid_loss: 0.460\n",
      "[epoch: 42, i:  7220]  train_loss: 0.455  |  valid_loss: 0.681\n",
      "[epoch: 42, i:  7307]  train_loss: 0.405  |  valid_loss: 0.708\n",
      "[epoch: 42, i:  7394]  train_loss: 0.443  |  valid_loss: 0.548\n",
      "[epoch: 42, i:  7481]  train_loss: 0.355  |  valid_loss: 0.621\n",
      "[epoch: 42, i:  7568]  train_loss: 0.453  |  valid_loss: 0.719\n",
      "[epoch: 42, i:  7655]  train_loss: 0.415  |  valid_loss: 0.609\n",
      "[epoch: 42, i:  7742]  train_loss: 0.402  |  valid_loss: 0.555\n",
      "[epoch: 42, i:  7829]  train_loss: 0.452  |  valid_loss: 0.407\n",
      "[epoch: 42, i:  7916]  train_loss: 0.447  |  valid_loss: 0.645\n",
      "[epoch: 42, i:  8003]  train_loss: 0.449  |  valid_loss: 0.426\n",
      "[epoch: 42, i:  8090]  train_loss: 0.426  |  valid_loss: 0.491\n",
      "[epoch: 42, i:  8177]  train_loss: 0.539  |  valid_loss: 0.561\n",
      "[epoch: 42, i:  8264]  train_loss: 0.448  |  valid_loss: 0.625\n",
      "[epoch: 42, i:  8351]  train_loss: 0.516  |  valid_loss: 0.681\n",
      "[epoch: 42, i:  8438]  train_loss: 0.512  |  valid_loss: 0.472\n",
      "[epoch: 42, i:  8525]  train_loss: 0.417  |  valid_loss: 0.532\n",
      "[epoch: 42, i:  8612]  train_loss: 0.461  |  valid_loss: 0.842\n",
      "[epoch: 42, i:  8699]  train_loss: 0.513  |  valid_loss: 0.616\n",
      "--> [End of epoch 42] train_accuracy: 84.40%  |  valid_accuracy: 80.75%\n",
      "--> [Start of epoch 43]  lr: 0.000050\n",
      "[epoch: 43, i:    86]  train_loss: 0.496  |  valid_loss: 0.571\n",
      "[epoch: 43, i:   173]  train_loss: 0.450  |  valid_loss: 0.656\n",
      "[epoch: 43, i:   260]  train_loss: 0.386  |  valid_loss: 0.707\n",
      "[epoch: 43, i:   347]  train_loss: 0.433  |  valid_loss: 0.521\n",
      "[epoch: 43, i:   434]  train_loss: 0.449  |  valid_loss: 0.586\n",
      "[epoch: 43, i:   521]  train_loss: 0.417  |  valid_loss: 0.357\n",
      "[epoch: 43, i:   608]  train_loss: 0.446  |  valid_loss: 0.608\n",
      "[epoch: 43, i:   695]  train_loss: 0.451  |  valid_loss: 0.579\n",
      "[epoch: 43, i:   782]  train_loss: 0.417  |  valid_loss: 0.683\n",
      "[epoch: 43, i:   869]  train_loss: 0.529  |  valid_loss: 0.709\n",
      "[epoch: 43, i:   956]  train_loss: 0.479  |  valid_loss: 0.496\n",
      "[epoch: 43, i:  1043]  train_loss: 0.473  |  valid_loss: 0.568\n",
      "[epoch: 43, i:  1130]  train_loss: 0.428  |  valid_loss: 0.509\n",
      "[epoch: 43, i:  1217]  train_loss: 0.461  |  valid_loss: 0.612\n",
      "[epoch: 43, i:  1304]  train_loss: 0.407  |  valid_loss: 0.438\n",
      "[epoch: 43, i:  1391]  train_loss: 0.451  |  valid_loss: 0.643\n",
      "[epoch: 43, i:  1478]  train_loss: 0.459  |  valid_loss: 0.535\n",
      "[epoch: 43, i:  1565]  train_loss: 0.460  |  valid_loss: 0.813\n",
      "[epoch: 43, i:  1652]  train_loss: 0.486  |  valid_loss: 0.580\n",
      "[epoch: 43, i:  1739]  train_loss: 0.448  |  valid_loss: 0.893\n",
      "[epoch: 43, i:  1826]  train_loss: 0.448  |  valid_loss: 0.725\n",
      "[epoch: 43, i:  1913]  train_loss: 0.459  |  valid_loss: 0.626\n",
      "[epoch: 43, i:  2000]  train_loss: 0.515  |  valid_loss: 0.633\n",
      "[epoch: 43, i:  2087]  train_loss: 0.402  |  valid_loss: 0.762\n",
      "[epoch: 43, i:  2174]  train_loss: 0.417  |  valid_loss: 0.504\n",
      "[epoch: 43, i:  2261]  train_loss: 0.446  |  valid_loss: 0.814\n",
      "[epoch: 43, i:  2348]  train_loss: 0.477  |  valid_loss: 0.448\n",
      "[epoch: 43, i:  2435]  train_loss: 0.477  |  valid_loss: 0.632\n",
      "[epoch: 43, i:  2522]  train_loss: 0.391  |  valid_loss: 0.423\n",
      "[epoch: 43, i:  2609]  train_loss: 0.546  |  valid_loss: 0.520\n",
      "[epoch: 43, i:  2696]  train_loss: 0.491  |  valid_loss: 0.655\n",
      "[epoch: 43, i:  2783]  train_loss: 0.452  |  valid_loss: 0.415\n",
      "[epoch: 43, i:  2870]  train_loss: 0.472  |  valid_loss: 0.712\n",
      "[epoch: 43, i:  2957]  train_loss: 0.469  |  valid_loss: 0.753\n",
      "[epoch: 43, i:  3044]  train_loss: 0.440  |  valid_loss: 0.656\n",
      "[epoch: 43, i:  3131]  train_loss: 0.432  |  valid_loss: 0.545\n",
      "[epoch: 43, i:  3218]  train_loss: 0.483  |  valid_loss: 0.648\n",
      "[epoch: 43, i:  3305]  train_loss: 0.451  |  valid_loss: 0.560\n",
      "[epoch: 43, i:  3392]  train_loss: 0.455  |  valid_loss: 0.497\n",
      "[epoch: 43, i:  3479]  train_loss: 0.490  |  valid_loss: 0.571\n",
      "[epoch: 43, i:  3566]  train_loss: 0.462  |  valid_loss: 0.557\n",
      "[epoch: 43, i:  3653]  train_loss: 0.465  |  valid_loss: 0.545\n",
      "[epoch: 43, i:  3740]  train_loss: 0.450  |  valid_loss: 0.414\n",
      "[epoch: 43, i:  3827]  train_loss: 0.451  |  valid_loss: 0.768\n",
      "[epoch: 43, i:  3914]  train_loss: 0.428  |  valid_loss: 0.446\n",
      "[epoch: 43, i:  4001]  train_loss: 0.449  |  valid_loss: 0.647\n",
      "[epoch: 43, i:  4088]  train_loss: 0.496  |  valid_loss: 0.498\n",
      "[epoch: 43, i:  4175]  train_loss: 0.479  |  valid_loss: 0.552\n",
      "[epoch: 43, i:  4262]  train_loss: 0.484  |  valid_loss: 0.423\n",
      "[epoch: 43, i:  4349]  train_loss: 0.386  |  valid_loss: 0.571\n",
      "[epoch: 43, i:  4436]  train_loss: 0.454  |  valid_loss: 0.524\n",
      "[epoch: 43, i:  4523]  train_loss: 0.395  |  valid_loss: 0.691\n",
      "[epoch: 43, i:  4610]  train_loss: 0.499  |  valid_loss: 0.535\n",
      "[epoch: 43, i:  4697]  train_loss: 0.395  |  valid_loss: 0.603\n",
      "[epoch: 43, i:  4784]  train_loss: 0.449  |  valid_loss: 0.673\n",
      "[epoch: 43, i:  4871]  train_loss: 0.402  |  valid_loss: 0.691\n",
      "[epoch: 43, i:  4958]  train_loss: 0.478  |  valid_loss: 0.774\n",
      "[epoch: 43, i:  5045]  train_loss: 0.471  |  valid_loss: 0.396\n",
      "[epoch: 43, i:  5132]  train_loss: 0.402  |  valid_loss: 0.568\n",
      "[epoch: 43, i:  5219]  train_loss: 0.487  |  valid_loss: 0.735\n",
      "[epoch: 43, i:  5306]  train_loss: 0.433  |  valid_loss: 0.600\n",
      "[epoch: 43, i:  5393]  train_loss: 0.477  |  valid_loss: 0.497\n",
      "[epoch: 43, i:  5480]  train_loss: 0.458  |  valid_loss: 0.764\n",
      "[epoch: 43, i:  5567]  train_loss: 0.447  |  valid_loss: 0.337\n",
      "[epoch: 43, i:  5654]  train_loss: 0.446  |  valid_loss: 0.545\n",
      "[epoch: 43, i:  5741]  train_loss: 0.422  |  valid_loss: 0.650\n",
      "[epoch: 43, i:  5828]  train_loss: 0.463  |  valid_loss: 0.567\n",
      "[epoch: 43, i:  5915]  train_loss: 0.451  |  valid_loss: 0.705\n",
      "[epoch: 43, i:  6002]  train_loss: 0.411  |  valid_loss: 0.476\n",
      "[epoch: 43, i:  6089]  train_loss: 0.457  |  valid_loss: 0.773\n",
      "[epoch: 43, i:  6176]  train_loss: 0.436  |  valid_loss: 0.565\n",
      "[epoch: 43, i:  6263]  train_loss: 0.543  |  valid_loss: 0.576\n",
      "[epoch: 43, i:  6350]  train_loss: 0.444  |  valid_loss: 0.495\n",
      "[epoch: 43, i:  6437]  train_loss: 0.422  |  valid_loss: 0.418\n",
      "[epoch: 43, i:  6524]  train_loss: 0.464  |  valid_loss: 0.746\n",
      "[epoch: 43, i:  6611]  train_loss: 0.471  |  valid_loss: 0.373\n",
      "[epoch: 43, i:  6698]  train_loss: 0.446  |  valid_loss: 0.677\n",
      "[epoch: 43, i:  6785]  train_loss: 0.398  |  valid_loss: 0.449\n",
      "[epoch: 43, i:  6872]  train_loss: 0.478  |  valid_loss: 0.759\n",
      "[epoch: 43, i:  6959]  train_loss: 0.472  |  valid_loss: 0.647\n",
      "[epoch: 43, i:  7046]  train_loss: 0.496  |  valid_loss: 0.552\n",
      "[epoch: 43, i:  7133]  train_loss: 0.372  |  valid_loss: 0.424\n",
      "[epoch: 43, i:  7220]  train_loss: 0.466  |  valid_loss: 0.705\n",
      "[epoch: 43, i:  7307]  train_loss: 0.432  |  valid_loss: 0.707\n",
      "[epoch: 43, i:  7394]  train_loss: 0.414  |  valid_loss: 0.559\n",
      "[epoch: 43, i:  7481]  train_loss: 0.413  |  valid_loss: 0.574\n",
      "[epoch: 43, i:  7568]  train_loss: 0.412  |  valid_loss: 0.689\n",
      "[epoch: 43, i:  7655]  train_loss: 0.449  |  valid_loss: 0.638\n",
      "[epoch: 43, i:  7742]  train_loss: 0.451  |  valid_loss: 0.588\n",
      "[epoch: 43, i:  7829]  train_loss: 0.484  |  valid_loss: 0.381\n",
      "[epoch: 43, i:  7916]  train_loss: 0.484  |  valid_loss: 0.623\n",
      "[epoch: 43, i:  8003]  train_loss: 0.420  |  valid_loss: 0.428\n",
      "[epoch: 43, i:  8090]  train_loss: 0.449  |  valid_loss: 0.449\n",
      "[epoch: 43, i:  8177]  train_loss: 0.496  |  valid_loss: 0.528\n",
      "[epoch: 43, i:  8264]  train_loss: 0.528  |  valid_loss: 0.574\n",
      "[epoch: 43, i:  8351]  train_loss: 0.440  |  valid_loss: 0.677\n",
      "[epoch: 43, i:  8438]  train_loss: 0.430  |  valid_loss: 0.447\n",
      "[epoch: 43, i:  8525]  train_loss: 0.526  |  valid_loss: 0.569\n",
      "[epoch: 43, i:  8612]  train_loss: 0.376  |  valid_loss: 0.865\n",
      "[epoch: 43, i:  8699]  train_loss: 0.412  |  valid_loss: 0.617\n",
      "--> [End of epoch 43] train_accuracy: 84.27%  |  valid_accuracy: 80.71%\n",
      "--> [Start of epoch 44]  lr: 0.000005\n",
      "[epoch: 44, i:    86]  train_loss: 0.474  |  valid_loss: 0.574\n",
      "[epoch: 44, i:   173]  train_loss: 0.413  |  valid_loss: 0.623\n",
      "[epoch: 44, i:   260]  train_loss: 0.445  |  valid_loss: 0.670\n",
      "[epoch: 44, i:   347]  train_loss: 0.417  |  valid_loss: 0.521\n",
      "[epoch: 44, i:   434]  train_loss: 0.402  |  valid_loss: 0.566\n",
      "[epoch: 44, i:   521]  train_loss: 0.467  |  valid_loss: 0.347\n",
      "[epoch: 44, i:   608]  train_loss: 0.427  |  valid_loss: 0.605\n",
      "[epoch: 44, i:   695]  train_loss: 0.443  |  valid_loss: 0.582\n",
      "[epoch: 44, i:   782]  train_loss: 0.467  |  valid_loss: 0.701\n",
      "[epoch: 44, i:   869]  train_loss: 0.488  |  valid_loss: 0.670\n",
      "[epoch: 44, i:   956]  train_loss: 0.435  |  valid_loss: 0.524\n",
      "[epoch: 44, i:  1043]  train_loss: 0.407  |  valid_loss: 0.567\n",
      "[epoch: 44, i:  1130]  train_loss: 0.444  |  valid_loss: 0.498\n",
      "[epoch: 44, i:  1217]  train_loss: 0.403  |  valid_loss: 0.632\n",
      "[epoch: 44, i:  1304]  train_loss: 0.470  |  valid_loss: 0.419\n",
      "[epoch: 44, i:  1391]  train_loss: 0.417  |  valid_loss: 0.639\n",
      "[epoch: 44, i:  1478]  train_loss: 0.488  |  valid_loss: 0.518\n",
      "[epoch: 44, i:  1565]  train_loss: 0.491  |  valid_loss: 0.790\n",
      "[epoch: 44, i:  1652]  train_loss: 0.443  |  valid_loss: 0.563\n",
      "[epoch: 44, i:  1739]  train_loss: 0.416  |  valid_loss: 0.872\n",
      "[epoch: 44, i:  1826]  train_loss: 0.451  |  valid_loss: 0.672\n",
      "[epoch: 44, i:  1913]  train_loss: 0.448  |  valid_loss: 0.604\n",
      "[epoch: 44, i:  2000]  train_loss: 0.444  |  valid_loss: 0.647\n",
      "[epoch: 44, i:  2087]  train_loss: 0.513  |  valid_loss: 0.765\n",
      "[epoch: 44, i:  2174]  train_loss: 0.447  |  valid_loss: 0.508\n",
      "[epoch: 44, i:  2261]  train_loss: 0.466  |  valid_loss: 0.826\n",
      "[epoch: 44, i:  2348]  train_loss: 0.458  |  valid_loss: 0.489\n",
      "[epoch: 44, i:  2435]  train_loss: 0.428  |  valid_loss: 0.637\n",
      "[epoch: 44, i:  2522]  train_loss: 0.425  |  valid_loss: 0.428\n",
      "[epoch: 44, i:  2609]  train_loss: 0.445  |  valid_loss: 0.539\n",
      "[epoch: 44, i:  2696]  train_loss: 0.426  |  valid_loss: 0.613\n",
      "[epoch: 44, i:  2783]  train_loss: 0.450  |  valid_loss: 0.400\n",
      "[epoch: 44, i:  2870]  train_loss: 0.505  |  valid_loss: 0.712\n",
      "[epoch: 44, i:  2957]  train_loss: 0.386  |  valid_loss: 0.696\n",
      "[epoch: 44, i:  3044]  train_loss: 0.451  |  valid_loss: 0.652\n",
      "[epoch: 44, i:  3131]  train_loss: 0.374  |  valid_loss: 0.584\n",
      "[epoch: 44, i:  3218]  train_loss: 0.438  |  valid_loss: 0.662\n",
      "[epoch: 44, i:  3305]  train_loss: 0.406  |  valid_loss: 0.569\n",
      "[epoch: 44, i:  3392]  train_loss: 0.481  |  valid_loss: 0.515\n",
      "[epoch: 44, i:  3479]  train_loss: 0.435  |  valid_loss: 0.586\n",
      "[epoch: 44, i:  3566]  train_loss: 0.493  |  valid_loss: 0.499\n",
      "[epoch: 44, i:  3653]  train_loss: 0.423  |  valid_loss: 0.582\n",
      "[epoch: 44, i:  3740]  train_loss: 0.456  |  valid_loss: 0.393\n",
      "[epoch: 44, i:  3827]  train_loss: 0.471  |  valid_loss: 0.831\n",
      "[epoch: 44, i:  3914]  train_loss: 0.457  |  valid_loss: 0.430\n",
      "[epoch: 44, i:  4001]  train_loss: 0.414  |  valid_loss: 0.624\n",
      "[epoch: 44, i:  4088]  train_loss: 0.379  |  valid_loss: 0.534\n",
      "[epoch: 44, i:  4175]  train_loss: 0.445  |  valid_loss: 0.562\n",
      "[epoch: 44, i:  4262]  train_loss: 0.439  |  valid_loss: 0.426\n",
      "[epoch: 44, i:  4349]  train_loss: 0.438  |  valid_loss: 0.579\n",
      "[epoch: 44, i:  4436]  train_loss: 0.452  |  valid_loss: 0.528\n",
      "[epoch: 44, i:  4523]  train_loss: 0.440  |  valid_loss: 0.701\n",
      "[epoch: 44, i:  4610]  train_loss: 0.425  |  valid_loss: 0.474\n",
      "[epoch: 44, i:  4697]  train_loss: 0.429  |  valid_loss: 0.650\n",
      "[epoch: 44, i:  4784]  train_loss: 0.452  |  valid_loss: 0.687\n",
      "[epoch: 44, i:  4871]  train_loss: 0.410  |  valid_loss: 0.680\n",
      "[epoch: 44, i:  4958]  train_loss: 0.476  |  valid_loss: 0.739\n",
      "[epoch: 44, i:  5045]  train_loss: 0.542  |  valid_loss: 0.397\n",
      "[epoch: 44, i:  5132]  train_loss: 0.454  |  valid_loss: 0.606\n",
      "[epoch: 44, i:  5219]  train_loss: 0.424  |  valid_loss: 0.727\n",
      "[epoch: 44, i:  5306]  train_loss: 0.470  |  valid_loss: 0.560\n",
      "[epoch: 44, i:  5393]  train_loss: 0.421  |  valid_loss: 0.497\n",
      "[epoch: 44, i:  5480]  train_loss: 0.440  |  valid_loss: 0.784\n",
      "[epoch: 44, i:  5567]  train_loss: 0.424  |  valid_loss: 0.321\n",
      "[epoch: 44, i:  5654]  train_loss: 0.417  |  valid_loss: 0.517\n",
      "[epoch: 44, i:  5741]  train_loss: 0.419  |  valid_loss: 0.656\n",
      "[epoch: 44, i:  5828]  train_loss: 0.427  |  valid_loss: 0.564\n",
      "[epoch: 44, i:  5915]  train_loss: 0.465  |  valid_loss: 0.673\n",
      "[epoch: 44, i:  6002]  train_loss: 0.466  |  valid_loss: 0.491\n",
      "[epoch: 44, i:  6089]  train_loss: 0.445  |  valid_loss: 0.752\n",
      "[epoch: 44, i:  6176]  train_loss: 0.459  |  valid_loss: 0.587\n",
      "[epoch: 44, i:  6263]  train_loss: 0.429  |  valid_loss: 0.551\n",
      "[epoch: 44, i:  6350]  train_loss: 0.455  |  valid_loss: 0.504\n",
      "[epoch: 44, i:  6437]  train_loss: 0.456  |  valid_loss: 0.401\n",
      "[epoch: 44, i:  6524]  train_loss: 0.411  |  valid_loss: 0.735\n",
      "[epoch: 44, i:  6611]  train_loss: 0.433  |  valid_loss: 0.362\n",
      "[epoch: 44, i:  6698]  train_loss: 0.498  |  valid_loss: 0.684\n",
      "[epoch: 44, i:  6785]  train_loss: 0.448  |  valid_loss: 0.424\n",
      "[epoch: 44, i:  6872]  train_loss: 0.432  |  valid_loss: 0.737\n",
      "[epoch: 44, i:  6959]  train_loss: 0.454  |  valid_loss: 0.644\n",
      "[epoch: 44, i:  7046]  train_loss: 0.524  |  valid_loss: 0.556\n",
      "[epoch: 44, i:  7133]  train_loss: 0.504  |  valid_loss: 0.446\n",
      "[epoch: 44, i:  7220]  train_loss: 0.382  |  valid_loss: 0.678\n",
      "[epoch: 44, i:  7307]  train_loss: 0.499  |  valid_loss: 0.698\n",
      "[epoch: 44, i:  7394]  train_loss: 0.441  |  valid_loss: 0.555\n",
      "[epoch: 44, i:  7481]  train_loss: 0.429  |  valid_loss: 0.592\n",
      "[epoch: 44, i:  7568]  train_loss: 0.538  |  valid_loss: 0.693\n",
      "[epoch: 44, i:  7655]  train_loss: 0.416  |  valid_loss: 0.593\n",
      "[epoch: 44, i:  7742]  train_loss: 0.476  |  valid_loss: 0.600\n",
      "[epoch: 44, i:  7829]  train_loss: 0.399  |  valid_loss: 0.407\n",
      "[epoch: 44, i:  7916]  train_loss: 0.440  |  valid_loss: 0.587\n",
      "[epoch: 44, i:  8003]  train_loss: 0.437  |  valid_loss: 0.411\n",
      "[epoch: 44, i:  8090]  train_loss: 0.430  |  valid_loss: 0.480\n",
      "[epoch: 44, i:  8177]  train_loss: 0.396  |  valid_loss: 0.522\n",
      "[epoch: 44, i:  8264]  train_loss: 0.509  |  valid_loss: 0.599\n",
      "[epoch: 44, i:  8351]  train_loss: 0.408  |  valid_loss: 0.670\n",
      "[epoch: 44, i:  8438]  train_loss: 0.438  |  valid_loss: 0.437\n",
      "[epoch: 44, i:  8525]  train_loss: 0.394  |  valid_loss: 0.535\n",
      "[epoch: 44, i:  8612]  train_loss: 0.422  |  valid_loss: 0.880\n",
      "[epoch: 44, i:  8699]  train_loss: 0.447  |  valid_loss: 0.583\n",
      "--> [End of epoch 44] train_accuracy: 84.63%  |  valid_accuracy: 80.73%\n",
      "--> [Start of epoch 45]  lr: 0.000005\n",
      "[epoch: 45, i:    86]  train_loss: 0.442  |  valid_loss: 0.599\n",
      "[epoch: 45, i:   173]  train_loss: 0.445  |  valid_loss: 0.634\n",
      "[epoch: 45, i:   260]  train_loss: 0.459  |  valid_loss: 0.661\n",
      "[epoch: 45, i:   347]  train_loss: 0.434  |  valid_loss: 0.533\n",
      "[epoch: 45, i:   434]  train_loss: 0.421  |  valid_loss: 0.575\n",
      "[epoch: 45, i:   521]  train_loss: 0.435  |  valid_loss: 0.334\n",
      "[epoch: 45, i:   608]  train_loss: 0.407  |  valid_loss: 0.598\n",
      "[epoch: 45, i:   695]  train_loss: 0.446  |  valid_loss: 0.566\n",
      "[epoch: 45, i:   782]  train_loss: 0.454  |  valid_loss: 0.700\n",
      "[epoch: 45, i:   869]  train_loss: 0.409  |  valid_loss: 0.660\n",
      "[epoch: 45, i:   956]  train_loss: 0.500  |  valid_loss: 0.518\n",
      "[epoch: 45, i:  1043]  train_loss: 0.466  |  valid_loss: 0.578\n",
      "[epoch: 45, i:  1130]  train_loss: 0.408  |  valid_loss: 0.475\n",
      "[epoch: 45, i:  1217]  train_loss: 0.414  |  valid_loss: 0.629\n",
      "[epoch: 45, i:  1304]  train_loss: 0.415  |  valid_loss: 0.416\n",
      "[epoch: 45, i:  1391]  train_loss: 0.384  |  valid_loss: 0.656\n",
      "[epoch: 45, i:  1478]  train_loss: 0.428  |  valid_loss: 0.533\n",
      "[epoch: 45, i:  1565]  train_loss: 0.397  |  valid_loss: 0.761\n",
      "[epoch: 45, i:  1652]  train_loss: 0.440  |  valid_loss: 0.556\n",
      "[epoch: 45, i:  1739]  train_loss: 0.478  |  valid_loss: 0.881\n",
      "[epoch: 45, i:  1826]  train_loss: 0.406  |  valid_loss: 0.681\n",
      "[epoch: 45, i:  1913]  train_loss: 0.471  |  valid_loss: 0.618\n",
      "[epoch: 45, i:  2000]  train_loss: 0.377  |  valid_loss: 0.634\n",
      "[epoch: 45, i:  2087]  train_loss: 0.410  |  valid_loss: 0.754\n",
      "[epoch: 45, i:  2174]  train_loss: 0.473  |  valid_loss: 0.520\n",
      "[epoch: 45, i:  2261]  train_loss: 0.431  |  valid_loss: 0.831\n",
      "[epoch: 45, i:  2348]  train_loss: 0.427  |  valid_loss: 0.456\n",
      "[epoch: 45, i:  2435]  train_loss: 0.416  |  valid_loss: 0.621\n",
      "[epoch: 45, i:  2522]  train_loss: 0.421  |  valid_loss: 0.438\n",
      "[epoch: 45, i:  2609]  train_loss: 0.457  |  valid_loss: 0.533\n",
      "[epoch: 45, i:  2696]  train_loss: 0.485  |  valid_loss: 0.618\n",
      "[epoch: 45, i:  2783]  train_loss: 0.439  |  valid_loss: 0.392\n",
      "[epoch: 45, i:  2870]  train_loss: 0.503  |  valid_loss: 0.678\n",
      "[epoch: 45, i:  2957]  train_loss: 0.472  |  valid_loss: 0.691\n",
      "[epoch: 45, i:  3044]  train_loss: 0.436  |  valid_loss: 0.657\n",
      "[epoch: 45, i:  3131]  train_loss: 0.405  |  valid_loss: 0.536\n",
      "[epoch: 45, i:  3218]  train_loss: 0.463  |  valid_loss: 0.667\n",
      "[epoch: 45, i:  3305]  train_loss: 0.462  |  valid_loss: 0.582\n",
      "[epoch: 45, i:  3392]  train_loss: 0.437  |  valid_loss: 0.504\n",
      "[epoch: 45, i:  3479]  train_loss: 0.488  |  valid_loss: 0.588\n",
      "[epoch: 45, i:  3566]  train_loss: 0.420  |  valid_loss: 0.517\n",
      "[epoch: 45, i:  3653]  train_loss: 0.453  |  valid_loss: 0.580\n",
      "[epoch: 45, i:  3740]  train_loss: 0.482  |  valid_loss: 0.392\n",
      "[epoch: 45, i:  3827]  train_loss: 0.437  |  valid_loss: 0.793\n",
      "[epoch: 45, i:  3914]  train_loss: 0.448  |  valid_loss: 0.465\n",
      "[epoch: 45, i:  4001]  train_loss: 0.488  |  valid_loss: 0.650\n",
      "[epoch: 45, i:  4088]  train_loss: 0.456  |  valid_loss: 0.528\n",
      "[epoch: 45, i:  4175]  train_loss: 0.403  |  valid_loss: 0.545\n",
      "[epoch: 45, i:  4262]  train_loss: 0.475  |  valid_loss: 0.427\n",
      "[epoch: 45, i:  4349]  train_loss: 0.509  |  valid_loss: 0.576\n",
      "[epoch: 45, i:  4436]  train_loss: 0.442  |  valid_loss: 0.527\n",
      "[epoch: 45, i:  4523]  train_loss: 0.489  |  valid_loss: 0.706\n",
      "[epoch: 45, i:  4610]  train_loss: 0.523  |  valid_loss: 0.469\n",
      "[epoch: 45, i:  4697]  train_loss: 0.419  |  valid_loss: 0.622\n",
      "[epoch: 45, i:  4784]  train_loss: 0.444  |  valid_loss: 0.652\n",
      "[epoch: 45, i:  4871]  train_loss: 0.397  |  valid_loss: 0.671\n",
      "[epoch: 45, i:  4958]  train_loss: 0.437  |  valid_loss: 0.725\n",
      "[epoch: 45, i:  5045]  train_loss: 0.423  |  valid_loss: 0.400\n",
      "[epoch: 45, i:  5132]  train_loss: 0.435  |  valid_loss: 0.581\n",
      "[epoch: 45, i:  5219]  train_loss: 0.437  |  valid_loss: 0.747\n",
      "[epoch: 45, i:  5306]  train_loss: 0.449  |  valid_loss: 0.601\n",
      "[epoch: 45, i:  5393]  train_loss: 0.428  |  valid_loss: 0.474\n",
      "[epoch: 45, i:  5480]  train_loss: 0.453  |  valid_loss: 0.804\n",
      "[epoch: 45, i:  5567]  train_loss: 0.474  |  valid_loss: 0.320\n",
      "[epoch: 45, i:  5654]  train_loss: 0.446  |  valid_loss: 0.509\n",
      "[epoch: 45, i:  5741]  train_loss: 0.452  |  valid_loss: 0.634\n",
      "[epoch: 45, i:  5828]  train_loss: 0.433  |  valid_loss: 0.559\n",
      "[epoch: 45, i:  5915]  train_loss: 0.494  |  valid_loss: 0.685\n",
      "[epoch: 45, i:  6002]  train_loss: 0.433  |  valid_loss: 0.487\n",
      "[epoch: 45, i:  6089]  train_loss: 0.489  |  valid_loss: 0.781\n",
      "[epoch: 45, i:  6176]  train_loss: 0.419  |  valid_loss: 0.575\n",
      "[epoch: 45, i:  6263]  train_loss: 0.404  |  valid_loss: 0.564\n",
      "[epoch: 45, i:  6350]  train_loss: 0.457  |  valid_loss: 0.508\n",
      "[epoch: 45, i:  6437]  train_loss: 0.470  |  valid_loss: 0.419\n",
      "[epoch: 45, i:  6524]  train_loss: 0.481  |  valid_loss: 0.746\n",
      "[epoch: 45, i:  6611]  train_loss: 0.464  |  valid_loss: 0.365\n",
      "[epoch: 45, i:  6698]  train_loss: 0.406  |  valid_loss: 0.707\n",
      "[epoch: 45, i:  6785]  train_loss: 0.460  |  valid_loss: 0.439\n",
      "[epoch: 45, i:  6872]  train_loss: 0.451  |  valid_loss: 0.716\n",
      "[epoch: 45, i:  6959]  train_loss: 0.455  |  valid_loss: 0.661\n",
      "[epoch: 45, i:  7046]  train_loss: 0.446  |  valid_loss: 0.572\n",
      "[epoch: 45, i:  7133]  train_loss: 0.512  |  valid_loss: 0.458\n",
      "[epoch: 45, i:  7220]  train_loss: 0.433  |  valid_loss: 0.681\n",
      "[epoch: 45, i:  7307]  train_loss: 0.392  |  valid_loss: 0.715\n",
      "[epoch: 45, i:  7394]  train_loss: 0.401  |  valid_loss: 0.536\n",
      "[epoch: 45, i:  7481]  train_loss: 0.444  |  valid_loss: 0.602\n",
      "[epoch: 45, i:  7568]  train_loss: 0.443  |  valid_loss: 0.702\n",
      "[epoch: 45, i:  7655]  train_loss: 0.425  |  valid_loss: 0.590\n",
      "[epoch: 45, i:  7742]  train_loss: 0.439  |  valid_loss: 0.575\n",
      "[epoch: 45, i:  7829]  train_loss: 0.481  |  valid_loss: 0.426\n",
      "[epoch: 45, i:  7916]  train_loss: 0.475  |  valid_loss: 0.620\n",
      "[epoch: 45, i:  8003]  train_loss: 0.429  |  valid_loss: 0.441\n",
      "[epoch: 45, i:  8090]  train_loss: 0.379  |  valid_loss: 0.486\n",
      "[epoch: 45, i:  8177]  train_loss: 0.427  |  valid_loss: 0.559\n",
      "[epoch: 45, i:  8264]  train_loss: 0.495  |  valid_loss: 0.593\n",
      "[epoch: 45, i:  8351]  train_loss: 0.456  |  valid_loss: 0.656\n",
      "[epoch: 45, i:  8438]  train_loss: 0.353  |  valid_loss: 0.416\n",
      "[epoch: 45, i:  8525]  train_loss: 0.513  |  valid_loss: 0.535\n",
      "[epoch: 45, i:  8612]  train_loss: 0.428  |  valid_loss: 0.851\n",
      "[epoch: 45, i:  8699]  train_loss: 0.503  |  valid_loss: 0.586\n",
      "--> [End of epoch 45] train_accuracy: 84.57%  |  valid_accuracy: 80.92%\n",
      "--> [Start of epoch 46]  lr: 0.000005\n",
      "[epoch: 46, i:    86]  train_loss: 0.421  |  valid_loss: 0.570\n",
      "[epoch: 46, i:   173]  train_loss: 0.452  |  valid_loss: 0.605\n",
      "[epoch: 46, i:   260]  train_loss: 0.444  |  valid_loss: 0.649\n",
      "[epoch: 46, i:   347]  train_loss: 0.442  |  valid_loss: 0.506\n",
      "[epoch: 46, i:   434]  train_loss: 0.451  |  valid_loss: 0.577\n",
      "[epoch: 46, i:   521]  train_loss: 0.440  |  valid_loss: 0.355\n",
      "[epoch: 46, i:   608]  train_loss: 0.421  |  valid_loss: 0.595\n",
      "[epoch: 46, i:   695]  train_loss: 0.423  |  valid_loss: 0.569\n",
      "[epoch: 46, i:   782]  train_loss: 0.431  |  valid_loss: 0.689\n",
      "[epoch: 46, i:   869]  train_loss: 0.481  |  valid_loss: 0.688\n",
      "[epoch: 46, i:   956]  train_loss: 0.437  |  valid_loss: 0.522\n",
      "[epoch: 46, i:  1043]  train_loss: 0.436  |  valid_loss: 0.557\n",
      "[epoch: 46, i:  1130]  train_loss: 0.435  |  valid_loss: 0.494\n",
      "[epoch: 46, i:  1217]  train_loss: 0.482  |  valid_loss: 0.594\n",
      "[epoch: 46, i:  1304]  train_loss: 0.401  |  valid_loss: 0.428\n",
      "[epoch: 46, i:  1391]  train_loss: 0.461  |  valid_loss: 0.679\n",
      "[epoch: 46, i:  1478]  train_loss: 0.460  |  valid_loss: 0.532\n",
      "[epoch: 46, i:  1565]  train_loss: 0.405  |  valid_loss: 0.815\n",
      "[epoch: 46, i:  1652]  train_loss: 0.421  |  valid_loss: 0.567\n",
      "[epoch: 46, i:  1739]  train_loss: 0.412  |  valid_loss: 0.878\n",
      "[epoch: 46, i:  1826]  train_loss: 0.453  |  valid_loss: 0.701\n",
      "[epoch: 46, i:  1913]  train_loss: 0.434  |  valid_loss: 0.590\n",
      "[epoch: 46, i:  2000]  train_loss: 0.488  |  valid_loss: 0.637\n",
      "[epoch: 46, i:  2087]  train_loss: 0.471  |  valid_loss: 0.733\n",
      "[epoch: 46, i:  2174]  train_loss: 0.495  |  valid_loss: 0.525\n",
      "[epoch: 46, i:  2261]  train_loss: 0.424  |  valid_loss: 0.817\n",
      "[epoch: 46, i:  2348]  train_loss: 0.444  |  valid_loss: 0.450\n",
      "[epoch: 46, i:  2435]  train_loss: 0.447  |  valid_loss: 0.640\n",
      "[epoch: 46, i:  2522]  train_loss: 0.452  |  valid_loss: 0.422\n",
      "[epoch: 46, i:  2609]  train_loss: 0.397  |  valid_loss: 0.531\n",
      "[epoch: 46, i:  2696]  train_loss: 0.401  |  valid_loss: 0.650\n",
      "[epoch: 46, i:  2783]  train_loss: 0.497  |  valid_loss: 0.402\n",
      "[epoch: 46, i:  2870]  train_loss: 0.444  |  valid_loss: 0.704\n",
      "[epoch: 46, i:  2957]  train_loss: 0.409  |  valid_loss: 0.711\n",
      "[epoch: 46, i:  3044]  train_loss: 0.452  |  valid_loss: 0.667\n",
      "[epoch: 46, i:  3131]  train_loss: 0.453  |  valid_loss: 0.584\n",
      "[epoch: 46, i:  3218]  train_loss: 0.474  |  valid_loss: 0.666\n",
      "[epoch: 46, i:  3305]  train_loss: 0.416  |  valid_loss: 0.571\n",
      "[epoch: 46, i:  3392]  train_loss: 0.439  |  valid_loss: 0.509\n",
      "[epoch: 46, i:  3479]  train_loss: 0.432  |  valid_loss: 0.562\n",
      "[epoch: 46, i:  3566]  train_loss: 0.409  |  valid_loss: 0.532\n",
      "[epoch: 46, i:  3653]  train_loss: 0.445  |  valid_loss: 0.597\n",
      "[epoch: 46, i:  3740]  train_loss: 0.460  |  valid_loss: 0.371\n",
      "[epoch: 46, i:  3827]  train_loss: 0.430  |  valid_loss: 0.794\n",
      "[epoch: 46, i:  3914]  train_loss: 0.448  |  valid_loss: 0.440\n",
      "[epoch: 46, i:  4001]  train_loss: 0.496  |  valid_loss: 0.638\n",
      "[epoch: 46, i:  4088]  train_loss: 0.409  |  valid_loss: 0.489\n",
      "[epoch: 46, i:  4175]  train_loss: 0.454  |  valid_loss: 0.583\n",
      "[epoch: 46, i:  4262]  train_loss: 0.461  |  valid_loss: 0.433\n",
      "[epoch: 46, i:  4349]  train_loss: 0.474  |  valid_loss: 0.582\n",
      "[epoch: 46, i:  4436]  train_loss: 0.424  |  valid_loss: 0.512\n",
      "[epoch: 46, i:  4523]  train_loss: 0.475  |  valid_loss: 0.692\n",
      "[epoch: 46, i:  4610]  train_loss: 0.438  |  valid_loss: 0.499\n",
      "[epoch: 46, i:  4697]  train_loss: 0.401  |  valid_loss: 0.596\n",
      "[epoch: 46, i:  4784]  train_loss: 0.444  |  valid_loss: 0.638\n",
      "[epoch: 46, i:  4871]  train_loss: 0.497  |  valid_loss: 0.692\n",
      "[epoch: 46, i:  4958]  train_loss: 0.424  |  valid_loss: 0.748\n",
      "[epoch: 46, i:  5045]  train_loss: 0.445  |  valid_loss: 0.392\n",
      "[epoch: 46, i:  5132]  train_loss: 0.459  |  valid_loss: 0.557\n",
      "[epoch: 46, i:  5219]  train_loss: 0.481  |  valid_loss: 0.732\n",
      "[epoch: 46, i:  5306]  train_loss: 0.459  |  valid_loss: 0.588\n",
      "[epoch: 46, i:  5393]  train_loss: 0.410  |  valid_loss: 0.470\n",
      "[epoch: 46, i:  5480]  train_loss: 0.465  |  valid_loss: 0.789\n",
      "[epoch: 46, i:  5567]  train_loss: 0.497  |  valid_loss: 0.310\n",
      "[epoch: 46, i:  5654]  train_loss: 0.433  |  valid_loss: 0.531\n",
      "[epoch: 46, i:  5741]  train_loss: 0.468  |  valid_loss: 0.629\n",
      "[epoch: 46, i:  5828]  train_loss: 0.443  |  valid_loss: 0.542\n",
      "[epoch: 46, i:  5915]  train_loss: 0.422  |  valid_loss: 0.683\n",
      "[epoch: 46, i:  6002]  train_loss: 0.466  |  valid_loss: 0.483\n",
      "[epoch: 46, i:  6089]  train_loss: 0.425  |  valid_loss: 0.758\n",
      "[epoch: 46, i:  6176]  train_loss: 0.449  |  valid_loss: 0.592\n",
      "[epoch: 46, i:  6263]  train_loss: 0.471  |  valid_loss: 0.570\n",
      "[epoch: 46, i:  6350]  train_loss: 0.429  |  valid_loss: 0.557\n",
      "[epoch: 46, i:  6437]  train_loss: 0.463  |  valid_loss: 0.393\n",
      "[epoch: 46, i:  6524]  train_loss: 0.486  |  valid_loss: 0.761\n",
      "[epoch: 46, i:  6611]  train_loss: 0.422  |  valid_loss: 0.368\n",
      "[epoch: 46, i:  6698]  train_loss: 0.409  |  valid_loss: 0.668\n",
      "[epoch: 46, i:  6785]  train_loss: 0.428  |  valid_loss: 0.455\n",
      "[epoch: 46, i:  6872]  train_loss: 0.415  |  valid_loss: 0.735\n",
      "[epoch: 46, i:  6959]  train_loss: 0.436  |  valid_loss: 0.649\n",
      "[epoch: 46, i:  7046]  train_loss: 0.384  |  valid_loss: 0.559\n",
      "[epoch: 46, i:  7133]  train_loss: 0.451  |  valid_loss: 0.451\n",
      "[epoch: 46, i:  7220]  train_loss: 0.401  |  valid_loss: 0.687\n",
      "[epoch: 46, i:  7307]  train_loss: 0.443  |  valid_loss: 0.690\n",
      "[epoch: 46, i:  7394]  train_loss: 0.453  |  valid_loss: 0.585\n",
      "[epoch: 46, i:  7481]  train_loss: 0.443  |  valid_loss: 0.596\n",
      "[epoch: 46, i:  7568]  train_loss: 0.386  |  valid_loss: 0.746\n",
      "[epoch: 46, i:  7655]  train_loss: 0.406  |  valid_loss: 0.622\n",
      "[epoch: 46, i:  7742]  train_loss: 0.429  |  valid_loss: 0.562\n",
      "[epoch: 46, i:  7829]  train_loss: 0.455  |  valid_loss: 0.396\n",
      "[epoch: 46, i:  7916]  train_loss: 0.441  |  valid_loss: 0.608\n",
      "[epoch: 46, i:  8003]  train_loss: 0.424  |  valid_loss: 0.408\n",
      "[epoch: 46, i:  8090]  train_loss: 0.418  |  valid_loss: 0.491\n",
      "[epoch: 46, i:  8177]  train_loss: 0.431  |  valid_loss: 0.564\n",
      "[epoch: 46, i:  8264]  train_loss: 0.448  |  valid_loss: 0.612\n",
      "[epoch: 46, i:  8351]  train_loss: 0.462  |  valid_loss: 0.683\n",
      "[epoch: 46, i:  8438]  train_loss: 0.417  |  valid_loss: 0.458\n",
      "[epoch: 46, i:  8525]  train_loss: 0.472  |  valid_loss: 0.573\n",
      "[epoch: 46, i:  8612]  train_loss: 0.408  |  valid_loss: 0.844\n",
      "[epoch: 46, i:  8699]  train_loss: 0.411  |  valid_loss: 0.599\n",
      "--> [End of epoch 46] train_accuracy: 84.70%  |  valid_accuracy: 80.58%\n",
      "--> [Start of epoch 47]  lr: 0.000005\n",
      "[epoch: 47, i:    86]  train_loss: 0.425  |  valid_loss: 0.562\n",
      "[epoch: 47, i:   173]  train_loss: 0.465  |  valid_loss: 0.644\n",
      "[epoch: 47, i:   260]  train_loss: 0.381  |  valid_loss: 0.684\n",
      "[epoch: 47, i:   347]  train_loss: 0.493  |  valid_loss: 0.516\n",
      "[epoch: 47, i:   434]  train_loss: 0.460  |  valid_loss: 0.601\n",
      "[epoch: 47, i:   521]  train_loss: 0.476  |  valid_loss: 0.344\n",
      "[epoch: 47, i:   608]  train_loss: 0.517  |  valid_loss: 0.613\n",
      "[epoch: 47, i:   695]  train_loss: 0.437  |  valid_loss: 0.574\n",
      "[epoch: 47, i:   782]  train_loss: 0.475  |  valid_loss: 0.716\n",
      "[epoch: 47, i:   869]  train_loss: 0.425  |  valid_loss: 0.679\n",
      "[epoch: 47, i:   956]  train_loss: 0.458  |  valid_loss: 0.498\n",
      "[epoch: 47, i:  1043]  train_loss: 0.479  |  valid_loss: 0.538\n",
      "[epoch: 47, i:  1130]  train_loss: 0.440  |  valid_loss: 0.482\n",
      "[epoch: 47, i:  1217]  train_loss: 0.415  |  valid_loss: 0.624\n",
      "[epoch: 47, i:  1304]  train_loss: 0.423  |  valid_loss: 0.422\n",
      "[epoch: 47, i:  1391]  train_loss: 0.393  |  valid_loss: 0.662\n",
      "[epoch: 47, i:  1478]  train_loss: 0.445  |  valid_loss: 0.549\n",
      "[epoch: 47, i:  1565]  train_loss: 0.423  |  valid_loss: 0.783\n",
      "[epoch: 47, i:  1652]  train_loss: 0.479  |  valid_loss: 0.569\n",
      "[epoch: 47, i:  1739]  train_loss: 0.400  |  valid_loss: 0.860\n",
      "[epoch: 47, i:  1826]  train_loss: 0.403  |  valid_loss: 0.695\n",
      "[epoch: 47, i:  1913]  train_loss: 0.335  |  valid_loss: 0.621\n",
      "[epoch: 47, i:  2000]  train_loss: 0.451  |  valid_loss: 0.617\n",
      "[epoch: 47, i:  2087]  train_loss: 0.427  |  valid_loss: 0.772\n",
      "[epoch: 47, i:  2174]  train_loss: 0.456  |  valid_loss: 0.509\n",
      "[epoch: 47, i:  2261]  train_loss: 0.472  |  valid_loss: 0.796\n",
      "[epoch: 47, i:  2348]  train_loss: 0.421  |  valid_loss: 0.465\n",
      "[epoch: 47, i:  2435]  train_loss: 0.451  |  valid_loss: 0.631\n",
      "[epoch: 47, i:  2522]  train_loss: 0.449  |  valid_loss: 0.421\n",
      "[epoch: 47, i:  2609]  train_loss: 0.418  |  valid_loss: 0.555\n",
      "[epoch: 47, i:  2696]  train_loss: 0.460  |  valid_loss: 0.626\n",
      "[epoch: 47, i:  2783]  train_loss: 0.496  |  valid_loss: 0.410\n",
      "[epoch: 47, i:  2870]  train_loss: 0.480  |  valid_loss: 0.708\n",
      "[epoch: 47, i:  2957]  train_loss: 0.489  |  valid_loss: 0.716\n",
      "[epoch: 47, i:  3044]  train_loss: 0.398  |  valid_loss: 0.687\n",
      "[epoch: 47, i:  3131]  train_loss: 0.473  |  valid_loss: 0.541\n",
      "[epoch: 47, i:  3218]  train_loss: 0.430  |  valid_loss: 0.633\n",
      "[epoch: 47, i:  3305]  train_loss: 0.441  |  valid_loss: 0.608\n",
      "[epoch: 47, i:  3392]  train_loss: 0.408  |  valid_loss: 0.510\n",
      "[epoch: 47, i:  3479]  train_loss: 0.452  |  valid_loss: 0.559\n",
      "[epoch: 47, i:  3566]  train_loss: 0.437  |  valid_loss: 0.504\n",
      "[epoch: 47, i:  3653]  train_loss: 0.411  |  valid_loss: 0.581\n",
      "[epoch: 47, i:  3740]  train_loss: 0.407  |  valid_loss: 0.379\n",
      "[epoch: 47, i:  3827]  train_loss: 0.434  |  valid_loss: 0.811\n",
      "[epoch: 47, i:  3914]  train_loss: 0.454  |  valid_loss: 0.413\n",
      "[epoch: 47, i:  4001]  train_loss: 0.382  |  valid_loss: 0.637\n",
      "[epoch: 47, i:  4088]  train_loss: 0.417  |  valid_loss: 0.501\n",
      "[epoch: 47, i:  4175]  train_loss: 0.450  |  valid_loss: 0.576\n",
      "[epoch: 47, i:  4262]  train_loss: 0.396  |  valid_loss: 0.437\n",
      "[epoch: 47, i:  4349]  train_loss: 0.481  |  valid_loss: 0.571\n",
      "[epoch: 47, i:  4436]  train_loss: 0.433  |  valid_loss: 0.546\n",
      "[epoch: 47, i:  4523]  train_loss: 0.497  |  valid_loss: 0.710\n",
      "[epoch: 47, i:  4610]  train_loss: 0.486  |  valid_loss: 0.466\n",
      "[epoch: 47, i:  4697]  train_loss: 0.415  |  valid_loss: 0.588\n",
      "[epoch: 47, i:  4784]  train_loss: 0.432  |  valid_loss: 0.659\n",
      "[epoch: 47, i:  4871]  train_loss: 0.423  |  valid_loss: 0.691\n",
      "[epoch: 47, i:  4958]  train_loss: 0.443  |  valid_loss: 0.739\n",
      "[epoch: 47, i:  5045]  train_loss: 0.511  |  valid_loss: 0.423\n",
      "[epoch: 47, i:  5132]  train_loss: 0.457  |  valid_loss: 0.594\n",
      "[epoch: 47, i:  5219]  train_loss: 0.494  |  valid_loss: 0.743\n",
      "[epoch: 47, i:  5306]  train_loss: 0.459  |  valid_loss: 0.615\n",
      "[epoch: 47, i:  5393]  train_loss: 0.506  |  valid_loss: 0.493\n",
      "[epoch: 47, i:  5480]  train_loss: 0.414  |  valid_loss: 0.824\n",
      "[epoch: 47, i:  5567]  train_loss: 0.419  |  valid_loss: 0.320\n",
      "[epoch: 47, i:  5654]  train_loss: 0.552  |  valid_loss: 0.520\n",
      "[epoch: 47, i:  5741]  train_loss: 0.466  |  valid_loss: 0.633\n",
      "[epoch: 47, i:  5828]  train_loss: 0.444  |  valid_loss: 0.581\n",
      "[epoch: 47, i:  5915]  train_loss: 0.426  |  valid_loss: 0.659\n",
      "[epoch: 47, i:  6002]  train_loss: 0.415  |  valid_loss: 0.481\n",
      "[epoch: 47, i:  6089]  train_loss: 0.441  |  valid_loss: 0.787\n",
      "[epoch: 47, i:  6176]  train_loss: 0.386  |  valid_loss: 0.567\n",
      "[epoch: 47, i:  6263]  train_loss: 0.416  |  valid_loss: 0.565\n",
      "[epoch: 47, i:  6350]  train_loss: 0.479  |  valid_loss: 0.512\n",
      "[epoch: 47, i:  6437]  train_loss: 0.443  |  valid_loss: 0.425\n",
      "[epoch: 47, i:  6524]  train_loss: 0.409  |  valid_loss: 0.756\n",
      "[epoch: 47, i:  6611]  train_loss: 0.439  |  valid_loss: 0.390\n",
      "[epoch: 47, i:  6698]  train_loss: 0.455  |  valid_loss: 0.682\n",
      "[epoch: 47, i:  6785]  train_loss: 0.432  |  valid_loss: 0.434\n",
      "[epoch: 47, i:  6872]  train_loss: 0.481  |  valid_loss: 0.761\n",
      "[epoch: 47, i:  6959]  train_loss: 0.404  |  valid_loss: 0.626\n",
      "[epoch: 47, i:  7046]  train_loss: 0.393  |  valid_loss: 0.548\n",
      "[epoch: 47, i:  7133]  train_loss: 0.392  |  valid_loss: 0.455\n",
      "[epoch: 47, i:  7220]  train_loss: 0.414  |  valid_loss: 0.674\n",
      "[epoch: 47, i:  7307]  train_loss: 0.438  |  valid_loss: 0.668\n",
      "[epoch: 47, i:  7394]  train_loss: 0.464  |  valid_loss: 0.585\n",
      "[epoch: 47, i:  7481]  train_loss: 0.428  |  valid_loss: 0.580\n",
      "[epoch: 47, i:  7568]  train_loss: 0.460  |  valid_loss: 0.725\n",
      "[epoch: 47, i:  7655]  train_loss: 0.450  |  valid_loss: 0.623\n",
      "[epoch: 47, i:  7742]  train_loss: 0.464  |  valid_loss: 0.586\n",
      "[epoch: 47, i:  7829]  train_loss: 0.412  |  valid_loss: 0.404\n",
      "[epoch: 47, i:  7916]  train_loss: 0.445  |  valid_loss: 0.606\n",
      "[epoch: 47, i:  8003]  train_loss: 0.427  |  valid_loss: 0.416\n",
      "[epoch: 47, i:  8090]  train_loss: 0.418  |  valid_loss: 0.476\n",
      "[epoch: 47, i:  8177]  train_loss: 0.477  |  valid_loss: 0.567\n",
      "[epoch: 47, i:  8264]  train_loss: 0.383  |  valid_loss: 0.585\n",
      "[epoch: 47, i:  8351]  train_loss: 0.433  |  valid_loss: 0.703\n",
      "[epoch: 47, i:  8438]  train_loss: 0.435  |  valid_loss: 0.442\n",
      "[epoch: 47, i:  8525]  train_loss: 0.500  |  valid_loss: 0.535\n",
      "[epoch: 47, i:  8612]  train_loss: 0.426  |  valid_loss: 0.867\n",
      "[epoch: 47, i:  8699]  train_loss: 0.411  |  valid_loss: 0.597\n",
      "--> [End of epoch 47] train_accuracy: 84.84%  |  valid_accuracy: 80.55%\n",
      "--> [Start of epoch 48]  lr: 0.000005\n",
      "[epoch: 48, i:    86]  train_loss: 0.424  |  valid_loss: 0.605\n",
      "[epoch: 48, i:   173]  train_loss: 0.468  |  valid_loss: 0.634\n",
      "[epoch: 48, i:   260]  train_loss: 0.413  |  valid_loss: 0.667\n",
      "[epoch: 48, i:   347]  train_loss: 0.384  |  valid_loss: 0.525\n",
      "[epoch: 48, i:   434]  train_loss: 0.421  |  valid_loss: 0.556\n",
      "[epoch: 48, i:   521]  train_loss: 0.495  |  valid_loss: 0.360\n",
      "[epoch: 48, i:   608]  train_loss: 0.449  |  valid_loss: 0.576\n",
      "[epoch: 48, i:   695]  train_loss: 0.468  |  valid_loss: 0.580\n",
      "[epoch: 48, i:   782]  train_loss: 0.475  |  valid_loss: 0.703\n",
      "[epoch: 48, i:   869]  train_loss: 0.405  |  valid_loss: 0.673\n",
      "[epoch: 48, i:   956]  train_loss: 0.436  |  valid_loss: 0.530\n",
      "[epoch: 48, i:  1043]  train_loss: 0.415  |  valid_loss: 0.558\n",
      "[epoch: 48, i:  1130]  train_loss: 0.450  |  valid_loss: 0.495\n",
      "[epoch: 48, i:  1217]  train_loss: 0.429  |  valid_loss: 0.587\n",
      "[epoch: 48, i:  1304]  train_loss: 0.481  |  valid_loss: 0.419\n",
      "[epoch: 48, i:  1391]  train_loss: 0.447  |  valid_loss: 0.637\n",
      "[epoch: 48, i:  1478]  train_loss: 0.427  |  valid_loss: 0.527\n",
      "[epoch: 48, i:  1565]  train_loss: 0.419  |  valid_loss: 0.772\n",
      "[epoch: 48, i:  1652]  train_loss: 0.464  |  valid_loss: 0.557\n",
      "[epoch: 48, i:  1739]  train_loss: 0.422  |  valid_loss: 0.882\n",
      "[epoch: 48, i:  1826]  train_loss: 0.465  |  valid_loss: 0.736\n",
      "[epoch: 48, i:  1913]  train_loss: 0.463  |  valid_loss: 0.614\n",
      "[epoch: 48, i:  2000]  train_loss: 0.438  |  valid_loss: 0.639\n",
      "[epoch: 48, i:  2087]  train_loss: 0.455  |  valid_loss: 0.759\n",
      "[epoch: 48, i:  2174]  train_loss: 0.457  |  valid_loss: 0.520\n",
      "[epoch: 48, i:  2261]  train_loss: 0.471  |  valid_loss: 0.814\n",
      "[epoch: 48, i:  2348]  train_loss: 0.432  |  valid_loss: 0.453\n",
      "[epoch: 48, i:  2435]  train_loss: 0.382  |  valid_loss: 0.636\n",
      "[epoch: 48, i:  2522]  train_loss: 0.438  |  valid_loss: 0.432\n",
      "[epoch: 48, i:  2609]  train_loss: 0.387  |  valid_loss: 0.548\n",
      "[epoch: 48, i:  2696]  train_loss: 0.426  |  valid_loss: 0.633\n",
      "[epoch: 48, i:  2783]  train_loss: 0.392  |  valid_loss: 0.404\n",
      "[epoch: 48, i:  2870]  train_loss: 0.450  |  valid_loss: 0.680\n",
      "[epoch: 48, i:  2957]  train_loss: 0.416  |  valid_loss: 0.663\n",
      "[epoch: 48, i:  3044]  train_loss: 0.384  |  valid_loss: 0.653\n",
      "[epoch: 48, i:  3131]  train_loss: 0.431  |  valid_loss: 0.586\n",
      "[epoch: 48, i:  3218]  train_loss: 0.468  |  valid_loss: 0.653\n",
      "[epoch: 48, i:  3305]  train_loss: 0.475  |  valid_loss: 0.571\n",
      "[epoch: 48, i:  3392]  train_loss: 0.415  |  valid_loss: 0.515\n",
      "[epoch: 48, i:  3479]  train_loss: 0.392  |  valid_loss: 0.572\n",
      "[epoch: 48, i:  3566]  train_loss: 0.362  |  valid_loss: 0.497\n",
      "[epoch: 48, i:  3653]  train_loss: 0.491  |  valid_loss: 0.552\n",
      "[epoch: 48, i:  3740]  train_loss: 0.383  |  valid_loss: 0.385\n",
      "[epoch: 48, i:  3827]  train_loss: 0.398  |  valid_loss: 0.813\n",
      "[epoch: 48, i:  3914]  train_loss: 0.427  |  valid_loss: 0.445\n",
      "[epoch: 48, i:  4001]  train_loss: 0.461  |  valid_loss: 0.619\n",
      "[epoch: 48, i:  4088]  train_loss: 0.424  |  valid_loss: 0.525\n",
      "[epoch: 48, i:  4175]  train_loss: 0.476  |  valid_loss: 0.545\n",
      "[epoch: 48, i:  4262]  train_loss: 0.474  |  valid_loss: 0.408\n",
      "[epoch: 48, i:  4349]  train_loss: 0.382  |  valid_loss: 0.587\n",
      "[epoch: 48, i:  4436]  train_loss: 0.418  |  valid_loss: 0.501\n",
      "[epoch: 48, i:  4523]  train_loss: 0.439  |  valid_loss: 0.706\n",
      "[epoch: 48, i:  4610]  train_loss: 0.429  |  valid_loss: 0.458\n",
      "[epoch: 48, i:  4697]  train_loss: 0.425  |  valid_loss: 0.591\n",
      "[epoch: 48, i:  4784]  train_loss: 0.430  |  valid_loss: 0.654\n",
      "[epoch: 48, i:  4871]  train_loss: 0.463  |  valid_loss: 0.722\n",
      "[epoch: 48, i:  4958]  train_loss: 0.479  |  valid_loss: 0.792\n",
      "[epoch: 48, i:  5045]  train_loss: 0.441  |  valid_loss: 0.420\n",
      "[epoch: 48, i:  5132]  train_loss: 0.450  |  valid_loss: 0.601\n",
      "[epoch: 48, i:  5219]  train_loss: 0.488  |  valid_loss: 0.743\n",
      "[epoch: 48, i:  5306]  train_loss: 0.483  |  valid_loss: 0.614\n",
      "[epoch: 48, i:  5393]  train_loss: 0.369  |  valid_loss: 0.480\n",
      "[epoch: 48, i:  5480]  train_loss: 0.451  |  valid_loss: 0.818\n",
      "[epoch: 48, i:  5567]  train_loss: 0.431  |  valid_loss: 0.343\n",
      "[epoch: 48, i:  5654]  train_loss: 0.452  |  valid_loss: 0.528\n",
      "[epoch: 48, i:  5741]  train_loss: 0.472  |  valid_loss: 0.621\n",
      "[epoch: 48, i:  5828]  train_loss: 0.415  |  valid_loss: 0.549\n",
      "[epoch: 48, i:  5915]  train_loss: 0.474  |  valid_loss: 0.697\n",
      "[epoch: 48, i:  6002]  train_loss: 0.444  |  valid_loss: 0.497\n",
      "[epoch: 48, i:  6089]  train_loss: 0.487  |  valid_loss: 0.750\n",
      "[epoch: 48, i:  6176]  train_loss: 0.431  |  valid_loss: 0.578\n",
      "[epoch: 48, i:  6263]  train_loss: 0.444  |  valid_loss: 0.554\n",
      "[epoch: 48, i:  6350]  train_loss: 0.398  |  valid_loss: 0.520\n",
      "[epoch: 48, i:  6437]  train_loss: 0.411  |  valid_loss: 0.412\n",
      "[epoch: 48, i:  6524]  train_loss: 0.425  |  valid_loss: 0.762\n",
      "[epoch: 48, i:  6611]  train_loss: 0.519  |  valid_loss: 0.359\n",
      "[epoch: 48, i:  6698]  train_loss: 0.463  |  valid_loss: 0.656\n",
      "[epoch: 48, i:  6785]  train_loss: 0.493  |  valid_loss: 0.424\n",
      "[epoch: 48, i:  6872]  train_loss: 0.455  |  valid_loss: 0.733\n",
      "[epoch: 48, i:  6959]  train_loss: 0.441  |  valid_loss: 0.606\n",
      "[epoch: 48, i:  7046]  train_loss: 0.462  |  valid_loss: 0.553\n",
      "[epoch: 48, i:  7133]  train_loss: 0.433  |  valid_loss: 0.461\n",
      "[epoch: 48, i:  7220]  train_loss: 0.451  |  valid_loss: 0.679\n",
      "[epoch: 48, i:  7307]  train_loss: 0.460  |  valid_loss: 0.678\n",
      "[epoch: 48, i:  7394]  train_loss: 0.415  |  valid_loss: 0.567\n",
      "[epoch: 48, i:  7481]  train_loss: 0.452  |  valid_loss: 0.568\n",
      "[epoch: 48, i:  7568]  train_loss: 0.397  |  valid_loss: 0.731\n",
      "[epoch: 48, i:  7655]  train_loss: 0.405  |  valid_loss: 0.607\n",
      "[epoch: 48, i:  7742]  train_loss: 0.408  |  valid_loss: 0.558\n",
      "[epoch: 48, i:  7829]  train_loss: 0.464  |  valid_loss: 0.396\n",
      "[epoch: 48, i:  7916]  train_loss: 0.436  |  valid_loss: 0.640\n",
      "[epoch: 48, i:  8003]  train_loss: 0.401  |  valid_loss: 0.434\n",
      "[epoch: 48, i:  8090]  train_loss: 0.426  |  valid_loss: 0.486\n",
      "[epoch: 48, i:  8177]  train_loss: 0.461  |  valid_loss: 0.531\n",
      "[epoch: 48, i:  8264]  train_loss: 0.450  |  valid_loss: 0.575\n",
      "[epoch: 48, i:  8351]  train_loss: 0.471  |  valid_loss: 0.652\n",
      "[epoch: 48, i:  8438]  train_loss: 0.407  |  valid_loss: 0.453\n",
      "[epoch: 48, i:  8525]  train_loss: 0.424  |  valid_loss: 0.550\n",
      "[epoch: 48, i:  8612]  train_loss: 0.416  |  valid_loss: 0.835\n",
      "[epoch: 48, i:  8699]  train_loss: 0.398  |  valid_loss: 0.590\n",
      "--> [End of epoch 48] train_accuracy: 84.87%  |  valid_accuracy: 80.77%\n",
      "--> [Start of epoch 49]  lr: 0.000005\n",
      "[epoch: 49, i:    86]  train_loss: 0.470  |  valid_loss: 0.588\n",
      "[epoch: 49, i:   173]  train_loss: 0.473  |  valid_loss: 0.619\n",
      "[epoch: 49, i:   260]  train_loss: 0.449  |  valid_loss: 0.671\n",
      "[epoch: 49, i:   347]  train_loss: 0.420  |  valid_loss: 0.497\n",
      "[epoch: 49, i:   434]  train_loss: 0.491  |  valid_loss: 0.559\n",
      "[epoch: 49, i:   521]  train_loss: 0.440  |  valid_loss: 0.356\n",
      "[epoch: 49, i:   608]  train_loss: 0.514  |  valid_loss: 0.612\n",
      "[epoch: 49, i:   695]  train_loss: 0.462  |  valid_loss: 0.578\n",
      "[epoch: 49, i:   782]  train_loss: 0.364  |  valid_loss: 0.687\n",
      "[epoch: 49, i:   869]  train_loss: 0.437  |  valid_loss: 0.675\n",
      "[epoch: 49, i:   956]  train_loss: 0.488  |  valid_loss: 0.535\n",
      "[epoch: 49, i:  1043]  train_loss: 0.451  |  valid_loss: 0.572\n",
      "[epoch: 49, i:  1130]  train_loss: 0.366  |  valid_loss: 0.506\n",
      "[epoch: 49, i:  1217]  train_loss: 0.496  |  valid_loss: 0.595\n",
      "[epoch: 49, i:  1304]  train_loss: 0.420  |  valid_loss: 0.405\n",
      "[epoch: 49, i:  1391]  train_loss: 0.406  |  valid_loss: 0.655\n",
      "[epoch: 49, i:  1478]  train_loss: 0.453  |  valid_loss: 0.533\n",
      "[epoch: 49, i:  1565]  train_loss: 0.446  |  valid_loss: 0.771\n",
      "[epoch: 49, i:  1652]  train_loss: 0.404  |  valid_loss: 0.564\n",
      "[epoch: 49, i:  1739]  train_loss: 0.449  |  valid_loss: 0.877\n",
      "[epoch: 49, i:  1826]  train_loss: 0.422  |  valid_loss: 0.706\n",
      "[epoch: 49, i:  1913]  train_loss: 0.462  |  valid_loss: 0.599\n",
      "[epoch: 49, i:  2000]  train_loss: 0.486  |  valid_loss: 0.625\n",
      "[epoch: 49, i:  2087]  train_loss: 0.456  |  valid_loss: 0.742\n",
      "[epoch: 49, i:  2174]  train_loss: 0.436  |  valid_loss: 0.499\n",
      "[epoch: 49, i:  2261]  train_loss: 0.499  |  valid_loss: 0.822\n",
      "[epoch: 49, i:  2348]  train_loss: 0.469  |  valid_loss: 0.457\n",
      "[epoch: 49, i:  2435]  train_loss: 0.470  |  valid_loss: 0.633\n",
      "[epoch: 49, i:  2522]  train_loss: 0.390  |  valid_loss: 0.434\n",
      "[epoch: 49, i:  2609]  train_loss: 0.421  |  valid_loss: 0.526\n",
      "[epoch: 49, i:  2696]  train_loss: 0.455  |  valid_loss: 0.642\n",
      "[epoch: 49, i:  2783]  train_loss: 0.430  |  valid_loss: 0.382\n",
      "[epoch: 49, i:  2870]  train_loss: 0.461  |  valid_loss: 0.700\n",
      "[epoch: 49, i:  2957]  train_loss: 0.432  |  valid_loss: 0.672\n",
      "[epoch: 49, i:  3044]  train_loss: 0.471  |  valid_loss: 0.681\n",
      "[epoch: 49, i:  3131]  train_loss: 0.452  |  valid_loss: 0.552\n",
      "[epoch: 49, i:  3218]  train_loss: 0.419  |  valid_loss: 0.666\n",
      "[epoch: 49, i:  3305]  train_loss: 0.380  |  valid_loss: 0.554\n",
      "[epoch: 49, i:  3392]  train_loss: 0.475  |  valid_loss: 0.509\n",
      "[epoch: 49, i:  3479]  train_loss: 0.472  |  valid_loss: 0.559\n",
      "[epoch: 49, i:  3566]  train_loss: 0.419  |  valid_loss: 0.510\n",
      "[epoch: 49, i:  3653]  train_loss: 0.415  |  valid_loss: 0.565\n",
      "[epoch: 49, i:  3740]  train_loss: 0.387  |  valid_loss: 0.386\n",
      "[epoch: 49, i:  3827]  train_loss: 0.438  |  valid_loss: 0.782\n",
      "[epoch: 49, i:  3914]  train_loss: 0.459  |  valid_loss: 0.446\n",
      "[epoch: 49, i:  4001]  train_loss: 0.469  |  valid_loss: 0.656\n",
      "[epoch: 49, i:  4088]  train_loss: 0.435  |  valid_loss: 0.508\n",
      "[epoch: 49, i:  4175]  train_loss: 0.466  |  valid_loss: 0.544\n",
      "[epoch: 49, i:  4262]  train_loss: 0.438  |  valid_loss: 0.427\n",
      "[epoch: 49, i:  4349]  train_loss: 0.493  |  valid_loss: 0.576\n",
      "[epoch: 49, i:  4436]  train_loss: 0.420  |  valid_loss: 0.522\n",
      "[epoch: 49, i:  4523]  train_loss: 0.498  |  valid_loss: 0.712\n",
      "[epoch: 49, i:  4610]  train_loss: 0.428  |  valid_loss: 0.461\n",
      "[epoch: 49, i:  4697]  train_loss: 0.443  |  valid_loss: 0.601\n",
      "[epoch: 49, i:  4784]  train_loss: 0.410  |  valid_loss: 0.662\n",
      "[epoch: 49, i:  4871]  train_loss: 0.446  |  valid_loss: 0.644\n",
      "[epoch: 49, i:  4958]  train_loss: 0.448  |  valid_loss: 0.774\n",
      "[epoch: 49, i:  5045]  train_loss: 0.414  |  valid_loss: 0.388\n",
      "[epoch: 49, i:  5132]  train_loss: 0.474  |  valid_loss: 0.542\n",
      "[epoch: 49, i:  5219]  train_loss: 0.428  |  valid_loss: 0.733\n",
      "[epoch: 49, i:  5306]  train_loss: 0.467  |  valid_loss: 0.603\n",
      "[epoch: 49, i:  5393]  train_loss: 0.462  |  valid_loss: 0.494\n",
      "[epoch: 49, i:  5480]  train_loss: 0.491  |  valid_loss: 0.787\n",
      "[epoch: 49, i:  5567]  train_loss: 0.414  |  valid_loss: 0.323\n",
      "[epoch: 49, i:  5654]  train_loss: 0.420  |  valid_loss: 0.523\n",
      "[epoch: 49, i:  5741]  train_loss: 0.406  |  valid_loss: 0.623\n",
      "[epoch: 49, i:  5828]  train_loss: 0.408  |  valid_loss: 0.567\n",
      "[epoch: 49, i:  5915]  train_loss: 0.426  |  valid_loss: 0.673\n",
      "[epoch: 49, i:  6002]  train_loss: 0.458  |  valid_loss: 0.484\n",
      "[epoch: 49, i:  6089]  train_loss: 0.448  |  valid_loss: 0.759\n",
      "[epoch: 49, i:  6176]  train_loss: 0.439  |  valid_loss: 0.587\n",
      "[epoch: 49, i:  6263]  train_loss: 0.511  |  valid_loss: 0.563\n",
      "[epoch: 49, i:  6350]  train_loss: 0.394  |  valid_loss: 0.521\n",
      "[epoch: 49, i:  6437]  train_loss: 0.402  |  valid_loss: 0.414\n",
      "[epoch: 49, i:  6524]  train_loss: 0.434  |  valid_loss: 0.740\n",
      "[epoch: 49, i:  6611]  train_loss: 0.398  |  valid_loss: 0.376\n",
      "[epoch: 49, i:  6698]  train_loss: 0.456  |  valid_loss: 0.684\n",
      "[epoch: 49, i:  6785]  train_loss: 0.448  |  valid_loss: 0.456\n",
      "[epoch: 49, i:  6872]  train_loss: 0.478  |  valid_loss: 0.775\n",
      "[epoch: 49, i:  6959]  train_loss: 0.401  |  valid_loss: 0.632\n",
      "[epoch: 49, i:  7046]  train_loss: 0.407  |  valid_loss: 0.544\n",
      "[epoch: 49, i:  7133]  train_loss: 0.448  |  valid_loss: 0.455\n",
      "[epoch: 49, i:  7220]  train_loss: 0.460  |  valid_loss: 0.678\n",
      "[epoch: 49, i:  7307]  train_loss: 0.440  |  valid_loss: 0.684\n",
      "[epoch: 49, i:  7394]  train_loss: 0.392  |  valid_loss: 0.535\n",
      "[epoch: 49, i:  7481]  train_loss: 0.424  |  valid_loss: 0.572\n",
      "[epoch: 49, i:  7568]  train_loss: 0.406  |  valid_loss: 0.709\n",
      "[epoch: 49, i:  7655]  train_loss: 0.445  |  valid_loss: 0.605\n",
      "[epoch: 49, i:  7742]  train_loss: 0.470  |  valid_loss: 0.565\n",
      "[epoch: 49, i:  7829]  train_loss: 0.419  |  valid_loss: 0.437\n",
      "[epoch: 49, i:  7916]  train_loss: 0.460  |  valid_loss: 0.597\n",
      "[epoch: 49, i:  8003]  train_loss: 0.413  |  valid_loss: 0.440\n",
      "[epoch: 49, i:  8090]  train_loss: 0.486  |  valid_loss: 0.445\n",
      "[epoch: 49, i:  8177]  train_loss: 0.410  |  valid_loss: 0.553\n",
      "[epoch: 49, i:  8264]  train_loss: 0.429  |  valid_loss: 0.623\n",
      "[epoch: 49, i:  8351]  train_loss: 0.491  |  valid_loss: 0.678\n",
      "[epoch: 49, i:  8438]  train_loss: 0.470  |  valid_loss: 0.447\n",
      "[epoch: 49, i:  8525]  train_loss: 0.404  |  valid_loss: 0.543\n",
      "[epoch: 49, i:  8612]  train_loss: 0.420  |  valid_loss: 0.835\n",
      "[epoch: 49, i:  8699]  train_loss: 0.439  |  valid_loss: 0.576\n",
      "--> [End of epoch 49] train_accuracy: 84.67%  |  valid_accuracy: 80.66%\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  # Total epochs.\n",
    "\n",
    "for epoch in range(epoch_start, epoch_start + epochs):  # Loop over the dataset multiple times.\n",
    "    running_train_loss = 0.0       # Initialize running train_loss for training set.\n",
    "    running_val_loss = 0.0     # Initialize running train_loss for validation set.\n",
    "    \n",
    "    # Initialize running total and correct for computing train accuracy.\n",
    "    train_correct = 0 \n",
    "    train_total = 0\n",
    "    # Initialize running total and correct for computing test accuracy.\n",
    "    valid_total = 0   \n",
    "    valid_correct = 0\n",
    "    \n",
    "    print('--> [Start of epoch {}]'.format(epoch) +\n",
    "          '  lr: {:.6f}'.format(opt.param_groups[0]['lr']))\n",
    "    \n",
    "    # print('--> [Start of epoch {}]'.format(epoch))\n",
    "    \n",
    "    validiter = iter(validloader)\n",
    "    for i, data in enumerate(iterable=trainloader, start=0):\n",
    "        \n",
    "        # Set the model to training mode.\n",
    "        dense_max.train()\n",
    "        \n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step\n",
    "        train_output = dense_max(inputs)\n",
    "        train_loss = loss_func(train_output, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "        \n",
    "        dense_max.eval()\n",
    "        \n",
    "        # record statistics.\n",
    "        running_train_loss += train_loss.item()\n",
    "        \n",
    "        # this is for recording training accuracy\n",
    "        _, train_predicted = torch.max(train_output.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (train_predicted == labels).sum().item()\n",
    "        \n",
    "        # only iterate through validation set according to train_per_valid ratio\n",
    "        if i % train_per_valid == train_per_valid - 1:\n",
    "            # Set the model to evaluation mode.\n",
    "            with torch.no_grad():\n",
    "                valid_inputs, valid_labels = next(validiter)\n",
    "                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)\n",
    "                valid_output = dense_max(valid_inputs)\n",
    "                valid_loss = loss_func(valid_output, valid_labels)\n",
    "                running_val_loss += valid_loss.item()\n",
    "                \n",
    "                _, valid_predicted = torch.max(valid_output.data, 1)\n",
    "                valid_total += valid_labels.size(0)\n",
    "                valid_correct += (valid_predicted == valid_labels).sum().item()\n",
    "            \n",
    "        # Record training/validation loss every several mini-batches.\n",
    "        if i % iter_n_per_record == iter_n_per_record - 1: \n",
    "            avg_train_loss = running_train_loss / iter_n_per_record\n",
    "            avg_train_losses.append(avg_train_loss)\n",
    "            avg_valid_loss = train_per_valid * running_val_loss / iter_n_per_record\n",
    "            avg_valid_losses.append(avg_valid_loss)\n",
    "            running_train_loss, running_val_loss = 0.0, 0.0\n",
    "            \n",
    "            # only print traing training loss once in a while to avoid cluster output\n",
    "            if i % iter_n_per_print == iter_n_per_print - 1:\n",
    "                print('[epoch: {}, i: {:5d}]'.format(epoch, i) +\n",
    "                      '  train_loss: {:.3f}'.format(avg_train_loss) + \n",
    "                      '  |  valid_loss: {:.3f}'.format(avg_valid_loss))\n",
    "                            \n",
    "\n",
    "    # exponential scheduling\n",
    "    # scheduler.step()\n",
    "                   \n",
    "    # calculating train accuracy\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    valid_accuracies.append(valid_accuracy)     \n",
    "    \n",
    "    # reduce on plateau scheduling\n",
    "    scheduler.step(valid_accuracy)\n",
    "    \n",
    "    # Store the networks after each epochs.\n",
    "    # in case we want to do simple average or weighted average\n",
    "    file_name = f'epoch_{epoch:04d}.pth'\n",
    "    save_path = directory_path / file_name\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': dense_max.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'avg_train_losses': avg_train_losses[epoch * iter_n_per_record : (epoch + 1) * iter_n_per_record],\n",
    "            'avg_valid_losses': avg_valid_losses[epoch * iter_n_per_record : (epoch + 1) * iter_n_per_record],\n",
    "            'train_accuracy': train_accuracies[epoch],\n",
    "            'valid_accuracy': valid_accuracies[epoch],\n",
    "            }, save_path)\n",
    "    \n",
    "    print('--> [End of epoch {}]'.format(epoch) +\n",
    "                      ' train_accuracy: {:.2f}%'.format(train_accuracy) + \n",
    "                      '  |  valid_accuracy: {:.2f}%'.format(valid_accuracy))\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(avg_train_losses[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(avg_valid_losses[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDenseNet loss curve + plateau scheduling + l2 regularization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(avg_train_losses[:1000])\n",
    "plt.plot(avg_valid_losses[:1000])\n",
    "plt.title('DenseNet loss curve + plateau scheduling + l2 regularization')\n",
    "plt.xlabel('mini-batch index / {}'.format(iter_n_per_record))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute the moving average of the data using a specified window size.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "window_size = 20  # Adjust this based on your preference\n",
    "train_losses_ma = moving_average(avg_train_losses, window_size)\n",
    "valid_losses_ma = moving_average(avg_valid_losses, window_size)\n",
    "\n",
    "# x_axis = np.arange(len(train_losses_ma))/100 \n",
    "x_axis = np.arange(window_size - 1, len(train_losses_ma) + window_size - 1)\n",
    "\n",
    "plt.plot(avg_train_losses)\n",
    "plt.plot(avg_valid_losses)\n",
    "plt.plot(x_axis, train_losses_ma, linewidth=2)\n",
    "plt.plot(x_axis, valid_losses_ma, linewidth=2)\n",
    "plt.title('DenseNet loss trend + exponential scheduling + l2 regularization')\n",
    "plt.xlabel('mini-batch index / {}'.format(iter_n_per_record))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend(['training loss', 'validation loss', 'training loss (moving avg)', 'validation loss (moving avg)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAHHCAYAAACStX1aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAACD8UlEQVR4nO3dd1hT1x8G8DcECDvspSzBBYILte4tzqq1WkdbVx11Vf1pq22tWltXtXVVra17te66rbsVdysqblEEEUSRKZvc3x8hkcgQIeEy3s/z8EBubu79Zr+cc+65EkEQBBARERGRVuiJXQARERFRecJwRURERKRFDFdEREREWsRwRURERKRFDFdEREREWsRwRURERKRFDFdEREREWsRwRURERKRFDFdEREREWsRwRVQOPX36FO+//z5sbGwgkUiwaNEisUsCAMyYMQMSiUTsMsqEdevWQSKR4PLlyzrf16BBg+Du7l6k2+b1nLq7u2PQoEHFL0xk5eV+vElxnv+CSCQSzJgxQ+vbLa37zUkn4Ur1oaD6MTIygrOzMwICArBkyRIkJibqYrc61apVK0gkElStWjXP648ePaq+vzt27Cjh6og0TZgwAUeOHMHUqVOxceNGdOzYUeySim358uVYt26d2GVQBXb79m18/vnnqFOnDszNzeHk5IQuXbqUSAAmTQcPHhQ9QBVEX5cb//bbb+Hh4YGMjAxERUXh1KlTGD9+PH788Ufs3bsXfn5+uty91hkZGeH+/fu4ePEiGjZsqHHd5s2bYWRkhNTUVJGqI3rlxIkT6N69OyZNmiR2KVqzfPly2NraVoiWhLLuzp070NMrfx0jv/32G1avXo1evXph1KhRiI+Pxy+//IJ33nkHhw8fRrt27cQusVRJSUmBvr5uYsbBgwfx888/5xmwdLnfwtLpq79Tp0748MMPMXjwYEydOhVHjhzBsWPHEB0djXfffRcpKSm63L3WeXp6onr16ti6davG8tTUVOzevRtdunQRqbKy5eXLl2KXUOJK+j5HR0fD0tJSa9tLTU2FQqHQ2vaofJPJZDAwMBC7DACvelK0oV+/fggPD8dvv/2G4cOHY/Lkybhw4QKsra2L3IpS3j4PFQqFupHByMhIlJAj1n5zKvF/Ldq0aYNp06bh0aNH2LRpk8Z1t2/fxvvvvw9ra2sYGRnB398fe/fu1VhH9UYJDAzExIkTYWdnB1NTU/Ts2RPPnj3TWPfy5csICAiAra0tjI2N4eHhgSFDhmiso1AosGjRIvj4+MDIyAgODg4YMWIEYmNj86y/X79++OOPPzS+aPbt24fk5GT06dMn1/qPHj3CqFGjUL16dRgbG8PGxga9e/dGaGioeh1BENC6dWvY2dkhOjpavTw9PR2+vr7w9PQs8A2Ynp6Ob775BvXr14dcLoepqSmaN2+OkydP5lpXoVBg8eLF8PX1hZGREezs7NCxY8dczdqbNm1Cw4YNYWJiAisrK7Ro0QJ//fWX+vr8+rRfH6Oger5Onz6NUaNGwd7eHpUrVy70Y6MSFxeHCRMmwN3dHTKZDJUrV8bHH3+M58+fIykpCaampvjss89y3e7x48eQSqWYM2dOvo9fYR6X0NBQSCSSPLulXn8sVGNQbt68if79+8PKygrNmjXDggULIJFI8OjRo1zbmDp1KgwNDTVedxcuXEDHjh0hl8thYmKCli1bIjAwsMD7oXq8BUHAzz//rO6qVnnw4AF69+4Na2trmJiY4J133sGBAwc0tnHq1ClIJBL8/vvv+Prrr1GpUiWYmJggISEhz32qHpsFCxbgp59+gpubG4yNjdGyZUsEBwcXWC8ArF27Fm3atIG9vT1kMhm8vb2xYsUKjXXc3d1x48YNnD59Wn2fWrVqpb4+Li4O48ePh4uLC2QyGby8vDBv3rxcgXDBggVo0qQJbGxsYGxsjPr16+fqxn+b5zo/S5cuhY+Pj/r94+/vjy1btmisExERgaFDh8LZ2RkymQweHh749NNPkZ6errFeWlraGz/rAODQoUNo3rw5TE1NYW5uji5duuDGjRu51tuzZw9q1aoFIyMj1KpVC7t37861juo1cOrUqUI/Njnl9zlQmM9thUKBGTNmwNnZGSYmJmjdujVu3rxZKsY/1a9fH2ZmZhrLbGxs0Lx5c9y6deuNt8/vs0Fl06ZNqF+/PoyNjWFtbY2+ffsiPDw813Z+/vlnVKlSBcbGxmjYsCH++ecftGrVSuM9oXrMX/88ze+5fV1h3iuA8j0xZswYbN68GT4+PpDJZDh8+LD6OtX7RfXaye9H5Z9//kHv3r3h6uoKmUwGFxcXTJgwQaMxZtCgQfj555/V+3h9G3m9T69cuYJOnTrBwsICZmZmaNu2Lc6fP6+xztu8Tt9ElGj30Ucf4csvv8Rff/2FYcOGAQBu3LiBpk2bolKlSpgyZQpMTU2xbds29OjRAzt37kTPnj01tjF27FhYWVlh+vTpCA0NxaJFizBmzBj88ccfAJT/uXfo0AF2dnaYMmUKLC0tERoail27dmlsZ8SIEVi3bh0GDx6McePG4eHDh1i2bBmuXLmCwMDAXP999e/fHzNmzMCpU6fQpk0bAMCWLVvQtm1b2Nvb57qvly5dwtmzZ9G3b19UrlwZoaGhWLFiBVq1aoWbN2/CxMQEEokEa9asgZ+fH0aOHKmucfr06bhx4wZOnToFU1PTfB/PhIQE/Pbbb+jXrx+GDRuGxMRErF69GgEBAbh48SLq1KmjXnfo0KFYt24dOnXqhE8++QSZmZn4559/cP78efj7+wMAZs6ciRkzZqBJkyb49ttvYWhoiAsXLuDEiRPo0KFDYZ7iXEaNGgU7Ozt888036qBYmMcGAJKSktQfXkOGDEG9evXw/Plz7N27F48fP0adOnXQs2dP/PHHH/jxxx8hlUrV+926dSsEQcCAAQMKrK8wj8vb6t27N6pWrYrZs2dDEAR07doVn3/+ObZt24bJkydrrLtt2zZ06NABVlZWAJTdep06dUL9+vUxffp06OnpqUPIP//8k6tbWqVFixbYuHEjPvroI7Rv3x4ff/yx+rqnT5+iSZMmSE5Oxrhx42BjY4P169fj3XffxY4dO3K9x2bNmgVDQ0NMmjQJaWlpMDQ0LPD+btiwAYmJiRg9ejRSU1OxePFitGnTBtevX4eDg0O+t1uxYgV8fHzw7rvvQl9fH/v27cOoUaOgUCgwevRoAMCiRYswduxYmJmZ4auvvgIA9TaTk5PRsmVLREREYMSIEXB1dcXZs2cxdepUREZGagzmX7x4Md59910MGDAA6enp+P3339G7d2/s379fay3Pv/76K8aNG4f3338fn332GVJTU3Ht2jVcuHAB/fv3BwA8efIEDRs2RFxcHIYPH44aNWogIiICO3bsQHJyssZj/abPOgDYuHEjBg4ciICAAMybNw/JyclYsWIFmjVrhitXrqgHK//111/o1asXvL29MWfOHMTExGDw4MHqf3h0rTD3ZerUqZg/fz66deuGgIAAXL16FQEBAaV6yEVUVBRsbW0Lvf7rnw0A8P3332PatGno06cPPvnkEzx79gxLly5FixYtcOXKFXVL9IoVKzBmzBg0b94cEyZMQGhoKHr06AErKyutPo9v8145ceIEtm3bhjFjxsDW1jbPwfF2dnbYuHGjxrKMjAxMmDBB4/W+fft2JCcn49NPP4WNjQ0uXryIpUuX4vHjx9i+fTsA5ff2kydPcPTo0VzbzMuNGzfQvHlzWFhY4PPPP4eBgQF++eUXtGrVCqdPn0ajRo001i/M6/SNBB1Yu3atAEC4dOlSvuvI5XKhbt266stt27YVfH19hdTUVPUyhUIhNGnSRKhatWqubbdr105QKBTq5RMmTBCkUqkQFxcnCIIg7N69+401/PPPPwIAYfPmzRrLDx8+nGt5y5YtBR8fH0EQBMHf318YOnSoIAiCEBsbKxgaGgrr168XTp48KQAQtm/frr5dcnJyrv2eO3dOACBs2LBBY/kvv/wiABA2bdoknD9/XpBKpcL48ePzrV8lMzNTSEtL01gWGxsrODg4CEOGDFEvO3HihABAGDduXK5tqB7Le/fuCXp6ekLPnj2FrKysPNcRBEEAIEyfPj3Xdtzc3ISBAweqL6uer2bNmgmZmZka6xb2sfnmm28EAMKuXbvyrfvIkSMCAOHQoUMa1/v5+QktW7bMdbucCvO4PHz4UAAgrF27Ntc6rz8W06dPFwAI/fr1y7Vu48aNhfr162ssu3jxosZ9VigUQtWqVYWAgACNxzw5OVnw8PAQ2rdvX+D9UdU0evRojWXjx48XAAj//POPelliYqLg4eEhuLu7q59v1eu4SpUqeT5Hr1M9NsbGxsLjx4/Vyy9cuCAAECZMmKBepnpscsprHwEBAUKVKlU0lvn4+OT5XM6aNUswNTUV7t69q7F8ypQpglQqFcLCwvLdV3p6ulCrVi2hTZs2ue5PYZ7rvHTv3l39WZGfjz/+WNDT08vz80n1nBf2sy4xMVGwtLQUhg0bprGdqKgoQS6XayyvU6eO4OTkpL6tIAjCX3/9JQAQ3Nzc1MtUr4GTJ09qbDOvxyav5zS/z4E33ZeoqChBX19f6NGjh8b2ZsyYIQDQ2GZhqfZdFK/fj7z8/fffgkQiEaZNm/bG7eX32RAaGipIpVLh+++/11h+/fp1QV9fX708LS1NsLGxERo0aCBkZGSo11u3bp0AQOP9obrfDx8+1NhmXs/twIEDNZ5/QSjce0UQlO8JPT094caNG7nu75veL6NGjRKkUqlw4sSJfPcrCIIwZ84cQSKRCI8ePVIvGz16dL7P6+v77dGjh2BoaCiEhISolz158kQwNzcXWrRooV5W2NdpYYg24tDMzEx91OCLFy9w4sQJ9OnTB4mJiXj+/DmeP3+OmJgYBAQE4N69e4iIiNC4/fDhwzWaAZs3b46srCx1l4sq5e/fvx8ZGRl51rB9+3bI5XK0b99evc/nz5+rm37z6lYDlK1Xu3btQnp6Onbs2AGpVJrrv34VY2Nj9d8ZGRmIiYmBl5cXLC0t8d9//+W6TwEBARg7diw++ugjeHp6Yvbs2QU8ikpSqVSd/BUKBV68eIHMzEz4+/tr7GPnzp2QSCSYPn16rm2oHss9e/ZAoVDgm2++yTUgtTjjFoYNG6bRogQU/rHZuXMnateunedjrKqpXbt2cHZ2xubNm9XXBQcH49q1a/jwww8LrK0wj0tRjBw5MteyDz74AP/++y9CQkLUy/744w/IZDJ0794dABAUFIR79+6hf//+iImJUb8uX758ibZt2+Lvv/8u0vingwcPomHDhhrdEGZmZhg+fDhCQ0Nx8+ZNjfUHDhyo8Ry9SY8ePVCpUiX15YYNG6JRo0Y4ePBggbfLuY/4+Hg8f/4cLVu2xIMHDxAfH//G/W7fvh3NmzeHlZWVxvu4Xbt2yMrKwt9//53nvmJjYxEfH4/mzZvnei8Wh6WlJR4/foxLly7leb1CocCePXvQrVu3PFtFX3/Nvemz7ujRo4iLi0O/fv007r9UKkWjRo3Un2ORkZEICgrCwIEDIZfL1dtr3749vL29i32/C+NN9+X48ePIzMzEqFGjNG43duzYQu8jNjZW43FISkoCAI1lz58/R3JycrHvT3R0NPr37w8PDw98/vnnhb7d658Nu3btgkKhQJ8+fTRqdHR0RNWqVdXP4eXLlxETE4Nhw4ZpjCkaMGCAutVbW97mvdKyZcu3fg1t2LABy5cvx/z589G6des89/vy5Us8f/4cTZo0gSAIuHLlylvfj6ysLPz111/o0aMHqlSpol7u5OSE/v3748yZM7mGPLzpdVoYoo34SkpKUnej3b9/H4IgYNq0aZg2bVqe60dHR2t8cLu6umpcr3phqcastGzZEr169cLMmTPx008/oVWrVujRowf69+8PmUwGALh37x7i4+Pz7M5T7TMvffv2xaRJk3Do0CFs3rwZXbt2hbm5eZ7rpqSkYM6cOVi7di0iIiLUTcAA8vziWL16NTw9PXHv3j2cPXu20F9u69evx8KFC3H79m2NMOnh4aH+OyQkBM7OzrC2ts53OyEhIdDT09P6h23OOlQK+9iEhISgV69eBW5fT08PAwYMwIoVK5CcnAwTExP1EZy9e/cu8LaFeVyKIq/73Lt3b0ycOBF//PEHvvzySwiCgO3bt6vHAgDK1yWgDDf5iY+Pf+sP00ePHuVq/gaAmjVrqq+vVatWgfUXJK9pSqpVq4Zt27YVeLvAwEBMnz4d586dy/WFFx8frxEE8nLv3j1cu3YNdnZ2eV6f8328f/9+fPfddwgKCkJaWpp6uTbn3vriiy9w7NgxNGzYEF5eXujQoQP69++Ppk2bAgCePXuGhIQEjce6IG/6rFO9XlTDFF6nel2pvhjyep6qV6+u1YCZnzfdF1WNXl5eGutZW1sX+vVet27dPL8EX399TJ8+vViH8r98+RJdu3ZFYmIizpw5k2ssVkFef2/du3cPgiDkO9WPanhKfo+Pvr6+1uepepv3ytt+VgQFBWHkyJHo168fJk6cqHFdWFgYvvnmG+zduzfX2OfC/LP1umfPniE5ORnVq1fPdV3NmjWhUCgQHh4OHx8f9fI3vU4LQ5Rw9fjxY8THx6tfIKr/widNmoSAgIA8b/P6i+n1VhAV1Re0ar6p8+fPY9++fThy5AiGDBmChQsX4vz58zAzM4NCoYC9vb1Ga0dO+X1YOzk5oVWrVli4cCECAwOxc+fOfO/r2LFjsXbtWowfPx6NGzeGXC6HRCJB375982x9OHXqlPqFfP36dTRu3Djfbats2rQJgwYNQo8ePTB58mTY29urB3HnbCEpCVlZWXkuzyskvu1j8yYff/wxfvjhB+zZswf9+vXDli1b0LVr1zd+ORdGfl+++d1fIO/77OzsjObNm2Pbtm348ssvcf78eYSFhWHevHnqdVT3/YcfftAYL5fT23yQF9XbtFoVVUhICNq2bYsaNWrgxx9/hIuLCwwNDXHw4EH89NNPhXodKBQKtG/fPt+Wg2rVqgFQDpR999130aJFCyxfvhxOTk4wMDDA2rVrNQabF+W5zqlmzZq4c+cO9u/fj8OHD2Pnzp1Yvnw5vvnmG8ycObNQ28jpTZ91qsdo48aNcHR0zLVeUY6aKu5jkJ833Rdt2Lx5s8bg57/++gs//PADjh49qrFezlaMt5Weno733nsP165dw5EjRwodlFVef28pFApIJBIcOnQoz8eoKO/34jyHhX2vqLzNZ0VsbCx69eqFatWq4bfffstVW/v27fHixQt88cUXqFGjBkxNTREREYFBgwaV2BHL2nidihKuVAPQVEFK9SI3MDDQ+jwh77zzDt555x18//332LJlCwYMGIDff/8dn3zyCTw9PXHs2DE0bdr0rb9I+vfvj08++QSWlpbo3Llzvuvt2LEDAwcOxMKFC9XLUlNTERcXl2vdyMhIjB07Fh06dFAPJA4ICICbm1uBtezYsQNVqlTBrl27NN5Qr3dzeXp64siRI3jx4kW+rTSenp5QKBS4efNmvl/sgDLJv34f0tPTERkZWWCtr9ddmMfG09OzUEed1apVC3Xr1sXmzZtRuXJlhIWFYenSpW+8XWEeF9V/Lq/X9jbNxCoffPABRo0ahTt37uCPP/6AiYkJunXrplEPoGxx0Ob7wc3NDXfu3Mm1/Pbt2+rri0PVgpLT3bt3C/yPet++fUhLS8PevXs1/lvMq0s+vy8LT09PJCUlvfGx2rlzJ4yMjHDkyBF16zWgPFoxJ20816ampvjggw/wwQcfqL+Iv//+e0ydOhV2dnawsLAo1Gu6MFSvF3t7+wIfA9Xzm9fz9PrrQpuv97ehqvH+/fsarSExMTGFbjVQtRCqPH78GAC09l5SKBT4+OOPcfz4cWzbtg0tW7Ys9jY9PT0hCAI8PDzU/wzkJefjk7MrLTMzE6GhoRpzRxbnOSzse+VtKRQKDBgwAHFxcTh27Jj6oCWV69ev4+7du1i/fr3GwTivB2Og8K3NdnZ2MDExyfezT09PDy4uLm95T96sxMdcnThxArNmzYKHh4f6CC57e3u0atUKv/zyS55fzm97CCSgTMevp0xVWFC1DPXp0wdZWVmYNWtWrttnZmbmGYBU3n//fUyfPh3Lly8v8CgqqVSaq46lS5fm+d/DsGHDoFAosHr1aqxatQr6+voYOnToG9OyKmXnXO/ChQs4d+6cxnq9evWCIAh5/vesum2PHj2gp6eHb7/9Ntd/CTm37+npqTGWBQBWrVr1Vv/ZFvax6dWrF65evZrnIeOv3/6jjz7CX3/9hUWLFsHGxgadOnV6Yx2FeVwsLCxga2ub6z4vX778jdvPa39SqRRbt27F9u3b0bVrV42jQevXrw9PT08sWLBAPV4kp6K8HwCgc+fOuHjxosbr4uXLl1i1ahXc3d2L3RW8Z88ejbGRFy9exIULFwp8DvJ67cbHx+f5IW5qaprne7JPnz44d+4cjhw5kuu6uLg4ZGZmqvclkUg0Xl+hoaHYs2ePxm2K+1zHxMRoXDY0NIS3tzcEQUBGRgb09PTQo0cP7Nu3L8+Zvd+2FScgIAAWFhaYPXt2nuNLVa8XJycn1KlTB+vXr9foXjl69Giu8XZubm6QSqVaeb2/jbZt20JfXz/XVBzLli3T6X7fxtixY/HHH39g+fLleO+997Syzffeew9SqRQzZ87M9fwLgqB+Tfn7+8PGxga//vqr+nUNKFvrXg+fqtCd8znMysrCqlWr3lhPYd8rb2vmzJk4cuQItm7dmmdXYl6fB4IgYPHixbnWVX1mFvQ9rdpmhw4d8Oeff2pMS/H06VNs2bIFzZo1U3eda5NOW64OHTqE27dvIzMzE0+fPsWJEydw9OhRuLm5Ye/evTAyMlKv+/PPP6NZs2bw9fXFsGHDUKVKFTx9+hTnzp3D48ePcfXq1bfa9/r167F8+XL07NkTnp6eSExMxK+//goLCwt1S1PLli0xYsQIzJkzB0FBQejQoQMMDAxw7949bN++HYsXL8b777+f5/blcnmh+uu7du2KjRs3Qi6Xw9vbG+fOncOxY8dgY2Ojsd7atWtx4MABrFu3Tn047dKlS/Hhhx9ixYoVuQZ4vr6PXbt2oWfPnujSpQsePnyIlStXwtvbW+PLuXXr1vjoo4+wZMkS3Lt3Dx07doRCocA///yD1q1bY8yYMfDy8sJXX32FWbNmoXnz5njvvfcgk8lw6dIlODs7q+eL+uSTTzBy5Ej06tUL7du3x9WrV3HkyJG3Ohy5sI/N5MmTsWPHDvTu3RtDhgxB/fr18eLFC+zduxcrV65E7dq11ev2798fn3/+OXbv3o1PP/20UBMZFuZxUd3nuXPn4pNPPoG/vz/+/vtv3L17t9D3V8Xe3h6tW7fGjz/+iMTERHzwwQca1+vp6eG3335Dp06d4OPjg8GDB6NSpUqIiIjAyZMnYWFhgX379r31fqdMmYKtW7eiU6dOGDduHKytrbF+/Xo8fPgQO3fuLPaM2l5eXmjWrBk+/fRTpKWlqQNuQQN9Va203bp1w4gRI5CUlIRff/0V9vb2uf7Rql+/PlasWIHvvvsOXl5esLe3R5s2bTB58mTs3bsXXbt2xaBBg1C/fn28fPkS169fx44dOxAaGgpbW1t06dIFP/74Izp27Ij+/fsjOjoaP//8M7y8vHDt2jWNfRXnue7QoQMcHR3RtGlTODg44NatW1i2bBm6dOmiHps5e/Zs/PXXX2jZsiWGDx+OmjVrIjIyEtu3b8eZM2feagJYCwsLrFixAh999BHq1auHvn37ws7ODmFhYThw4ACaNm2qDidz5sxBly5d0KxZMwwZMgQvXrxQz8mV87NCLpejd+/eWLp0KSQSCTw9PbF///58x6Fqi4ODAz777DMsXLgQ7777Ljp27IirV6/i0KFDsLW1Ff28lIsWLcLy5cvRuHFjmJiY5JqrsWfPngVOm5MfT09PfPfdd5g6dap6agVzc3M8fPgQu3fvxvDhwzFp0iQYGhpixowZGDt2LNq0aYM+ffogNDQU69atg6enp8bj4+Pjg3feeQdTp05Vt8r//vvvGqEsP2/zXims69evY9asWWjRogWio6NzPXYffvghatSoAU9PT0yaNAkRERGwsLDAzp0782y1rF+/PgBg3LhxCAgIgFQqRd++ffPc93fffYejR4+iWbNmGDVqFPT19fHLL78gLS0N8+fPL9L9eaNCH1f4FlSHM6p+DA0NBUdHR6F9+/bC4sWLhYSEhDxvFxISInz88ceCo6OjYGBgIFSqVEno2rWrsGPHjlzbfv0Q5tcPL/3vv/+Efv36Ca6uroJMJhPs7e2Frl27CpcvX86131WrVgn169cXjI2NBXNzc8HX11f4/PPPhSdPnqjXyTkVQ37ymoohNjZWGDx4sGBrayuYmZkJAQEBwu3btzUO8Q0PDxfkcrnQrVu3XNvs2bOnYGpqKjx48CDf/SoUCmH27NmCm5ubIJPJhLp16wr79+/P8/DazMxM4YcffhBq1KghGBoaCnZ2dkKnTp2Ef//9V2O9NWvWCHXr1hVkMplgZWUltGzZUjh69Kj6+qysLOGLL74QbG1tBRMTEyEgIEC4f/9+vodg53XIeWEeG5WYmBhhzJgxQqVKlQRDQ0OhcuXKwsCBA4Xnz5/n2m7nzp0FAMLZs2fzfcxeV5jHJTk5WRg6dKggl8sFc3NzoU+fPkJ0dHS+UzE8e/Ys3/39+uuvAgDB3NxcSElJyXOdK1euCO+9955gY2MjyGQywc3NTejTp49w/PjxN94f5DEVgyAo32Pvv/++YGlpKRgZGQkNGzYU9u/fr7FOXq/jgqgOz//hhx+EhQsXCi4uLoJMJhOaN28uXL16VWPdvA7b37t3r+Dn5ycYGRkJ7u7uwrx584Q1a9bkOow8KipK6NKli2Bubp7rsPPExERh6tSpgpeXl2BoaCjY2toKTZo0ERYsWCCkp6er11u9erVQtWpVQSaTCTVq1BDWrl2b7/QQhXmu8/LLL78ILVq0UD9vnp6ewuTJk4X4+HiN9R49eiR8/PHHgp2dnSCTyYQqVaoIo0ePVk+rUtjPupzLAwICBLlcLhgZGQmenp7CoEGDcn3m7dy5U6hZs6Ygk8kEb29vYdeuXXl+Vjx79kzo1auXYGJiIlhZWQkjRowQgoODizUVQ2HuS2ZmpjBt2jTB0dFRMDY2Ftq0aSPcunVLsLGxEUaOHJnfw54vbU7FMHDgQI3vttd/Xp/24HVv+mzYuXOn0KxZM8HU1FQwNTUVatSoIYwePVq4c+eOxnpLlixRf943bNhQCAwMFOrXry907NhRY72QkBChXbt2gkwmExwcHIQvv/xSOHr0aKGmYijseyW/zxrVdar3i+q5zu9H5ebNm0K7du0EMzMzwdbWVhg2bJhw9erVXK+7zMxMYezYsYKdnZ0gkUg0tpHX+/S///4TAgICBDMzM8HExERo3bp1ru+It33PFUSSXQhRudGzZ09cv34d9+/fF7uUCiE0NBQeHh744YcfytW5DKn0iIuLg5WVFb777jv1JLL0ikKhgJ2dHd577z38+uuvYpdDEGHMFZEuRUZG4sCBA/joo4/ELoWIiiCvc86qZtnPeXqXiio1NTXXuKwNGzbgxYsXfHxKEXHPbEikJQ8fPkRgYCB+++03GBgYYMSIEWKXRERF8Mcff2DdunXo3LkzzMzMcObMGWzduhUdOnTIdSRgRXT+/HlMmDABvXv3ho2NDf777z+sXr0atWrVeuOcflRyGK6oXDh9+jQGDx4MV1dXrF+/Ps/5foio9PPz84O+vj7mz5+PhIQE9SD37777TuzSSgV3d3e4uLhgyZIl6oHqH3/8MebOnfvG839SyeGYKyIiIiIt4pgrIiIiIi1iuCIiIiLSonI/5kqhUODJkycwNzcXfQI6IiIiKhxBEJCYmAhnZ+diT3Jc0sp9uHry5IlOzhtEREREuhceHq4+c0lZUe7Dlep0E+Hh4To5fxARERFpX0JCAlxcXNTf42VJuQ9Xqq5ACwsLhisiIqIypiwO6SlbnZhEREREpRzDFREREZEWMVwRERERaVG5H3NVWFlZWcjIyBC7DCKtMjAwgFQqFbsMIqIKRdRwlZWVhRkzZmDTpk2IioqCs7MzBg0ahK+//lo9gE0QBEyfPh2//vor4uLi0LRpU6xYsQJVq1bVSg2CICAqKgpxcXFa2R5RaWNpaQlHR8cyOSiUiKgsEjVczZs3DytWrMD69evh4+ODy5cvY/DgwZDL5Rg3bhwAYP78+ViyZAnWr18PDw8PTJs2DQEBAbh58yaMjIyKXYMqWNnb28PExIRfQFRuCIKA5ORkREdHAwCcnJxEroiIqGIQNVydPXsW3bt3R5cuXQAoz/a9detWXLx4EYDyy2HRokX4+uuv0b17dwDAhg0b4ODggD179qBv377F2n9WVpY6WNnY2BTvzhCVQsbGxgCA6Oho2Nvbs4uQiKgEiDqgvUmTJjh+/Dju3r0LALh69SrOnDmDTp06AQAePnyIqKgotGvXTn0buVyORo0a4dy5c3luMy0tDQkJCRo/+VGNsTIxMdHWXSIqdVSvb44pJCIqGaK2XE2ZMgUJCQmoUaMGpFIpsrKy8P3332PAgAEAlF12AODg4KBxOwcHB/V1r5szZw5mzpz5VnWwK5DKM76+iYhKlqgtV9u2bcPmzZuxZcsW/Pfff1i/fj0WLFiA9evXF3mbU6dORXx8vPonPDxcixUTERERFUzUcDV58mRMmTIFffv2ha+vLz766CNMmDABc+bMAQA4OjoCAJ4+fapxu6dPn6qve51MJlOf6oanvCkcd3d3LFq0SOwyiIiIygVRw1VycjL09DRLkEqlUCgUAAAPDw84Ojri+PHj6usTEhJw4cIFNG7cuERrLU1atWqF8ePHa217ly5dwvDhw7W2PSIioopM1DFX3bp1w/fffw9XV1f4+PjgypUr+PHHHzFkyBAAyrEi48ePx3fffYeqVauqp2JwdnZGjx49xCy91BMEAVlZWdDXf/NTbGdnVwIVlay3uf9ERPQWBAHISgcyU4GMVOVvYyvAiD1FKqK2XC1duhTvv/8+Ro0ahZo1a2LSpEkYMWIEZs2apV7n888/x9ixYzF8+HA0aNAASUlJOHz4sFbmuCqLBg0ahNOnT2Px4sWQSCSQSCQIDQ3FqVOnIJFIcOjQIdSvXx8ymQxnzpxBSEgIunfvDgcHB5iZmaFBgwY4duyYxjZf7xaUSCT47bff0LNnT5iYmKBq1arYu3dvgXVt3LgR/v7+MDc3h6OjI/r376+eX0nlxo0b6Nq1KywsLGBubo7mzZsjJCREff2aNWvg4+MDmUwGJycnjBkzBgAQGhoKiUSCoKAg9bpxcXGQSCQ4deoUABTr/qelpeGLL76Ai4sLZDIZvLy8sHr1agiCAC8vLyxYsEBj/aCgIEgkEty/f7/Ax4SISKcEAchMB1ITgKRoIC4MeHYXiLwKhF8EHpwG7v4F3PwTuLYN+Hc9cOEXIHAxcGoecGwmcHgqsH8CsPtTYPtgYGs/YEMPYE0nYFUrYHljYHEdYGFNYJ478J0jMNMK+M4emOsKLKwGLPZT7oPURP233tzcHIsWLSpwvI9EIsG3336Lb7/9Vuf1CIKAlIwsne8nL8YG0kId1bV48WLcvXsXtWrVUj8mdnZ2CA0NBaA8AnPBggWoUqUKrKysEB4ejs6dO+P777+HTCbDhg0b0K1bN9y5cweurq757mfmzJmYP38+fvjhByxduhQDBgzAo0ePYG1tnef6GRkZmDVrFqpXr47o6GhMnDgRgwYNwsGDBwEAERERaNGiBVq1aoUTJ07AwsICgYGByMzMBACsWLECEydOxNy5c9GpUyfEx8cjMDDwbR7CIt//jz/+GOfOncOSJUtQu3ZtPHz4EM+fP4dEIsGQIUOwdu1aTJo0Sb2PtWvXokWLFvDy8nrr+oioghMEIOEJ8OQKEPsQyEjRbAHKTM1elgZkZv/OeTnnepmpgKAQ+Q5JAH2jUlBH6cI+kxxSMrLg/c0RUfZ989sAmBi++emQy+UwNDSEiYlJnoP6v/32W7Rv31592draGrVr11ZfnjVrFnbv3o29e/eqW4byMmjQIPTr1w8AMHv2bCxZsgQXL15Ex44d81xf1ZULAFWqVMGSJUvULY1mZmb4+eefIZfL8fvvv8PAwAAAUK1aNfVtvvvuO/zvf//DZ599pl7WoEGDNz0cubzt/b979y62bduGo0ePqudTq1Klisbj8M033+DixYto2LAhMjIysGXLllytWUREeUp8qgxSOX9eRr/5dkWhb6T8MTAG9GWAfvbvPC+r1jV69bf6ckG3y76sup3UEOB0L7kwXJUz/v7+GpeTkpIwY8YMHDhwAJGRkcjMzERKSgrCwsIK3I6fn5/6b1NTU1hYWOTq5svp33//xYwZM3D16lXExsaqD0oICwuDt7c3goKC0Lx5c3Wwyik6OhpPnjxB27Zt3+au5ult739QUBCkUilatmyZ5/acnZ3RpUsXrFmzBg0bNsS+ffuQlpaG3r17F7tWIipnXsYAkdkBKiL7d+KT3OtJpIC9N2BfAzA0fctwk08o0pcx5JQiDFc5GBtIcfPbANH2rQ2mpqYalydNmoSjR49iwYIF8PLygrGxMd5//32kp6cXuJ3XQ5BEIlEHpte9fPkSAQEBCAgIwObNm2FnZ4ewsDAEBASo96M6DUteCroOgPqIUkEQ1Mvym238be//m/YNAJ988gk++ugj/PTTT1i7di0++OADzupPVNGlxAGRQZotUnF5/dMqAeyqA851Aed6yt+OtZRBicothqscJBJJobrmxGZoaIisrMKNDQsMDMSgQYPQs2dPAMqWHNX4LG25ffs2YmJiMHfuXLi4uAAALl++rLGOn58f1q9fj4yMjFzBzdzcHO7u7jh+/Dhat26da/uqoxkjIyNRt25dANAY3F6QN91/X19fKBQKnD59WuM0Szl17twZpqamWLFiBQ4fPoy///67UPsmonIiLVE5SDxnkHrxIO91bbyyg1T2j6MfIDMr2XpJdKU/SVAu7u7uuHDhAkJDQ2FmZpbvIHMAqFq1Knbt2oVu3bpBIpFg2rRp+bZAFZWrqysMDQ2xdOlSjBw5EsHBwRpHfALAmDFjsHTpUvTt2xdTp06FXC7H+fPn0bBhQ1SvXh0zZszAyJEjYW9vj06dOiExMRGBgYEYO3YsjI2N8c4772Du3Lnw8PBAdHQ0vv7660LV9qb77+7ujoEDB2LIkCHqAe2PHj1CdHQ0+vTpA0A599qgQYMwdepUVK1atULPsUZU7qUnA1HXNYPU87sAhNzrWrlrBimn2oCRvKQrplKI4aoMmjRpEgYOHAhvb2+kpKTg4cOH+a6rmjesSZMmsLW1xRdffFHgyayLws7ODuvWrcOXX36JJUuWoF69eliwYAHeffdd9To2NjY4ceIEJk+ejJYtW0IqlaJOnTpo2rQpAGDgwIFITU3FTz/9hEmTJsHW1hbvv/+++vZr1qzB0KFDUb9+fVSvXh3z589Hhw4d3lhbYe7/ihUr8OWXX2LUqFGIiYmBq6srvvzyS411hg4ditmzZ2Pw4MHFeaiIqDTJSAWe3gCe/Ac8CVIGqWe38j7yzaIy4FxHM0yZ5P+PLVVsEiHnQJZyKCEhAXK5HPHx8blOhZOamoqHDx/Cw8Ojws6bRYXzzz//oG3btggPD891IvHSjq9zIijng3p2K3uw+X/K39E3AUVm7nXNHF6Nj3KuqwxVZvYlXnJFV9D3d2nHliuiAqSlpeHZs2eYMWMGevfuXeaCFVGFlJUJPL+j2bUXFQxkpeVe18TmtSBVF7BwKvmaqVxhuCIqwNatWzF06FDUqVMHGzZsELscInqdIguIua8ZpCKvKSfcfJ2RXDNEOdcF5C6cwoC0juGKqACDBg3CoEGDxC6DiADl9Afx4cAzVatUkHI6hPSk3OsammePkarzKkhZeTBIUYlguCIiIvEpFEBSFBD/WDlfVHx49t/hr/5Oy+dgHAMT5ZQHqhBVqR5g7QnoiXr6XKrAGK6IiEj3MlKBhIh8glM4EB8BKPKeHFiDiY2yBSpn155tNUDKrzMqPfhqJCKi4hEEIDVOs5Xp9RBVmPPpSaSARSVAXhmwdFGOh1L/7QrIKylPF0NUyjFcERFRwRRZQGJ2l118eHZwUv2dHaDSE9+8HQMTZWDSCE6ur/42d2ILFJULfBUTEVV0GSnKbrn4sFdhSR2cwpXdeXnNB/U6E9scwSmPEGVsxQHlVCEwXBERlXeq6Qpi7muOc1L9/fLZm7ehpw9YOL8WnCpn/+2q/JsnIyYCwHBVYbm7u2P8+PEYP348AOVJq3fv3o0ePXrkuX5oaCg8PDxw5coV1KlTp8j71dZ2iCgf6cnKmccjryrPkRd1DXh6M+95n3IyMH3V0qQOTq6v/jZ3AvSkJXMfiMo4hisCAERGRsLKykqr2xw0aBDi4uKwZ88e9TIXFxdERkbC1tZWq/siqpBexijDU9Q15cSZUdeBmHt5nxvPwBSwq5ZjjNNrrU/ssiPSGoYrAgA4OjqWyH6kUmmJ7au0ycjIgIGBgdhlUFkkCEDco1cBKir7d0JE3uub2innfXLyU/529AOsq3DeJ6ISwndaGbNq1So4OztDodD8z7R79+4YMmQIACAkJATdu3eHg4MDzMzM0KBBAxw7dqzA7UokEo0WposXL6Ju3bowMjKCv78/rly5orF+VlYWhg4dCg8PDxgbG6N69epYvHix+voZM2Zg/fr1+PPPPyGRSCCRSHDq1CmEhoZCIpEgKChIve7p06fRsGFDyGQyODk5YcqUKcjMfDV4tlWrVhg3bhw+//xzWFtbw9HRETNmzCjw/ly6dAnt27eHra0t5HI5WrZsif/++09jnbi4OIwYMQIODg4wMjJCrVq1sH//fvX1gYGBaNWqFUxMTGBlZYWAgADExsYCUHarLlq0SGN7derU0ahLIpFgxYoVePfdd2Fqaorvv//+jY+bypo1a+Dj46N+TMaMGQMAGDJkCLp27aqxbkZGBuzt7bF69eoCHxMqI7IylMEpaAtwaAqwtgsw1w1YXBvY9hHw93zg7uFXwcq6CuDdA2gzDRiwA/jfHWDyfeCjXUC7GUCt9wBbLwYrohLElqucBAHISBZn3wYmhWqS7927N8aOHYuTJ0+ibdu2AIAXL17g8OHDOHjwIAAgKSkJnTt3xvfffw+ZTIYNGzagW7duuHPnDlxdXd+4j6SkJHTt2hXt27fHpk2b8PDhQ3z22Wca6ygUClSuXBnbt2+HjY0Nzp49i+HDh8PJyQl9+vTBpEmTcOvWLSQkJGDt2rUAAGtrazx58kRjOxEREejcuTMGDRqEDRs24Pbt2xg2bBiMjIw0gsr69esxceJEXLhwAefOncOgQYPQtGlTtG/fPs/7kJiYiIEDB2Lp0qUQBAELFy5E586dce/ePZibm0OhUKBTp05ITEzEpk2b4OnpiZs3b0IqVY4pCQoKQtu2bTFkyBAsXrwY+vr6OHnyJLKyst74+OU0Y8YMzJ07F4sWLYK+vv4bHzcAWLFiBSZOnIi5c+eiU6dOiI+PR2BgIADgk08+QYsWLRAZGQknJ+XJZffv34/k5GR88MEHb1UblQKpCcDTG5pde89uA1npudeVGgL2NQFHX8CxdvbvWoDMvOTrJqICMVzllJEMzHYWZ99fPinU5HhWVlbo1KkTtmzZog5XO3bsgK2tLVq3bg0AqF27NmrXrq2+zaxZs7B7927s3btX3QJSkC1btkChUGD16tUwMjKCj48PHj9+jE8//VS9joGBAWbOnKm+7OHhgXPnzmHbtm3o06cPzMzMYGxsjLS0tAK7AZcvXw4XFxcsW7YMEokENWrUwJMnT/DFF1/gm2++gV72f9t+fn6YPn06AKBq1apYtmwZjh8/nm+4atOmjcblVatWwdLSEqdPn0bXrl1x7NgxXLx4Ebdu3UK1atUAAFWqVFGvP3/+fPj7+2P58uXqZT4+Pm987F7Xv39/DB48WGNZQY8bAHz33Xf43//+pxFoGzRoAABo0qQJqlevjo0bN+Lzzz8HAKxduxa9e/eGmZnZW9dHJSgxKrtb79qrbr0XD/JeVyZXhicnv+wQ5aechVzfsGRrJqIiYbgqgwYMGIBhw4Zh+fLlkMlk2Lx5M/r27asOIklJSZgxYwYOHDiAyMhIZGZmIiUlBWFhYYXa/q1bt+Dn5wcjIyP1ssaNG+da7+eff8aaNWsQFhaGlJQUpKenv/URgLdu3ULjxo0hydFq17RpUyQlJeHx48fqljY/Pz+N2zk5OSE6Ov8Zn58+fYqvv/4ap06dQnR0NLKyspCcnKx+DIKCglC5cmV1sHpdUFAQevfu/Vb3JS/+/v65lhX0uEVHR+PJkyfq4JyXTz75BKtWrcLnn3+Op0+f4tChQzhx4kSxayUtUSiUoSkq+2g91Tip/GYot6iUPS4qR5iydOPgcqIyjOEqJwMTZQuSWPsupG7dukEQBBw4cAANGjTAP//8g59++kl9/aRJk3D06FEsWLAAXl5eMDY2xvvvv4/09Dy6Goro999/x6RJk7Bw4UI0btwY5ubm+OGHH3DhwgWt7SOn1weCSySSXOPOcho4cCBiYmKwePFiuLm5QSaToXHjxurHwNi44Pl43nS9np4eBEHQWJaRkfu8aKammq2Rb3rc3rRfAPj4448xZcoUnDt3DmfPnoWHhweaN2/+xtuRDmSkKqc9yDnIPCoYyHiZe12JHmBTVbM1ytEPMLUp+bqJSKcYrnKSSMrEeauMjIzw3nvvYfPmzbh//z6qV6+OevXqqa8PDAzEoEGD0LNnTwDKlqzQ0NBCb79mzZrYuHEjUlNT1a1X58+f11gnMDAQTZo0wahRo9TLQkJCNNYxNDR84xilmjVrYufOnRAEQd16FRgYCHNzc1SuXLnQNb8uMDAQy5cvR+fOnQEA4eHheP78ufp6Pz8/PH78GHfv3s2z9crPzw/Hjx/X6MLLyc7ODpGRkerLCQkJePjwYaHqKuhxMzc3h7u7O44fP67u5n2djY0NevTogbVr1+LcuXO5uh1JR1JiNVuioq4Bz+4AQh6vcX1jwMEnR2uUH2DvDRgW/p8oIiq7GK7KqAEDBqBr1664ceMGPvzwQ43rqlatil27dqFbt26QSCSYNm1aga08r+vfvz+++uorDBs2DFOnTkVoaCgWLFiQax8bNmzAkSNH4OHhgY0bN+LSpUvw8PBQr+Pu7o4jR47gzp07sLGxgVwuz7WvUaNGYdGiRRg7dizGjBmDO3fuYPr06Zg4caK6m7Moqlatio0bN8Lf3x8JCQmYPHmyRqtQy5Yt0aJFC/Tq1Qs//vgjvLy8cPv2bUgkEnTs2BFTp06Fr68vRo0ahZEjR8LQ0BAnT55E7969YWtrizZt2mDdunXo1q0bLC0t8c0336gHw7+prjc9bjNmzMDIkSNhb2+vHnQfGBiIsWPHqtf55JNP0LVrV2RlZWHgwIFFfpwoB0EAkl8opzyIC8vx8wiIvq08NUxejK01pzxw8gOsPXmOPKIKjO/+MqpNmzawtrbGnTt30L9/f43rfvzxRwwZMgRNmjSBra0tvvjiCyQkJBR622ZmZti3bx9GjhyJunXrwtvbG/PmzUOvXr3U64wYMQJXrlzBBx98AIlEgn79+mHUqFE4dOiQep1hw4bh1KlT8Pf3R1JSEk6ePAl3d3eNfVWqVAkHDx7E5MmTUbt2bVhbW2Po0KH4+uuvi/bAZFu9ejWGDx+OevXqwcXFBbNnz8akSZM01tm5cycmTZqEfv364eXLl/Dy8sLcuXMBANWqVcNff/2FL7/8Eg0bNoSxsTEaNWqEfv36AQCmTp2Khw8fomvXrpDL5Zg1a1ahWq4K87gNHDgQqamp+OmnnzBp0iTY2tri/fff19hOu3bt4OTkBB8fHzg7i3QQRlkjCMrWp1zhKQyIzV6WV3deTpZu2a1RtV+Nk7Jw5vgoItIgEV4fOFLOJCQkQC6XIz4+HhYWFhrXpaam4uHDh/Dw8NAYvE1U2iUlJaFSpUpYu3Yt3nvvvQLXrTCvc3V4ytHi9HqISk9683bMHJWzmFu6AlZuyt/WnsogZWyp87tBREoFfX+Xdmy5IipDFAoFnj9/joULF8LS0hLvvvuu2CWVHEEAUuNetTLl9ZOe+Obt5AxPGj9u2ScfLscBlIhKBMMVURkSFhYGDw8PVK5cGevWrYO+fjl6C6vCU37BKS4MSCtE97aZwxvC05uPyCQiKo5y9MlMVP65u7vnmgKiTEmJyyc4PSp8eDK1zzs4WboqT0TM8EREImO4IiLte/EQeHASeHb3tZan+Dff1tQun/CU3fLE6QyIqJRjuALKdksA0RuUyOs7LQkIPQPcPwaEHM//tC4AYGKrGZys3F61PMldGJ6IqMyr0OFKNet3cnJyoWbGJiqLkpOVJyN/fZb7YhEE4GkwcP+4MlCFnQcUOWao19MHXBoBleq9anVSdduVgYl6iYiKo0KHK6lUCktLS/U56kxMTDTOcUdUlgmCgOTkZERHR8PS0rJQk5wW6GWMsqvv/nEg5ASQFKV5vaUb4NUW8GoHuDcHjMrWodNERNpSocMVADg6OgJAgScBJirLLC0t1a/zt5KVCURcVrZM3T8OPLkCIEcXo4GJMkSpApV1FU6mSUQEhitIJBI4OTnB3t4+zxPvEpVlBgYGb9diFReuHDN1/xjw4O/cA9AdagGebZSByrUxoC/TbsFEROVAhQ9XKlKptPjdJkRlTUYKEBr4KlA9v6t5vbGVMkx5tlX+tnASp04iojKE4YqoIhEE4NmdV0f1hQYCWWmvrpfoAZUbKLv5PNsCznUAPf7TQUT0NhiuiMq7lFjgwensQHUCSIjQvN6iMuDVRhmoPFry/HlERMXEcEVU3iiylIPPVdMkRFwGBMWr6/WNALemrwai21bjQHQiIi1iuCIqDxIis8dNHVdOl5ASq3m9XQ1lN59XG2Ww4iliiIh0huGKqCzKTAPCzmVPk3ACiL6heb1MDni2yg5UbZWnjSEiohLBcEVUFggCEBPy6qi+0DNARnKOFSTK2dBVA9Er1QekfHsTEYmBn75EpVVqAvDw71eBKi5M83ozR2WrlGcb5Y+JtTh1EhGRBoYrotIkJQ74bz1w9wgQfgFQZL66TmqonLjTq62ydcrBhwPRiYhKIYYrotIgNQG4sBI4u0xzVnRrT2VXn1dbwL0ZT3pMRFQGMFwRiSktEbjwC3B2KZAap1xm7w00GKoMVVbuYlZHRERFwHBFJIb0l8DFX4HAxUDKC+Uy2+pAqymAdw9AT0/U8oiI8iMIAmJepiP8RTLCXiTjcWwKOng7oKqDudillRoMV0QlKT0ZuLwGOPMTkPxcuczGC2g1FfDpyVPNEFGpkJKehfDYZITFJCt/v0hG+IsUhL9QXk5Oz9JY39rUkOEqB4YropKQkQr8u1YZqpKeKpdZeShbqmq9z2kTiKhEZSkERManaASmsBevQtTzpLQCby+RAI4WRnCxMoGLtQlcrU1KqPKygZ/oRLqUmQb8twH4ZyGQGKlcZukKtPwC8OvLUEVEOiEIAuKSMzRanZRdeMrLT+JSkJElFLgNcyN9uGYHJxfVj5UxXK1NUMnKGDJ9trTnh5/sRLqQmQ4EbQL+XvDqRMlyF6DFJKDOAEBqIG59RFTmpWZk4XFsCsJjk5Xjn9RdeCl4/CIZiWmZBd7eQCpBZSsTVM4OTKoQ5WptAhcrE8hN+DlVVAxXRNqUlQFc3Qqc/gGIz57009wZaPE/oO5HgL5M3PqIqMxQKAQ8TUxVtzqFq36yW5+eJhTcdQcA9uayHIHJWN0C5WptAgcLI0j1OFeeLjBcEWlDViZw7Q/g7/lAbKhymZkj0Px/QL2PAQMjUcsjotIpITUDYTGvuutyBqnHcSlIz1QUeHtTQ+mr8KT+rWyJqmxlAiMDdt2JgeGKqDgUWcD1HcDpucCLB8plpnZAs4mA/2DAwFjc+oioVBAEAZHxqQgKj8PV8DgEhcfhdlQi4lMyCrydVE+CSpbGGoEpZ5CyMjGAhGdqKHUYroiKQpEF3NgNnJ4HPL+rXGZiAzQdr5wAlDOpE1Vo8SkZuP44Hlcfx+FKWByuPo7Ds8S8u/FszQxzhCZj9ZgnF2sTOMmNoC/lvHdlDcMV0dtQKIBbfwKn5gLPbiuXGVsBTT8DGgwDZGbi1kdEJS49U4HbUQkIym6Ruhoeh5BnL3OtJ9WToIajOWq7WKKOiyVqOcvhZmMCUxm/issbUZ9Rd3d3PHr0KNfyUaNG4eeff0arVq1w+vRpjetGjBiBlStXllSJREqCANzeD5ycA0TfUC4zkgNNxgINRwBGFuLWR0QlQhAEhMYkq7v2gsLjcPNJAtKzco+NcrU2QW0XS9SuLEddV0t4O8lhbMgxUBWBqOHq0qVLyMp6NctrcHAw2rdvj969e6uXDRs2DN9++636sokJJyqjEiQIwN3DwMnZQNQ15TKZBdB4NPDOp8qARUTl1vOkNFxVjZN6HI+r4XF5jpOyNDFA7crKFqk6LpbwqyyHjRmPDq6oRA1XdnZ2Gpfnzp0LT09PtGzZUr3MxMQEjo6OJV0aVXSCANw7CpyaDTy5olxmaKYMVI1HK7sCiahcSUnPQvATZYC6kh2oHsem5FrPUF8PtZwt1N17dVws4WptwoHlpFZqOnrT09OxadMmTJw4UeMFunnzZmzatAmOjo7o1q0bpk2bxtYr0h1BAEJOKFuqIi4rlxmYAo1GKLsATazFrY+ItCJLIeBedGJ2954yUN15mogsheas5RIJ4GlnhjoulsowVdkS1R3NYajPQeaUv1ITrvbs2YO4uDgMGjRIvax///5wc3ODs7Mzrl27hi+++AJ37tzBrl278t1OWloa0tJeHZGRkJCgy7KpvBAE4OHfylAVfl65TN8YaDhMOVjd1Fbc+oioyFTTIOQcJ3U9Ij7XyYcB5aSbqiBV18UStSrLYWHEmcrp7UgEQSj45EIlJCAgAIaGhti3b1++65w4cQJt27bF/fv34enpmec6M2bMwMyZM3Mtj4+Ph4UFBx1THkLPKAeqPzqjvKxvBPgPBZqNB8zsRS2NiN5eQmoGroUrp0FQham8pkEwNZTCt7IcdVysUMdFjtoulnCSc2660iIhIQFyubxMfn+XinD16NEjVKlSBbt27UL37t3zXe/ly5cwMzPD4cOHERAQkOc6ebVcubi4lMknh3Qs7Dxw8ntlixUASA2B+oOBZhMACydxayOiQlFNg5BznFRhpkGo42IJTzsznv6lFCvL4apUdAuuXbsW9vb26NKlS4HrBQUFAQCcnPL/4pPJZJDJeIQGFSD8knKgesgJ5WU9A6D+QOWs6vJK4tZGRPlSKASExrzEtcfxb5wGwcXaGHVcrFC7shx1XCzh48xpEKjkiB6uFAoF1q5di4EDB0Jf/1U5ISEh2LJlCzp37gwbGxtcu3YNEyZMQIsWLeDn5ydixVRmRfwHnJoD3PtLeVlPH6j7IdB8EmDpIm5tRKRBoRDw6EUyrj2OQ3BEPK5HxONGRAIS0zJzrauaBkE1TorTIJDYRA9Xx44dQ1hYGIYMGaKx3NDQEMeOHcOiRYvw8uVLuLi4oFevXvj6669FqpTKrMiryjFVdw8pL0ukQJ1+QIvJgJW7qKURkXLA+aOYZFyLiFcGqcfK33kFKZm+HrydLdRde7UrW8LNhtMgUOlSKsZc6VJZ7rOlYooKVrZU3d6vvCzRA/z6Ai0mATZ5HxBBRLqlClLXs4PUtcfxCH4Sj8TUvINUTScL+FWWo1YlOXwryVHV3ozn2qsgyvL3t+gtV0RaF31Lee6/m3uyF0gA395Ayy8AWy8xKyOqUARBQNgLZZC6nqNFKiGPIGWoClLZIcq3shxe9mYwYJCiMojhisqPZ3eB0/OA4J0ABAASwKenMlTZ1xC7OqJyTRAEhL9IwfWIeFyLUI6TCo5IyPNUMaog5VvJQhmkKlmiqgODFJUfDFdU9mWmKbv/AhcDQvZRQ97dgZZTAAdvcWsjKocEQcDj2Owgld0adT0iPu8gJdVDTSdzdbeeb2U5qjmYM0hRucZwRWVb1HVg90jgabDycvXOQOsvAUdfcesiKidUQSo4Iv7VgPOIeMQl5x2kamQHKb9KynFS1Rx4qhiqeBiuqGzKygTOLlYeBajIAExsgW6LgJrdxK6MqMwSBAERcSm4/jhePU4qOCIesXkEKQOpBDUcLeBbObtFikGKSI3hisqemBBla9Xji8rL1bsA3RYDZnbi1kVUhqiClKolStW9l1+Qqu5oDt9Klq+ClKMZZPqclJMoLwxXVHYIAnDpN+DoN0BGMiCzADrNA2r3U566nojy9DItE1EJqbj3NEmje+/Fy/Rc6+rrKYNUzukPqjuaM0gRvQWGKyob4iOAP0cDD04qL3u0ALov58zqVKEJgoDY5AxExaciKiEFUfFpiIpPQVRCKiLjU/E0+3dec0gBr4KUb/b4KL/KDFJE2sBwRaWbIADXtgEHJwNp8YC+EdD+W6DBMECPYzuo/MrMUuBZUpoyOMWnIirh1e/IHMvSM3OfVy8vZjJ9uFqbKINUZeWA8+qO5jAyYJAi0jaGKyq9Xj4H9k8Abu1VXq5UH+ixErCrJm5dRMWUmpGFqHjN1iXl7xREJShbn54lpkFRyPNn2JgawlFuBEcLI83fciM4yY3gYGEEcyMD3d4pIlJjuKLS6fZBYN844OUz5QmWW04Bmk0ApHzJUuklCAISUjKzW5dSXgtOr1qb8prGIC/6ehI4WBjBwUIGJ7kxHCyyw1J2aHK0MIK9hYzdeESlDL+pqHRJTQAOTwWCNikv29UEeq4EnOuIWhZRlkJATFJarm6517vtUjKyCrU9YwOpulUpZ2BSXXaUG8HWVAY9PR6sQVTWMFxR6fHwH2DPKCA+DIAEaDIWaP0VYGAkdmVUAb14mY6/bkTh8I0o3I1KRHRiGjIL2U9nZWKgEZIcLYzhKJfBUW6s7rKzMNKHhEe5EpVLDFckvowU4Pi3wPnlysuWbsrWKrcm4tZFFc7zpDQcuRGFg9cjcf7BC2S9Fqb0JIC9eXYrU45xTa+PdeIgcaKKjeGKxBXxH7B7BPD8rvJy/UFAh+8AmbmoZVHFEZ2YiiPBUTh4PQoXHsZoDCKvVckCnWo5obGnDZzlxrA1M4Q+z4lHRG/AcEXiyMoA/l4A/P0DIGQBZo7Au0uBah3ErowqgKcJqTh0PRIHg6NwKfQFhByBqnZlOTr5OqFzLSe42piIVyQRlVkMV1Tyom8rW6sig5SXfd4DuiwETKxFLYvKtydxKTgUHIVD1yNx+VGsxnV1XS3RuZYTOtZyhIs1AxURFQ/DFZUchUI5rur4t0BWGmBkqQxVvu+LXRmVU49jk3HoehQOBkfiSlicxnX13azQ2VcZqCpZGotTIBGVSwxXVDJiHymPBHx0RnnZq72yG9DCSdy6qNwJi0nGoeBIHLweiauP49XLJRKggZs1Ovs6omMtJzjKeRQqEekGwxXpliAAVzYq565KTwIMTIGA75UD13kYOmlJ6POXOJgdqIIjEtTL9SRAQw9rZQuVjyPsLRioiEj3GK5IdxKfAnvHAveOKC+7NgZ6rACsPcSti8qFkGdJykHp16NwM1IzUDX2tEGnWk4I8HGEnblMxCqJqCJiuCLduLFHeV7AlBeA1BBoMw1oPBrQ4/w/VHT3oxNx4FoUDgVH4nZUonq5VE+CJp426OzrhA7eDrAxY6AiIvEwXJF2pcQCBycD17crLzv6AT1/ARy8xa2LyiRBEHD3aRIOXI/EoeuRuBedpL5OX0+Cpl626OLrhPbeDrAyNRSxUiKiVxiuSHvuHwP+HAMkRgISKdB8ItDic0CfX3pUeIIg4FZkIg4FR+LA9Ug8ePZSfZ2BVILmVe3QqZYjOng7Qm5iIGKlRER5Y7ii4ktLAo5OAy6vUV628VK2VlX2F7cuKjMEQcCNJwk4eD0Sh4Kj8PD5q0BlKNVDi2p26OzriLY1HSA3ZqAiotKN4YqKJ+w8sHskEPtQebnRSKDtdMCQEzFSwQRBwLXH8TgYHIlD16MQ9iJZfZ1MXw+tqtuhs68T2tSwh7kRAxURlR0MV1Q0mWnAydnA2SWAoAAsKgM9fgaqtBK7MirFBEFAUHgcDmYf5RcRl6K+zshAD62r26OzrxNa17CHmYwfT0RUNvHTi95e1HVg1wgg+obycu3+QKe5gJFc3LqoVFIoBFwJj8WBa1E4HByJJ/Gp6uuMDaRoU9MenWs5oXUNO5gY8iOJiMo+fpJR4WVlAmcXAyfnAIoMwMQW6LYYqNlV7MqolFEoBFx+FIuD1yNxODgKUQmvApWpoRRtazqgs68jWlazh7Ehp+cgovKF4YoKJyZEebLlx5eUl2t0BbouAszsRC2LSg9VoDpw7QkOBUchOjFNfZ25TB/tvB3QqZYjWlSzg5EBAxURlV8MV1QwhQK4vBo4+g2QkQzILIBO84HafXn6GkKWQsDl0Bfqo/w0ApWRPtp7O6CLrxOaVbWFTJ+BiogqBoYryl98BPDnaODBSeVlj5ZA958BSxdx6yJRqQLVgexA9ey1QNXB2xFd/BzR1IuBiogqJoYryk0QgGt/AAc/B9LiAX1joP23QINPAD09sasjEWQpBFzK0UKVV6Dq6ueEpl62MNTna4SIKjaGK9L08jmwfzxwa5/yciV/5YSgtl6ilkUlr6BAZWGkjw4+jujiy0BFRPQ6hit65fZBYN844OUzQE8faDUFaDoBkPJlUlFkKQRcfKgMVIdvMFARERUFvzUJSE0ADk8FgjYpL9t7Az1XAk61xa2LSkTOQHUoOArPkzQDVYCPIzr7OaGpJwMVEVFhMFxVdDEhwIbuQHw4AAnQdBzQ+itAXyZ2ZaRDqkB14PoTHA5+qhGo5MYG6ODtwEBFRFREDFcV3T8/KoOVlTvQYyXg1ljsikhHshQCLjyMyZ7YM+9A1cXPCU0YqIiIioXhqiLLTAduZw9c776cwaocUgWqA9ciceRGFJ4npauvkxsbIMDHAZ19GaiIiLSJ4aoie3AKSI0HzBwA13fEroa0JDNLkd3lV3CgauplCwMpAxURkbYxXFVkN/cof3t3B/Q42WNZpgpU+69H4khwFGJevgpUliYGCPBWDkpv4mnDQEVEpGMMVxVVZjpwa7/yb5+e4tZCRZKZpcAFVQsVAxURUanBcFVRPTipnH3dzBFwYZdgWVGYQNXFzwmNGaiIiETDcFVR3dij/O3dnae0KeUysxQ4/0AZqP66kTtQdfRxRGdfBioiotKC4aoiykwDbh9Q/s0uwVIpZ6A6ciMKL3IEKisTA+XEngxURESlEsNVRRSS3SVo7gS4NBK7GsrhcWwyVp4OwcHruQNVx1rKQPVOFQYqIqLSjOGqItI4SpBf0qVBfHIGfj51H+vOhiI9UwGAgYqIqKxiuKpo2CVYqqRmZGHjuUdYdvI+4lMyAACNq9jg01aeaOJpA30GKiKiMofhqqIJOQGkJQDmzkDlhmJXU2EpFAL+vBqBBUfuIiIuBQBQ3cEcUzrXQKtqdpBIJCJXSERERcVwVdHwKEHRnbn3HHMO3cKNJwkAAEcLI0zsUA296lWGVI+hioiorGO4qkgyUoE7B5V/s0uwxN18koC5h2/j77vPAADmMn2MbOWJIU09YGzIGfKJiMoLhquKRKNLsIHY1VQYEXEpWPjXHey+EgFBAAykEnz4jhvGtqkKa1NDscsjIiItY7iqSFRHCfr0YJdgCYhPycDyU/exNvDVEYBd/ZwwOaA63GxMRa6OiIh0heGqoshIBW6zS7AkpGW+OgIwLll5BGAjD2t82bkmartYilscERHpHMNVRRFyHEhPBCwqAZX8xa6mXFIoBOy79gQ/HLmDx7HKIwCrOZhhSqcaaF3dnkcAEhFVEAxXFcWN3crf3j3YJagDgfeVRwAGRyiPAHSwkOF/7aujV30eAUhEVNEwXFUEGSnAnUPKv9klqFW3IhMw99BtnM4+AtBMpo9PeQQgEVGFxnBVEdw/DqQnAXIXoDK7BLXhSVwKfjx6Fzv/ewxBAPT1VEcAesHGTCZ2eUREJCKGq4pA3SXYHeC4n2KJT8nAytMhWHPmIdKyjwDs4ueEyR2qw92WRwASEREg6uAbd3d3SCSSXD+jR48GAKSmpmL06NGwsbGBmZkZevXqhadPn4pZctmTkQLcPaz8m12CRZaWmYXVZx6i5Q8nseJUCNIyFWjoYY3do5rg5/71GKyIiEhN1JarS5cuISsrS305ODgY7du3R+/evQEAEyZMwIEDB7B9+3bI5XKMGTMG7733HgIDA8Uquey5f+xVl2Cl+mJXU+aojgBc8NcdhL9QHgHoZW+GKR1roG1NHgFIRES5iRqu7OzsNC7PnTsXnp6eaNmyJeLj47F69Wps2bIFbdq0AQCsXbsWNWvWxPnz5/HOO++IUXLZwy7BIjsb8hxzDt7G9Yh4AIC9uQwT21fD+/UrQ1/KIy6JiChvpWbMVXp6OjZt2oSJEydCIpHg33//RUZGBtq1a6dep0aNGnB1dcW5c+cYrgojIwW4o+oSfE/cWsqQ21EJmHfoNk7eeXUE4IgWVTC0uQdMDEvNW4aIiEqpUvNNsWfPHsTFxWHQoEEAgKioKBgaGsLS0lJjPQcHB0RFReW7nbS0NKSlpakvJyQk6KLcsuHeUSDjJSB3BSrVE7uaUi8yPgU//nUXO3IcATigkSvGtq0KWx4BSEREhVRqwtXq1avRqVMnODs7F2s7c+bMwcyZM7VUVRmn6hL0YZdgQRJSM7DyVAhW5zwC0NcJkwKqw4MD1YmI6C2VinD16NEjHDt2DLt27VIvc3R0RHp6OuLi4jRar54+fQpHR8d8tzV16lRMnDhRfTkhIQEuLi46qbtUS08G7h5R/s2jBPOUnqnA5guPsOT4PcRmnwOwobs1pnSugXquViJXR0REZVWpCFdr166Fvb09unTpol5Wv359GBgY4Pjx4+jVqxcA4M6dOwgLC0Pjxo3z3ZZMJoNMxi4c3M/uErR0BZzZJZiTIAjYfy0SPxy5g7AXyQAATztTTOlUE+14BCARERWT6OFKoVBg7dq1GDhwIPT1X5Ujl8sxdOhQTJw4EdbW1rCwsMDYsWPRuHFjDmYvjJznEmRYUDsXEoO5h27h6mPlEYB25jJMaFcNffx5BCAREWmH6OHq2LFjCAsLw5AhQ3Jd99NPP0FPTw+9evVCWloaAgICsHz5chGqLGPYJZjL3aeJmHvoNk7cjgYAmBpKMaKlJz7hEYBERKRlEkEQhLe90cmTJ9G6dWtd1KN1CQkJkMvliI+Ph4WFhdjllIwbe4DtAwFLN+CzqxW65SoqPhU/Hb2L7f+GQ5F9BGC/hq4Y17Yq7MzZfUxEVFqV5e/vIv3L3rFjR1SuXBmDBw/GwIEDK+aA8dJMfZRgjwobrBJSM/DLaeURgKkZyiMAO9VyxOSA6qhiZyZydUREVJ4VaZBJREQExowZgx07dqBKlSoICAjAtm3bkJ6eru366G2lv6zQXYLpmQqsC3yIVj+cws8nQ5CaoYC/mxV2ftoEKz6sz2BFREQ6V6RuwZz+++8/rF27Flu3bgUA9O/fH0OHDkXt2rW1UmBxleVmxSK5sRvYPgiwcgfGBVWYlitBEHDwehTmH7mNRzHKIwCr2JliSscaaO/twCMAiYjKmLL8/V3skbz16tWDo6MjbGxsMHfuXKxZswbLly9H48aNsXLlSvj4+GijTiqsCniUYJZCwDd/BmPzhTAAgK2ZDBPaV8UH/i48ApCIiEpckb95MjIysGPHDnTu3Blubm44cuQIli1bhqdPn+L+/ftwc3ND7969tVkrvUn6S+DuX8q/K0iXYGpGFsZs+Q+bL4RBIgHGtfHC6cmtMKCRG4MVERGJokgtV2PHjsXWrVshCAI++ugjzJ8/H7Vq1VJfb2pqigULFhT7VDb0lu4eATJTACsPwKl0dMvqUkJqBoZvuIzzD17AUKqHRX3roLOvk9hlERFRBVekcHXz5k0sXboU7733Xr6zodva2uLkyZPFKo7ekvoowZ7lvkswOjEVg9Zcws3IBJjJ9LHqo/po4mUrdllERERFC1fHjx9/84b19dGyZcuibJ6KIi0JuKfqEuwhaim69ijmJT5afRFhL5Jha2aIdYMbolYludhlERERASjimKs5c+ZgzZo1uZavWbMG8+bNK3ZRVAT3jgCZqYB1FcDRT+xqdCY4Ih69VpxF2ItkuFqbYMfIJgxWRERUqhQpXP3yyy+oUaNGruU+Pj5YuXJlsYuiIqgAXYJnQ56j76rzeJ6UDm8nC+z4tDHcbU3FLouIiEhDkboFo6Ki4OSUe+CwnZ0dIiMji10UvaW0JODeUeXf3j1ELUVXDl6PxPjfg5CepcA7Vayx6mN/WBgZiF0WERFRLkVquXJxcUFgYGCu5YGBgTxCUAx3D2d3CXoCjr5iV6N1m84/wugt/yE9S4GOPo5YN7ghgxUREZVaRWq5GjZsGMaPH4+MjAy0adMGgHKQ++eff47//e9/Wi2QCqGcdgkKgoDFx+9h0bF7AID+jVwxq3stSPXKz30kIqLyp0jhavLkyYiJicGoUaPU5xM0MjLCF198galTp2q1QHqDtMRXXYLl6CjBLIWA6XuDsem8ctb1cW2rYkK7qjyNDRERlXrFOrdgUlISbt26BWNjY1StWjXfOa/EVJbPTVQo13cAO4cCNl7AmMvlouUqLTMLE/4IwsHrUZBIgG/f9cFHjd3FLouIiEpQWf7+Lta5Bc3MzNCgQQNt1UJFUc66BBNTMzB8w7849yAGhlI9/PRBHXTx46zrRERUdhQ5XF2+fBnbtm1DWFiYumtQZdeuXcUujAohNaFcHSX4LDENg9ZexI0nCTA1lOLXj/056zoREZU5RTpa8Pfff0eTJk1w69Yt7N69GxkZGbhx4wZOnDgBuZwTOpaYu4eBrDTApirg4CN2NcUSFpOM91eexY0nCbAxNcTvwxszWBERUZlUpHA1e/Zs/PTTT9i3bx8MDQ2xePFi3L59G3369IGrq6u2a6T83Nij/F3GuwRvPInHeyvO4lFMMlysjbHj0ybwrcyQTkREZVORwlVISAi6dOkCADA0NMTLly8hkUgwYcIErFq1SqsFUj5SE4D7Zf8owXMhMej7y3k8T0pDTScL7BzZBB6cdZ2IiMqwIoUrKysrJCYmAgAqVaqE4OBgAEBcXBySk5O1Vx3l784hICsdsK0G2HuLXU2RHA6OxMA1F5GYlolGHtb4Y8Q7sLcwErssIiKiYinSgPYWLVrg6NGj8PX1Re/evfHZZ5/hxIkTOHr0KNq2bavtGikvN/cof5fRLsHNFx5h2p5gKAQgwMcBi/vWhZGBVOyyiIiIiq1I4WrZsmVITU0FAHz11VcwMDDA2bNn0atXL3z99ddaLZDykBoP3D+m/LuMHSUoCAKWHL+Pn47dBQD0a+iK73pw1nUiIio/3jpcZWZmYv/+/QgICAAA6OnpYcqUKVovjAqg7hKsDtjXFLuaQstSCJi57wY2nHsEABjXxgsT2lfjrOtERFSuvPWYK319fYwcOVLdckUiKINHCaZlZmHc71ew4dwjSCTAzHd9MLFDdQYrIiIqd4o0oL1hw4YICgrScilUKClxQMhx5d9l5CjBpLRMDFl3CQeuRcJAKsGSvnUxsIm72GURERHpRJHGXI0aNQoTJ05EeHg46tevD1NTzUPn/fz8tFIc5UHVJWhXo0x0CT5PUs66HhyhnHX9l4/80awqJwclIqLyq0jhqm/fvgCAcePGqZdJJBIIggCJRIKsrCztVEe55TxKsJQLf5GMj1ZfQGhMMmxMDbF2cAP4VbYUuywiIiKdKlK4evjwobbroMJIiQPuZ3cJlvKjBG8+ScDAtRfxLDENla2MsXFoI04OSkREFUKRwpWbm5u266DCuHMQUGQAdjUB+xpiV5Ov8w9iMGz9ZSSmZaKGoznWD2kIB04OSkREFUSRwtWGDRsKvP7jjz8uUjH0BjmPEiylDgdHYdzvV5CeqUBDD2v8+rE/5MYGYpdFRERUYiSCIAhveyMrKyuNyxkZGUhOToahoSFMTEzw4sULrRVYXAkJCZDL5YiPj4eFhYXY5RRdSizwQ1Vly9Xoi4BddbErymXrxTB8tfs6FALQwdsBS/px1nUiIiqasvz9XaSWq9jY2FzL7t27h08//RSTJ08udlGUh9vZXYL23qUuWAmCgGUn7mPhUeWs630buOC7HrWgLy3STB9ERERlWpHCVV6qVq2KuXPn4sMPP8Tt27e1tVlSubFb+buUdQkqsmddX5896/qY1l74XwfOuk5ERBWX1sIVoJy9/cmTJ9rcJAHKLsEHJ5V/l6KjBNMys/C/bVex/1okJBJgeldvDGrqIXZZREREoipSuNq7d6/GZUEQEBkZiWXLlqFp06ZaKYxyuH0AUGQC9j6AXTWxqwGgnHV95MZ/ceb+cxhIJVjYpw7ere0sdllERESiK1K46tGjh8ZliUQCOzs7tGnTBgsXLtRGXZRTKesSfJ6UhsFrL+F6RDxMDKX45aP6aF7VTuyyiIiISoUihSuFQqHtOig/yS+AB6eUf5eCcwmGv0jGx2su4uHzl7A2NcQ6zrpORESkQatjrkgHVF2CDrUA26qilnIrMgED11xEdGIaKlkaY+PQhqhiZyZqTURERKVNkY6V79WrF+bNm5dr+fz589G7d+9iF0U5qLsEe4haxoUHMejzyzlEJ6ahhqM5do1qwmBFRESUhyKFq7///hudO3fOtbxTp074+++/i10UZUt+ATw8rfzbW7zxVkduROGjNReRmJqJBu5W+GNEY57OhoiIKB9F6hZMSkqCoaFhruUGBgZISEgodlGU7fb+7C5BX8DWS5QSfr8Yhi+zZ11vV9MBy/pz1nUiIqKCFKnlytfXF3/88Ueu5b///ju8vb2LXRRlE7FLUBAE/HzyPqbsUgarPv6VsfLDegxWREREb1Cklqtp06bhvffeQ0hICNq0aQMAOH78OLZu3Yrt27drtcAKK/kF8CC7S7CEp2BQKAR8u/8m1p0NBQCMbu2JSR2qc9Z1IiKiQihSuOrWrRv27NmD2bNnY8eOHTA2Noafnx+OHTuGli1barvGiunWPkDIAhx9ARvPEttteqYC/9t+FfuuKmfan97NG4M56zoREVGhFXkqhi5duqBLly7arIVyEmHi0KS0THy66V/8c+859PUkWNinNrrXqVRi+yciIioPihSuLl26BIVCgUaNGmksv3DhAqRSKfz9/bVSXIX1MgZ4mH3UZQmdSzAmKQ2D113CtcfKWddXflgfLapx1nUiIqK3VaQB7aNHj0Z4eHiu5RERERg9enSxi6rwbmd3CTrVLpEuwfAXyei98hyuPY6HlYkBtgx7h8GKiIioiIrUcnXz5k3Uq1cv1/K6devi5s2bxS6qwlN1CZZAq9WjmJfovfKcetb1DUMbwpOTgxIRERVZkVquZDIZnj59mmt5ZGQk9PV5Rp1iefn8VZdgCUzBsOJUCKIT01DNwQw7P23CYEVERFRMRQpXHTp0wNSpUxEfH69eFhcXhy+//BLt27fXWnEV0q19gKAAnOoA1lV0uqv0TAUOBUcBAGZ084GjnLOuExERFVeRmpkWLFiAFi1awM3NDXXr1gUABAUFwcHBARs3btRqgRVOCU4cGnj/OeJTMmBrJkOjKjY63x8REVFFUKRwValSJVy7dg2bN2/G1atXYWxsjMGDB6Nfv34wMDDQdo0VR9IzIPQf5d8lMN5KNZdVF19HSPU4QSgREZE2FHmAlKmpKZo1awZXV1ekp6cDAA4dOgQAePfdd7VTXUVzO7tL0LkuYK3biTtTM7Lw103luLlutZ11ui8iIqKKpEjh6sGDB+jZsyeuX78OiUQCQRA0To2SlZWltQIrlBI8SvDUnWdISsuEk9wI9VytdL4/IiKiiqJIA9o/++wzeHh4IDo6GiYmJggODsbp06fh7++PU6dOabnECiLpGRB6Rvl3CYy32n9N2SXY1c8JeuwSJCIi0poitVydO3cOJ06cgK2tLfT09CCVStGsWTPMmTMH48aNw5UrV7RdZ/l3a292l2A9wMpdp7tKTs/E8VvRAICufuwSJCIi0qYitVxlZWXB3NwcAGBra4snT5StIG5ubrhz5472qqtISvAowWO3opGSkQVXaxP4VZbrfH9EREQVSZFarmrVqoWrV6/Cw8MDjRo1wvz582FoaIhVq1ahShXdzs1ULiVFA48ClX+XwHir/VdfdQnmHCtHRERExVeklquvv/4aCoUCAPDtt9/i4cOHaN68OQ4ePIglS5a81bYiIiLw4YcfwsbGBsbGxvD19cXly5fV1w8aNAgSiUTjp2PHjkUpu/RSdQlWqg9Yuel0VwmpGTh15xkAHiVIRESkC0VquQoICFD/7eXlhdu3b+PFixewsrJ6q5aQ2NhYNG3aFK1bt8ahQ4dgZ2eHe/fuwcpK8+i1jh07Yu3aterLMpmsKGWXXjf2KH+XQKvV0RtPkZ6lgJe9GWo4mut8f0RERBWN1k4EaG1t/da3mTdvHlxcXDSCk4dH7vmdZDIZHB0di1VfqZX4tESPEtx3jV2CREREulSkbkFt2bt3L/z9/dG7d2/Y29ujbt26+PXXX3Otd+rUKdjb26N69er49NNPERMTk+8209LSkJCQoPFTqt3aC0AAKvkDlq463VXsy3ScufccAI8SJCIi0hVRw9WDBw+wYsUKVK1aFUeOHMGnn36KcePGYf369ep1OnbsiA0bNuD48eOYN28eTp8+jU6dOuU7UemcOXMgl8vVPy4uLiV1d4pG1SVYAq1Wh29EIVMhoKaTBbzszXS+PyIioopIIgiCINbODQ0N4e/vj7Nnz6qXjRs3DpcuXcK5c+fyvM2DBw/g6emJY8eOoW3btrmuT0tLQ1pamvpyQkICXFxcEB8fDwsLC+3fieJIjAIW1gAgAOOv67zlasBv5xF4Pwafd6yOUa28dLovIiKi4khISIBcLi+d399vIGrLlZOTE7y9vTWW1axZE2FhYfnepkqVKrC1tcX9+/fzvF4mk8HCwkLjp9S6md0lWLmBzoPVs8Q0nAtRdqd29WWXIBERka6IGq6aNm2aa9LRu3fvws0t/+kIHj9+jJiYGDg5Oem6PN27uUf526enznd1KDgSCgGo7WIJVxsTne+PiIioohI1XE2YMAHnz5/H7Nmzcf/+fWzZsgWrVq3C6NGjAQBJSUmYPHkyzp8/j9DQUBw/fhzdu3eHl5eXxnQQZVJCJPAouzvUu7vOd7cve+LQbn7lIJQSERGVYqKGqwYNGmD37t3YunUratWqhVmzZmHRokUYMGAAAEAqleLatWt49913Ua1aNQwdOhT169fHP//8U/bnulIdJVi5ISCvrNNdRcan4FJoLACgC8MVERGRTmltnqui6tq1K7p27ZrndcbGxjhy5EgJV1RC1EcJ6r5L8MC1SABAA3crOMmNdb4/IiKiikzUlqsKK+EJEJZ9NGRJdAlmhyue7oaIiEj3GK7EoDpK0KURIK+k012FxSTjangc9CRAp1rsEiQiItI1hisxlOBRgvuvKweyN/a0gZ15GR+nRkREVAYwXJW0nF2CNd/V+e72XVV2CfJ0N0RERCWD4aqk3fxT+dvlHZ13Cd6PTsKtyATo60nQ0aecnviaiIiolGG4KmkleJTg/mvKLsFmVW1hZWqo8/0RERERw1XJio8Aws8r//bWbZegIAg5Jg5llyAREVFJYbgqSaouQdfGgIVuA8/tqESEPHsJQ6ke2vs46HRfRERE9ArDVUkqyaMEs7sEW1W3g4WRgc73R0REREoMVyUl/jEQfgGAROdHCSq7BLOPEuTEoURERCWK4aqkaHQJ6nYyz+sR8Qh7kQxjAyna1bTX6b6IiIhIE8NVSbmxW/m7BLoEVQPZ29S0h4mh6KePJCIiqlAYrkpCXDjw+BIAic6PElQoBPWJmnmUIBERUcljuCoJqi5BtyaAuW4n8/wvLBZP4lNhJtNHq+p2Ot0XERER5cZwVRJKsEtwf3arVQdvBxgZSHW+PyIiItLEcKVrcWFAxGWUxFGCWQoBB66rjhLU7aB5IiIiyhvDla6puwSbAua6nczzwsMYPEtMg9zYAM282CVIREQkBoYrXVN3CfbQ+a5Uc1t19HGEoT6fWiIiIjHwG1iXYh8BEf8CEj2ddwlmZClwODj7KEFOHEpERCQahitdKsEuwcD7zxGbnAEbU0O8U8Vap/siIiKi/DFc6VIJdgmqjhLs7OsEfSmfViIiIrHwW1hXYh8BT/4rkS7BtMwsHLkRBQDo6sejBImIiMTEcKUrN/cof7s1Bcx0e36/v+8+R2JqJhwsZGjgzi5BIiIiMTFc6YoI5xLs4usMPT2JzvdHRERE+WO40oXYUODJlRLpEkxJz8KxW08BAN04cSgREZHoGK504cYe5W/3ZoCZbifzPHE7GsnpWahsZYw6LpY63RcRERG9GcOVLpTouQSVXYJd/ZwhkbBLkIiISGwMV9r24iEQGVQiXYJJaZk4cTsaAI8SJCIiKi0YrrRNdZSge3PA1Fanuzp28ynSMhWoYmsKH2cLne6LiIiICofhSttEOEqwq58TuwSJiIhKCYYrbXrxAIi8CkikQM1uOt1VfHIG/r73DADPJUhERFSaMFxpk+ooQQ/ddwkeuRGFjCwB1R3MUdXBXKf7IiIiosJjuNKmkuwSzD5KkHNbERERlS4MV9oSEwJEXVN2CdbQbZdgTFIazobEAFBOwUBERESlB8OVtqiOEqzSEjC10emuDgVHIUshwLeSHO62pjrdFxEREb0dhittUXUJevfQ+a5yHiVIREREpQvDlTbEhABR10vkKMGnCam4GPoCANCF4YqIiKjUYbjSBlWrVZVWgIm1Tnd14FokBAGo52qJylYmOt0XERERvT2GK21QTcHg00Pnu9qvPkqQA9mJiIhKI4ar4np+H3h6HdDTB2p01emuHscm47+wOEgkQGdfdgkSERGVRgxXxXWzZLsEAaCRhzUcLIx0ui8iIiIqGoar4lJ1CZbEUYLXVEcJskuQiIiotGK4Ko5nd4Gnwdldgl10uquHz18iOCIBUj0JOtVy1Om+iIiIqOgYropDPXFoa513Ce7PntuqiacNbMxkOt0XERERFR3DVXGU6FGCyvFWPEqQiIiodGO4Kqpnd4DoG4Cegc67BO8+TcSdp4kwkEoQ4M0uQSIiotKM4aqoVK1Wnq0BYyud7krVJdiymh3kJgY63RcREREVD8NVUcU9Uv7W8VGCgiBgX3aXII8SJCIiKv30xS6gzOqxHGj9FWAk1+lubjxJwMPnLyHT10M7bwed7ouIiIiKj+GqOOSVdL4L1dxWbWrYw0zGp4uIiKi0Y7dgKSYIAvZf5VGCREREZQnDVSl2JTwOEXEpMDGUonV1e7HLISIiokJguCrFVK1W7b0dYGwoFbkaIiIiKgyGq1JKoRBw4DrPJUhERFTWMFyVUpdCX+BpQhrMjfTRopqt2OUQERFRITFclVKqowQDfBwh02eXIBERUVnBcFUKZWYpcOh6FAAeJUhERFTWMFyVQucexCDmZTqsTAzQxNNG7HKIiIjoLTBclUKqowQ7+TrBQMqniIiIqCwR/Zs7IiICH374IWxsbGBsbAxfX19cvnxZfb0gCPjmm2/g5OQEY2NjtGvXDvfu3ROxYt1Kz1TgULDqXIJOIldDREREb0vUcBUbG4umTZvCwMAAhw4dws2bN7Fw4UJYWVmp15k/fz6WLFmClStX4sKFCzA1NUVAQABSU1NFrFx3ztx/hoTUTNiZy9DIg12CREREZY2oJ6ubN28eXFxcsHbtWvUyDw8P9d+CIGDRokX4+uuv0b17dwDAhg0b4ODggD179qBv374lXrOu7cvuEuzi6wSpnkTkaoiIiOhtidpytXfvXvj7+6N3796wt7dH3bp18euvv6qvf/jwIaKiotCuXTv1MrlcjkaNGuHcuXN5bjMtLQ0JCQkaP2VFakYWjt58CgDoVptdgkRERGWRqOHqwYMHWLFiBapWrYojR47g008/xbhx47B+/XoAQFSUcjoCBwcHjds5ODior3vdnDlzIJfL1T8uLi66vRNadOpONJLSMuEsN0JdF6s334CIiIhKHVHDlUKhQL169TB79mzUrVsXw4cPx7Bhw7By5coib3Pq1KmIj49X/4SHh2uxYt3ady17IHttZ+ixS5CIiKhMEjVcOTk5wdvbW2NZzZo1ERYWBgBwdHQEADx9+lRjnadPn6qve51MJoOFhYXGT1nwMi0Tx28p7yePEiQiIiq7RA1XTZs2xZ07dzSW3b17F25ubgCUg9sdHR1x/Phx9fUJCQm4cOECGjduXKK16trx29FIzVDAzcYEvpXkYpdDRERERSTq0YITJkxAkyZNMHv2bPTp0wcXL17EqlWrsGrVKgCARCLB+PHj8d1336Fq1arw8PDAtGnT4OzsjB49eohZutbtu6o8l2BXPydIJOwSJCIiKqtEDVcNGjTA7t27MXXqVHz77bfw8PDAokWLMGDAAPU6n3/+OV6+fInhw4cjLi4OzZo1w+HDh2FkZCRi5dqVkJqB03eeAeC5BImIiMo6iSAIgthF6FJCQgLkcjni4+NL7firHf8+xqTtV+Flb4ajE1qw5YqIiCq8svD9nR/RT39DwP5ryi7Bbn7ODFZERERlHMOVyGJfpuPMvecAgK6cOJSIiKjMY7gS2eEbUchUCPB2soCnnZnY5RAREVExMVyJTH2UIFutiIiIygWGKxFFJ6bi/IMYAMrxVkRERFT2MVyJ6ND1KCgEoLaLJVysTcQuh4iIiLSA4UpEr44SZJcgERFRecFwJZIncSm4FBoLAOjCcEVERFRuMFyJ5OD1SABAQ3drOMmNRa6GiIiItIXhSiQ8SpCIiKh8YrgSQVhMMq4+joeeBOhUi+GKiIioPGG4EsG+7IHsjT1tYGcuE7kaIiIi0iaGKxHsv6Ycb8W5rYiIiMofhqsSdj86CbciE6CvJ0HHWo5il0NERERaxnBVwlRzWzWvagtLE0ORqyEiIiJtY7gqQYIgvDpKkF2CRERE5RLDVQm6HZWIkGcvYaivh/Y+DmKXQ0RERDrAcFWCVK1WrarZwcLIQORqiIiISBcYrkqIIAivjhKszS5BIiKi8orhqoRcexyPsBfJMDaQom1Ne7HLISIiIh1huCohqqME29a0h4mhvsjVEBERka4wXJUAheJVlyCPEiQiIirfGK5KwH9hsYiMT4WZTB+tqtuJXQ4RERHpEMNVCVAdJdjB2wFGBlKRqyEiIiJdYrjSsSyFgAPXowDwKEEiIqKKgOFKxy48iMHzpDTIjQ3Q1MtW7HKIiIhIxxiudGxf9kD2TrUcYajPh5uIiKi847e9DmVkKXAomEcJEhERVSQMVzoUeP854pIzYGtmiHeqWItdDhEREZUAhisd2ndV1SXoBH0pH2oiIqKKgN/4OpKWmYW/bvAoQSIiooqG4UpHTt95hsS0TDhaGMHfzUrscoiIiKiEMFzpiOp0N138nKCnJxG5GiIiIiopDFc6kJyeiaM3nwIAuvo5iVwNERERlSSGKx04cTsaKRlZqGxljDoulmKXQ0RERCWI4UoH9l99NbeVRMIuQSIiooqE4UrLElMzcOJONACgW212CRIREVU0DFdaduzWU6RnKlDF1hTeThZil0NEREQljOFKy1QTh3atzS5BIiKiiojhSoviktPxz71nAIBuPEqQiIioQmK40qIjN6KQkSWghqM5qjqYi10OERERiYDhSotUE4dybisiIqKKi+FKS54npSHw/nMAyikYiIiIqGJiuNKSQ8FRUAiAbyU53G1NxS6HiIiIRMJwpSX7rj4BwLmtiIiIKjqGKy2Iik/FpdAXAIAu7BIkIiKq0BiutODA9UgIAlDfzQqVLI3FLoeIiIhExHClBfuvKbsEeZQgERERMVwVU/iLZFwJi4NEAnTxZbgiIiKq6BiuiunAdeXcVo08rGFvYSRyNURERCQ2hqtienWUIAeyExEREcNVsTx4loQbTxIg1ZOgUy12CRIRERHDVbGoTnfT1MsW1qaGIldDREREpQHDVTHwKEEiIiJ6HcNVEd2JSsTdp0kwkEoQ4OModjlERERUSjBcFZGq1aplNTvIjQ1EroaIiIhKC4arIopPyYCBVMKjBImIiEiDRBAEQewidCkhIQFyuRzx8fGwsLDQ6rbjkzMgM9CDkYFUq9slIiKq6HT5/a1r+mIXUJbJTdgdSERERJpE7RacMWMGJBKJxk+NGjXU17dq1SrX9SNHjhSxYiIiIqKCid5y5ePjg2PHjqkv6+trljRs2DB8++236ssmJiYlVhsRERHR2xI9XOnr68PRMf+pDExMTAq8noiIiKg0Ef1owXv37sHZ2RlVqlTBgAEDEBYWpnH95s2bYWtri1q1amHq1KlITk4ucHtpaWlISEjQ+CEiIiIqKaK2XDVq1Ajr1q1D9erVERkZiZkzZ6J58+YIDg6Gubk5+vfvDzc3Nzg7O+PatWv44osvcOfOHezatSvfbc6ZMwczZ84swXtBRERE9EqpmoohLi4Obm5u+PHHHzF06NBc1584cQJt27bF/fv34enpmec20tLSkJaWpr6ckJAAFxeXMnkoJxERUUXFqRi0xNLSEtWqVcP9+/fzvL5Ro0YAUGC4kslkkMlkOquRiIiIqCCij7nKKSkpCSEhIXByyvtEyEFBQQCQ7/VEREREYhO15WrSpEno1q0b3Nzc8OTJE0yfPh1SqRT9+vVDSEgItmzZgs6dO8PGxgbXrl3DhAkT0KJFC/j5+YlZNhEREVG+RA1Xjx8/Rr9+/RATEwM7Ozs0a9YM58+fh52dHVJTU3Hs2DEsWrQIL1++hIuLC3r16oWvv/5azJKJiIiIClSqBrTrQlkeEEdERFRRleXv71I15oqIiIiorGO4IiIiItKiUjUVgy6oej05UzsREVHZofreLoujl8p9uEpMTAQAuLi4iFwJERERva3ExETI5XKxy3gr5X5Au0KhwJMnT2Bubg6JRKK17apmfg8PDy9zA+3KKz4npQufj9KFz0fpwufjzQRBQGJiIpydnaGnV7ZGMZX7lis9PT1UrlxZZ9u3sLDgG6OU4XNSuvD5KF34fJQufD4KVtZarFTKVhQkIiIiKuUYroiIiIi0iOGqiGQyGaZPn86TRJcifE5KFz4fpQufj9KFz0f5Vu4HtBMRERGVJLZcEREREWkRwxURERGRFjFcEREREWkRwxURERGRFjFcFdHPP/8Md3d3GBkZoVGjRrh48aLYJVVIc+bMQYMGDWBubg57e3v06NEDd+7cEbssyjZ37lxIJBKMHz9e7FIqtIiICHz44YewsbGBsbExfH19cfnyZbHLqpCysrIwbdo0eHh4wNjYGJ6enpg1a1aZPH8e5Y/hqgj++OMPTJw4EdOnT8d///2H2rVrIyAgANHR0WKXVuGcPn0ao0ePxvnz53H06FFkZGSgQ4cOePnypdilVXiXLl3CL7/8Aj8/P7FLqdBiY2PRtGlTGBgY4NChQ7h58yYWLlwIKysrsUurkObNm4cVK1Zg2bJluHXrFubNm4f58+dj6dKlYpdGWsSpGIqgUaNGaNCgAZYtWwZAef5CFxcXjB07FlOmTBG5uort2bNnsLe3x+nTp9GiRQuxy6mwkpKSUK9ePSxfvhzfffcd6tSpg0WLFoldVoU0ZcoUBAYG4p9//hG7FALQtWtXODg4YPXq1eplvXr1grGxMTZt2iRiZaRNbLl6S+np6fj333/Rrl079TI9PT20a9cO586dE7EyAoD4+HgAgLW1tciVVGyjR49Gly5dNN4nJI69e/fC398fvXv3hr29PerWrYtff/1V7LIqrCZNmuD48eO4e/cuAODq1as4c+YMOnXqJHJlpE3l/sTN2vb8+XNkZWXBwcFBY7mDgwNu374tUlUEKFsQx48fj6ZNm6JWrVpil1Nh/f777/jvv/9w6dIlsUshAA8ePMCKFSswceJEfPnll7h06RLGjRsHQ0NDDBw4UOzyKpwpU6YgISEBNWrUgFQqRVZWFr7//nsMGDBA7NJIixiuqNwYPXo0goODcebMGbFLqbDCw8Px2Wef4ejRozAyMhK7HILynw5/f3/Mnj0bAFC3bl0EBwdj5cqVDFci2LZtGzZv3owtW7bAx8cHQUFBGD9+PJydnfl8lCMMV2/J1tYWUqkUT58+1Vj+9OlTODo6ilQVjRkzBvv378fff/+NypUri11OhfXvv/8iOjoa9erVUy/LysrC33//jWXLliEtLQ1SqVTECiseJycneHt7ayyrWbMmdu7cKVJFFdvkyZMxZcoU9O3bFwDg6+uLR48eYc6cOQxX5QjHXL0lQ0ND1K9fH8ePH1cvUygUOH78OBo3bixiZRWTIAgYM2YMdu/ejRMnTsDDw0Pskiq0tm3b4vr16wgKClL/+Pv7Y8CAAQgKCmKwEkHTpk1zTU9y9+5duLm5iVRRxZacnAw9Pc2vXqlUCoVCIVJFpAtsuSqCiRMnYuDAgfD390fDhg2xaNEivHz5EoMHDxa7tApn9OjR2LJlC/7880+Ym5sjKioKACCXy2FsbCxydRWPubl5rvFupqamsLGx4Tg4kUyYMAFNmjTB7Nmz0adPH1y8eBGrVq3CqlWrxC6tQurWrRu+//57uLq6wsfHB1euXMGPP/6IIUOGiF0aaRGnYiiiZcuW4YcffkBUVBTq1KmDJUuWoFGjRmKXVeFIJJI8l69duxaDBg0q2WIoT61ateJUDCLbv38/pk6dinv37sHDwwMTJ07EsGHDxC6rQkpMTMS0adOwe/duREdHw9nZGf369cM333wDQ0NDscsjLWG4IiIiItIijrkiIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIq9U6dOgWJRIK4uDixSyEieiOGKyIq11JSUmBqaor79+/nef3333+PJk2awMTEBJaWlrmuv3r1Kvr16wcXFxcYGxujZs2aWLx4scY6qvD3+o/qdExEVLHw3IJEVK4dPXoUbm5u8PLyyvP69PR09O7dG40bN8bq1atzXf/vv//C3t4emzZtgouLC86ePYvhw4dDKpVizJgxGuveuXMHFhYW6sv29vbavTNEVCaw5YqICqRQKDBnzhx4eHjA2NgYtWvXxo4dO9TXq1ptDhw4AD8/PxgZGeGdd95BcHCwxnZ27twJHx8fyGQyuLu7Y+HChRrXp6Wl4YsvvoCLiwtkMhm8vLxyhZ1///0X/v7+MDExQZMmTXDnzp031v/nn3/i3Xffzff6mTNnYsKECfD19c3z+iFDhmDx4sVo2bIlqlSpgg8//BCDBw/Grl27cq1rb28PR0dH9Y+eHj9iiSoivvOJqEBz5szBhg0bsHLlSty4cQMTJkzAhx9+iNOnT2usN3nyZCxcuBCXLl2CnZ0dunXrhoyMDADKUNSnTx/07dsX169fx4wZMzBt2jSsW7dOffuPP/4YW7duxZIlS3Dr1i388ssvMDMz09jHV199hYULF+Ly5cvQ19fHkCFDCqxdoVBg//796N69u3YejGzx8fGwtrbOtbxOnTpwcnJC+/btERgYqNV9ElEZIhAR5SM1NVUwMTERzp49q7F86NChQr9+/QRBEISTJ08KAITff/9dfX1MTIxgbGws/PHHH4IgCEL//v2F9u3ba2xj8uTJgre3tyAIgnDnzh0BgHD06NE861Dt49ixY+plBw4cEAAIKSkp+dYfGBgo2NvbC1lZWW+8r2vXrhXkcvkb1wsMDBT09fWFI0eOqJfdvn1bWLlypXD58mUhMDBQGDx4sKCvry/8+++/b9weEZU/HHNFRPm6f/8+kpOT0b59e43l6enpqFu3rsayxo0bq/+2trZG9erVcevWLQDArVu3crUeNW3aFIsWLUJWVhaCgoIglUrRsmXLAuvx8/NT/+3k5AQAiI6Ohqura57r//nnn+jatavWuueCg4PRvXt3TJ8+HR06dFAvr169OqpXr66+3KRJE4SEhOCnn37Cxo0btbJvIio7GK6IKF9JSUkAgAMHDqBSpUoa18lkMq3tx9jYuFDrGRgYqP+WSCQAlF1/+dm7dy/mzp1bvOKy3bx5E23btsXw4cPx9ddfv3H9hg0b4syZM1rZNxGVLQxXRJQvb29vyGQyhIWFvbFV6fz58+oWpNjYWNy9exc1a9YEANSsWTPXGKTAwEBUq1YNUqkUvr6+UCgUOH36NNq1a6eV2u/du4dHjx7lanUrihs3bqBNmzYYOHAgvv/++0LdJigoSN26RkQVC8MVEeXL3NwckyZNwoQJE6BQKNCsWTPEx8cjMDAQFhYWGDhwoHrdb7/9FjY2NnBwcMBXX30FW1tb9OjRAwDwv//9Dw0aNMCsWbPwwQcf4Ny5c1i2bBmWL18OAHB3d8fAgQMxZMgQLFmyBLVr18ajR48QHR2NPn36FKn2P//8E+3atYOJiUmB64WFheHFixcICwtTd1ECgJeXF8zMzBAcHIw2bdogICAAEydOVM9dJZVKYWdnBwBYtGgRPDw84OPjg9TUVPz22284ceIE/vrrryLVTkRlnNiDvoiodFMoFMKiRYuE6tWrCwYGBoKdnZ0QEBAgnD59WhCEV4PN9+3bJ/j4+AiGhoZCw4YNhatXr2psZ8eOHYK3t7dgYGAguLq6Cj/88IPG9SkpKcKECRMEJycnwdDQUPDy8hLWrFmjsY/Y2Fj1+leuXBEACA8fPsyz7mbNmgm//vrrG+/fwIEDBQC5fk6ePCkIgiBMnz49z+vd3NzU25g3b57g6ekpGBkZCdbW1kKrVq2EEydOvHHfRFQ+SQRBEERJdURULpw6dQqtW7dGbGxsnjOci+H58+dwcnLC48eP4eDgIHY5RFTBcJ4rIip3Xrx4gR9//JHBiohEwTFXRFTuVKtWDdWqVRO7DCKqoNgtSERERKRF7BYkIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIiIiItYrgiIiIi0iKGKyIiIiItYrgiIiIi0qL/A58/YOrtvPVKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies)\n",
    "plt.plot(valid_accuracies)\n",
    "plt.title('DenseMax accuracy curve for plateau scheduling + l2 regularization')\n",
    "plt.xlabel('epoch / {}'.format(iter_n_per_record))\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train accuracy', 'validation accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to show an image.\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # Unnormalize.\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACwCAYAAACviAzDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPEElEQVR4nO29eXRd1Xn3/5zhzqPGK8mSbBnb2GAzeUKBNyGJWyBZJBTeNslLizP8mpXWTgNeq0lImnQ1LTW/dq1m6CJktYtA+msoCX0DaUlCSgxhSG08YDN5xvKswZJ8dXXne87Zvz9o7n6eR9ZFAvnKw/NZS2udrX11zj5777Pv0f4+g6GUUiAIgiAIglAnzNlugCAIgiAIFxfy8iEIgiAIQl2Rlw9BEARBEOqKvHwIgiAIglBX5OVDEARBEIS6Ii8fgiAIgiDUFXn5EARBEAShrsjLhyAIgiAIdUVePgRBEARBqCvy8iEIgiAIQl05ay8f999/P8ybNw+CwSCsXr0atm7derYuJQiCIAjCeYRxNnK7/OhHP4I777wTvve978Hq1avhW9/6Fjz22GOwb98+aG1trfm3nufByZMnIRaLgWEYM900QRAEQRDOAkopGB8fh46ODjDNt9nbUGeBVatWqXXr1lXLruuqjo4OtXHjxrf922PHjikAkB/5kR/5kR/5kZ/z8OfYsWNv+11vwwxTLpdhx44dcM8991R/Z5omrFmzBjZv3jzh86VSCUqlUrWs/mcj5u6774ZAIDDTzRMEQRAE4SxQKpXgm9/8JsRisbf97Iy/fAwPD4PrupBKpcjvU6kU7N27d8LnN27cCH/1V3814feBQEBePgRBEAThPGMqJhOz7u1yzz33wNjYWPXn2LFjs90kQRAEQRDOIjO+89Hc3AyWZcHg4CD5/eDgILS1tU34vOxwCIIgCMLFxYzvfPj9fli+fDls2rSp+jvP82DTpk3Q29s705cTBEEQBOE8Y8Z3PgAANmzYAGvXroUVK1bAqlWr4Fvf+hbkcjn41Kc+9a7PPXfsp6RsKK967PfR2zGYq0+5rA1bHbdC6vx+f/XY9TxSpzzFzutWj02Ltk9VIvpz4JI6n79YPbaAt5Vew/Wc6nHFoe3xPKSnGfQ8jku1thL6LFfhPNR3XKMrl2n/uK6+Du5zAAAT3WeZ9V3OIUXIl/VnI5ethclYv349KTsOPVG93bBn7Hpq8vKEKvavgUKfMCdWagw6BgYrK8Bzgp5HTcPzvlaf4PM88MADNc8z931oHrh0nEdODVSPS8UiqZt/yQJSTibi1WOfRe/L79MPqp/XsXXCNnTbXadA6qIRH7oGvX8blS22MJw+PUrK2CDP5/OROtvQf2uY9BqOVyblWt6MpqEr87k8vYZN141gMFg9LpfpNRy0boaCIVJnsPv89j/8v5O2p7NLh1mINi8idSHLT8rxWLR6PF6i62guM1I9Nk22NrKnyEYdFLLpDnvQQn3A1t8JiyWqdj130jqP1eH28D43Wd/Vep4MNCcNfs+8PTXOiVUGv8kUB0XLhl+3Lz+yh9Q9u+X1Sa85Vc7Ky8fHPvYxOHXqFHz961+HgYEBuOqqq+Cpp56aYIQqCIIgCMLFx1l5+QB46z9X/t+rIAiCIAjCrHu7CIIgCIJwcXHWdj7OFuUJGjXSZJm9QQAipGyC1rBsm+pkRDvl8p+PXrOENFHHo7qdjbR4i9mD2Og0hkdtKsApkSK2o/DYNcqG1mddi+p0Zf5ZV1/UYNqggexKgj6ue9OyaSMdvMLabujzKGbnoph4allTe9+1eOfNMmfLxgSPyQRrC6b3e7gvFTc2QnYcTL82gD4X9Epn3+bj7YiG9Rw2WdzDUk7XeWVqtxD00+tHQvpvbdY0/DwFbHrPIT+b66i/Si6dzwFbP3t+9szg4bJtOj7Y5uStzyINn41PANmf8ccll6fPHq7GdmsAAAqtdyabSz5mf4DtTioluhbhtSDEPROn8Vx4SvedYzWQuoqPrtWupW0+TB+z+Shkq8fKzZE6Zj4DJaX/tsJsJYpoHjBzEChXqH2RidajQp7aAeG1itvvYNs506Rjp7j9DhpsPpaOg9YJ9jgbBvsOQmPb0ED7ORDStkYmWyc8vm4E9L242SjMNLLzIQiCIAhCXZGXD0EQBEEQ6sp5J7soj/luKpQXhrnpGS7djvIqepvLCtH3Lrz1yXf8uSuTH22tOYpus3kV/cf87/DWmcG2pbnrpIFcz5QVJHUFV+8RDozQrbxcmZ43m9X1lqLtiQWR+yFzx4yHqUtdKKD71jPZdiGSA7hcwnZBoeJNbTueb9tPZxv/bPBurk/kCX4evIfKdrAVl1bQ/wqlCp3rNt7udelYWkattnNJZmaYTn/ZSLYzmWznt3T7fCaTQEzaB0H8WeYGWypoycZiUmXQpnO9UtJb7ibQayhH1ynm5u4iOcvvo+c0+RigZ5G7O7tIks3nqdQ0cuoUKaea9bY6d8u1/Lp9FhP1+JzACpLNzlNC66rN+rXC5mEtTKU/67K1yGXrj2vofg7GaD83zdVek+bYaVIXzWdJuVzU3w9ulK6jXiJZPY4xCQ+3FQBIhtZyia5/ODRDMMjcVbErPXsmuGyJyzwjrIP62eOPLFs3/LZeC0Ih5hoNWO6j3x0ecDdhbCcw87Kz7HwIgiAIglBX5OVDEARBEIS6Ii8fgiAIgiDUlfPO5sN2qRsYWCjkNHNfDVhMj8T+d0xTw25O3OfR4XYKSBP1+amm1jbv0upxJj1M6oZHtH7rs6krlQnMZdbRQ1NQYVK354jWfVWgidRVLOqyVkY6Z3aMhng+Maj10miQ6df9aVLubtPtbYpxzRyHXqd9zqTUCVrvZNTSQ88WdbErmdAf+prKo5UOE3cryGbowKFDpC7VpkNXeyw8dksjdbcLIhc67yzd83TGy49sOTyHtt1CurSPuUr6mGZtuvr58vuY9m7pa/iYzZLPpHPfM3S96dH1xikil132rBVRv4eZzZTF7CiIcM/GIIfCyO/Y8TKpqxSoDUhDfKVuT4Cuadg8g6dEAGaPZmJbAPaMesjOTrG/m2CDVwMHkJsn0PXPs2j7SsjeyWK2TxHkFxsPM5u7l7eRcnlY24C0L72U1Bmn9NpYMuhYRplty3hBu/QG2RdEANn9mU3UJdVErrbcbboUpjYodkWf16qw60f03AqMjdG/67qMlPPJRPXYc6jLsIvmYdCjYzDBDtFFLt/uzO9TyM6HIAiCIAh1RV4+BEEQBEGoK/LyIQiCIAhCXTnvbD64aG7YSX3MdGaHp35HcQHKTFv2I99/1+W6JrNTQNfhIZZXr/md6vGO/95M6k4iG5CcQ7vecalWeOT4UPW47/gJUhdoaK8ed6Z6aFsDMVIuI33UF22h1yxqPXRk6CSpCzdQW5LjWZ3avMhsEVIxrXmGWRhpt0I1ahzBt1aEibeL81EPG5DpXG/q9iIsFoNP66quonWFLLU3SI9p3XlwmNrvhGJas26K0TlgGjymDQq5b0wjzge3w5n6X9bEj2yxFLuGD08YZu9lAY/ro+t9QOdhBWnfLrOtseJc+0a2JCwEtueg/nKpXUk2k64eR5meb7L5gdPU2z66FqRRbI/RDH1+Qiw0fBl1QblCx9L2I3sitha6LrWXcdB6WC7TfvYjmy7Fnn3PnZoN11ugFAA8joai7XEd1LfMWMJANhZFg851n0dtN4xmbQuVH6djWenbXz12DGqj49HhgxwO8c76wF/RbS0fY7F50JjwMPpFFnfEKup6mzYVSm36ngsD9NmPGXRdNxLN1WOX242h58nH0zewOWIhWyzbnHnbMNn5EARBEAShrsjLhyAIgiAIdeW8k11KJt1mG8vrbTaXuRU1ROnWXhy529lsGxS7+E2IhMzcybBbbj5Pw/s+8+RPq8eDabp9OZjVf3fkBP27IyePkbIV1DKMa8VJXSSut9l8YSrX2EG6fRhAW+5Bk25JDpd1dsb2zm5SVyzQbJGHDmnZZTRN+9mao9swr4W2x8dCfRsoVDNzmibwLJzcDfWdovhpauwmknDHbyO7uGhL2WNbnTiTL85yCQBwaiRTPc7kaL8WSiybZ173mBmg7te5gp6/0TDb4mf3iEWGd6NezZT0FTD0fboGfdawey0Oew5whtDnHgqLzkKf2+bkIcItg2UbJfIO60vkzu8yV9/suB7Lo7ytTC7BMkhXnI4lDqH+yquvkrorLr+clD10LyWX7tUHkTzhMfmokGeys63b4zCp1LJ1+yoO7fNSiX62FljO9ti6oPj/wSi8QZlJNC5qa2KcjV1LipRDrXOrx46iLqqAws+r5jZSVfDRcbcHRnSBpZDIoTVXpahc7fP0fRWZfB+JsbAI47ovS2yO2iHk9srWCbuplZQNn+4fV1FpMIZOazEZyDGo27Jh4vLMZxmXnQ9BEARBEOqKvHwIgiAIglBX5OVDEARBEIS6ct7ZfJwqUO1ptJKsHj/3m1+TussWUU3t/ZdrF6QGi9l8ID3SZJqeaVItzEVuYcyLEfqO6LDXowWqt6lwY/XYijJ3yMYMKYeSyepxuUg1vjJyj4w30HuMR2l5aEDbamROMxctpHkGWerlo6dpaHhfXGupQ/1HSF10YLx63Ban5wkx7d1hIfAnI5cv0F+wEPc2GiPF6izbOuMxAIDBDHqwDYjpTf4ubnLHUmbvkEUaP3e7DSFXxSJLQd6PbD6GTtM54LFrVpDxRn6cpg4fQq63x0/0k7rLFs4n5UvmdVaPLRZKm7Rdsf7gJh4kfDetmtBfNbCQrZbHXbORLVZhjPYPMHsDZaJQ1iE67/xo3vn5nKhQ+yYXn9dlnyVuwdRuIpfTNgWDg7RtkTi1hVIovYOyaVvLWf23QRYm/lQ6Tcovv65tQiIB2tYF8/W428x2pZQfJ+WQreu9En32XORe7NKlEKDIxqQWaEq4Hg/hPmEC6c8yd14fshEKHDxAm7PjBVJ2ViL7HZOtxyhthZ/ZjhSBjl8UpZuwAvQ8XkS3x1DUbdut6PPGmpKkzndihJQhq59pX4p+P8Ax/VmbzaXiKWoXZCE7QG8RDb1e9Ov2mczN3u8wOxO03vDo/DOB7HwIgiAIglBX5OVDEARBEIS6ct7JLnaCbiHnR/T7U8VPI72N5uk2ZL6sI8rF/SxyIXbn4tv4FnWFK5a1tHCK+YsOj+stuHCSul01tGh31pxHtyubgWXBRO5bZR9tazGnt0yLWXqeuczVK4+klaEy3U410Jbu2ChzmWPbogW0JWj5aX8MZrTbcP8YlYjmNjMJa4rbd+kC7dhomMpJpq33f13mCk3UE7b7zzzYwES6i2HWeBd/mwirA/06Cm1jYyOpCwX1VmepSPs5HNB1bS3NpE6xxufyum8jfrq9Wy7qsbVYJ2dLLDMrarvBZDEqGfHMwkDLkxYmdFdNgkizmZBZE8kuASYRRZn7dQK5A5pjVEoJoPkc5Dv8TOIz0Rj52VY9uPqa5Qx9LmMR/dkGNgf6jg+Q8qFjurz/4CZSd3o4XT3OFuk18pU3SNkGFJk0R11Jl126qHr8kQ/fROrmsHWiFNT9U8zRvivndFvjikXTLFD5phY+C2V/Za6b3PXWQxE1bfY/cvS0bp9znEZmjjOZavykbns5mCB1CvT3gTEwROoiHcwNNo4kCKBrXAhFIvanaX8UkTu2M0zlUD8bWyejxy8wSsMrVApI7gvR78B0Hw3T4A9p2SXWPpfUWSioqjLp81TibuVobSh7M6+7yM6HIAiCIAh1RV4+BEEQBEGoK9N++Xj++efhlltugY6ODjAMA5544glSr5SCr3/969De3g6hUAjWrFkDBw4cOPPJBEEQBEG46Ji2zUcul4Mrr7wSPv3pT8Ntt902of7v/u7v4Dvf+Q784Ac/gJ6eHvja174GN954I+zevRuCweAZzjg9Lr1iFSkf37KvehxNUD1yVe9qUg5b2kW0nKPaHLYhMHzU/sJVDaQca+2qHu96lb5YRZNat58zl4ZCVkg/9jE7Dq9E3a7KZa2x4bYBAFhIi3vjlVdIXTxAPxuOaO0ywkKxnxwYrB473M6FaaeNKAR0+jR1Szs9qst9/VR37kjRsMU2s7WZDDtONWmX2WNUTKQZGyyzJg7XzWxXeHZRbGOgasRa52HZWfR3kqXUYLYJgGxSkiykcqWCrmmxsWPu2Njmw7Do+BjImCUQ4mGSWbZn5B8+wYUOux5P8Jal/YOvMvGjUzf6OHb4cPW4UqHzYzyjn1O3Qm1XTpyg2Z5Po7mfY7ZQrU3aBiMaYdlEbTpeZeQObfvpWmDa2tYmx+x3irjDFF1aj56krut9x7VrdK5M7XeCCR0u24jQAaJPMEDEr8ey/8h+UnfypH6+X3jhN6RuCXO/bklqG4NCNk3qchm9NlWWXErqsmM0TUQtAn7d74rNdfCY8Ryy5zGZbU8WZRLPrriS1MXt5aScH9fzp8LCKxgBNEZl5s4bonMkh0LX81QLFVe3x2dSW5YCGh8eoLzAXIjzWd3WCLt+EZ0nEKWzoDFGv59c9H2RZWsBoLDxoQpdUx12X7jbK9Mx4poi0375uPnmm+Hmm28+Y51SCr71rW/BX/zFX8BHP/pRAAD4l3/5F0ilUvDEE0/Axz/+8XfXWkEQBEEQzntm1Oajr68PBgYGYM2aNdXfJRIJWL16NWzevPmMf1MqlSCTyZAfQRAEQRAuXGb05WPgf6JpplI0s2AqlarWcTZu3AiJRKL609XVdcbPCYIgCIJwYTDrcT7uuece2LBhQ7WcyWRqvoCEE9QWYO587cteYJG7u3sWkHIz0tfTfYdJXQXF+XAdGsdi1Xtvpeedv6J63LOMnmfHTm2D0RCl9g4nh7Tua7MwvAEf0+aQxJZlfvfpUa3BNkbp33FlzkW2HM0t1CamhLTt4dPUVsOw6HtpDIVtty0WDhpp328eO07qWhqoZr6wk4UNnoTv/8u/0vYwmxQf0jWjMaqPLujR8VRWXkHDC7PM5iQ0Ow+LrrCGz/RQh8UWwXEd/AHaHhyvw++nthpNDShMPFOFbRbLw4/DcPuYJoxSnaczVIdPj9GxHR9LV48rPIw9irnRxMJBL1xA7QR8OCU5m3jczqQWL/z3Fv13Bov/gGx2CgX6HBweoDEe8CX5ODcktE1DJMiePdZUHwq/brNQ2qat+z3P4jTY6BqK2eQMjNJw+BUUjCYcS9IGgB5LHGodYGLY+mJR90k8RmNDXLt8WfU4N0ZTKxRZyoajR/WcefPNN0ldAYXZPjJC50shT8fEDtC1ExOJ6LXAYWNQcfk81OPusBgTBrLDCaVo7I5MjvbXqTHd7wZLm1HOo5D7LN5NOU3P4yDjqICfrrkZtIYEfewr1dRlj9mflfLczkW3b6xA1xdkUgZhm/ZHrJN+X1q42mR2Lni/YUL2BPYQo4faOwvx1Wd056Ot7a0v28HBQfL7wcHBah0nEAhAPB4nP4IgCIIgXLjM6MtHT08PtLW1waZNOmJfJpOBl156CXp7e2fyUoIgCIIgnKdMW3bJZrNw8ODBarmvrw927doFjY2N0N3dDXfddRf8zd/8DSxcuLDqatvR0QG33nrrjDTYCjB30cE91eOrlq8kdZEE3QK0xrVrnuvQLSYbbSEfOkbdcK9v6KGNCOusoLEI3Z4L2rp9IRaGPIi33NkW3JyOdlLejbY+/X66xZ5B7mM9XYtI3aLFVGYYHdXbqdF4ktSdRCGFDeYilmyg4aHH0Fa+xSSZUFiftzBO++PAUZY9E7mMpc68GfbWefJ0W7hcoGUfkiDGqaoAYVTnLllM6oqKbpWbaMs0wNwqsZTgckmGyTCJRi1pcVc8QG7CPEyxhaUVliKZb3R6aFv0MMqeDABwYkiP5egIddsuFFiW0hLa1i/Q/iihjK6dXdR2q7urk5Qjfrx8sP6ZRlbbXQf0vYRDVJZTSA4tOXRuJRqoBItdOctFKgecyur5Y7HxiQWp+7PjoqzVPjomFopPbdj07wI5vR1frlDD+dFRKnvg/uLTpezqPfbxHB27Mks70NWin9OmBvpA4Sy7o6dPkbqmJF1TVlypwwIc76cuzGMok/je43RumWzd6KFThmCjvgzF6NqYzVNZyka6mcukAxtlYzXZ8+wBLRsWcptmbcWlSpnOrRCTwW0kn/hYVmTsXus6TC4p6vFy2BPtCzHXVhS638/mnQ/JdD6HyUcsDoCBrhN0mZTiOviD9PrsFzRLxdSf56ky7ZeP7du3w/vf//5q+bf2GmvXroWHH34YvvjFL0Iul4PPfvazkE6n4frrr4ennnpqRmJ8CIIgCIJw/jPtl48bbrhhgmEexjAM+MY3vgHf+MY33lXDBEEQBEG4MJHcLoIgCIIg1JVZd7WdLr4g9YYpIne3Uon62vqYzUU4gt3tqL4fQNpg1Ka66sP/9CAp3/Kx9foaORq/xB/Q73OmSfW/nvlzqsdDo9RNsJilGnVbqw7TPpqhemSprO95/gLqTnzJAmoDMrbz5epxbpzqqtgtzWEprQvMxiKZ1C5trqJ2HIkGrY86ZXrPlkn78vhJbZuQugIm5Q9uu52US8wlNBLS48ddxELIFsFghhM8iJ3n6Dnjs6k0aKMQx4rpvAUWBlx5+pomCwWP3YJtrhf7UHp7s7ZdCQ5xXPToXI/Eta1RQzJJ6twy/WzQ0n2XHqEGM8dPHK4eL2Cu6pZJlwtsB8PtKKYTjTmD7K+UR/sujFIChCw6Pp1dl5ByBd3nKRZXaBjZwaRSraQu0ExtWXJp/VnPpBMo0aCNGgIBGta6iLo579B5FozQdcut6GfRYukB/MhN1+en86USpOVV12hbjUVzO2h7ynpN6XuT9t2b+3aTcu9K7Zbb1UXPc/RVnZaiwmwIPJc+77Xwo3vxB+lc8hR1TQ4hV3LHoNcYz+hnz2Xus8EEtVVLRZANEXMXxesGt2mw2P/lFrLHIi7vb4NC6yq3+XBZuHelsC0L/awfW6gw27AS+57B1TazMXNBzzWDPbOGR+8LZWyYYOc3E8jOhyAIgiAIdUVePgRBEARBqCvy8iEIgiAIQl0572w+DJaKOY9sJYrMLsDH0sKPjyBt1aL2ID5IV4/bk1RHPLDnACmfPK7jnECe2m4cOX64enx12ypSN2eu9sPvGKIO8bmDR0i5MZCsHseSzaTuzTf7dFs75pC6NLNpqCDNcfAU9dH3kH+4wUKm55nNh2EirRAoERR6HTwae8FvsDgFw2fO8cPxKiweBtdg0XHUT+MthIJ63AtF2h/5CtXXDx86rNvK4nx098ytHvcdo+P85FObSLli6nkZDNDQ0WHUHp4qO4Ei+iYTNMbF1VdTo5iWZm1jcEknHXcThSW3mCaMYw0A0JgFhVaqkXe0J/XxHBp7xuUpwFF4amyDAzBBlq6JD8XuaWml9gZBFBdmeJiG7s/lqO0RzgFerFAdPNGin705zJYllqC2G/FmbRMyguLkAAC4SBdnU4mEf8+zuBXlCgsfDii0t58+e8GAns8+FseilUWAbmnQ5SCLDdGC7FPiLCT4yNGjpHzkzcPV47ZGut6MDerw975GmqKhbE39K8RGa4hl0PsKsnU9PaTjooxm+0ndqX49DxpidL1ZetkyUvYh274Ssw2rIHsVk6Vv4OuNiWL3c5subDvBPUFdEpOEB9bghlH4GizdBrkGXRttdh68FvDz+LA9EV/IWXNMZE/jTiNdwlSRnQ9BEARBEOqKvHwIgiAIglBXzjvZhW9VWWgLqr2ZbsHh7W4AgGde1SHLGxy6dbWwEW+bM9c3m0oQp4YO6+aU6LZs9yU6FLvFrh+O6+3d5hR17xthWS/HkHst2+2G1la9LWwzaanIXF3LaPu5wLbfHXRih12kWKLboo6j31ObmqmromHovvMbtK8CzE3OVZNnvcQ88Z//RcpehbqLmiiMcpS5VMfQ1vS8hbSfW5poeP6mdp0Bt5HdVzCiJZL0HiqLvbbnGCkX0HYr86YFG+1nxiNUdlnQraWd3lXX0LZFqAwTQVvcfAe3jMbdcek451EWWwCACgofHgrT9iSTest/cIAmiBwepiHCQyhLaaqN9l04TOdlLRqQrGixbfxSSc8ng/2vNDqSJuVMBrmvsufCQhlDj5yg9xXPUEkkkUii9tD+KSHXfoPN7QDOaBqhczKkeHZcNIBsGz0S0n/rU3TedzZRiTGM3FdzmTSpc5D0Y7At9R4mPe3Zq0PcL1p0Kf0wkidOnqSh14MsDQMAL2uwPGEzF1mPSRnjKIXEqVNUqk2f1m3Y/+pWUrf3lc2kvGCBTjcxb8ESUtfQjKRvJiu4LGs1KN0+LkBYJGw7rcWu9dy11WNusB5Zg5nrLzoPF2smZOOu4edOXH/537HP4vnNv1dmAtn5EARBEAShrsjLhyAIgiAIdUVePgRBEARBqCvnnc0HT2eciGrdORlj7n5Mt8sorZcOn6aaWnNMd0WEuaW5JtVdD588XD1ONSRI3VykMRbpn8HWHXuqxyf6qa1ILErd/XwovPAbB6lbHH5n9Nj7Y4lpc1mUkjvZSPVYBxkO9A8OkbpIjN6XjUIBh8NUz/b7kZ5doe68bo7eZ6qV2jFMxradr5NyyEfdV0sl7ULr99M+WH3tyurxkRPUNmOEeu3B0st1eGo/c4PNI7sXH7PfueYa6gZbRKnO/T76WC2cr+2ALl9C9fSO5mT1OB6m89crUrubYwM6LfrQadqv/cO6LsdC9afTaVIuV3RbfczN0x/QfeA6zDWRua+Gk3osl8LlpC6RmNo4A1D7jHyB3rOFjBUsFv7edem427a25/EUrfMHdHuam6kLcTRK+z2I5kEiwELuo3nIw98rFHrccejDn4hTWyMThdL3XHrPNnKv9UrUFiwRYNd09Fi6zNanjFKvF9hcCrPn+8iAfm53v0ntrUolvYZUinQOKGa7MVUsto7zrOeLL11cPV6whLqV58e1DcgbL79M6nZu30LKLzyvbbX27KZryqIlV1WPF15K7UGSDUlSxu7Q1oR7xmPi1ahjz5NH7ew8NmdInavP4zKDL4+dd6pOsQa3+TDofZnIJd+Z4Bb87pGdD0EQBEEQ6oq8fAiCIAiCUFfOO9mFZ89sa9WRC232LuUx19L2Tr39vR1JJwAAaUNH7lMW3bZONNPtsURcyzK+IN1enodkl2iCuv4+9P3/r3qcZ23LFKgbYx5FS2S7+NCGssgWR6kLaC7A26qlpr37aKTWwUG9VZ9hGW+TSXrReERvG1vM/c+HsmdaeeqK1xJh289BPX485iPm1DEW8bWRylKdndq187IrFtL2oK3pN3ZRV7wU296NooyiQ8NUk4nE9dZ0U5z+3Uduei8pmyikZyJBt7Sbm/Q8GB2lslTfET0mY2kajTUzRiN4jiP363SOztHRjM5O6zC3ZJ+Pyoj+gC6bLFtlIq77Lsmy4zYwySyA5Dd/iEpxWRYhtxZNKPooj2wbDem2ei6LYGzSMWlF0VENm90zinTpZ1JKkGVYtWzdJ1xaMXCqT1aHI8vmc/R54llKsVuuYtmM82N6jpw4TJ/ZURaWMhnS50k1JUldMKjHhLtKKpvKiHZYu6efOk6j+Xa167UxVqb3kSlN3QUTu5aaJt3iVyx7MI4oarHop8mmrurx9TdQF+8FC3pI+cXnfl097uuja1Nup16DM8xNedkVV5JyV5e+ps3cwV1HryEud59F0r/izqxM9jCQxMimFhgmdvVl33M8Min67ISIq7h9E1xt+Xknl3pmAtn5EARBEAShrsjLhyAIgiAIdUVePgRBEARBqCvnnc0HcesEgHiD1osdl95OgOmai3p0KO3tO6h+nfHpcMOeQbX21ByqOe7eo0P4vud9nyJ1m/9bu3rlcizDbHm4ejw0QF1A+XtgtqLLNlANv8HU9iFzQvQaY6eoRuxY2lYi1UrtJlwUNrnANPpiIU/KOeQO6XhUz64UdZbJVh/V5Tui1Bag5Oj6WjYfJ/a/QcoZ5qp4y+/+SfX4pps+SOp+9Yx2FWxN0nFuDbMMuCjMddCgem0qoXXwWIJmEw2ysOQO0nO5TYGDQhoP7KO689EhHeq7XKEarB2kbY3FtKt0a5D2a6U8uZuej7mOW8jOw2I2H7GY7q94nPadZVHdN5vTc2RwcJjUFYt0/tQijOwNKswlNITC0SfjVN/3mCuw7ddusKEobTt2IzSZZu8p5mKIn0X27xn24FXMrdJBc9tx6f1nRmj/4Bb4mM1HdkzbYvWfpPYXqUY6D5MRHZo+z+wxPGS74rClHrsFAwDM6dQ2DZcunE/qrrpMl/cfouvWztf2wFQxkJ2HadD2mDa1gfMh136XuYAaqN9N5oK/cBF1gfdQWoj+/v9L6k4P6749UBojdYMn9pHyJQu16++Sy+k1WlPaddtm3zlORbev4vBUE9Q+D89Ro1YWWWY/ZNRwrlW8jowBPy0zHkGGJxOy7M4AsvMhCIIgCEJdkZcPQRAEQRDqirx8CIIgCIJQV847m49IlOrgDc1a83SYjlg0qR4YjGq9NJmksRiOHtMhe69fSUNFF7NUYwvHdCjy/hPHSd3B/ft1e1jYZOzanstQjTHWREM+j41pzTgRpTYEly5aVj3e9speUvfynj5Svv79H6oe+1jq+UMHtX1IOkM1ah62vVjQdh5zU1RPD6H04Y1Mk1Y21Tmd8tTC9BbzNI7FsiuXkfIHPviB6nFTksZTuW61jsFhMj09xlKtx9F8svwslLZfx4bgsRg8oGM7dlrHZogz3dcDPfDzL11K6lo7F1WPR09T+50Yi7NRQTq9wcKH+9Dk4qm6i0Vqz5NFMSgUC/GcRWnYj/XTuCfcDqiS1+d1XXqecIT2QS1yyN4oFuJ2JvqZHjpFY6RkxtKk7Hm6TxawtPDJRr1OWD5uQ0DL2EanXKa2CHkU06ZYov3hlPX4GS61wVEleh6cwiGZpGkPQn4dV8M26LxLMhuqREyXy+waedQf5RJtj2nQ57IB2TSFA3RuHUcxdyz2+F5+KY2xcwqF+eeYyIaAx2uy2H36UbXHYoLgwBY8NkWZ2T51ds2rHs+bN4/UbRvU89th9kOnhtK0jOxD9ux5ldT19Gh7wUsuof2RSunQ8DEW0h4MakdRLKN4IWyd9CF7Jh67g4dXx9XK4OHeySdpc1gsD1yyphy0ferIzocgCIIgCHVlWi8fGzduhJUrV0IsFoPW1la49dZbYd8+ahVcLBZh3bp10NTUBNFoFG6//XYYHByc5IyCIAiCIFxsTEt2ee6552DdunWwcuVKcBwHvvKVr8Dv/u7vwu7duyESeWv7+u6774af/exn8Nhjj0EikYD169fDbbfdBr/5zW9mpMGeQ7c6E43aBTNXoFu/eeZOht0Ku7s6Sd3+N1CY6zwL8RzpJuWuS/Txkf00DPgJ5BrX27uKtgdtacc6aKbGxg4aFvjoqJZTCiXaHn9Eb9PGW7pI3dUxel+n0Fb14SO7SF0ur6WD9Bh1n21taSHlhNL3NTdKZY7WuN4W9RlULilXqENtBG23UodmyvzFV5Hyx+/8f0g57+oty30H6cuth7Yzg8xFt8K2FkfTaM54dG65KJw3U/TAA7rFPZ7Rd2MN0q3fk0Napiux7W8PZQmNMDfgQweopNd3VGc35uHDG5v1mPDt97ExKvGNDGu3T8XkEhOFuTZYyOtIiGZ/TSJX4CDL+lvI1nKkpgRQ+PeRYZpd+c3Tuq08a2uygbqOt7enqsdlliG0UtbSjsdcHDNM4isgecl16DUtJL/5ffR/NyylBCO0r0IsR0IRrQUec9mNRFEqAyZP+FlGVbymcZfqInLtNKzJ3VUBACoVvRYcH6EZk/M5PX+4K2lbO11vamEhCcDicgBzQwUDjd+EMOD4b7m/KP0szpYbi1FJmLiz8gzFPPS50u0bP03n6M5hlGX3lW2krrFJz9G2NrpWt7XPY21F6RyYDN+S0iElDObyzuezg6RUh7nlkvDqPIS7R+ezQvKj8mrJN++Mab18PPXUU6T88MMPQ2trK+zYsQPe+973wtjYGDz44IPwyCOPwAc+8JYm/9BDD8GSJUtgy5YtcO21185cywVBEARBOC95VzYfv/2PqrHxrf/Ed+zYAZVKBdasWVP9zOLFi6G7uxs2b958xnOUSiXIZDLkRxAEQRCEC5d3/PLheR7cddddcN1118HSpW9Z8A8MDIDf75+QDTOVSsHAwMAZzvKWHUkikaj+4OyBgiAIgiBceLxjV9t169bB66+/Di+++OK7asA999wDGzZsqJYzmUzNF5DxEer+F0KukyUWmtnw6O3hlMXNjdRuYb95qHo8NEo14BGL6l2JqNbfFi+l7lOHDmtdvkKlOOLOunAhdcla2HMJKR/p1zrrG2+8RtszjFKZB6hNQwMLK338DW070j9Md5UM5IpsBenftXfREMtzkT7YHaN6dtDUemipyFNKUx2ahxiejP99x/8h5YY2qi2/8rq2h+DudWWkT7rMjVIxXRO7kBnM9czFmierMye8tuv6ikP7YHhE26TgENwAANisIhlPkjru5jk6guYl0/CHh7VNQ4nZ2TgsdL5b1s+J5afPSDio50SAhV63HHrNchH3O53sOCz625FGbsonT9Bw4hHkxr34Mupu3dhMw62Hw3peFgv0GT59WqckqFSYS6qi60YYhc5PxKmNQySgyyFmY2EjuwGXudo6Dr1GBS0ORZM+EzhcNk897zI7NhyR37ZoaAHl6XEvlugcGDlFw70Po/Dv4+PUGut0Ol095nZJgRhdR2thKGzzQeu4S6iB7BgMNXnYb26rgV1SAQAKWX0vAwP0u+PkSV0eC9O/87HnC7vkR4J0bodt/bfc5fxEv16nDhw+ROoKhU2k7Lj6ms0tHaRu2bLLqscLF9Dvx5YW+hzEE9qtPBBioQ8AtZ3ZcTjs+woM5Kp9Flxt39HLx/r16+HJJ5+E559/Hjo79ZdCW1sblMtlSKfTZPdjcHAQ2traznAmgEAgAIHA1GMCCIIgCIJwfjMt2UUpBevXr4fHH38cnnnmGejpoR4ay5cvB5/PB5s26Te6ffv2wdGjR6G3t3dmWiwIgiAIwnnNtHY+1q1bB4888gj89Kc/hVgsVrXjSCQSEAqFIJFIwGc+8xnYsGEDNDY2Qjweh89//vPQ29s7Y54uhw7SravuhUuqx0GTbm16Zbr9bKPtsiDbOovFtHwRjdOtqsWLabTEX/3Xz6vH+TFqyxJu0u5+B49Tl6yuTu2y23PpNaQuwLa/53frz6ZHqevb7j3aLdhTdMv2+GnaBxnkflx06Q5TJq1loFbmBnZkhLqdNnYlq8cjfKfKQy67TFZRNpVoSp7e8q6137Vz13ZSfvW1XaRsgD6vZbHtbyTFWTbf/ucZXvVWp+2n7+J4jvh89O/8rA9MFA3VUvSzcb92tzOZTFax8PiwaLBst9kf1hJEJc+kA5RBuczcQ40Ky3iLNKMy28Z3Uaba3Dg9T5jN0ZaEvhebZfnFisTbOd02tuhnpoFJKTYeH/bMjmepe3g2q/sgEGByH3Il9ZgbbkeKupUHkPRksci2ytNjlCvSOysid+s0knkAAEZGaeTPApKFliyh64sP7RrzzW6LpSLF7rSlHJVLjqPM2TzyaLlM14l8TrdnLE1ds/0oyizv803PPEPK7119NUwKiqrqsQyqymHZYJFEw5RSMJC8xF1ALeZC/MrLO6rH2dO0D5pQdNhj/bQuzrJY+9E65jHpNB5FkVtZ9Fy/ra/hC1DJyjKZvH86XT0+3EezeqdP67F8eTtbi1hk5i4kmXe00zAR7R16ne9I0bpIlLquGyHd8YY58+rEtF4+HnjgAQAAuOGGG8jvH3roIfjkJz8JAADf/OY3wTRNuP3226FUKsGNN94I3/3ud2eksYIgCIIgnP9M6+WDB145E8FgEO6//364//7733GjBEEQBEG4cJHcLoIgCIIg1JXzLqvtroPUjqJ7qQ5h7gHV0Azu1ol0xgxzJ0untatZU+NVpO5DN72flK+6cnH1+Mc/eZxe09CaXyJBNbQ5HdozKMrcKi2Htr2xTQ9New/VqMdCWuN7edcuUtefZWGCfdoVONFO3eKaF+g6bhvhsjDk+5TWKw8OUJ8sP/KbK7AMqjk2BI6n++dmKu8TXnjuaVLOZ9L0mj6tpYbC1E0YT2tL0SnOs2CaPmzzQe85GNA6Lw8f7g/S7KJ2RPdt0E/drwOm1mhtrl8Hkasvy+xZKVFdvohcZrENAwCAh10V2Xls5iZM0isz24hkRJcTEdp30RB1Rwz49DV9Bp2jBguFXosK2lHl/WyjMPIuCxXNM6HayDWYmUZAENlxFHK07wpjdC0ooCK3AzJRSHXFbHT27dldPT5y+DCp4xmuFXIl7WinnoCNCT1/Cnlqe8XLaWQnMIJclgEACsjmzWVtzfPzoOCOJpsvYVvPg/6T1BWax2+qZfNRQbZI3D3ecOhcw1l3eWBvBbqOu+xms3QsiwV9zUsXLSF111y1onq849XXSd2WbVtJOZ3V67PL3KZb27Vb7PXXX0/qbDSfDx+hqTi2bKGBN5deprOpxxN0DRlE/cxzpfG1oC2lQ7P39MwjdTh8QG6c2vbwcAI+W6/5RTZeM4HsfAiCIAiCUFfk5UMQBEEQhLoiLx+CIAiCINSV887mY/8YjRsx7Gq9X/movYFZZpoWsjfgYYs72rUBwv96D43BEfRRG4eeuXOqxx/+3x8ndf/++M902wbo9fvHtN5WLB4kdX6gmuxoQZcPHmF5cZD+ploWk6qGFLVF8JCOZxhU3/eQ3YJnUD2/wuI/jKEU9kEf/WzQ1sJrzqBacoXFx1Ae1g4n1xFTLdTPvr9A/fBdN109jv9PYsPfYqP7zAzTGCnjGWpbU3Fx/Admp1ArjbRJ78sX0vNH+WjbHUM/ZiYz+gj79RhEQnTs3MrkNksQoOcxkL1KkMXjCDE7isaY1nK7WDj+znYdmpmF7oBSkerpptLPm83E92RcP6d5aoowgf3791SPL7/8MlIXQrYafDhMFgXDQ6nEB4eobVguo5/FUoHGaXCZbRi2j5i/YB6pa2nV/eOyBvmQfUqSxYnAsUMAaHR8Hvp877591eNsjsbV4J/F6Qo85o2YQ3ZteXbP+Tx9DsrIvijgo/Pn6KB+9tIo1DoAgOu9vQfkb8Hekty+gBdxunsW5R88ZA/CA6GEwvQZ+l83fBB9lJ7IRvFLFl21itQtXb6SlHG4Fz7vmpu0vdf8+TRNho3Gfd7CK0hdRzeN7xIK6WcmwWw+cN+NjtIHCttxAAC0tmgboliMnsdC9jsmC6DienT9q6Ax8Iypj/NUkZ0PQRAEQRDqirx8CIIgCIJQV8472WVfmr4v/fRFnfH1qrnNpK7NT8PZhtF2YjtLdNferLdJL5lPM6gCy3rZf0pve33/0Z+Ruh27tLsdz7JLdncVvQ/FXPHcgG6Py7b4bRRa3DGofOSYLOMsHmHmPlssI7dB5ptoM9dbC20xqyILA46c4Xw8a6xBy+XK1LIjqgqVbxIRum09jlx6Ky7dml68ZKk+Twd1Lx5i2TyHUDbPbJrKa9gdkbsqKpduf0dsvb25+MoFpO4kcuU8laEyUKGs214o0nu22PZuAIWNj/i4i6we95aGJKlr76BzfcEcHc68NUDnTxaFaR9lIcEt5nYajmhX8ijLdNzUpOtO9lEXQ04FyTnFbJrUmei5mJBZ2KLLl4vCph84sJ/UjY/p8/qZrOAP0LmOQ7p7LNWniTMWM2myCcl/3NU3X6BztIDKx44dJ3X4b9njA4qlU86X9TzkkkhuWEtNPnbPDgu576BsrDkWXt1BoeB51tYJekkNCkj6sTJUwrMVy5iM1lyHZUx20Bjw9nhMCsNKlMOeYQOnGfDoeTq6ad4y8JBLvEcH10Rred9RGla/UNbtMdjYxRL0Grjtp8doW20kl0Ti82jb2Lo+Oqb7+eQgbQ8Oax8w6ZrKEgKDEdXXLJ6m691MIDsfgiAIgiDUFXn5EARBEAShrsjLhyAIgiAIdeW8s/nIMp3qVy9rbXf/m4dI3c3LqdveJR1al+87dIDUvXelthMIMj19vEz1yB8/ta16/PJuGm44j1NDM7sJHJqZp5TG4YQBqA2Gy/TIErKrqDDN02BhrksohTxPDGgjt0+L+bOFw0wPRLor8+wCF7mScrcvh7mL+mNJVKLukJiRk1QHdytUcywgrTl/7Cipa7T0PbcEqd2Pr0TtKkKmbm/BYmm+FW57ba07X9C2I+9deTmpu3zJsurx0aPU/mEkrW1ASiycOrA5YiP38BBL9d6M3GmTEXrPLmv7wLDur33D/aTOQK6B8VZqLxOKU7fcMHLZbWymn40yV8FahNA8LDPbCOzGbTD3eJPNWRPZNcTjUXoeFEY/GqHumBZzRQ4H9XPLbSMO7N1bPR4bpXr6GEpp7yra5z4/bTsOBR9gYruBxjZfpC6yQ8zNMo9cby3WPw2JZPW4zNIe5AvU5sKp6PZ6E+w6sBEKtS8wuFFKDZ5//tnq8ZjzKqmL2MzNHD2nFWbHgd3jXZeOD1/jKsgOiK+j2O20WKJ1LrPnMZBNis9mrutJbWsYjSZZW9Gaz92JJ/SlLpvMPgT3s8m+A22blk30WT4+uHsMto4bBvsuCaNrFpn9F51q7wjZ+RAEQRAEoa7Iy4cgCIIgCHXlvJNdmppbSHn0tN5H6kcZHgEA/vuVvaTsVuaiEt2qamnT7rWGRbfVtm6nGQ9/9ozORljy6HYhoC05vnVG2sK22BXbk8PRGvlWIs4467PpEBp8P8zS92mzOgu5KsZidJvaYm23FNq+ZG7CHpJ2uCbT3ka332NxVM5PLru0tdOopcePMhmmhKMcUmmnb7+OEDnmp+PDRySHIq7mHLqF6xHXPC6T0S3TcklvY7/84n+Ruhsium+Xsn4tJLSUwd06eVbmInKrHGNZY7HL8JG9NOvlcCFDykWfbnuolfZzQ1uyehyIM3mCZbUNoyiegTCVegxr6ksLjjbsOnT+4CzRvH9KJSodYFfbEHsuTCSlFnI0umdplEqnR/Na+vHYGBjoWfQxeRa7p/uCTCJi3VEu6/OOn6bSSrGYRcdUJuSO6kE0nyoFuqZUQLehwCKc8jJ28zSYn7CDxke5dP76fVNznQcACKJM1BWLzS2PdlAAhRrwDOZSjdpqsrZyd2zP0/08UYJAUpNiWXZZTyu05hosvAFWc0ygY2Bb+vqlEn1muestvqTjMPkIyddcIufRumvJN5gyywCsmERexMmvLSr3dXTMhXeL7HwIgiAIglBX5OVDEARBEIS6Ii8fgiAIgiDUlfPO5oPbLfhQyGmnSDXpvkGqdZdyOnvme69ZROpCyfbq8ViR6s7PvbSdlAvIBbPC7AQCKFQzD/WLw3VzLKZrEpMC5qIVQHq6wcVkVjYCWlvFWRMBaMjeCtP7xpkujrNXlpgun2jQrmZtKCsqAEA0SNtTQJk2a736di/qJuVMjo5l7jgOk87CxiNXwVHWVj/r5zIaS+4eWSt0tKEmrzvw6lZSPjaudeAWk2rd2J7HZfps1qRtH1Bapz/IXIaPo4y8+TC9x1h3BymnerReG0zS7Ktk/jBtORqldkFh5Hpr+qidlJqGC2YmrccyP54mdUMn9TNdLFLN3GVZiCuVMjpmruto/posA6+PZa2mLujMRRa57PIQ6hXk9lnIUe2/VKLP0zgKga1oUyES12sIt71SFTonSlk9DxyHXnMM2RhwGw/udoptHDw1eTZn26Z2LobnTPLJieCs0dkcTTMQtvj8QW1lCwXO5FtmaRgch4UBN/VnFbPrwPPFc1j4eeZq6yJ7I247grMJcxMLpfQ9l5jb9ITQ8DjrL7MBVMRd3mV1zC0YfXlwixx8DavM+4OOZb5BP9/tXdTNvgPE5kMQBEEQhPMMefkQBEEQBKGuyMuHIAiCIAh15byz+eC+/jg1vWfRcOZloHrtYFbrby/vo779H8prLWxcUf/nE6dpOYi0bydPr1FEOms4zGwsfPYZPwdwhtDRBg7nS4dJIV1esfdHH0sPnkVhk8sO1Z2xDQiPJcLtOnJFrY9Gk9Suo6FFp2wvM915714aa8WHtOblNWTDeAONP9GSaiXlfmTzMUHXRMclZsdRYaYaOPS4O4304BM+iRpRYfp6bliHJjYDSVJnofDYJ5mWuwvoHDlo6zvLRan2HunSKexbOuaQuqaWFCkHUHjxMrsThfT+gM3iwvAysoeweFyNacRfHjisUyQoZieFdXEef8IOMPsDC8dioJ/1I5uUMIv9wj+LbbUcFucjm9U6eblE6zxkqGCyUNWeS58Lf0DHRUnNoTY52axOaZ85TW0jnDKLD4Tax2NT5MvYHoTZwHCbJRxBnZ3Hh/rdAm7HRtfGWhw7puMlHein9xFhIeZtbIs14QnX4+64bAw8asfgD5iT1mHbERalfUIYeRxbwzBYzB88L/kcRfZ53AaQp1Pw3MljrZjIVs0w6LznqTrwM1xjmKECtO/cRvpczFmm05MkaBifWuZwU0Z2PgRBEARBqCvTevl44IEH4IorroB4PA7xeBx6e3vhF7/4RbW+WCzCunXroKmpCaLRKNx+++0wODhY44yCIAiCIFxsTEt26ezshPvuuw8WLlwISin4wQ9+AB/96Edh586dcPnll8Pdd98NP/vZz+Cxxx6DRCIB69evh9tuuw1+85vfzFyLeWpAtMVkWWw7StGtX9fU9X1DdLvw+z/+efX4AzesIHV9J2lGvxzOVMhlD5QV1GJbiWG0decPUXmkME4lEez2pJgE4kPuq3wrnLtL4a1xvj1XwGGkWR13MUwiGaQp1U7qTo3o7J7p4QFSlz5CswcvmN8DUyHEstEGWOZRn1/3pcvcD/GdOAbfH2RuhGqS47dhgjMi2qbNsr7ci7a/E34qxe0t6pfzN5gsNsLCmzd16b5r76HSShKFow9EqEus6dEt3Ap+ZlhGTAvJE/aEbKv0PEQSMfg28dT/r7E8LVN5LDw/Dm8+4frMrdxUeGuaXqOEwtE7FdrPWC4BmOgCicHu6T4/nZMWckO1eUoE9gwHA/o8gRA9z+iIbmtunK5TPibPWqify0zKdfD2ew13TAAahpu7kQfRGpPNpEldPjcGU8VUKPw8lwNcunZjWWhC5lwLhVdXk693ADSEAfekx/NFsZDpfAIpGkOdgOUUHgrCQW2vsLZ67PtKoWzGXC7BWc75jRgTxlZfU9m0sQ7KrB7vaCN1ncto+Anb0PMyvf812qBOKuW+E6b18nHLLbeQ8r333gsPPPAAbNmyBTo7O+HBBx+ERx55BD7wgQ8AAMBDDz0ES5YsgS1btsC11177rhsrCIIgCML5zzu2+XBdFx599FHI5XLQ29sLO3bsgEqlAmvWrKl+ZvHixdDd3Q2bN2+e9DylUgkymQz5EQRBEAThwmXaLx+vvfYaRKNRCAQC8LnPfQ4ef/xxuOyyy2BgYAD8fj8kk0ny+VQqBQMDA2c+GQBs3LgREolE9aerq2vaNyEIgiAIwvnDtF1tL730Uti1axeMjY3Bv//7v8PatWvhueeee8cNuOeee2DDhg3VciaTqfkC0sRebopFrYnmWEppv0X1dQfprjwc9HNbX60e952kbrjpHPXDGs1qjZp5lkIE6e0Oc60KBCbX04MhquNZSNu1ffSzONyww+wLjAluV8iVtELvo4zCC4eC1AaluamJlBubtZ1HWdF31pJfT6NCgLbVY2nHcyzE8GRUmAtdrkC171hSt7eYY2G3Ub+7TC92uV0H+oUxudQ/AcXsBBRyqcuZtO0vlLUufiRP60bCun12is779s4WUu5p0eWmBB0fE827HNOAi8zuxUYafpDZ0gTD2tbG9tM5EQxRG5QAmjM8vfx08JCfI3cBVUgnV8x2RTG/aWKDwq6B05e73C6APV/4ObW4Czz6Wz6VsF2AW6Fhvl3mfl326b4rFKgNCrbz8JiLrOFnrv0oZcOEvkNTn7eV23zgepuHdC/r5+v0CHUgqJSn9jwDADgovLrL/q7MUgmQUPEes+1BRY/ZP5isD8poTDxuc4HsizyP3rOffT/gZYSfB9sicfMUD4cwZ/ZM3LaG2Iuw8TGQnQtwd2J20Qr6DqhE6NxuvPSS6vGceXS9KTLnkDf36rQioUqW1EEnvGum/fLh9/thwYIFAACwfPly2LZtG3z729+Gj33sY1AulyGdTpPdj8HBQWhra5vkbG896PhhFwRBEAThwuZdx/nwPA9KpRIsX74cfD4fbNq0qVq3b98+OHr0KPT29r7bywiCIAiCcIEwrZ2Pe+65B26++Wbo7u6G8fFxeOSRR+DXv/41/PKXv4REIgGf+cxnYMOGDdDY2AjxeBw+//nPQ29vr3i6CIIgCIJQZVovH0NDQ3DnnXdCf38/JBIJuOKKK+CXv/wl/M7v/A4AAHzzm98E0zTh9ttvh1KpBDfeeCN897vfndEGF5nNAIqeCyUWI9dnUb3LQZKaYrqmGdKa+WEW18NksTQcpDU7zH+/WNRab46lpce+9FxqivipZh5CcUBMpofimBehMI3pUC5TPfLUqI7B4bFwujby+W6I07gabY1JWm7TcSTSzMYik9YhoLNjaVKXbKRh0odPDaMSDdOOqbj0Gpaf6qMNLbq9lSgbZxT3g4UAgQqzw1HI5oN1MwkzPUEj54EkcIwHm8XVCOn2lRK0Py5Jan/5hkaa3j4ap49nNKznYSBI64oo7UCZp9xm9hgWCvM/ISAGKvuYXRKPKeND5+HxFXhciVoUUchwm6cSQO2ZEMKdpXc3kd2NyZ5vbLsxIfQ7K2P7EB7uHYcpd1k6+QoaA4utU5UstVlyUXsiJWq/g+08TDY+pQJLGc/jHpGqyet4uHUbzRE+lqODQ9XjSomuaXz61ASd1vKxOCPs+fahtQlctkGPjFkslkKDN0chQy6D2WkFkf1MQ5w+lybw2C+Tj7uFwvoHmM2b4yCbMnZOHm7dRfYp4xk6X7Bpi8fm/ZhBz2M363uZu4jG7mho0Gvuib0HSd3wwUP0POg+g77pDPTUmNbLx4MPPlizPhgMwv333w/333//u2qUIAiCIAgXLpLbRRAEQRCEunLeZbXl244BtOUVZnfjVejWJ46g67EA2R4KReyxrTynzFzYXH3Nia6Busy31fBW8OlRmq1ylLU1HtOyQoJleI2jMO1BoO6QrkflChttO1oBel+lov5skEkFNvM7dfJj6JheI5seqR57Fep7HGSZR4tTzHbKt2WTTVReikaQ62SJjgGWXRyXh17nYaVRSG72Lo63vE3ucsnCFtto2zjM5IkYGstUNEnqogHtDh5hodf9rO/KqJj10+sX8LYwc70Lsm1av4VDhNNtYixJGNzlkrsxIjdCv5+5//mmntUWZ2Lm/exDbeBSimL3iUd2YlR9HLqabpuDO7mrNs+i7SB39TLLMFtAUotbyJM6h7naRtB5QwkqPzqoXytFeg0uw2C4NAjY5ZyH62ayWAStKbkMXZsyOKQ6O49pTv0rxMK6d5mtvyyDswLdBxbQ+Wuj8sSMxMwNFk0Eno3Wc/Q18jYNbsmzjAOSMnHWWAAAD2UOL1a4DISz4fIQ7uwSqHkusDS7qO3cVTzeyjKAL9JpGEz2Pbdv20u6rUPDpM5ic91Gc6KWhPdOkZ0PQRAEQRDqirx8CIIgCIJQV+TlQxAEQRCEumIoLuTOMplMBhKJBHz5y1+WyKeCIAiCcJ5QKpXgvvvug7GxMYjH4zU/KzsfgiAIgiDUFXn5EARBEAShrsjLhyAIgiAIdUVePgRBEARBqCvy8iEIgiAIQl055yKc/tb5plQqvc0nBUEQBEE4V/jt9/ZUnGjPOVfb48ePQ1dX12w3QxAEQRCEd8CxY8egs7Oz5mfOuZcPz/Pg5MmToJSC7u5uOHbs2Nv6C1+MZDIZ6Orqkv6ZBOmf2kj/1Eb6pzbSP5NzMfeNUgrGx8eho6NjQi4mzjknu5imCZ2dnZDJvJXoJx6PX3QDOB2kf2oj/VMb6Z/aSP/URvpnci7WvkkkElP6nBicCoIgCIJQV+TlQxAEQRCEunLOvnwEAgH4y7/8S8nvMgnSP7WR/qmN9E9tpH9qI/0zOdI3U+OcMzgVBEEQBOHC5pzd+RAEQRAE4cJEXj4EQRAEQagr8vIhCIIgCEJdkZcPQRAEQRDqirx8CIIgCIJQV87Zl4/7778f5s2bB8FgEFavXg1bt26d7SbVnY0bN8LKlSshFotBa2sr3HrrrbBv3z7ymWKxCOvWrYOmpiaIRqNw++23w+Dg4Cy1eHa57777wDAMuOuuu6q/u9j758SJE/CHf/iH0NTUBKFQCJYtWwbbt2+v1iul4Otf/zq0t7dDKBSCNWvWwIEDB2axxfXDdV342te+Bj09PRAKheCSSy6Bv/7rvyZJsS6m/nn++efhlltugY6ODjAMA5544glSP5W+GB0dhTvuuAPi8Tgkk0n4zGc+A9lsto53cfao1T+VSgW+9KUvwbJlyyASiUBHRwfceeedcPLkSXKOC7l/po06B3n00UeV3+9X3//+99Ubb7yh/viP/1glk0k1ODg4202rKzfeeKN66KGH1Ouvv6527dqlPvShD6nu7m6VzWarn/nc5z6nurq61KZNm9T27dvVtddeq97znvfMYqtnh61bt6p58+apK664Qn3hC1+o/v5i7p/R0VE1d+5c9clPflK99NJL6tChQ+qXv/ylOnjwYPUz9913n0okEuqJJ55Qr7zyivrIRz6ienp6VKFQmMWW14d7771XNTU1qSeffFL19fWpxx57TEWjUfXtb3+7+pmLqX9+/vOfq69+9avqJz/5iQIA9fjjj5P6qfTFTTfdpK688kq1ZcsW9cILL6gFCxaoT3ziE3W+k7NDrf5Jp9NqzZo16kc/+pHau3ev2rx5s1q1apVavnw5OceF3D/T5Zx8+Vi1apVat25dtey6ruro6FAbN26cxVbNPkNDQwoA1HPPPaeUemvC+3w+9dhjj1U/s2fPHgUAavPmzbPVzLozPj6uFi5cqJ5++mn1vve9r/rycbH3z5e+9CV1/fXXT1rveZ5qa2tTf//3f1/9XTqdVoFAQP3bv/1bPZo4q3z4wx9Wn/70p8nvbrvtNnXHHXcopS7u/uFfrlPpi927dysAUNu2bat+5he/+IUyDEOdOHGibm2vB2d6OeNs3bpVAYA6cuSIUuri6p+pcM7JLuVyGXbs2AFr1qyp/s40TVizZg1s3rx5Fls2+4yNjQEAQGNjIwAA7NixAyqVCumrxYsXQ3d390XVV+vWrYMPf/jDpB8ApH/+4z/+A1asWAG///u/D62trXD11VfDP//zP1fr+/r6YGBggPRPIpGA1atXXxT98573vAc2bdoE+/fvBwCAV155BV588UW4+eabAUD6BzOVvti8eTMkk0lYsWJF9TNr1qwB0zThpZdeqnubZ5uxsTEwDAOSySQASP9wzrmstsPDw+C6LqRSKfL7VCoFe/funaVWzT6e58Fdd90F1113HSxduhQAAAYGBsDv91cn929JpVIwMDAwC62sP48++ii8/PLLsG3btgl1F3v/HDp0CB544AHYsGEDfOUrX4Ft27bBn/3Zn4Hf74e1a9dW++BMz9rF0D9f/vKXIZPJwOLFi8GyLHBdF+6991644447AAAu+v7BTKUvBgYGoLW1ldTbtg2NjY0XXX8Vi0X40pe+BJ/4xCeqmW2lfyjn3MuHcGbWrVsHr7/+Orz44ouz3ZRzhmPHjsEXvvAFePrppyEYDM52c845PM+DFStWwN/+7d8CAMDVV18Nr7/+Onzve9+DtWvXznLrZp8f//jH8MMf/hAeeeQRuPzyy2HXrl1w1113QUdHh/SP8I6pVCrwB3/wB6CUggceeGC2m3POcs7JLs3NzWBZ1gSPhMHBQWhra5ulVs0u69evhyeffBKeffZZ6OzsrP6+ra0NyuUypNNp8vmLpa927NgBQ0NDcM0114Bt22DbNjz33HPwne98B2zbhlQqdVH3T3t7O1x22WXkd0uWLIGjR48CAFT74GJ91v78z/8cvvzlL8PHP/5xWLZsGfzRH/0R3H333bBx40YAkP7BTKUv2traYGhoiNQ7jgOjo6MXTX/99sXjyJEj8PTTT1d3PQCkfzjn3MuH3++H5cuXw6ZNm6q/8zwPNm3aBL29vbPYsvqjlIL169fD448/Ds888wz09PSQ+uXLl4PP5yN9tW/fPjh69OhF0Vcf/OAH4bXXXoNdu3ZVf1asWAF33HFH9fhi7p/rrrtugmv2/v37Ye7cuQAA0NPTA21tbaR/MpkMvPTSSxdF/+TzeTBNugRalgWe5wGA9A9mKn3R29sL6XQaduzYUf3MM888A57nwerVq+ve5nrz2xePAwcOwK9+9Stoamoi9Rd7/0xgti1ez8Sjjz6qAoGAevjhh9Xu3bvVZz/7WZVMJtXAwMBsN62u/Mmf/IlKJBLq17/+terv76/+5PP56mc+97nPqe7ubvXMM8+o7du3q97eXtXb2zuLrZ5dsLeLUhd3/2zdulXZtq3uvfdedeDAAfXDH/5QhcNh9a//+q/Vz9x3330qmUyqn/70p+rVV19VH/3oRy9YV1LO2rVr1Zw5c6qutj/5yU9Uc3Oz+uIXv1j9zMXUP+Pj42rnzp1q586dCgDUP/zDP6idO3dWvTWm0hc33XSTuvrqq9VLL72kXnzxRbVw4cILxpW0Vv+Uy2X1kY98RHV2dqpdu3aR9bpUKlXPcSH3z3Q5J18+lFLqH//xH1V3d7fy+/1q1apVasuWLbPdpLoDAGf8eeihh6qfKRQK6k//9E9VQ0ODCofD6vd+7/dUf3//7DV6luEvHxd7//znf/6nWrp0qQoEAmrx4sXqn/7pn0i953nqa1/7mkqlUioQCKgPfvCDat++fbPU2vqSyWTUF77wBdXd3a2CwaCaP3+++upXv0q+LC6m/nn22WfPuN6sXbtWKTW1vhgZGVGf+MQnVDQaVfF4XH3qU59S4+Pjs3A3M0+t/unr65t0vX722Wer57iQ+2e6GEqhcH6CIAiCIAhnmXPO5kMQBEEQhAsbefkQBEEQBKGuyMuHIAiCIAh1RV4+BEEQBEGoK/LyIQiCIAhCXZGXD0EQBEEQ6oq8fAiCIAiCUFfk5UMQBEEQhLoiLx+CIAiCINQVefkQBEEQBKGuyMuHIAiCIAh15f8HdxvpomgNdv8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n",
      "Predicted:    cat  ship  ship plane\n"
     ]
    }
   ],
   "source": [
    "# Check several images.\n",
    "dataiter = iter(validloader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "outputs = dense_max(images.to(device))\n",
    "\n",
    "# max compare along the row, return the index of the max value, which is the predicted class\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81.07 %\n"
     ]
    }
   ],
   "source": [
    "dense_max.to(device)\n",
    "dense_max.eval()\n",
    "# Get test accuracy.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_max(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 80 %\n",
      "Accuracy of   car : 93 %\n",
      "Accuracy of  bird : 73 %\n",
      "Accuracy of   cat : 67 %\n",
      "Accuracy of  deer : 81 %\n",
      "Accuracy of   dog : 72 %\n",
      "Accuracy of  frog : 82 %\n",
      "Accuracy of horse : 84 %\n",
      "Accuracy of  ship : 87 %\n",
      "Accuracy of truck : 88 %\n"
     ]
    }
   ],
   "source": [
    "dense_max.to(device)\n",
    "dense_max.eval()\n",
    "\n",
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = dense_max(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs181",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
